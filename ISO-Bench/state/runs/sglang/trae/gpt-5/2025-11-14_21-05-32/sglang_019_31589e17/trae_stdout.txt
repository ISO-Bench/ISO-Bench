Changed working directory to: /home/ubuntu/OmniPerf-Bench/perf-agents-bench/.work/worktrees/sglang_core/sglang_019_31589e17
Initialising MCP tools...
â•­â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ Task Details â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•®
â”‚ [1mTask:[0m I've uploaded a python code repository in the directory /home/ubuntu/OmniPerf-Bench/perf-agents-bench/.work/worktrees/sglang_core/sglang_019_31589e17.     â”‚
â”‚ Consider the following test script showing an example usage of the repository:                                                                                   â”‚
â”‚                                                                                                                                                                  â”‚
â”‚ <test_script>                                                                                                                                                    â”‚
â”‚ # This is a performance optimization task                                                                                                                        â”‚
â”‚ # The specific operations to optimize are in the files listed below                                                                                              â”‚
â”‚ # Focus on performance improvements in the target functions                                                                                                      â”‚
â”‚                                                                                                                                                                  â”‚
â”‚ </test_script>                                                                                                                                                   â”‚
â”‚                                                                                                                                                                  â”‚
â”‚ Can you help me implement the necessary changes to the repository so that the runtime of the <test_script> is optimized?                                         â”‚
â”‚                                                                                                                                                                  â”‚
â”‚ Basic guidelines:                                                                                                                                                â”‚
â”‚ 1. Your task is to make changes to non-test files in the /home/ubuntu/OmniPerf-Bench/perf-agents-bench/.work/worktrees/sglang_core/sglang_019_31589e17 directory â”‚
â”‚ to improve the performance of the <test_script>.                                                                                                                 â”‚
â”‚ 2. Make changes while ensuring the repository is functionally equivalent to the original.                                                                        â”‚
â”‚ 3. Do not overoptimize for just the specific inputs in <test_script>. Make general performance improvements for the usage scenario shown.                        â”‚
â”‚ 4. You may need to rebuild the repo for your changes to take effect before testing. Some rebuilds may take time to run, so be patient with running them.         â”‚
â”‚                                                                                                                                                                  â”‚
â”‚ Follow these steps to improve performance:                                                                                                                       â”‚
â”‚ 1. As a first step, explore the repository structure.                                                                                                            â”‚
â”‚ 2. Create a script ONLY inside /home/ubuntu/OmniPerf-Bench/perf-agents-bench/.work/worktrees/sglang_core/sglang_019_31589e17/.bench_scratch (e.g.,               â”‚
â”‚ /home/ubuntu/OmniPerf-Bench/perf-agents-bench/.work/worktrees/sglang_core/sglang_019_31589e17/.bench_scratch/test_opt.py) to reproduce and time the example,     â”‚
â”‚ then execute it with python <filename.py> from the repo root.                                                                                                    â”‚
â”‚ 3. Edit the source code of the repository to improve performance.                                                                                                â”‚
â”‚ 4. Rebuild and rerun your script to confirm that performance has improved.                                                                                       â”‚
â”‚                                                                                                                                                                  â”‚
â”‚ Here is an example of the kind of optimizations that have been shown to improve performance in this codebase:                                                    â”‚
â”‚                                                                                                                                                                  â”‚
â”‚ <example_optimization_diff>                                                                                                                                      â”‚
â”‚ diff --git a/python/sglang/srt/models/deepseek_v2.py b/python/sglang/srt/models/deepseek_v2.py                                                                   â”‚
â”‚ index 29f18f0ef..b4fc4d7a7 100644                                                                                                                                â”‚
â”‚ --- a/python/sglang/srt/models/deepseek_v2.py                                                                                                                    â”‚
â”‚ +++ b/python/sglang/srt/models/deepseek_v2.py                                                                                                                    â”‚
â”‚ @@ -454,6 +454,7 @@ class DeepseekV2MoE(nn.Module):                                                                                                              â”‚
â”‚                  num_expert_group=self.num_expert_group,                                                                                                         â”‚
â”‚                  correction_bias=self.correction_bias,                                                                                                           â”‚
â”‚                  routed_scaling_factor=self.routed_scaling_factor,                                                                                               â”‚
â”‚ +                num_token_non_padded=state.forward_batch.num_token_non_padded,                                                                                  â”‚
â”‚                  expert_location_dispatch_info=ExpertLocationDispatchInfo.init_new(                                                                              â”‚
â”‚                      layer_id=self.layer_id,                                                                                                                     â”‚
â”‚                  ),                                                                                                                                              â”‚
â”‚ diff --git a/python/sglang/srt/two_batch_overlap.py b/python/sglang/srt/two_batch_overlap.py                                                                     â”‚
â”‚ index 6b0241f40..b417de7ce 100644                                                                                                                                â”‚
â”‚ --- a/python/sglang/srt/two_batch_overlap.py                                                                                                                     â”‚
â”‚ +++ b/python/sglang/srt/two_batch_overlap.py                                                                                                                     â”‚
â”‚ @@ -110,7 +110,7 @@ def compute_split_indices_for_cuda_graph_replay(                                                                                             â”‚
â”‚                                                                                                                                                                  â”‚
â”‚  class TboCudaGraphRunnerPlugin:                                                                                                                                 â”‚
â”‚      def __init__(self):                                                                                                                                         â”‚
â”‚ -        pass  # TODO add logic here                                                                                                                             â”‚
â”‚ +        self._tbo_children_num_token_non_padded = torch.zeros((2,), dtype=torch.int32)                                                                          â”‚
â”‚                                                                                                                                                                  â”‚
â”‚      def capture_one_batch_size(self, batch: ForwardBatch, num_tokens: int):                                                                                     â”‚
â”‚          if not global_server_args_dict["enable_two_batch_overlap"]:                                                                                             â”‚
â”‚ @@ -124,7 +124,14 @@ class TboCudaGraphRunnerPlugin:                                                                                                             â”‚
â”‚          # For simplicity, when two_batch_overlap is enabled, we only capture CUDA Graph for tbo=true                                                            â”‚
â”‚          assert batch.tbo_split_seq_index is not None, f"{num_tokens=}"                                                                                          â”‚
â”‚                                                                                                                                                                  â”‚
â”‚ -        TboForwardBatchPreparer.prepare(batch)                                                                                                                  â”‚
â”‚ </example_optimization_diff>                                                                                                                                     â”‚
â”‚                                                                                                                                                                  â”‚
â”‚ IMPORTANT: The above diff is an EXAMPLE of optimizations that were successful in a different context.                                                            â”‚
â”‚ These changes have NOT been applied to your codebase yet.                                                                                                        â”‚
â”‚ Your task is to:                                                                                                                                                 â”‚
â”‚ 1. Understand the optimization pattern shown (e.g., torch.zeros â†’ torch.empty)                                                                                   â”‚
â”‚ 2. Look at the CURRENT code in the target files                                                                                                                  â”‚
â”‚ 3. Find places where you can apply SIMILAR optimizations                                                                                                         â”‚
â”‚ 4. MAKE THE CHANGES yourself using str_replace_editor                                                                                                            â”‚
â”‚                                                                                                                                                                  â”‚
â”‚ The codebase you're working with is at the BASE commit - it does NOT have these optimizations yet.                                                               â”‚
â”‚ You need to IMPLEMENT similar optimizations yourself.                                                                                                            â”‚
â”‚                                                                                                                                                                  â”‚
â”‚ HERE'S WHAT YOU NEED TO DO:                                                                                                                                      â”‚
â”‚ 1. The files CURRENTLY contain torch.zeros() calls that need optimization                                                                                        â”‚
â”‚ 2. You need to CHANGE torch.zeros to torch.empty where appropriate                                                                                               â”‚
â”‚ 3. You need to REMOVE .fill_() operations that are unnecessary                                                                                                   â”‚
â”‚ 4. These are NEW changes you're making - not already in the code                                                                                                 â”‚
â”‚                                                                                                                                                                  â”‚
â”‚ START WITH THIS COMMAND to see what needs changing:                                                                                                              â”‚
â”‚ ```bash                                                                                                                                                          â”‚
â”‚ grep -n 'torch.zeros\|fill_' vllm/model_executor/layers/fused_moe/moe_align_block_size.py benchmarks/kernels/benchmark_moe_align_block_size.py                   â”‚
â”‚ ```                                                                                                                                                              â”‚
â”‚                                                                                                                                                                  â”‚
â”‚ CRITICAL: You MUST make actual code changes. Look for patterns like:                                                                                             â”‚
â”‚ - Analyze the target files for performance bottlenecks                                                                                                           â”‚
â”‚ - Look for unnecessary memory allocations or initializations                                                                                                     â”‚
â”‚ - Consider more efficient algorithms or data structures                                                                                                          â”‚
â”‚                                                                                                                                                                  â”‚
â”‚ Target files to optimize:                                                                                                                                        â”‚
â”‚ - python/sglang/srt/models/deepseek_v2.py                                                                                                                        â”‚
â”‚ - python/sglang/srt/two_batch_overlap.py                                                                                                                         â”‚
â”‚                                                                                                                                                                  â”‚
â”‚ IMPORTANT: You MUST make actual code changes to at least one file.                                                                                               â”‚
â”‚ The task will fail if no files are modified.                                                                                                                     â”‚
â”‚                                                                                                                                                                  â”‚
â”‚ ## Constraints                                                                                                                                                   â”‚
â”‚ - No public API breakage                                                                                                                                         â”‚
â”‚ - All TestPack checks must pass                                                                                                                                  â”‚
â”‚                                                                                                                                                                  â”‚
â”‚ ## Target Files (ONLY modify these)                                                                                                                              â”‚
â”‚ - `python/sglang/srt/models/deepseek_v2.py`                                                                                                                      â”‚
â”‚ - `python/sglang/srt/two_batch_overlap.py`                                                                                                                       â”‚
â”‚                                                                                                                                                                  â”‚
â”‚ ## SPECIFIC OPTIMIZATION TARGETS:                                                                                                                                â”‚
â”‚ Based on the human commit analysis, focus on these areas:                                                                                                        â”‚
â”‚ - Memory allocation patterns (torch.zeros vs torch.empty)                                                                                                        â”‚
â”‚ - Tensor initialization strategies                                                                                                                               â”‚
â”‚ - Kernel parameter optimization                                                                                                                                  â”‚
â”‚ - Buffer reuse and caching                                                                                                                                       â”‚
â”‚                                                                                                                                                                  â”‚
â”‚ ### Human Developer's Approach:                                                                                                                                  â”‚
â”‚ ```                                                                                                                                                              â”‚
â”‚ Speed up when having padding tokens two-batch overlap (#6668)                                                                                                    â”‚
â”‚                                                                                                                                                                  â”‚
â”‚ Co-authored-by: Cheng Wan <54331508+ch-wan@users.noreply.github.com>                                                                                             â”‚
â”‚ ```                                                                                                                                                              â”‚
â”‚                                                                                                                                                                  â”‚
â”‚ ### Files Modified (statistics):                                                                                                                                 â”‚
â”‚ ```                                                                                                                                                              â”‚
â”‚ python/sglang/srt/models/deepseek_v2.py |  1 +                                                                                                                   â”‚
â”‚  python/sglang/srt/two_batch_overlap.py  | 82 ++++++++++++++++++++++++++++-----                                                                                  â”‚
â”‚  2 files changed, 71 insertions(+), 12 deletions(-)                                                                                                              â”‚
â”‚ ```                                                                                                                                                              â”‚
â”‚                                                                                                                                                                  â”‚
â”‚ ## IMMEDIATE ACTION REQUIREMENTS:                                                                                                                                â”‚
â”‚ 1. Start editing files by iteration 3                                                                                                                            â”‚
â”‚ 2. Create and run /home/ubuntu/OmniPerf-Bench/perf-agents-bench/.work/worktrees/sglang_core/sglang_019_31589e17/.bench_scratch/test_opt.py before and after      â”‚
â”‚ edits (do not create timing scripts outside /home/ubuntu/OmniPerf-Bench/perf-agents-bench/.work/worktrees/sglang_core/sglang_019_31589e17/.bench_scratch)        â”‚
â”‚ 3. Make at least 3 concrete optimizations                                                                                                                        â”‚
â”‚ 4. Commit changes by iteration 8                                                                                                                                 â”‚
â”‚ 5. Use finish command by iteration 10                                                                                                                            â”‚
â”‚                                                                                                                                                                  â”‚
â”‚ ## TASK COMPLETION COMMAND:                                                                                                                                      â”‚
â”‚ When you have made optimizations:                                                                                                                                â”‚
â”‚ ```bash                                                                                                                                                          â”‚
â”‚ git add -A                                                                                                                                                       â”‚
â”‚ git reset -q .bench_scratch || true                                                                                                                              â”‚
â”‚ git commit -m 'Optimize MoE align sum kernels performance'                                                                                                       â”‚
â”‚ git diff $(git merge-base HEAD origin/HEAD || git rev-parse HEAD~1) -- . ':(exclude).bench_scratch' >                                                            â”‚
â”‚ /home/ubuntu/OmniPerf-Bench/perf-agents-bench/.work/worktrees/sglang_core/sglang_019_31589e17/model_patch.diff || true                                           â”‚
â”‚ finish                                                                                                                                                           â”‚
â”‚ ```                                                                                                                                                              â”‚
â”‚                                                                                                                                                                  â”‚
â”‚ START IMPLEMENTING IMMEDIATELY. NO MORE ANALYSIS.                                                                                                                â”‚
â”‚                                                                                                                                                                  â”‚
â”‚ [1mModel Provider:[0m openai                                                                                                                                           â”‚
â”‚ [1mModel:[0m gpt-5-2025-08-07                                                                                                                                          â”‚
â”‚ [1mMax Steps:[0m 120                                                                                                                                                   â”‚
â”‚ [1mTrajectory File:[0m /home/ubuntu/OmniPerf-Bench/perf-agents-bench/state/runs/sglang_core-ae58875a/sglang_019_31589e17/trajectory.json                               â”‚
â”‚ [1mTools:[0m bash, str_replace_based_edit_tool, sequentialthinking, task_done                                                                                          â”‚
â”‚ [1mProject_path:[0m /home/ubuntu/OmniPerf-Bench/perf-agents-bench/.work/worktrees/sglang_core/sglang_019_31589e17                                                      â”‚
â”‚ [1mIssue:[0m I've uploaded a python code repository in the directory /home/ubuntu/OmniPerf-Bench/perf-agents-bench/.work/worktrees/sglang_core/sglang_019_31589e17.    â”‚
â”‚ Consider the following test script showing an example usage of the repository:                                                                                   â”‚
â”‚                                                                                                                                                                  â”‚
â”‚ <test_script>                                                                                                                                                    â”‚
â”‚ # This is a performance optimization task                                                                                                                        â”‚
â”‚ # The specific operations to optimize are in the files listed below                                                                                              â”‚
â”‚ # Focus on performance improvements in the target functions                                                                                                      â”‚
â”‚                                                                                                                                                                  â”‚
â”‚ </test_script>                                                                                                                                                   â”‚
â”‚                                                                                                                                                                  â”‚
â”‚ Can you help me implement the necessary changes to the repository so that the runtime of the <test_script> is optimized?                                         â”‚
â”‚                                                                                                                                                                  â”‚
â”‚ Basic guidelines:                                                                                                                                                â”‚
â”‚ 1. Your task is to make changes to non-test files in the /home/ubuntu/OmniPerf-Bench/perf-agents-bench/.work/worktrees/sglang_core/sglang_019_31589e17 directory â”‚
â”‚ to improve the performance of the <test_script>.                                                                                                                 â”‚
â”‚ 2. Make changes while ensuring the repository is functionally equivalent to the original.                                                                        â”‚
â”‚ 3. Do not overoptimize for just the specific inputs in <test_script>. Make general performance improvements for the usage scenario shown.                        â”‚
â”‚ 4. You may need to rebuild the repo for your changes to take effect before testing. Some rebuilds may take time to run, so be patient with running them.         â”‚
â”‚                                                                                                                                                                  â”‚
â”‚ Follow these steps to improve performance:                                                                                                                       â”‚
â”‚ 1. As a first step, explore the repository structure.                                                                                                            â”‚
â”‚ 2. Create a script ONLY inside /home/ubuntu/OmniPerf-Bench/perf-agents-bench/.work/worktrees/sglang_core/sglang_019_31589e17/.bench_scratch (e.g.,               â”‚
â”‚ /home/ubuntu/OmniPerf-Bench/perf-agents-bench/.work/worktrees/sglang_core/sglang_019_31589e17/.bench_scratch/test_opt.py) to reproduce and time the example,     â”‚
â”‚ then execute it with python <filename.py> from the repo root.                                                                                                    â”‚
â”‚ 3. Edit the source code of the repository to improve performance.                                                                                                â”‚
â”‚ 4. Rebuild and rerun your script to confirm that performance has improved.                                                                                       â”‚
â”‚                                                                                                                                                                  â”‚
â”‚ Here is an example of the kind of optimizations that have been shown to improve performance in this codebase:                                                    â”‚
â”‚                                                                                                                                                                  â”‚
â”‚ <example_optimization_diff>                                                                                                                                      â”‚
â”‚ diff --git a/python/sglang/srt/models/deepseek_v2.py b/python/sglang/srt/models/deepseek_v2.py                                                                   â”‚
â”‚ index 29f18f0ef..b4fc4d7a7 100644                                                                                                                                â”‚
â”‚ --- a/python/sglang/srt/models/deepseek_v2.py                                                                                                                    â”‚
â”‚ +++ b/python/sglang/srt/models/deepseek_v2.py                                                                                                                    â”‚
â”‚ @@ -454,6 +454,7 @@ class DeepseekV2MoE(nn.Module):                                                                                                              â”‚
â”‚                  num_expert_group=self.num_expert_group,                                                                                                         â”‚
â”‚                  correction_bias=self.correction_bias,                                                                                                           â”‚
â”‚                  routed_scaling_factor=self.routed_scaling_factor,                                                                                               â”‚
â”‚ +                num_token_non_padded=state.forward_batch.num_token_non_padded,                                                                                  â”‚
â”‚                  expert_location_dispatch_info=ExpertLocationDispatchInfo.init_new(                                                                              â”‚
â”‚                      layer_id=self.layer_id,                                                                                                                     â”‚
â”‚                  ),                                                                                                                                              â”‚
â”‚ diff --git a/python/sglang/srt/two_batch_overlap.py b/python/sglang/srt/two_batch_overlap.py                                                                     â”‚
â”‚ index 6b0241f40..b417de7ce 100644                                                                                                                                â”‚
â”‚ --- a/python/sglang/srt/two_batch_overlap.py                                                                                                                     â”‚
â”‚ +++ b/python/sglang/srt/two_batch_overlap.py                                                                                                                     â”‚
â”‚ @@ -110,7 +110,7 @@ def compute_split_indices_for_cuda_graph_replay(                                                                                             â”‚
â”‚                                                                                                                                                                  â”‚
â”‚  class TboCudaGraphRunnerPlugin:                                                                                                                                 â”‚
â”‚      def __init__(self):                                                                                                                                         â”‚
â”‚ -        pass  # TODO add logic here                                                                                                                             â”‚
â”‚ +        self._tbo_children_num_token_non_padded = torch.zeros((2,), dtype=torch.int32)                                                                          â”‚
â”‚                                                                                                                                                                  â”‚
â”‚      def capture_one_batch_size(self, batch: ForwardBatch, num_tokens: int):                                                                                     â”‚
â”‚          if not global_server_args_dict["enable_two_batch_overlap"]:                                                                                             â”‚
â”‚ @@ -124,7 +124,14 @@ class TboCudaGraphRunnerPlugin:                                                                                                             â”‚
â”‚          # For simplicity, when two_batch_overlap is enabled, we only capture CUDA Graph for tbo=true                                                            â”‚
â”‚          assert batch.tbo_split_seq_index is not None, f"{num_tokens=}"                                                                                          â”‚
â”‚                                                                                                                                                                  â”‚
â”‚ -        TboForwardBatchPreparer.prepare(batch)                                                                                                                  â”‚
â”‚ </example_optimization_diff>                                                                                                                                     â”‚
â”‚                                                                                                                                                                  â”‚
â”‚ IMPORTANT: The above diff is an EXAMPLE of optimizations that were successful in a different context.                                                            â”‚
â”‚ These changes have NOT been applied to your codebase yet.                                                                                                        â”‚
â”‚ Your task is to:                                                                                                                                                 â”‚
â”‚ 1. Understand the optimization pattern shown (e.g., torch.zeros â†’ torch.empty)                                                                                   â”‚
â”‚ 2. Look at the CURRENT code in the target files                                                                                                                  â”‚
â”‚ 3. Find places where you can apply SIMILAR optimizations                                                                                                         â”‚
â”‚ 4. MAKE THE CHANGES yourself using str_replace_editor                                                                                                            â”‚
â”‚                                                                                                                                                                  â”‚
â”‚ The codebase you're working with is at the BASE commit - it does NOT have these optimizations yet.                                                               â”‚
â”‚ You need to IMPLEMENT similar optimizations yourself.                                                                                                            â”‚
â”‚                                                                                                                                                                  â”‚
â”‚ HERE'S WHAT YOU NEED TO DO:                                                                                                                                      â”‚
â”‚ 1. The files CURRENTLY contain torch.zeros() calls that need optimization                                                                                        â”‚
â”‚ 2. You need to CHANGE torch.zeros to torch.empty where appropriate                                                                                               â”‚
â”‚ 3. You need to REMOVE .fill_() operations that are unnecessary                                                                                                   â”‚
â”‚ 4. These are NEW changes you're making - not already in the code                                                                                                 â”‚
â”‚                                                                                                                                                                  â”‚
â”‚ START WITH THIS COMMAND to see what needs changing:                                                                                                              â”‚
â”‚ ```bash                                                                                                                                                          â”‚
â”‚ grep -n 'torch.zeros\|fill_' vllm/model_executor/layers/fused_moe/moe_align_block_size.py benchmarks/kernels/benchmark_moe_align_block_size.py                   â”‚
â”‚ ```                                                                                                                                                              â”‚
â”‚                                                                                                                                                                  â”‚
â”‚ CRITICAL: You MUST make actual code changes. Look for patterns like:                                                                                             â”‚
â”‚ - Analyze the target files for performance bottlenecks                                                                                                           â”‚
â”‚ - Look for unnecessary memory allocations or initializations                                                                                                     â”‚
â”‚ - Consider more efficient algorithms or data structures                                                                                                          â”‚
â”‚                                                                                                                                                                  â”‚
â”‚ Target files to optimize:                                                                                                                                        â”‚
â”‚ - python/sglang/srt/models/deepseek_v2.py                                                                                                                        â”‚
â”‚ - python/sglang/srt/two_batch_overlap.py                                                                                                                         â”‚
â”‚                                                                                                                                                                  â”‚
â”‚ IMPORTANT: You MUST make actual code changes to at least one file.                                                                                               â”‚
â”‚ The task will fail if no files are modified.                                                                                                                     â”‚
â”‚                                                                                                                                                                  â”‚
â”‚ ## Constraints                                                                                                                                                   â”‚
â”‚ - No public API breakage                                                                                                                                         â”‚
â”‚ - All TestPack checks must pass                                                                                                                                  â”‚
â”‚                                                                                                                                                                  â”‚
â”‚ ## Target Files (ONLY modify these)                                                                                                                              â”‚
â”‚ - `python/sglang/srt/models/deepseek_v2.py`                                                                                                                      â”‚
â”‚ - `python/sglang/srt/two_batch_overlap.py`                                                                                                                       â”‚
â”‚                                                                                                                                                                  â”‚
â”‚ ## SPECIFIC OPTIMIZATION TARGETS:                                                                                                                                â”‚
â”‚ Based on the human commit analysis, focus on these areas:                                                                                                        â”‚
â”‚ - Memory allocation patterns (torch.zeros vs torch.empty)                                                                                                        â”‚
â”‚ - Tensor initialization strategies                                                                                                                               â”‚
â”‚ - Kernel parameter optimization                                                                                                                                  â”‚
â”‚ - Buffer reuse and caching                                                                                                                                       â”‚
â”‚                                                                                                                                                                  â”‚
â”‚ ### Human Developer's Approach:                                                                                                                                  â”‚
â”‚ ```                                                                                                                                                              â”‚
â”‚ Speed up when having padding tokens two-batch overlap (#6668)                                                                                                    â”‚
â”‚                                                                                                                                                                  â”‚
â”‚ Co-authored-by: Cheng Wan <54331508+ch-wan@users.noreply.github.com>                                                                                             â”‚
â”‚ ```                                                                                                                                                              â”‚
â”‚                                                                                                                                                                  â”‚
â”‚ ### Files Modified (statistics):                                                                                                                                 â”‚
â”‚ ```                                                                                                                                                              â”‚
â”‚ python/sglang/srt/models/deepseek_v2.py |  1 +                                                                                                                   â”‚
â”‚  python/sglang/srt/two_batch_overlap.py  | 82 ++++++++++++++++++++++++++++-----                                                                                  â”‚
â”‚  2 files changed, 71 insertions(+), 12 deletions(-)                                                                                                              â”‚
â”‚ ```                                                                                                                                                              â”‚
â”‚                                                                                                                                                                  â”‚
â”‚ ## IMMEDIATE ACTION REQUIREMENTS:                                                                                                                                â”‚
â”‚ 1. Start editing files by iteration 3                                                                                                                            â”‚
â”‚ 2. Create and run /home/ubuntu/OmniPerf-Bench/perf-agents-bench/.work/worktrees/sglang_core/sglang_019_31589e17/.bench_scratch/test_opt.py before and after      â”‚
â”‚ edits (do not create timing scripts outside /home/ubuntu/OmniPerf-Bench/perf-agents-bench/.work/worktrees/sglang_core/sglang_019_31589e17/.bench_scratch)        â”‚
â”‚ 3. Make at least 3 concrete optimizations                                                                                                                        â”‚
â”‚ 4. Commit changes by iteration 8                                                                                                                                 â”‚
â”‚ 5. Use finish command by iteration 10                                                                                                                            â”‚
â”‚                                                                                                                                                                  â”‚
â”‚ ## TASK COMPLETION COMMAND:                                                                                                                                      â”‚
â”‚ When you have made optimizations:                                                                                                                                â”‚
â”‚ ```bash                                                                                                                                                          â”‚
â”‚ git add -A                                                                                                                                                       â”‚
â”‚ git reset -q .bench_scratch || true                                                                                                                              â”‚
â”‚ git commit -m 'Optimize MoE align sum kernels performance'                                                                                                       â”‚
â”‚ git diff $(git merge-base HEAD origin/HEAD || git rev-parse HEAD~1) -- . ':(exclude).bench_scratch' >                                                            â”‚
â”‚ /home/ubuntu/OmniPerf-Bench/perf-agents-bench/.work/worktrees/sglang_core/sglang_019_31589e17/model_patch.diff || true                                           â”‚
â”‚ finish                                                                                                                                                           â”‚
â”‚ ```                                                                                                                                                              â”‚
â”‚                                                                                                                                                                  â”‚
â”‚ START IMPLEMENTING IMMEDIATELY. NO MORE ANALYSIS.                                                                                                                â”‚
â”‚                                                                                                                                                                  â”‚
â”‚ [1mMust_patch:[0m true                                                                                                                                                 â”‚
â”‚ [1mPatch_path:[0m /home/ubuntu/OmniPerf-Bench/perf-agents-bench/state/runs/sglang_core-ae58875a/sglang_019_31589e17/model_patch.diff                                   â”‚
â•°â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•¯
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Status      â”‚ âœ… Step 1: Completed                                                                                   â”‚
â”‚ LLM         â”‚ ðŸ’¬ I'll start by exploring the repository and inspecting the suggested locations for torch.zeros/fillâ€¦ â”‚
â”‚ Response    â”‚ usage to identify optimization opportunities. Then I'll open the target files to plan concrete         â”‚
â”‚             â”‚ changes.                                                                                               â”‚
â”‚ Tools       â”‚ ðŸ”§ bash                                                                                                â”‚
â”‚ bash        â”‚ â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚
â”‚             â”‚ â”‚ {'command': "ls -la                            â”‚ total 108                                       â”‚   â”‚
â”‚             â”‚ â”‚ /home/ubuntu/OmniPerf-Bench/perf-agents-benchâ€¦ â”‚ drwxrwxr-x 15 ubuntu ubuntu  4096 Nov 14 23:12  â”‚   â”‚
â”‚             â”‚ â”‚ | sed -n '1,120p'", 'restart': False}          â”‚ .                                               â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ drwxrwxr-x 22 ubuntu ubuntu  4096 Nov 14 23:12  â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ ..                                              â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ -rw-rw-r--  1 ubuntu ubuntu    35 Nov 14 23:12  â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ .clang-format-ignore                            â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ drwxrwxr-x  2 ubuntu ubuntu  4096 Nov 14 23:12  â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ .devcontainer                                   â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ -rw-rw-r--  1 ubuntu ubuntu   341 Nov 14 23:12  â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ .editorconfig                                   â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ -rw-rw-r--  1 ubuntu ubuntu   113 Nov 14 23:12  â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ .git                                            â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ drwxrwxr-x  4 ubuntu ubuntu  4096 Nov 14 23:12  â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ .github                                         â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ -rw-rw-r--  1 ubuntu ubuntu  3695 Nov 14 23:12  â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ .gitignore                                      â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ -rw-rw-r--  1 ubuntu ubuntu     0 Nov 14 23:12  â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ .gitmodules                                     â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ -rw-rw-r--  1 ubuntu ubuntu    50 Nov 14 23:12  â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ .isort.cfg                                      â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ -rw-rw-r--  1 ubuntu ubuntu  1568 Nov 14 23:12  â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ .pre-commit-config.yaml                         â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ drwxrwxr-x  3 ubuntu ubuntu  4096 Nov 14 23:12  â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ 3rdparty                                        â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ -rw-rw-r--  1 ubuntu ubuntu 11346 Nov 14 23:12  â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ LICENSE                                         â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ -rw-rw-r--  1 ubuntu ubuntu  1759 Nov 14 23:12  â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ Makefile                                        â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ -rw-rw-r--  1 ubuntu ubuntu  6838 Nov 14 23:12  â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ README.md                                       â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ drwxrwxr-x  2 ubuntu ubuntu  4096 Nov 14 23:12  â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ assets                                          â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ drwxrwxr-x 32 ubuntu ubuntu  4096 Nov 14 23:12  â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ benchmark                                       â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ drwxrwxr-x  2 ubuntu ubuntu  4096 Nov 14 23:12  â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ docker                                          â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ drwxrwxr-x  9 ubuntu ubuntu  4096 Nov 14 23:12  â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ docs                                            â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ drwxrwxr-x  6 ubuntu ubuntu  4096 Nov 14 23:12  â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ examples                                        â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ drwxrwxr-x  3 ubuntu ubuntu  4096 Nov 14 23:12  â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ python                                          â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ drwxrwxr-x  4 ubuntu ubuntu  4096 Nov 14 23:12  â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ scripts                                         â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ drwxrwxr-x  8 ubuntu ubuntu  4096 Nov 14 23:12  â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ sgl-kernel                                      â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ drwxrwxr-x  6 ubuntu ubuntu  4096 Nov 14 23:12  â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ sgl-router                                      â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ drwxrwxr-x  4 ubuntu ubuntu  4096 Nov 14 23:12  â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ test                                            â”‚   â”‚
â”‚             â”‚ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚
â”‚ Total       â”‚ Input: 8217 Output: 815                                                                                â”‚
â”‚ Tokens      â”‚                                                                                                        â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Status      â”‚ âœ… Step 2: Completed                                                                                   â”‚
â”‚ Tools       â”‚ ðŸ”§ bash                                                                                                â”‚
â”‚ bash        â”‚ â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚
â”‚             â”‚ â”‚ {'command': "grep -n 'torch.zeros\\|fill_'     â”‚                                                 â”‚   â”‚
â”‚             â”‚ â”‚ vllm/model_executor/layers/fused_moe/moe_aligâ€¦ â”‚                                                 â”‚   â”‚
â”‚             â”‚ â”‚ benchmarks/kernels/benchmark_moe_align_block_â€¦ â”‚                                                 â”‚   â”‚
â”‚             â”‚ â”‚ || true", 'restart': False}                    â”‚                                                 â”‚   â”‚
â”‚             â”‚ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚
â”‚ Total       â”‚ Input: 17939 Output: 878                                                                               â”‚
â”‚ Tokens      â”‚                                                                                                        â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Status      â”‚ âœ… Step 3: Completed                                                                                   â”‚
â”‚ Tools       â”‚ ðŸ”§ str_replace_based_edit_tool                                                                         â”‚
â”‚ str_replacâ€¦ â”‚ â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚
â”‚             â”‚ â”‚ {'command': 'view', 'file_text': '',           â”‚ Here's the result of running `cat -n` on        â”‚   â”‚
â”‚             â”‚ â”‚ 'insert_line': 1, 'new_str': '', 'old_str':    â”‚ /home/ubuntu/OmniPerf-Bench/perf-agents-bench/â€¦ â”‚   â”‚
â”‚             â”‚ â”‚ '', 'path':                                    â”‚      1  # Copyright 2023-2024 SGLang Team       â”‚   â”‚
â”‚             â”‚ â”‚ '/home/ubuntu/OmniPerf-Bench/perf-agents-bencâ€¦ â”‚      2  # Licensed under the Apache License,    â”‚   â”‚
â”‚             â”‚ â”‚ 'view_range': [1, -1]}                         â”‚ Version 2.0 (the "License");                    â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚      3  # you may not use this file except in   â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ compliance with the License.                    â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚      4  # You may obtain a copy of the License  â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ at                                              â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚      5  #                                       â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚      6  #                                       â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ http://www.apache.org/licenses/LICENSE-2.0      â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚      7  #                                       â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚      8  # Unless required by applicable law or  â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ agreed to in writing, software                  â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚      9  # distributed under the License is      â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ distributed on an "AS IS" BASIS,                â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚     10  # WITHOUT WARRANTIES OR CONDITIONS OF   â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ ANY KIND, either express or implied.            â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚     11  # See the License for the specific      â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ language governing permissions and              â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚     12  # limitations under the License.        â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚     13  #                                       â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ ==============================================â€¦ â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚     14                                          â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚     15  # Adapted from:                         â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚     16  #                                       â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ https://github.com/vllm-project/vllm/blob/fb6aâ€¦ â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚     17  """Inference-only DeepseekV2 model."""  â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚     18                                          â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚     19  import logging                          â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚     20  import os                               â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚     21  from enum import IntEnum, auto          â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚     22  from typing import Any, Dict, Iterable, â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ Optional, Tuple                                 â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚     23                                          â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚     24  import torch                            â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚     25  import torch.nn.functional as F         â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚     26  from torch import nn                    â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚     27  from tqdm import tqdm                   â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚     28  from transformers import                â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ PretrainedConfig                                â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚     29                                          â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚     30  from sglang.srt.distributed import (    â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚     31                                          â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ get_tensor_model_parallel_world_size,           â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚     32      parallel_state,                     â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚     33      tensor_model_parallel_all_reduce,   â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚     34  )                                       â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚     35  from sglang.srt.layers.activation       â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ import SiluAndMul                               â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚     36  from sglang.srt.layers.communicator     â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ import (                                        â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚     37      LayerCommunicator,                  â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚     38      LayerScatterModes,                  â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚     39      enable_moe_dense_fully_dp,          â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚     40  )                                       â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚     41  from sglang.srt.layers.dp_attention     â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ import (                                        â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚     42      get_attention_tp_rank,              â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚     43      get_attention_tp_size,              â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚     44      get_local_attention_dp_size,        â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚     45  )                                       â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚     46  from sglang.srt.layers.layernorm import â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ RMSNorm                                         â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚     47  from sglang.srt.layers.linear import (  â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚     48      ColumnParallelLinear,               â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚     49      MergedColumnParallelLinear,         â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚     50      ReplicatedLinear,                   â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚     51      RowParallelLinear,                  â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚     52  )                                       â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚     53  from sglang.srt.layers.logits_processor â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ import LogitsProcessor                          â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚     54  from sglang.srt.layers.moe.ep_moe.layer â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ import get_moe_impl_class                       â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚     55  from                                    â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ sglang.srt.layers.moe.ep_moe.token_dispatcher   â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ import DeepEPDispatcher                         â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚     56  from sglang.srt.layers.moe.topk import  â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ select_experts                                  â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚     57  from                                    â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ sglang.srt.layers.quantization.base_config      â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ import QuantizationConfig                       â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚     58  from                                    â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ sglang.srt.layers.quantization.deep_gemm import â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ _ENABLE_JIT_DEEPGEMM                            â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚     59  from                                    â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ sglang.srt.layers.quantization.fp8_kernel       â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ import (                                        â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚     60      is_fp8_fnuz,                        â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚     61      per_tensor_quant_mla_fp8,           â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚     62                                          â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ per_token_group_quant_mla_deep_gemm_masked_fp8, â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚     63  )                                       â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚     64  from                                    â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ sglang.srt.layers.quantization.fp8_utils import â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ (                                               â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚     65      block_quant_dequant,                â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚     66      block_quant_to_tensor_quant,        â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚     67      channel_quant_to_tensor_quant,      â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚     68      normalize_e4m3fn_to_e4m3fnuz,       â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚     69  )                                       â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚     70  from                                    â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ sglang.srt.layers.quantization.int8_utils       â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ import (                                        â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚     71      block_dequant as                    â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ int8_block_dequant,                             â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚     72  )                                       â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚     73  from sglang.srt.layers.radix_attention  â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ import RadixAttention                           â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚     74  from sglang.srt.layers.rotary_embedding â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ import get_rope                                 â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚     75  from                                    â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ sglang.srt.layers.vocab_parallel_embedding      â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ import (                                        â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚     76      ParallelLMHead,                     â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚     77      VocabParallelEmbedding,             â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚     78  )                                       â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚     79  from                                    â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ sglang.srt.managers.expert_distribution import  â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ (                                               â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚     80                                          â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ get_global_expert_distribution_recorder,        â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚     81  )                                       â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚     82  from                                    â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ sglang.srt.managers.expert_location import      â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ ModelConfigForExpertLocation                    â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚     83  from                                    â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ sglang.srt.managers.expert_location_dispatch    â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ import ExpertLocationDispatchInfo               â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚     84  from sglang.srt.managers.schedule_batch â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ import global_server_args_dict                  â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚     85  from                                    â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ sglang.srt.model_executor.forward_batch_info    â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ import ForwardBatch                             â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚     86  from                                    â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ sglang.srt.model_loader.weight_utils import     â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ default_weight_loader                           â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚     87  from sglang.srt.two_batch_overlap       â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ import (                                        â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚     88      MaybeTboDeepEPDispatcher,           â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚     89      model_forward_maybe_tbo,            â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚     90  )                                       â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚     91  from sglang.srt.utils import (          â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚     92      BumpAllocator,                      â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚     93      DeepEPMode,                         â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚     94      add_prefix,                         â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚     95      get_bool_env_var,                   â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚     96      get_int_env_var,                    â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚     97      is_cuda,                            â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚     98      is_hip,                             â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚     99      is_non_idle_and_non_empty,          â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    100      log_info_on_rank0,                  â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    101  )                                       â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    102                                          â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    103  _is_hip = is_hip()                      â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    104  _is_cuda = is_cuda()                    â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    105  _is_fp8_fnuz = is_fp8_fnuz()            â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    106                                          â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    107  if _is_cuda:                            â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    108      from sgl_kernel import              â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ awq_dequantize, bmm_fp8, merge_state_v2         â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    109                                          â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    110      from                                â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ sglang.srt.layers.quantization.deep_gemm import â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ (                                               â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    111          grouped_gemm_nt_f8f8bf16_masked â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ as deep_gemm_grouped_gemm_nt_f8f8bf16_masked,   â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    112      )                                   â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    113  else:                                   â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    114      from vllm._custom_ops import        â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ awq_dequantize                                  â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    115                                          â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    116  if _is_hip:                             â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    117      from                                â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ sglang.srt.layers.attention.triton_ops.rocm_mlâ€¦ â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ import (                                        â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    118                                          â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ decode_attention_fwd_grouped_rope,              â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    119      )                                   â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    120                                          â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    121  logger = logging.getLogger(__name__)    â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    122                                          â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    123                                          â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    124  class AttnForwardMethod(IntEnum):       â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    125      # Use multi-head attention          â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    126      MHA = auto()                        â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    127                                          â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    128      # Use absorbed multi-latent         â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ attention                                       â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    129      MLA = auto()                        â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    130                                          â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    131      # Use multi-head attention, but     â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ with KV cache chunked.                          â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    132      # This method can avoid OOM when    â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ prefix lengths are long.                        â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    133      MHA_CHUNKED_KV = auto()             â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    134                                          â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    135      # Use MLA but with fused RoPE       â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    136      MLA_FUSED_ROPE = auto()             â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    137                                          â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    138                                          â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    139  class DeepseekV2MLP(nn.Module):         â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    140      def __init__(                       â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    141          self,                           â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    142          hidden_size: int,               â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    143          intermediate_size: int,         â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    144          hidden_act: str,                â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    145          quant_config:                   â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ Optional[QuantizationConfig] = None,            â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    146          reduce_results: bool = True,    â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    147          prefix: str = "",               â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    148          tp_rank: Optional = None,       â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    149          tp_size: Optional = None,       â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    150      ) -> None:                          â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    151          super().__init__()              â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    152          self.tp_size = tp_size          â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    153                                          â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    154          self.gate_up_proj =             â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ MergedColumnParallelLinear(                     â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    155              hidden_size,                â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    156               * 2,                       â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    157              bias=False,                 â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    158              quant_config=quant_config,  â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    159                                          â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ prefix=add_prefix("gate_up_proj", prefix),      â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    160              tp_rank=tp_rank,            â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    161              tp_size=tp_size,            â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    162          )                               â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    163          self.down_proj =                â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ RowParallelLinear(                              â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    164              intermediate_size,          â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    165              hidden_size,                â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    166              bias=False,                 â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    167              quant_config=quant_config,  â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    168                                          â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ reduce_results=reduce_results,                  â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    169                                          â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ prefix=add_prefix("down_proj", prefix),         â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    170              tp_rank=tp_rank,            â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    171              tp_size=tp_size,            â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    172          )                               â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    173          if hidden_act != "silu":        â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    174              raise ValueError(           â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    175                  f"Unsupported           â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ activation: {hidden_act}. "                     â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    176                  "Only silu is supported â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ for now."                                       â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    177              )                           â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    178          self.act_fn = SiluAndMul()      â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    179                                          â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    180      def forward(self, x,                â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ forward_batch=None):                            â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    181          if (self.tp_size == 1) and      â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ x.shape[0] == 0:                                â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    182              return x                    â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    183                                          â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    184          gate_up, _ =                    â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ self.gate_up_proj(x)                            â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    185          x = self.act_fn(gate_up)        â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    186          x, _ = self.down_proj(x)        â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    187          return x                        â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    188                                          â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    189                                          â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    190  class MoEGate(nn.Module):               â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    191      def __init__(                       â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    192          self,                           â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    193          config,                         â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    194          prefix: str = "",               â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    195      ):                                  â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    196          super().__init__()              â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    197          self.weight = nn.Parameter(     â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    198                                          â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ torch.empty((config.n_routed_experts,           â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ config.hidden_size))                            â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    199          )                               â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    200          if config.topk_method ==        â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ "noaux_tc":                                     â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    201                                          â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ self.e_score_correction_bias = nn.Parameter(    â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    202                                          â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ torch.empty((config.n_routed_experts))          â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    203              )                           â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    204          else:                           â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    205                                          â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ self.e_score_correction_bias = None             â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    206                                          â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    207      def forward(self, hidden_states):   â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    208          logits =                        â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ F.linear(hidden_states, self.weight, None)      â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    209          return logits                   â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    210                                          â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    211                                          â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    212  class DeepseekV2MoE(nn.Module):         â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    213                                          â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    214      def __init__(                       â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    215          self,                           â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    216          config: PretrainedConfig,       â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    217          layer_id: int,                  â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    218          quant_config:                   â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ Optional[QuantizationConfig] = None,            â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    219          prefix: str = "",               â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    220      ):                                  â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    221          super().__init__()              â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    222          self.tp_size =                  â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ get_tensor_model_parallel_world_size()          â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    223          self.routed_scaling_factor =    â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ config.routed_scaling_factor                    â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    224          self.n_shared_experts =         â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ config.n_shared_experts                         â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    225          self.n_share_experts_fusion =   â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ global_server_args_dict["n_share_experts_fusioâ€¦ â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    226          self.config = config            â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    227          self.layer_id = layer_id        â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    228                                          â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    229          if self.tp_size >               â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ config.n_routed_experts:                        â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    230              raise ValueError(           â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    231                  f"Tensor parallel size  â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ {self.tp_size} is greater than "                â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    232                  f"the number of experts â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ {config.n_routed_experts}."                     â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    233              )                           â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    234                                          â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    235          if config.hidden_act != "silu": â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    236              raise ValueError(           â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    237                  f"Unsupported           â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ activation: {config.hidden_act}. "              â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    238                  "Only silu is supported â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ for now."                                       â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    239              )                           â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    240                                          â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    241          self.gate =                     â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ MoEGate(config=config,                          â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ prefix=add_prefix("gate", prefix))              â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    242                                          â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    243          self.experts =                  â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ get_moe_impl_class()(                           â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    244                                          â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ num_experts=config.n_routed_experts             â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    245              +                           â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ self.n_share_experts_fusion                     â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    246              +                           â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ global_server_args_dict["ep_num_redundant_expeâ€¦ â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    247                                          â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ top_k=config.num_experts_per_tok +              â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ min(self.n_share_experts_fusion, 1),            â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    248                                          â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ hidden_size=config.hidden_size,                 â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    249                                          â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ intermediate_size=config.moe_intermediate_size, â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    250              layer_id=self.layer_id,     â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    251                                          â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ renormalize=config.norm_topk_prob,              â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    252              quant_config=quant_config,  â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    253              use_grouped_topk=True,      â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    254                                          â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ num_expert_group=config.n_group,                â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    255                                          â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ topk_group=config.topk_group,                   â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    256                                          â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ correction_bias=self.gate.e_score_correction_bâ€¦ â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    257                                          â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ routed_scaling_factor=self.routed_scaling_factâ€¦ â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    258                                          â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ prefix=add_prefix("experts", prefix),           â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    259              **(                         â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    260                                          â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ dict(deepep_mode=DeepEPMode[global_server_argsâ€¦ â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    261                  if                      â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ global_server_args_dict["enable_deepep_moe"]    â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    262                  else {}                 â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    263              ),                          â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    264          )                               â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    265                                          â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    266          if config.n_shared_experts is   â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ not None and self.n_share_experts_fusion == 0:  â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    267              intermediate_size =         â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ config.moe_intermediate_size *                  â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ config.n_shared_experts                         â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    268              # disable tp for shared     â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ experts when enable deepep moe                  â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    269              self.shared_experts =       â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ DeepseekV2MLP(                                  â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    270                                          â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ hidden_size=config.hidden_size,                 â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    271                                          â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ intermediate_size=intermediate_size,            â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    272                                          â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ hidden_act=config.hidden_act,                   â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    273                                          â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ quant_config=quant_config,                      â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    274                  reduce_results=False,   â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    275                                          â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ prefix=add_prefix("shared_experts", prefix),    â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    276                  **(                     â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    277                      dict(tp_rank=0,     â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ tp_size=1)                                      â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    278                      if                  â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ global_server_args_dict["enable_deepep_moe"]    â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    279                      else {}             â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    280                  ),                      â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    281              )                           â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    282                                          â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    283          self.top_k =                    â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ config.num_experts_per_tok                      â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    284                                          â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    285          if                              â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ global_server_args_dict["enable_deepep_moe"]:   â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    286              # TODO: we will support tp  â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ < ep in the future                              â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    287              self.ep_size =              â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ get_tensor_model_parallel_world_size()          â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    288              self.num_experts = (        â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    289                  config.n_routed_experts â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    290                  +                       â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ global_server_args_dict["ep_num_redundant_expeâ€¦ â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    291              )                           â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    292              self.renormalize =          â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ config.norm_topk_prob                           â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    293              self.topk_group =           â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ config.topk_group                               â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    294              self.num_expert_group =     â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ config.n_group                                  â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    295              self.correction_bias = (    â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    296                                          â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ self.gate.e_score_correction_bias.data          â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    297                  if                      â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ self.gate.e_score_correction_bias is not None   â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    298                  else None               â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    299              )                           â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    300                                          â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    301              self.deepep_dispatcher =    â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ MaybeTboDeepEPDispatcher(                       â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    302                                          â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ group=parallel_state.get_tp_group().device_groâ€¦ â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    303                  router_topk=self.top_k, â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    304                  permute_fusion=True,    â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    305                                          â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ num_experts=self.num_experts,                   â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    306                                          â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ num_local_experts=config.n_routed_experts //    â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ self.tp_size,                                   â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    307                                          â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ hidden_size=config.hidden_size,                 â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    308                                          â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ params_dtype=config.torch_dtype,                â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    309                                          â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ deepep_mode=DeepEPMode[global_server_args_dictâ€¦ â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    310                  async_finish=True,      â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    311                  return_recv_hook=True,  â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    312              )                           â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    313                                          â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    314          self._enable_deepep_moe =       â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ global_server_args_dict["enable_deepep_moe"]    â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    315                                          â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    316      def get_moe_weights(self):          â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    317          return [                        â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    318              x.data                      â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    319              for name, x in              â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ self.experts.named_parameters()                 â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    320              if name not in              â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ ["correction_bias"]                             â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    321          ]                               â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    322                                          â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    323      def forward(                        â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    324          self, hidden_states:            â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ torch.Tensor, forward_batch:                    â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ Optional[ForwardBatch] = None                   â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    325      ) -> torch.Tensor:                  â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    326          if not self._enable_deepep_moe: â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    327              return                      â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ self.forward_normal(hidden_states)              â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    328          else:                           â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    329              return                      â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ self.forward_deepep(hidden_states,              â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ forward_batch)                                  â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    330                                          â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    331      def forward_normal(self,            â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ hidden_states: torch.Tensor) -> torch.Tensor:   â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    332          shared_output =                 â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ self._forward_shared_experts(hidden_states)     â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    333          # router_logits: (num_tokens,   â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ n_experts)                                      â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    334          router_logits =                 â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ self.gate(hidden_states)                        â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    335          final_hidden_states =           â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ self.experts(                                   â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    336                                          â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ hidden_states=hidden_states,                    â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ router_logits=router_logits                     â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    337          )                               â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    338          final_hidden_states *=          â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ self.routed_scaling_factor                      â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    339          if shared_output is not None:   â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    340              final_hidden_states =       â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ final_hidden_states + shared_output             â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    341          if self.tp_size > 1:            â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    342              final_hidden_states =       â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ tensor_model_parallel_all_reduce(final_hidden_â€¦ â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    343          return final_hidden_states      â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    344                                          â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    345      def forward_deepep(                 â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    346          self, hidden_states:            â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ torch.Tensor, forward_batch: ForwardBatch       â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    347      ) -> torch.Tensor:                  â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    348          forward_mode =                  â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ forward_batch.forward_mode                      â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    349          shared_output = None            â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    350          if                              â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ is_non_idle_and_non_empty(forward_mode,         â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ hidden_states):                                 â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    351              # router_logits:            â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ (num_tokens, n_experts)                         â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    352              router_logits =             â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ self.gate(hidden_states)                        â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    353              shared_output =             â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ self._forward_shared_experts(hidden_states)     â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    354              topk_weights, topk_idx =    â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ select_experts(                                 â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    355                                          â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ hidden_states=hidden_states,                    â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    356                                          â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ router_logits=router_logits,                    â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    357                  top_k=self.top_k,       â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    358                  use_grouped_topk=True,  â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    359                                          â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ renormalize=self.renormalize,                   â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    360                                          â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ topk_group=self.topk_group,                     â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    361                                          â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ num_expert_group=self.num_expert_group,         â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    362                                          â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ correction_bias=self.correction_bias,           â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    363                                          â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ routed_scaling_factor=self.routed_scaling_factâ€¦ â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    364                                          â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ num_token_non_padded=forward_batch.num_token_nâ€¦ â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    365                                          â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ expert_location_dispatch_info=ExpertLocationDiâ€¦ â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    366                                          â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ layer_id=self.layer_id,                         â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    367                  ),                      â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    368              )                           â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    369          else:                           â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    370              topk_idx = torch.full(      â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    371                  (0, self.top_k), -1,    â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ dtype=torch.int, device=hidden_states.device    â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    372              )                           â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    373              topk_weights = torch.empty( â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    374                  (0, self.top_k),        â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ dtype=torch.float32,                            â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ device=hidden_states.device                     â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    375              )                           â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    376          if self.ep_size > 1:            â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    377              # TODO(ch-wan): allow users â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ to set num_max_dispatch_tokens_per_rank value   â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    378              (                           â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    379                  hidden_states,          â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    380                  topk_idx,               â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    381                  topk_weights,           â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    382                  reorder_topk_ids,       â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    383                                          â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ num_recv_tokens_per_expert,                     â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    384                  seg_indptr,             â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    385                  masked_m,               â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    386                  expected_m,             â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    387              ) =                         â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ self.deepep_dispatcher.dispatch(                â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    388                                          â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ hidden_states=hidden_states,                    â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    389                  topk_idx=topk_idx,      â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    390                                          â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ topk_weights=topk_weights,                      â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    391                                          â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ forward_mode=forward_mode,                      â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    392              )                           â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    393          final_hidden_states =           â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ self.experts(                                   â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    394                                          â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ hidden_states=hidden_states,                    â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    395              topk_idx=topk_idx,          â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    396              topk_weights=topk_weights,  â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    397                                          â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ reorder_topk_ids=reorder_topk_ids,              â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    398              seg_indptr=seg_indptr,      â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    399              masked_m=masked_m,          â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    400              expected_m=expected_m,      â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    401                                          â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ num_recv_tokens_per_expert=num_recv_tokens_perâ€¦ â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    402              forward_mode=forward_mode,  â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    403          )                               â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    404          if self.ep_size > 1:            â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    405              final_hidden_states =       â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ self.deepep_dispatcher.combine(                 â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    406                                          â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ hidden_states=final_hidden_states,              â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    407                  topk_idx=topk_idx,      â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    408                                          â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ topk_weights=topk_weights,                      â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    409                                          â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ forward_mode=forward_mode,                      â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    410              )                           â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    411          final_hidden_states *=          â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ self.routed_scaling_factor                      â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    412                                          â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    413          if shared_output is not None:   â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    414              final_hidden_states =       â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ final_hidden_states + shared_output             â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    415                                          â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    416          return final_hidden_states      â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    417                                          â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    418      def _forward_shared_experts(self,   â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ hidden_states):                                 â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    419          if self.n_share_experts_fusion  â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ == 0:                                           â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    420              return                      â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ self.shared_experts(hidden_states)              â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    421          else:                           â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    422              return None                 â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    423                                          â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    424      def op_gate(self, state):           â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    425          if is_non_idle_and_non_empty(   â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    426                                          â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ state.forward_batch.forward_mode,               â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ state.hidden_states_mlp_input                   â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    427          ):                              â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    428              # router_logits:            â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ (num_tokens, n_experts)                         â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    429              state.router_logits =       â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ self.gate(state.hidden_states_mlp_input)        â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    430          else:                           â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    431              state.router_logits = None  â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    432                                          â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    433      def op_shared_experts(self, state): â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    434          hidden_states_mlp_input =       â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ state.pop("hidden_states_mlp_input")            â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    435          if (self.n_share_experts_fusion â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ == 0) and is_non_idle_and_non_empty(            â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    436                                          â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ state.forward_batch.forward_mode,               â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ hidden_states_mlp_input                         â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    437          ):                              â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    438              state.shared_output =       â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ self.shared_experts(hidden_states_mlp_input)    â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    439          else:                           â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    440              state.shared_output = None  â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    441                                          â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    442      def op_select_experts(self, state): â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    443          <response clipped><NOTE>To save â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ on context only part of this file has been      â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ shown to you. You should retry this tool after  â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ you have searched inside the file with `grep    â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ -n` in order to find the line numbers of what   â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ you are looking for.</NOTE>                     â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                                                 â”‚   â”‚
â”‚             â”‚ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚
â”‚ Total       â”‚ Input: 27792 Output: 972                                                                               â”‚
â”‚ Tokens      â”‚                                                                                                        â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Status      â”‚ âœ… Step 4: Completed                                                                                   â”‚
â”‚ Tools       â”‚ ðŸ”§ bash                                                                                                â”‚
â”‚ bash        â”‚ â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚
â”‚             â”‚ â”‚ {'command': 'grep -n                           â”‚ 87:from sglang.srt.two_batch_overlap import (   â”‚   â”‚
â”‚             â”‚ â”‚ "torch\\.zeros\\|fill_\\|torch\\.empty\\|cudaâ€¦ â”‚ 198:                                            â”‚   â”‚
â”‚             â”‚ â”‚ -n                                             â”‚ torch.empty((config.n_routed_experts,           â”‚   â”‚
â”‚             â”‚ â”‚ /home/ubuntu/OmniPerf-Bench/perf-agents-benchâ€¦ â”‚ config.hidden_size))                            â”‚   â”‚
â”‚             â”‚ â”‚ | sed -n \'1,200p\'', 'restart': False}        â”‚ 202:                                            â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ torch.empty((config.n_routed_experts))          â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ 373:            topk_weights = torch.empty(     â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ 465:            state.topk_weights_local =      â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ torch.empty(                                    â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ 882:        k = torch.empty_like(q)             â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ 1091:            k_pe_output =                  â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ torch.empty_like(k_input[..., self.kv_lora_rank â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ :])                                             â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ 1097:        attn_output = torch.empty(         â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ 1109:            attn_logits = torch.empty(     â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ 1252:            k = torch.empty(               â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ 1266:            tmp_output =                   â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ torch.empty_like(accum_output)                  â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ 1267:            tmp_lse =                      â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ torch.empty_like(accum_lse)                     â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ 1310:        k = torch.empty_like(q)            â”‚   â”‚
â”‚             â”‚ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚
â”‚ Total       â”‚ Input: 42849 Output: 1089                                                                              â”‚
â”‚ Tokens      â”‚                                                                                                        â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Status      â”‚ âœ… Step 5: Completed                                                                                   â”‚
â”‚ Tools       â”‚ ðŸ”§ str_replace_based_edit_tool                                                                         â”‚
â”‚ str_replacâ€¦ â”‚ â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚
â”‚             â”‚ â”‚ {'command': 'view', 'file_text': '',           â”‚ Here's the result of running `cat -n` on        â”‚   â”‚
â”‚             â”‚ â”‚ 'insert_line': 1, 'new_str': '', 'old_str':    â”‚ /home/ubuntu/OmniPerf-Bench/perf-agents-bench/â€¦ â”‚   â”‚
â”‚             â”‚ â”‚ '', 'path':                                    â”‚      1  import dataclasses                      â”‚   â”‚
â”‚             â”‚ â”‚ '/home/ubuntu/OmniPerf-Bench/perf-agents-bencâ€¦ â”‚      2  import logging                          â”‚   â”‚
â”‚             â”‚ â”‚ 'view_range': [1, -1]}                         â”‚      3  from typing import TYPE_CHECKING, Dict, â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ List, Optional, Sequence                        â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚      4                                          â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚      5  import torch                            â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚      6                                          â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚      7  from                                    â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ sglang.srt.layers.attention.base_attn_backend   â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ import AttentionBackend                         â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚      8  from sglang.srt.layers.communicator     â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ import (                                        â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚      9      CommunicateContext,                 â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚     10      CommunicateSimpleFn,                â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚     11      CommunicateSummableTensorPairFn,    â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚     12      ScatterMode,                        â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚     13  )                                       â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚     14  from sglang.srt.layers.dp_attention     â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ import get_attention_tp_size                    â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚     15  from                                    â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ sglang.srt.layers.moe.ep_moe.token_dispatcher   â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ import DeepEPDispatcher                         â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚     16  from                                    â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ sglang.srt.layers.quantization.deep_gemm import â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ configure_deep_gemm_num_sms                     â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚     17  from sglang.srt.managers.schedule_batch â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ import global_server_args_dict                  â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚     18  from                                    â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ sglang.srt.model_executor.forward_batch_info    â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ import ForwardBatch, ForwardMode                â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚     19  from sglang.srt.operations import       â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ execute_operations,                             â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ execute_overlapped_operations                   â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚     20  from sglang.srt.operations_strategy     â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ import OperationsStrategy                       â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚     21  from sglang.srt.utils import            â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ BumpAllocator, DeepEPMode, get_bool_env_var     â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚     22                                          â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚     23  if TYPE_CHECKING:                       â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚     24      from                                â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ sglang.srt.model_executor.cuda_graph_runner     â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ import CudaGraphRunner                          â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚     25                                          â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚     26  _tbo_debug =                            â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ get_bool_env_var("SGLANG_TBO_DEBUG")            â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚     27                                          â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚     28  logger = logging.getLogger(__name__)    â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚     29                                          â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚     30                                          â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚     31  # --------------------------------      â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ Compute Basic Info                              â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ ---------------------------------------         â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚     32                                          â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚     33                                          â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚     34  # TODO: may smartly disable TBO when    â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ batch size is too small b/c it will slow down   â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚     35  def compute_split_seq_index(            â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚     36      forward_mode: "ForwardMode",        â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚     37      num_tokens: int,                    â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚     38      extend_lens: Optional[Sequence],    â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚     39  ) -> Optional:                          â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚     40      if forward_mode.is_extend():        â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚     41          assert extend_lens is not None  â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚     42          return                          â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ _split_array_by_half_sum(extend_lens)           â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚     43      elif forward_mode.is_decode():      â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚     44          return num_tokens // 2          â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚     45      elif forward_mode.is_idle():        â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚     46          assert num_tokens == 0          â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚     47          return 0                        â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚     48      else:                               â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚     49          raise NotImplementedError       â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚     50                                          â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚     51                                          â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚     52  def _split_array_by_half_sum(arr:       â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ Sequence) -> int:                               â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚     53      overall_sum = sum(arr)              â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚     54      left_sum = 0                        â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚     55      min_diff = float("inf")             â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚     56      best_index = 0                      â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚     57                                          â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚     58      for i in range(1, len(arr)):        â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚     59          left_sum += arr                 â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚     60          right_sum = overall_sum -       â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ left_sum                                        â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚     61          diff = abs(left_sum -           â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ right_sum)                                      â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚     62          if diff <= min_diff:            â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚     63              min_diff = diff             â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚     64              best_index = i              â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚     65          else:                           â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚     66              break                       â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚     67                                          â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚     68      return best_index                   â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚     69                                          â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚     70                                          â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚     71  def compute_split_token_index(          â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚     72      split_seq_index: int,               â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚     73      forward_mode: "ForwardMode",        â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚     74      extend_seq_lens:                    â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ Optional[Sequence],                             â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚     75  ) -> int:                               â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚     76      if forward_mode.is_extend():        â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚     77          assert extend_seq_lens is not   â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ None                                            â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚     78          return                          â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ sum(extend_seq_lens[:split_seq_index])          â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚     79      elif forward_mode.is_decode():      â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚     80          return split_seq_index          â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚     81      elif forward_mode.is_idle():        â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚     82          assert split_seq_index == 0     â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚     83          return 0                        â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚     84      else:                               â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚     85          raise NotImplementedError       â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚     86                                          â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚     87                                          â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚     88  def                                     â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ compute_split_indices_for_cuda_graph_replay(    â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚     89      forward_mode: ForwardMode,          â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚     90      cuda_graph_num_tokens: int,         â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚     91  ):                                      â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚     92      forward_mode_for_tbo_split = (      â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚     93          forward_mode if forward_mode != â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ ForwardMode.IDLE else ForwardMode.DECODE        â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚     94      )                                   â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚     95      tbo_split_seq_index =               â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ compute_split_seq_index(                        â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚     96                                          â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ forward_mode=forward_mode_for_tbo_split,        â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚     97                                          â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ num_tokens=cuda_graph_num_tokens,               â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚     98          extend_lens=None,               â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚     99      )                                   â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    100      tbo_split_token_index =             â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ compute_split_token_index(                      â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    101                                          â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ split_seq_index=tbo_split_seq_index,            â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    102                                          â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ forward_mode=forward_mode_for_tbo_split,        â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    103          extend_seq_lens=None,           â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    104      )                                   â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    105      return tbo_split_seq_index,         â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ tbo_split_token_index                           â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    106                                          â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    107                                          â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    108  # --------------------------------      â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ Preparation                                     â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ ---------------------------------------         â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    109                                          â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    110                                          â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    111  class TboCudaGraphRunnerPlugin:         â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    112      def __init__(self):                 â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    113          pass  # TODO add logic here     â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    114                                          â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    115      def capture_one_batch_size(self,    â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ batch: ForwardBatch, num_tokens: int):          â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    116          if not                          â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ global_server_args_dict["enable_two_batch_overâ€¦ â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    117              return                      â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    118                                          â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    119          batch.tbo_split_seq_index =     â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ compute_split_seq_index(                        â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    120                                          â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ forward_mode=batch.forward_mode,                â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    121              num_tokens=num_tokens,      â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    122              extend_lens=None,           â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    123          )                               â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    124          # For simplicity, when          â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ two_batch_overlap is enabled, we only capture   â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ CUDA Graph for tbo=true                         â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    125          assert                          â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ batch.tbo_split_seq_index is not None,          â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ f"{num_tokens=}"                                â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    126                                          â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    127                                          â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ TboForwardBatchPreparer.prepare(batch)          â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    128                                          â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    129      def replay_prepare(                 â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    130          self, forward_mode:             â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ ForwardMode, bs: int, num_token_non_padded: int â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    131      ):                                  â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    132          if not                          â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ global_server_args_dict["enable_two_batch_overâ€¦ â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    133              return                      â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    134                                          â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    135          pass  # TODO add logic here     â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    136                                          â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    137                                          â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    138  class TboDPAttentionPreparer:           â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    139      def prepare_all_gather(             â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    140          self, local_batch, deepep_mode, â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ enable_deepep_moe, enable_two_batch_overlap     â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    141      ):                                  â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    142          self.enable_two_batch_overlap = â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ enable_two_batch_overlap                        â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    143                                          â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    144          if local_batch is not None:     â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    145                                          â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ self.local_tbo_split_seq_index =                â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ compute_split_seq_index(                        â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    146                                          â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ forward_mode=local_batch.forward_mode,          â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    147                                          â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ num_tokens=local_batch.input_ids.shape[0],      â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    148                                          â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ extend_lens=local_batch.extend_lens,            â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    149              )                           â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    150              resolved_deepep_mode =      â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ deepep_mode.resolve(local_batch.forward_mode)   â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    151              local_can_run_tbo =         â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ (self.local_tbo_split_seq_index is not None)    â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ and not (                                       â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    152                                          â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ local_batch.forward_mode.is_extend()            â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    153                  and enable_deepep_moe   â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    154                  and                     â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ (resolved_deepep_mode ==                        â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ DeepEPMode.low_latency)                         â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    155              )                           â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    156          else:                           â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    157                                          â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ self.local_tbo_split_seq_index = 0              â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    158              local_can_run_tbo = True    â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    159                                          â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    160          local_forward_mode =            â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ self._compute_local_forward_mode(local_batch)   â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    161                                          â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    162          return local_can_run_tbo,       â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ local_forward_mode                              â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    163                                          â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    164      def compute_output(self,            â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ partial_global_info):                           â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    165          local_can_run_tbo_aggregated =  â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ min(partial_global_info[:, 0, 0].tolist())      â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    166          forward_modes =                 â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ partial_global_info[:, 0, 1].tolist()           â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    167                                          â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    168          global_forward_mode,            â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ forward_mode_agree =                            â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ self._compute_global_forward_mode(              â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    169              forward_modes               â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    170          )                               â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    171                                          â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    172          can_run_tbo = (                 â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    173                                          â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ self.enable_two_batch_overlap                   â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    174              and                         â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ local_can_run_tbo_aggregated                    â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    175              and forward_mode_agree      â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    176          )                               â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    177                                          â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    178          tbo_split_seq_index =           â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ self.local_tbo_split_seq_index if can_run_tbo   â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ else None                                       â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    179          global_forward_mode =           â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ global_forward_mode if can_run_tbo else None    â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    180          return tbo_split_seq_index,     â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ global_forward_mode                             â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    181                                          â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    182      @staticmethod                       â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    183      def                                 â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ _compute_local_forward_mode(local_batch):       â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    184          return (                        â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    185              local_batch.forward_mode if â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ local_batch is not None else ForwardMode.IDLE   â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    186          ).value                         â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    187                                          â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    188      @staticmethod                       â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    189      def                                 â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ _compute_global_forward_mode(forward_modes):    â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    190          converted_forward_modes = [     â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    191              ForwardMode.DECODE.value if â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ x == ForwardMode.IDLE.value else x              â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    192              for x in forward_modes      â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    193          ]                               â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    194          forward_mode_agree =            â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ TboDPAttentionPreparer._is_all_same(            â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    195              converted_forward_modes     â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    196          )                               â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    197          global_forward_mode = (         â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    198                                          â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ ForwardMode(converted_forward_modes[0]) if      â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ forward_mode_agree else None                    â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    199          )                               â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    200          return global_forward_mode,     â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ forward_mode_agree                              â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    201                                          â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    202      @staticmethod                       â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    203      def _is_all_same(x):                â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    204          return all(value == x[0] for    â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ value in x)                                     â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    205                                          â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    206                                          â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    207  class TboForwardBatchPreparer:          â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    208      @classmethod                        â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    209      def prepare(cls, batch:             â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ ForwardBatch):                                  â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    210          from                            â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ sglang.srt.layers.attention.tbo_backend import  â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ TboAttnBackend                                  â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    211                                          â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    212          if batch.tbo_split_seq_index is â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ None:                                           â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    213              return                      â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    214                                          â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    215          tbo_split_token_index =         â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ compute_split_token_index(                      â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    216                                          â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ split_seq_index=batch.tbo_split_seq_index,      â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    217                                          â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ forward_mode=batch.forward_mode,                â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    218                                          â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ extend_seq_lens=batch.extend_seq_lens_cpu,      â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    219          )                               â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    220                                          â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    221          if _tbo_debug:                  â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    222              logger.info(                â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    223                                          â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ f"TboForwardBatchPreparer.prepare "             â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    224                                          â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ f"tbo_split_seq_index={batch.tbo_split_seq_indâ€¦ â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ "                                               â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    225                                          â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ f"tbo_split_token_index={tbo_split_token_index} â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ "                                               â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    226                                          â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ f"extend_seq_lens={batch.extend_seq_lens_cpu}"  â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    227              )                           â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    228                                          â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    229          assert                          â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ isinstance(batch.attn_backend, TboAttnBackend)  â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    230          attn_backend_child_a,           â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ attn_backend_child_b =                          â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ batch.attn_backend.children                     â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    231                                          â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    232          child_a = cls.filter_batch(     â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    233              batch,                      â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    234              start_token_index=0,        â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    235                                          â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ end_token_index=tbo_split_token_index,          â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    236              start_seq_index=0,          â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    237                                          â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ end_seq_index=batch.tbo_split_seq_index,        â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    238                                          â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ output_attn_backend=attn_backend_child_a,       â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    239          )                               â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    240          child_b = cls.filter_batch(     â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    241              batch,                      â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    242                                          â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ start_token_index=tbo_split_token_index,        â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    243                                          â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ end_token_index=batch.input_ids.shape[0],       â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    244                                          â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ start_seq_index=batch.tbo_split_seq_index,      â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    245                                          â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ end_seq_index=batch.batch_size,                 â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    246                                          â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ output_attn_backend=attn_backend_child_b,       â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    247          )                               â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    248                                          â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    249          assert batch.tbo_children is    â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ None                                            â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    250          batch.tbo_children =            â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    251                                          â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    252      @classmethod                        â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    253      def filter_batch(                   â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    254          cls,                            â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    255          batch: ForwardBatch,            â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    256          *,                              â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    257          start_token_index: int,         â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    258          end_token_index: int,           â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    259          start_seq_index: int,           â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    260          end_seq_index: int,             â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    261          output_attn_backend:            â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ AttentionBackend,                               â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    262      ):                                  â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    263          from                            â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ sglang.srt.managers.schedule_batch import       â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ global_server_args_dict                         â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    264                                          â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    265          num_tokens =                    â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ batch.input_ids.shape[0]                        â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    266          num_seqs = batch.batch_size     â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    267                                          â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    268          output_dict = dict()            â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    269                                          â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    270          for key in [                    â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    271              "input_ids",                â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    272              "positions",                â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    273              "out_cache_loc",            â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    274          ]:                              â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    275              old_value = getattr(batch,  â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ key)                                            â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    276              assert (                    â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    277                  old_value.shape[0] ==   â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ num_tokens                                      â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    278              ), f"{key=} {old_value=}    â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ {num_tokens=} {batch=}"                         â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    279              output_dict = old_value     â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    280                                          â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    281          for key in [                    â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    282              "req_pool_indices",         â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    283              "seq_lens",                 â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    284              "seq_lens_cpu",             â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    285              "extend_seq_lens",          â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    286              "extend_prefix_lens",       â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    287              "extend_start_loc",         â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    288              "extend_prefix_lens_cpu",   â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    289              "extend_seq_lens_cpu",      â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    290                                          â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ "extend_logprob_start_lens_cpu",                â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    291              "lora_paths",               â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    292          ]:                              â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    293              old_value = getattr(batch,  â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ key)                                            â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    294              if old_value is None:       â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    295                  continue                â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    296              assert (                    â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    297                  len(old_value) ==       â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ num_seqs                                        â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    298              ), f"{key=} {old_value=}    â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ {num_seqs=} {batch=}"                           â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    299              output_dict = old_value     â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    300                                          â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    301          for key in [                    â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    302              "forward_mode",             â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    303              "return_logprob",           â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    304              "req_to_token_pool",        â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    305              "token_to_kv_pool",         â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    306              "can_run_dp_cuda_graph",    â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    307              "global_forward_mode",      â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    308              "spec_info",                â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    309              "spec_algorithm",           â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    310              "capture_hidden_mode",      â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    311              "padded_static_len",        â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    312              "mrope_positions",  # only  â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ used by qwen2-vl, thus not care                 â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    313          ]:                              â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    314              output_dict =               â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ getattr(batch, key)                             â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    315                                          â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    316          assert (                        â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    317                                          â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ _compute_extend_num_tokens(batch.input_ids,     â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ batch.forward_mode)                             â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    318              == batch.extend_num_tokens  â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    319          ), f"{batch=}"                  â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    320          extend_num_tokens =             â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ _compute_extend_num_tokens(                     â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    321              output_dict["input_ids"],   â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ output_dict["forward_mode"]                     â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    322          )                               â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    323                                          â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    324          # TODO improve, e.g. unify w/   â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ `init_raw`                                      â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    325          if                              â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ global_server_args_dict["moe_dense_tp_size"] == â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ 1:                                              â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    326              sum_len = end_token_index - â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ start_token_index                               â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    327              gathered_buffer =           â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ torch.zeros(                                    â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    328                  (sum_len,               â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ batch.gathered_buffer.shape[1]),                â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    329                                          â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ dtype=batch.gathered_buffer.dtype,              â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    330                                          â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ device=batch.gathered_buffer.device,            â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    331              )                           â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    332          else:                           â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    333              gathered_buffer = None      â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    334                                          â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    335          output_dict.update(             â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    336              dict(                       â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    337                                          â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ batch_size=end_seq_index - start_seq_index,     â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    338                  seq_lens_sum=(          â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    339                                          â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ output_dict["seq_lens_cpu"].sum()               â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    340                      if "seq_lens_cpu"   â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ in output_dict                                  â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    341                      else None           â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    342                  ),                      â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    343                                          â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ extend_num_tokens=extend_num_tokens,            â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    344                                          â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ attn_backend=output_attn_backend,               â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    345                                          â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ tbo_split_seq_index=None,                       â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    346                                          â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ tbo_parent_token_range=(start_token_index,      â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ end_token_index),                               â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    347                  tbo_children=None,      â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    348                                          â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ global_num_tokens_gpu=None,                     â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    349                                          â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ global_num_tokens_cpu=None,                     â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    350                                          â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ gathered_buffer=gathered_buffer,                â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    351                                          â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ global_num_tokens_for_logprob_gpu=None,         â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    352                                          â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ global_num_tokens_for_logprob_cpu=None,         â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    353                  sampling_info=None,     â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    354                  # For logits and        â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ logprobs post processing, thus we do not care   â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    355                                          â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ temp_scaled_logprobs=False,                     â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    356                  temperature=None,       â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    357                                          â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ top_p_normalized_logprobs=False,                â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    358                  top_p=None,             â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    359                  mm_inputs=None,         â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    360                                          â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ num_token_non_padded=None,                      â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    361              )                           â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    362          )                               â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    363                                          â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    364          errors = []                     â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    365          for field in                    â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ dataclasses.fields(ForwardBatch):               â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    366              if getattr(batch,           â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ field.name) is not None and field.name not in   â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ output_dict:                                    â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    367                  errors.append(          â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    368                      f"Field             â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ {field.name} has value, but is not yet          â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ supported (value={getattr(batch, field.name)}   â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ batch={batch})"                                 â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    369                  )                       â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    370          if len(errors) > 0:             â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    371              raise                       â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ Exception(f"{len(errors)} errors happen:\n" +   â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ "\n\n".join(errors))                            â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    372                                          â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    373          return                          â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ ForwardBatch(**output_dict)                     â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    374                                          â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    375                                          â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    376  def                                     â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ _compute_extend_num_tokens(input_ids,           â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ forward_mode: ForwardMode):                     â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    377      if forward_mode.is_extend():        â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    378          return input_ids.shape[0]       â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    379      elif forward_mode.is_decode() or    â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ forward_mode.is_idle():                         â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    380          return None                     â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    381      raise NotImplementedError           â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    382                                          â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    383                                          â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    384  # --------------------------------      â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ Execution                                       â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ ---------------------------------------         â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    385                                          â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    386                                          â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    387  def model_forward_maybe_tbo(            â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    388      layers,                             â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    389      enable_tbo: bool,                   â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    390      positions: torch.Tensor,            â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    391      forward_batch: ForwardBatch,        â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    392      hidden_states: torch.Tensor,        â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    393      input_data_scatter_mode:            â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ ScatterMode,                                    â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    394      residual: Optional,                 â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    395      zero_allocator:                     â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ Optional[BumpAllocator] = None,                 â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    396  ):                                      â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    397      inputs = dict(                      â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    398          positions=positions,            â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    399          hidden_states=hidden_states,    â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    400          forward_batch=forward_batch,    â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    401          residual=residual,              â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    402          zero_allocator=zero_allocator,  â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    403      )                                   â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    404      layer_input_scatter_mode =          â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ layers[0].layer_scatter_modes.layer_input_mode  â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    405      operations_strategy =               â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ OperationsStrategy.init_new_tbo(                â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    406          layers,                         â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ forward_batch.global_forward_mode               â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    407      )                                   â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    408      if enable_tbo:                      â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    409          return _model_forward_tbo(      â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    410              inputs=inputs,              â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    411                                          â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ operations_strategy=operations_strategy,        â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    412                                          â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ input_data_scatter_mode=input_data_scatter_modâ€¦ â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    413                                          â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ layer_input_scatter_mode=layer_input_scatter_mâ€¦ â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    414          )                               â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    415      else:                               â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    416          return                          â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ _model_forward_non_tbo(inputs,                  â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ operations_strategy)                            â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    417                                          â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    418                                          â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    419  def _model_forward_tbo(                 â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    420      inputs,                             â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    421      operations_strategy:                â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ OperationsStrategy,                             â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    422      input_data_scatter_mode:            â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ ScatterMode,                                    â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    423      layer_input_scatter_mode:           â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ ScatterMode,                                    â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    424  ):                                      â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    425      inputs_arr =                        â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ _model_forward_tbo_split_inputs(                â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    426          **inputs,                       â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    427                                          â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ input_data_scatter_mode=input_data_scatter_modâ€¦ â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    428                                          â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ layer_input_scatter_mode=layer_input_scatter_mâ€¦ â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    429      )                                   â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    430      del inputs                          â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    431                                          â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    432      with                                â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ configure_deep_gemm_num_sms(operations_strategâ€¦ â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    433          outputs_arr =                   â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ execute_overlapped_operations(                  â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    434              inputs_arr=inputs_arr,      â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    435              operations_arr= * 2,        â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    436              delta_stages=[0,            â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ operations_strategy.tbo_delta_stages],          â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    437          )                               â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    438                                          â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    439      return                              â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ _model_forward_tbo_merge_outputs(*outputs_arr)  â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    440                                          â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    441                                          â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    442  def _model_forward_non_tbo(inputs,      â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ operations_strategy: OperationsStrategy):       â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    443      outputs =                           â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ execute_operations(inputs,                      â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ operations_strategy.operations)                 â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    444      return outputs["hidden_states"],    â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ outputs["residual"]                             â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    445                                          â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    446                                          â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    447  def _model_forward_tbo_split_inputs(    â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    448      hidden_states: torch.Tensor,        â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    449      residual: torch.Tensor,             â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    450      positions: torch.Tensor,            â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    451      forward_batch: ForwardBatch,        â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    452      zero_allocator:                     â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ Optional[BumpAllocator],                        â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    453      input_data_scatter_mode:            â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ ScatterMode,                                    â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    454      layer_input_scatter_mode:           â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ ScatterMode,                                    â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    455  ) -> List[Dict]:                        â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    456      tbo_splitter_scatter_mode =         â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ ScatterMode.TP_ATTN_FULL                        â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    457      context =                           â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ CommunicateContext.init_new()                   â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    458                                          â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    459      hidden_states, residual =           â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ CommunicateSummableTensorPairFn.execute(        â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    460                                          â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ hidden_states_input_mode=input_data_scatter_moâ€¦ â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    461                                          â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ residual_input_mode=input_data_scatter_mode,    â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    462                                          â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ output_mode=tbo_splitter_scatter_mode,          â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    463          hidden_states=hidden_states,    â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    464          residual=residual,              â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    465          forward_batch=forward_batch,    â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    466          context=context,                â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    467      )                                   â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    468                                          â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    469      inputs_arr =                        â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ _model_forward_tbo_split_inputs_raw(            â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    470          hidden_st<response              â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ clipped><NOTE>To save on context only part of   â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ this file has been shown to you. You should     â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ retry this tool after you have searched inside  â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ the file with `grep -n` in order to find the    â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ line numbers of what you are looking            â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ for.</NOTE>                                     â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                                                 â”‚   â”‚
â”‚             â”‚ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚
â”‚ Total       â”‚ Input: 58233 Output: 1245                                                                              â”‚
â”‚ Tokens      â”‚                                                                                                        â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Status      â”‚ âœ… Step 6: Completed                                                                                   â”‚
â”‚ Tools       â”‚ ðŸ”§ bash                                                                                                â”‚
â”‚ bash        â”‚ â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚
â”‚             â”‚ â”‚ {'command': 'grep -n                           â”‚ 130:        self, forward_mode: ForwardMode,    â”‚   â”‚
â”‚             â”‚ â”‚ "torch\\.zeros\\|fill_\\|empty(\\|num_token_nâ€¦ â”‚ bs: int, num_token_non_padded: int              â”‚   â”‚
â”‚             â”‚ â”‚ /home/ubuntu/OmniPerf-Bench/perf-agents-benchâ€¦ â”‚ 249:        assert batch.tbo_children is None   â”‚   â”‚
â”‚             â”‚ â”‚ | sed -n \'1,200p\'', 'restart': False}        â”‚ 250:        batch.tbo_children =                â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ 327:            gathered_buffer = torch.zeros(  â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ 347:                tbo_children=None,          â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ 360:                num_token_non_padded=None,  â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ 520:            forward_batch.tbo_children      â”‚   â”‚
â”‚             â”‚ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚
â”‚ Total       â”‚ Input: 78887 Output: 1436                                                                              â”‚
â”‚ Tokens      â”‚                                                                                                        â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Status      â”‚ âœ… Step 7: Completed                                                                                   â”‚
â”‚ Tools       â”‚ ðŸ”§ bash                                                                                                â”‚
â”‚ bash        â”‚ â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚
â”‚             â”‚ â”‚ {'command': 'grep -R "gathered_buffer" -n      â”‚ /home/ubuntu/OmniPerf-Bench/perf-agents-bench/â€¦ â”‚   â”‚
â”‚             â”‚ â”‚ /home/ubuntu/OmniPerf-Bench/perf-agents-benchâ€¦ â”‚ # TODO(ch-wan): SP layernorm should use a       â”‚   â”‚
â”‚             â”‚ â”‚ | sed -n \'1,200p\'', 'restart': False}        â”‚ different logic to manage gathered_buffer       â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ /home/ubuntu/OmniPerf-Bench/perf-agents-bench/â€¦ â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ self.gathered_buffer = torch.zeros(             â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ /home/ubuntu/OmniPerf-Bench/perf-agents-bench/â€¦ â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ gathered_buffer =                               â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ self.gathered_buffer[:num_tokens]               â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ /home/ubuntu/OmniPerf-Bench/perf-agents-bench/â€¦ â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ gathered_buffer = None                          â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ /home/ubuntu/OmniPerf-Bench/perf-agents-bench/â€¦ â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ gathered_buffer=gathered_buffer,                â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ /home/ubuntu/OmniPerf-Bench/perf-agents-bench/â€¦ â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ gathered_buffer: Optional = None                â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ /home/ubuntu/OmniPerf-Bench/perf-agents-bench/â€¦ â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ ret.gathered_buffer = torch.zeros(              â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ /home/ubuntu/OmniPerf-Bench/perf-agents-bench/â€¦ â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ gathered_buffer = torch.zeros(                  â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ /home/ubuntu/OmniPerf-Bench/perf-agents-bench/â€¦ â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ (sum_len, batch.gathered_buffer.shape[1]),      â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ /home/ubuntu/OmniPerf-Bench/perf-agents-bench/â€¦ â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ dtype=batch.gathered_buffer.dtype,              â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ /home/ubuntu/OmniPerf-Bench/perf-agents-bench/â€¦ â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ device=batch.gathered_buffer.device,            â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ /home/ubuntu/OmniPerf-Bench/perf-agents-bench/â€¦ â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ gathered_buffer = None                          â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ /home/ubuntu/OmniPerf-Bench/perf-agents-bench/â€¦ â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ gathered_buffer=gathered_buffer,                â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ /home/ubuntu/OmniPerf-Bench/perf-agents-bench/â€¦ â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ gathered_buffer: Optional = None                â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ /home/ubuntu/OmniPerf-Bench/perf-agents-bench/â€¦ â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ forward_batch_gathered_buffer: Optional = None  â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ /home/ubuntu/OmniPerf-Bench/perf-agents-bench/â€¦ â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ gathered_buffer=forward_batch.gathered_buffer,  â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ /home/ubuntu/OmniPerf-Bench/perf-agents-bench/â€¦ â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ forward_batch_gathered_buffer=forward_batch.gaâ€¦ â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ /home/ubuntu/OmniPerf-Bench/perf-agents-bench/â€¦ â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ gathered_buffer = torch.zeros(                  â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ /home/ubuntu/OmniPerf-Bench/perf-agents-bench/â€¦ â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ self.gathered_buffer = gathered_buffer          â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ /home/ubuntu/OmniPerf-Bench/perf-agents-bench/â€¦ â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ logits_metadata.gathered_buffer,                â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ /home/ubuntu/OmniPerf-Bench/perf-agents-bench/â€¦ â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ forward_batch.gathered_buffer[:                 â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ forward_batch.input_ids.shape[0]],              â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ /home/ubuntu/OmniPerf-Bench/perf-agents-bench/â€¦ â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ forward_batch.gathered_buffer,                  â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ /home/ubuntu/OmniPerf-Bench/perf-agents-bench/â€¦ â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ # important: forward batch.gathered_buffer is   â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ used both after scatter and after gather.       â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ /home/ubuntu/OmniPerf-Bench/perf-agents-bench/â€¦ â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ forward_batch.gathered_buffer[:                 â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ forward_batch.input_ids.shape[0]],              â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ /home/ubuntu/OmniPerf-Bench/perf-agents-bench/â€¦ â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ forward_batch.gathered_buffer[:                 â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ forward_batch.input_ids.shape[0]],              â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ /home/ubuntu/OmniPerf-Bench/perf-agents-bench/â€¦ â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ forward_batch.gathered_buffer,                  â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ /home/ubuntu/OmniPerf-Bench/perf-agents-bench/â€¦ â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ # important: forward batch.gathered_buffer is   â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ used both after scatter and after gather.       â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ /home/ubuntu/OmniPerf-Bench/perf-agents-bench/â€¦ â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ forward_batch.gathered_buffer[:                 â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ forward_batch.input_ids.shape[0]],              â”‚   â”‚
â”‚             â”‚ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚
â”‚ Total       â”‚ Input: 99871 Output: 1579                                                                              â”‚
â”‚ Tokens      â”‚                                                                                                        â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Status      â”‚ âœ… Step 8: Completed                                                                                   â”‚
â”‚ Tools       â”‚ ðŸ”§ bash                                                                                                â”‚
â”‚ bash        â”‚ â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚
â”‚             â”‚ â”‚ {'command': 'grep -R "num_token_non_padded" -n â”‚ /home/ubuntu/OmniPerf-Bench/perf-agents-bench/â€¦ â”‚   â”‚
â”‚             â”‚ â”‚ /home/ubuntu/OmniPerf-Bench/perf-agents-benchâ€¦ â”‚ self.num_token_non_padded = torch.zeros((1,),   â”‚   â”‚
â”‚             â”‚ â”‚ | sed -n \'1,200p\'', 'restart': False}        â”‚ dtype=torch.int32)                              â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ /home/ubuntu/OmniPerf-Bench/perf-agents-bench/â€¦ â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ self.num_token_non_padded[...] = num_tokens     â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ /home/ubuntu/OmniPerf-Bench/perf-agents-bench/â€¦ â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ num_token_non_padded=self.num_token_non_padded, â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ /home/ubuntu/OmniPerf-Bench/perf-agents-bench/â€¦ â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ num_token_non_padded =                          â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ len(forward_batch.input_ids)                    â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ /home/ubuntu/OmniPerf-Bench/perf-agents-bench/â€¦ â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ self.num_token_non_padded[...] =                â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ num_token_non_padded                            â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ /home/ubuntu/OmniPerf-Bench/perf-agents-bench/â€¦ â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ num_token_non_padded=num_token_non_padded,      â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ /home/ubuntu/OmniPerf-Bench/perf-agents-bench/â€¦ â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ num_token_non_padded: Optional = None  # scalar â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ tensor                                          â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ /home/ubuntu/OmniPerf-Bench/perf-agents-bench/â€¦ â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ num_token_non_padded=torch.tensor(              â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ /home/ubuntu/OmniPerf-Bench/perf-agents-bench/â€¦ â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ self, forward_mode: ForwardMode, bs: int,       â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ num_token_non_padded: int                       â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ /home/ubuntu/OmniPerf-Bench/perf-agents-bench/â€¦ â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ num_token_non_padded=None,                      â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ /home/ubuntu/OmniPerf-Bench/perf-agents-bench/â€¦ â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ num_token_non_padded: Optional = None,          â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ /home/ubuntu/OmniPerf-Bench/perf-agents-bench/â€¦ â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ _mask_topk_ids_padded_region(topk_ids,          â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ num_token_non_padded)                           â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ /home/ubuntu/OmniPerf-Bench/perf-agents-bench/â€¦ â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ num_token_non_padded: Optional = None,          â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ /home/ubuntu/OmniPerf-Bench/perf-agents-bench/â€¦ â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ _mask_topk_ids_padded_region(topk_ids,          â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ num_token_non_padded)                           â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ /home/ubuntu/OmniPerf-Bench/perf-agents-bench/â€¦ â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ num_token_non_padded: Optional = None,          â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ /home/ubuntu/OmniPerf-Bench/perf-agents-bench/â€¦ â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ if num_token_non_padded is None:                â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ /home/ubuntu/OmniPerf-Bench/perf-agents-bench/â€¦ â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ topk_ids = -1                                   â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ /home/ubuntu/OmniPerf-Bench/perf-agents-bench/â€¦ â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ num_token_non_padded: Optional = None,          â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ /home/ubuntu/OmniPerf-Bench/perf-agents-bench/â€¦ â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ if num_token_non_padded is None:                â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ /home/ubuntu/OmniPerf-Bench/perf-agents-bench/â€¦ â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ )(topk_ids, num_token_non_padded)               â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ /home/ubuntu/OmniPerf-Bench/perf-agents-bench/â€¦ â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ num_token_non_padded=num_token_non_padded,      â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ /home/ubuntu/OmniPerf-Bench/perf-agents-bench/â€¦ â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ num_token_non_padded: Optional = None,          â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ /home/ubuntu/OmniPerf-Bench/perf-agents-bench/â€¦ â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ num_token_non_padded=num_token_non_padded,      â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ /home/ubuntu/OmniPerf-Bench/perf-agents-bench/â€¦ â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ num_token_non_padded=num_token_non_padded,      â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ /home/ubuntu/OmniPerf-Bench/perf-agents-bench/â€¦ â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ num_token_non_padded is None                    â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ /home/ubuntu/OmniPerf-Bench/perf-agents-bench/â€¦ â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ ), "num_token_non_padded is not yet supported   â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ in fused_topk_native"                           â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ /home/ubuntu/OmniPerf-Bench/perf-agents-bench/â€¦ â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ num_token_non_padded is None                    â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ /home/ubuntu/OmniPerf-Bench/perf-agents-bench/â€¦ â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ ), "num_token_non_padded is not yet supported   â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ in fused_topk"                                  â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ /home/ubuntu/OmniPerf-Bench/perf-agents-bench/â€¦ â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ num_token_non_padded is None                    â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ /home/ubuntu/OmniPerf-Bench/perf-agents-bench/â€¦ â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ ), "num_token_non_padded is not yet supported   â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ in custom_routing_function"                     â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ /home/ubuntu/OmniPerf-Bench/perf-agents-bench/â€¦ â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ num_token_non_padded=forward_batch.num_token_nâ€¦ â”‚   â”‚
â”‚             â”‚ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚
â”‚ Total       â”‚ Input: 122702 Output: 1915                                                                             â”‚
â”‚ Tokens      â”‚                                                                                                        â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Status      â”‚ âœ… Step 9: Completed                                                                                   â”‚
â”‚ Tools       â”‚ ðŸ”§ str_replace_based_edit_tool                                                                         â”‚
â”‚ str_replacâ€¦ â”‚ â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚
â”‚             â”‚ â”‚ {'command': 'view', 'file_text': '',           â”‚                                                 â”‚   â”‚
â”‚             â”‚ â”‚ 'insert_line': 1, 'new_str': '', 'old_str':    â”‚                                                 â”‚   â”‚
â”‚             â”‚ â”‚ '', 'path':                                    â”‚                                                 â”‚   â”‚
â”‚             â”‚ â”‚ '/home/ubuntu/OmniPerf-Bench/perf-agents-bencâ€¦ â”‚                                                 â”‚   â”‚
â”‚             â”‚ â”‚ 'view_range': [200, 700]}                      â”‚                                                 â”‚   â”‚
â”‚             â”‚ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚
â”‚ Total       â”‚ Input: 147892 Output: 2009                                                                             â”‚
â”‚ Tokens      â”‚                                                                                                        â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Status      â”‚ âœ… Step 10: Completed                                                                                  â”‚
â”‚ Tools       â”‚ ðŸ”§ str_replace_based_edit_tool                                                                         â”‚
â”‚ str_replacâ€¦ â”‚ â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚
â”‚             â”‚ â”‚ {'command': 'view', 'file_text': '',           â”‚ Here's the result of running `cat -n` on        â”‚   â”‚
â”‚             â”‚ â”‚ 'insert_line': 1, 'new_str': '', 'old_str':    â”‚ /home/ubuntu/OmniPerf-Bench/perf-agents-bench/â€¦ â”‚   â”‚
â”‚             â”‚ â”‚ '', 'path':                                    â”‚      1  # Copyright 2023-2024 SGLang Team       â”‚   â”‚
â”‚             â”‚ â”‚ '/home/ubuntu/OmniPerf-Bench/perf-agents-bencâ€¦ â”‚      2  # Licensed under the Apache License,    â”‚   â”‚
â”‚             â”‚ â”‚ 'view_range': [1, -1]}                         â”‚ Version 2.0 (the "License");                    â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚      3  # you may not use this file except in   â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ compliance with the License.                    â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚      4  # You may obtain a copy of the License  â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ at                                              â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚      5  #                                       â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚      6  #                                       â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ http://www.apache.org/licenses/LICENSE-2.0      â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚      7  #                                       â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚      8  # Unless required by applicable law or  â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ agreed to in writing, software                  â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚      9  # distributed under the License is      â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ distributed on an "AS IS" BASIS,                â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚     10  # WITHOUT WARRANTIES OR CONDITIONS OF   â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ ANY KIND, either express or implied.            â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚     11  # See the License for the specific      â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ language governing permissions and              â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚     12  # limitations under the License.        â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚     13  #                                       â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ ==============================================â€¦ â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚     14  """Run the model with cuda graph and    â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ torch.compile."""                               â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚     15                                          â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚     16  from __future__ import annotations      â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚     17                                          â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚     18  import bisect                           â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚     19  import inspect                          â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚     20  import os                               â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚     21  from contextlib import contextmanager   â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚     22  from typing import TYPE_CHECKING,       â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ Callable, Optional, Union                       â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚     23                                          â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚     24  import torch                            â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚     25  import tqdm                             â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚     26                                          â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚     27  from sglang.srt import                  â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ two_batch_overlap                               â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚     28  from sglang.srt.custom_op import        â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ CustomOp                                        â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚     29  from sglang.srt.distributed import      â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ get_tensor_model_parallel_rank                  â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚     30  from                                    â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ sglang.srt.distributed.parallel_state import    â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ GroupCoordinator, graph_capture                 â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚     31  from sglang.srt.layers.logits_processor â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ import LogitsProcessorOutput                    â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚     32  from                                    â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ sglang.srt.layers.moe.fused_moe_native import   â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ fused_moe_forward_native                        â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚     33  from sglang.srt.layers.torchao_utils    â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ import save_gemlite_cache                       â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚     34  from sglang.srt.managers.schedule_batch â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ import global_server_args_dict                  â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚     35  from                                    â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ sglang.srt.model_executor.forward_batch_info    â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ import (                                        â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚     36      CaptureHiddenMode,                  â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚     37      ForwardBatch,                       â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚     38      ForwardMode,                        â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚     39      PPProxyTensors,                     â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚     40  )                                       â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚     41  from sglang.srt.patch_torch import      â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ monkey_patch_torch_compile                      â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚     42  from sglang.srt.two_batch_overlap       â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ import (                                        â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚     43      TboCudaGraphRunnerPlugin,           â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚     44      TboForwardBatchPreparer,            â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚     45  )                                       â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚     46  from sglang.srt.utils import (          â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚     47      get_available_gpu_memory,           â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚     48      get_device_memory_capacity,         â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚     49      rank0_log,                          â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚     50  )                                       â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚     51                                          â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚     52  if TYPE_CHECKING:                       â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚     53      from                                â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ sglang.srt.model_executor.model_runner import   â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ ModelRunner                                     â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚     54                                          â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚     55  # Detect whether the current forward    â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ pass is in capture mode                         â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚     56  is_capture_mode = False                 â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚     57                                          â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚     58                                          â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚     59  def get_is_capture_mode():              â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚     60      return is_capture_mode              â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚     61                                          â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚     62                                          â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚     63  def _to_torch(model: torch.nn.Module,   â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ reverse: bool, num_tokens: int):                â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚     64      for sub in model._modules.values(): â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚     65          if isinstance(sub, CustomOp):   â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚     66              if reverse:                 â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚     67                  sub._forward_method =   â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ sub.forward_cuda                                â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚     68                  setattr(sub,            â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ "is_torch_compile", False)                      â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚     69              else:                       â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚     70                  # NOTE: Temporarily     â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ workaround MoE                                  â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚     71                  if "FusedMoE" in        â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ sub.__class__.__name__:                         â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚     72                      if num_tokens == 1: â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚     73                          # The           â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ performance of torch.compile on this layer is   â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ not always good when bs > 1,                    â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚     74                          # so we decide  â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ to only use torch.compile when bs =1            â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚     75                                          â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ sub._forward_method = fused_moe_forward_native  â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚     76                  else:                   â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚     77                      sub._forward_method â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ = sub.forward_native                            â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚     78                  setattr(sub,            â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ "is_torch_compile", True)                       â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚     79          if isinstance(sub,              â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ torch.nn.Module):                               â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚     80              _to_torch(sub, reverse,     â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ num_tokens)                                     â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚     81                                          â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚     82                                          â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚     83  @contextmanager                         â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚     84  def patch_model(                        â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚     85      model: torch.nn.Module,             â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚     86      enable_compile: bool,               â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚     87      num_tokens: int,                    â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚     88      tp_group: GroupCoordinator,         â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚     89  ):                                      â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚     90      """Patch the model to make it       â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ compatible with with torch.compile"""           â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚     91      backup_ca_comm = None               â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚     92                                          â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚     93      try:                                â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚     94          if enable_compile:              â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚     95              _to_torch(model,            â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ reverse=False, num_tokens=num_tokens)           â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚     96              backup_ca_comm =            â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ tp_group.ca_comm                                â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚     97              # Use custom-allreduce      â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ here.                                           â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚     98              # We found the custom       â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ allreduce is much faster than the built-in      â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ allreduce in torch,                             â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚     99              # even with                 â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ ENABLE_INTRA_NODE_COMM=1.                       â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    100              # tp_group.ca_comm = None   â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    101              yield torch.compile(        â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    102                                          â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ torch.no_grad()(model.forward),                 â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    103                  mode=os.environ.get(    â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    104                                          â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ "SGLANG_TORCH_COMPILE_MODE",                    â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ "max-autotune-no-cudagraphs"                    â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    105                  ),                      â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    106                  dynamic=False,          â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    107              )                           â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    108          else:                           â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    109              yield model.forward         â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    110      finally:                            â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    111          if enable_compile:              â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    112              _to_torch(model,            â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ reverse=True, num_tokens=num_tokens)            â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    113              tp_group.ca_comm =          â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ backup_ca_comm                                  â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    114                                          â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    115                                          â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    116  def set_torch_compile_config():         â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    117      import torch._dynamo.config         â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    118      import torch._inductor.config       â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    119                                          â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    120                                          â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ torch._inductor.config.coordinate_descent_tuniâ€¦ â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ = True                                          â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    121                                          â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ torch._inductor.config.triton.unique_kernel_naâ€¦ â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ = True                                          â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    122                                          â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ torch._inductor.config.fx_graph_cache = True  # â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ Experimental feature to reduce compilation      â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ times, will be on by default in future          â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    123                                          â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    124      # FIXME: tmp workaround             â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    125                                          â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ torch._dynamo.config.accumulated_cache_size_liâ€¦ â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ = 1024                                          â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    126      if hasattr(torch._dynamo.config,    â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ "cache_size_limit"):                            â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    127                                          â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ torch._dynamo.config.cache_size_limit = 1024    â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    128                                          â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    129      monkey_patch_torch_compile()        â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    130                                          â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    131                                          â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    132  def                                     â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ get_batch_sizes_to_capture(model_runner:        â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ ModelRunner):                                   â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    133      server_args =                       â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ model_runner.server_args                        â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    134      capture_bs =                        â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ server_args.cuda_graph_bs                       â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    135                                          â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    136      if capture_bs is None:              â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    137          if                              â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ server_args.speculative_algorithm is None:      â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    138              if                          â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ server_args.disable_cuda_graph_padding:         â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    139                  capture_bs =            â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ list(range(1, 33)) + list(range(40, 161, 16))   â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    140              else:                       â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    141                  capture_bs = [1, 2, 4,  â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ 8] + list(range(16, 161, 8))                    â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    142          else:                           â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    143              # Since speculative         â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ decoding requires more cuda graph memory, we    â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    144              # capture less.             â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    145              capture_bs = (              â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    146                  list(range(1, 9)) +     â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ list(range(10, 33, 2)) + list(range(40, 161,    â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ 16))                                            â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    147              )                           â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    148                                          â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    149          gpu_mem =                       â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ get_device_memory_capacity()                    â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    150          if gpu_mem is not None and      â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ gpu_mem > 96 * 1024:                            â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    151              capture_bs +=               â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ list(range(160, 257, 8))                        â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    152                                          â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    153      if max(capture_bs) >                â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ model_runner.req_to_token_pool.size:            â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    154          # In some case (e.g., with a    â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ small GPU or --max-running-requests), the       â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ #max-running-requests                           â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    155          # is very small. We add more    â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ values here to make sure we capture the maximum â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ bs.                                             â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    156          capture_bs +=  + [              â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    157                                          â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ model_runner.req_to_token_pool.size             â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    158          ]                               â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    159                                          â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    160      if                                  â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ server_args.enable_two_batch_overlap:           â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    161          capture_bs =                    â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    162                                          â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    163      if server_args.cuda_graph_max_bs:   â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    164          capture_bs =                    â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    165          if max(capture_bs) <            â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ server_args.cuda_graph_max_bs:                  â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    166              capture_bs += list(         â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    167                  range(max(capture_bs),  â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ server_args.cuda_graph_max_bs + 1, 16)          â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    168              )                           â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    169      capture_bs =                        â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    170      capture_bs =                        â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ list(sorted(set(capture_bs)))                   â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    171      assert len(capture_bs) > 0 and      â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ capture_bs[0] > 0                               â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    172      compile_bs = (                      â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    173                                          â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    174          if                              â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ server_args.enable_torch_compile                â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    175          else []                         â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    176      )                                   â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    177      return capture_bs, compile_bs       â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    178                                          â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    179                                          â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    180  # Reuse this memory pool across all     â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ cuda graph runners.                             â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    181  global_graph_memory_pool = None         â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    182                                          â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    183                                          â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    184  def get_global_graph_memory_pool():     â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    185      return global_graph_memory_pool     â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    186                                          â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    187                                          â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    188  def set_global_graph_memory_pool(val):  â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    189      global global_graph_memory_pool     â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    190      global_graph_memory_pool = val      â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    191                                          â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    192                                          â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    193  class CudaGraphRunner:                  â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    194      """A CudaGraphRunner runs the       â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ forward pass of a model with cuda graph and     â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ torch.compile."""                               â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    195                                          â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    196      def __init__(self, model_runner:    â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ ModelRunner):                                   â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    197          # Parse args                    â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    198          self.model_runner =             â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ model_runner                                    â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    199          self.graphs = {}                â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    200          self.output_buffers = {}        â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    201          self.enable_torch_compile =     â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ model_runner.server_args.enable_torch_compile   â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    202          self.disable_padding =          â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ model_runner.server_args.disable_cuda_graph_paâ€¦ â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    203          self.is_encoder_decoder =       â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ model_runner.model_config.is_encoder_decoder    â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    204          self.enable_dp_attention =      â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ model_runner.server_args.enable_dp_attention    â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    205          self.enable_sp_layernorm =      â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ model_runner.server_args.enable_sp_layernorm    â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    206          self.speculative_algorithm =    â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ model_runner.server_args.speculative_algorithm  â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    207          self.tp_size =                  â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ model_runner.server_args.tp_size                â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    208          self.dp_size =                  â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ model_runner.server_args.dp_size                â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    209          self.pp_size =                  â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ model_runner.server_args.pp_size                â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    210                                          â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    211          # Batch sizes to capture        â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    212          self.capture_bs,                â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ self.compile_bs =                               â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ get_batch_sizes_to_capture(model_runner)        â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    213          rank0_log(f"Capture cuda graph  â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ bs {self.capture_bs}")                          â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    214          self.capture_forward_mode =     â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ ForwardMode.DECODE                              â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    215          self.capture_hidden_mode =      â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ CaptureHiddenMode.NULL                          â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    216          self.num_tokens_per_bs = 1      â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    217          if                              â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ model_runner.spec_algorithm.is_eagle():         â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    218              if                          â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ self.model_runner.is_draft_worker:              â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    219                  raise                   â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ RuntimeError("This should not happen")          â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    220              else:                       â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    221                                          â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ self.capture_forward_mode =                     â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ ForwardMode.TARGET_VERIFY                       â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    222                  self.num_tokens_per_bs  â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ = (                                             â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    223                                          â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ self.model_runner.server_args.speculative_num_â€¦ â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    224                  )                       â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    225                                          â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    226          # Attention backend             â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    227          self.max_bs =                   â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ max(self.capture_bs)                            â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    228          self.max_num_token =            â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ self.max_bs * self.num_tokens_per_bs            â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    229          if                              â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ global_server_args_dict["attention_backend"] == â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ "flashmla":                                     â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    230                                          â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ self.model_runner.attn_backend.init_cuda_graphâ€¦ â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    231          else:                           â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    232                                          â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ self.model_runner.attn_backend.init_cuda_graphâ€¦ â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    233          self.seq_len_fill_value = (     â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    234                                          â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ self.model_runner.attn_backend.get_cuda_graph_â€¦ â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    235          )                               â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    236          # FIXME(lsyin): leave it here   â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ for now, I don't know whether it is necessary   â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    237          self.encoder_len_fill_value = 0 â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    238          self.seq_lens_cpu = torch.full( â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    239              (self.max_bs,),             â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ self.seq_len_fill_value, dtype=torch.int32      â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    240          )                               â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    241                                          â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    242          if self.enable_torch_compile:   â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    243              set_torch_compile_config()  â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    244                                          â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    245          if                              â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ self.model_runner.server_args.lora_paths is not â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ None:                                           â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    246                                          â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ self.model_runner.lora_manager.init_cuda_graphâ€¦ â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    247                                          â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    248          # Graph inputs                  â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    249          with torch.device("cuda"):      â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    250              self.input_ids =            â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ torch.zeros((self.max_num_token,),              â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ dtype=torch.int64)                              â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    251              self.req_pool_indices =     â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ torch.zeros((self.max_bs,), dtype=torch.int32)  â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    252              self.seq_lens = torch.full( â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    253                  (self.max_bs,),         â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ self.seq_len_fill_value, dtype=torch.int32      â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    254              )                           â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    255              self.out_cache_loc =        â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ torch.zeros((self.max_num_token,),              â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ dtype=torch.int64)                              â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    256              self.positions =            â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ torch.zeros((self.max_num_token,),              â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ dtype=torch.int64)                              â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    257              self.mrope_positions =      â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ torch.zeros((3, self.max_bs),                   â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ dtype=torch.int64)                              â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    258              self.num_token_non_padded = â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ torch.zeros((1,), dtype=torch.int32)            â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    259              self.tbo_plugin =           â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ TboCudaGraphRunnerPlugin()                      â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    260                                          â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    261              # pipeline parallelism      â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    262              if self.pp_size > 1:        â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    263                  self.pp_proxy_tensors = â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ {                                               â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    264                      "hidden_states":    â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ torch.zeros(                                    â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    265                          (self.max_bs,   â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ self.model_runner.model_config.hidden_size),    â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    266                                          â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ dtype=torch.bfloat16,                           â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    267                      ),                  â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    268                      "residual":         â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ torch.zeros(                                    â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    269                          (self.max_bs,   â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ self.model_runner.model_config.hidden_size),    â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    270                                          â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ dtype=torch.bfloat16,                           â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    271                      ),                  â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    272                  }                       â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    273                                          â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    274              # Speculative_inference     â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    275              if (                        â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    276                                          â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ model_runner.spec_algorithm.is_eagle3()         â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    277                  and not                 â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ model_runner.is_draft_worker                    â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    278              ):                          â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    279                  self.hidden_states =    â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ torch.zeros(                                    â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    280                      (                   â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    281                                          â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ self.max_num_token,                             â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    282                          3 *             â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ self.model_runner.model_config.hidden_size,     â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    283                      ),                  â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    284                                          â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ dtype=self.model_runner.dtype,                  â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    285                  )                       â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    286                                          â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ self.model_runner.model.set_eagle3_layers_to_câ€¦ â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    287              elif                        â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ model_runner.spec_algorithm.is_eagle():         â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    288                  self.hidden_states =    â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ torch.zeros(                                    â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    289                                          â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ (self.max_num_token,                            â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ self.model_runner.model_config.hidden_size),    â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    290                                          â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ dtype=self.model_runner.dtype,                  â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    291                  )                       â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    292                                          â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    293              if self.is_encoder_decoder: â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    294                  # NOTE: encoder_lens    â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ can influence the full_text_row_masked_out_mask â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ tensor when doing mixed batch                   â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    295                  self.encoder_lens =     â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ torch.full(                                     â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    296                      (self.max_bs,),     â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ self.encoder_len_fill_value, dtype=torch.int32  â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    297                  )                       â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    298              else:                       â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    299                  self.encoder_lens =     â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ None                                            â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    300              if self.enable_dp_attention â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ or self.enable_sp_layernorm:                    â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    301                  # TODO(ch-wan): SP      â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ layernorm should use a different logic to       â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ manage gathered_buffer                          â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    302                  self.gathered_buffer =  â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ torch.zeros(                                    â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    303                      (                   â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    304                          self.max_bs *   â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ self.dp_size * self.num_tokens_per_bs,          â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    305                                          â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ self.model_runner.model_config.hidden_size,     â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    306                      ),                  â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    307                                          â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ dtype=self.model_runner.dtype,                  â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    308                  )                       â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    309                                          â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ self.global_num_tokens_gpu = torch.zeros(       â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    310                      (self.dp_size,),    â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ dtype=torch.int32                               â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    311                  )                       â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    312                                          â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    313          # Capture                       â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    314          try:                            â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    315              with                        â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ self.model_capture_mode():                      â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    316                  self.capture()          â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    317          except RuntimeError as e:       â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    318              raise Exception(            â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    319                  f"Capture CUDA graph    â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ failed: {e}\n"                                  â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    320                  "Possible solutions:\n" â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    321                  "1. set                 â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ --mem-fraction-static to a smaller value (e.g., â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ 0.8 or 0.7)\n"                                  â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    322                  "2. set                 â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ --cuda-graph-max-bs to a smaller value (e.g.,   â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ 16)\n"                                          â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    323                  "3. disable torch       â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ compile by not using --enable-torch-compile\n"  â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    324                  "4. disable CUDA graph  â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ by --disable-cuda-graph. (Not recommended. Huge â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ performance loss)\n"                            â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    325                  "Open an issue on       â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ GitHub                                          â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ https://github.com/sgl-project/sglang/issues/nâ€¦ â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ \n"                                             â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    326              )                           â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    327                                          â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    328      @contextmanager                     â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    329      def model_capture_mode(self):       â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    330          global is_capture_mode          â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    331          is_capture_mode = True          â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    332                                          â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    333          yield                           â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    334                                          â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    335          is_capture_mode = False         â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    336                                          â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    337      def can_run(self, forward_batch:    â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ ForwardBatch):                                  â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    338          if self.enable_dp_attention or  â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ self.enable_sp_layernorm:                       â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    339              total_global_tokens =       â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ sum(forward_batch.global_num_tokens_cpu)        â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    340                                          â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    341              is_bs_supported =           â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ forward_batch.can_run_dp_cuda_graph and (       â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    342                  total_global_tokens in  â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ self.graphs                                     â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    343                  if self.disable_padding â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    344                  else                    â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ total_global_tokens <= self.max_bs              â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    345              )                           â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    346          else:                           â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    347              is_bs_supported = (         â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    348                                          â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ forward_batch.batch_size in self.graphs         â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    349                  if self.disable_padding â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    350                  else                    â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ forward_batch.batch_size <= self.max_bs         â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    351              )                           â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    352                                          â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    353          # NOTE: cuda graph cannot       â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ handle mixed batch (encoder_len = 0)            â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    354          # If mixed batch cannot be      â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ supported, then encoder_lens can be removed in  â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ cuda graph                                      â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    355          # because the                   â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ full_text_row_masked_out_mask tensor will       â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ always be ones                                  â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    356          is_encoder_lens_supported = (   â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    357                                          â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ torch.all(forward_batch.encoder_lens > 0)       â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    358              if self.is_encoder_decoder  â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    359              else True                   â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    360          )                               â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    361                                          â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    362          is_tbo_supported = (            â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    363              forward_batch.can_run_tbo   â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    364              if                          â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ self.model_runner.server_args.enable_two_batchâ€¦ â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    365              else True                   â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    366          )                               â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    367                                          â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    368          return is_bs_supported and      â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ is_encoder_lens_supported and is_tbo_supported  â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    369                                          â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    370      def capture(self):                  â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    371          with graph_capture() as         â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ graph_capture_context:                          â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    372              self.stream =               â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ graph_capture_context.stream                    â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    373              avail_mem =                 â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ get_available_gpu_memory(                       â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    374                                          â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ self.model_runner.device,                       â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ self.model_runner.gpu_id, empty_cache=False     â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    375              )                           â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    376              # Reverse the order to      â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ enable better memory sharing across cuda        â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ graphs.                                         â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    377              capture_range = (           â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    378                                          â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ tqdm.tqdm(list(reversed(self.capture_bs)))      â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    379                  if                      â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ get_tensor_model_parallel_rank() == 0           â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    380                  else                    â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ reversed(self.capture_bs)                       â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    381              )                           â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    382              for bs in capture_range:    â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    383                  if                      â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ get_tensor_model_parallel_rank() == 0:          â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    384                      avail_mem =         â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ get_available_gpu_memory(                       â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    385                                          â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ self.model_runner.device,                       â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    386                                          â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ self.model_runner.gpu_id,                       â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    387                                          â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ empty_cache=False,                              â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    388                      )                   â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    389                                          â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ capture_range.set_description(                  â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    390                          f"Capturing     â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ batches ({avail_mem=:.2f} GB)"                  â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    391                      )                   â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    392                                          â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    393                  with patch_model(       â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    394                                          â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ self.model_runner.model,                        â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    395                      bs in               â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ self.com<response clipped><NOTE>To save on      â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ context only part of this file has been shown   â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ to you. You should retry this tool after you    â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ have searched inside the file with `grep -n` in â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ order to find the line numbers of what you are  â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ looking for.</NOTE>                             â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                                                 â”‚   â”‚
â”‚             â”‚ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚
â”‚ Total       â”‚ Input: 173263 Output: 2101                                                                             â”‚
â”‚ Tokens      â”‚                                                                                                        â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Status      â”‚ âœ… Step 11: Completed                                                                                  â”‚
â”‚ Tools       â”‚ ðŸ”§ bash                                                                                                â”‚
â”‚ bash        â”‚ â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚
â”‚             â”‚ â”‚ {'command': 'grep -n                           â”‚ 43:    TboCudaGraphRunnerPlugin,                â”‚   â”‚
â”‚             â”‚ â”‚ "TboCudaGraphRunnerPlugin\\|tbo_children_num_â€¦ â”‚ 259:            self.tbo_plugin =               â”‚   â”‚
â”‚             â”‚ â”‚ /home/ubuntu/OmniPerf-Bench/perf-agents-benchâ€¦ â”‚ TboCudaGraphRunnerPlugin()                      â”‚   â”‚
â”‚             â”‚ â”‚ | sed -n \'1,200p\'', 'restart': False}        â”‚ 402:                    ) =                     â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ self.capture_one_batch_size(bs, forward)        â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ 409:    def capture_one_batch_size(self, bs:    â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ int, forward: Callable):                        â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ 487:                                            â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ self.tbo_plugin.capture_one_batch_size(forwardâ€¦ â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ num_tokens=num_tokens)                          â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ 554:    def replay_prepare(                     â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ 584:        self.tbo_plugin.replay_prepare(     â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ 633:                                            â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ self.replay_prepare(forward_batch,              â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ pp_proxy_tensors)                               â”‚   â”‚
â”‚             â”‚ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚
â”‚ Total       â”‚ Input: 203447 Output: 2213                                                                             â”‚
â”‚ Tokens      â”‚                                                                                                        â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Status      â”‚ âœ… Step 12: Completed                                                                                  â”‚
â”‚ Tools       â”‚ ðŸ”§ str_replace_based_edit_tool                                                                         â”‚
â”‚ str_replacâ€¦ â”‚ â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚
â”‚             â”‚ â”‚ {'command': 'view', 'file_text': '',           â”‚ Here's the result of running `cat -n` on        â”‚   â”‚
â”‚             â”‚ â”‚ 'insert_line': 1, 'new_str': '', 'old_str':    â”‚ /home/ubuntu/OmniPerf-Bench/perf-agents-bench/â€¦ â”‚   â”‚
â”‚             â”‚ â”‚ '', 'path':                                    â”‚    380                  else                    â”‚   â”‚
â”‚             â”‚ â”‚ '/home/ubuntu/OmniPerf-Bench/perf-agents-bencâ€¦ â”‚ reversed(self.capture_bs)                       â”‚   â”‚
â”‚             â”‚ â”‚ 'view_range': [380, 660]}                      â”‚    381              )                           â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    382              for bs in capture_range:    â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    383                  if                      â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ get_tensor_model_parallel_rank() == 0:          â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    384                      avail_mem =         â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ get_available_gpu_memory(                       â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    385                                          â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ self.model_runner.device,                       â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    386                                          â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ self.model_runner.gpu_id,                       â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    387                                          â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ empty_cache=False,                              â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    388                      )                   â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    389                                          â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ capture_range.set_description(                  â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    390                          f"Capturing     â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ batches ({avail_mem=:.2f} GB)"                  â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    391                      )                   â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    392                                          â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    393                  with patch_model(       â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    394                                          â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ self.model_runner.model,                        â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    395                      bs in               â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ self.compile_bs,                                â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    396                      num_tokens=bs *     â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ self.num_tokens_per_bs,                         â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    397                                          â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ tp_group=self.model_runner.tp_group,            â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    398                  ) as forward:           â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    399                      (                   â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    400                          graph,          â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    401                          output_buffers, â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    402                      ) =                 â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ self.capture_one_batch_size(bs, forward)        â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    403                      self.graphs = graph â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    404                      self.output_buffers â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ = output_buffers                                â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    405                                          â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    406                  # Save gemlite cache    â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ after each capture                              â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    407                  save_gemlite_cache()    â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    408                                          â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    409      def capture_one_batch_size(self,    â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ bs: int, forward: Callable):                    â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    410          graph = torch.cuda.CUDAGraph()  â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    411          stream = self.stream            â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    412          num_tokens = bs *               â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ self.num_tokens_per_bs                          â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    413                                          â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    414          # Graph inputs                  â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    415          input_ids =                     â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ self.input_ids[:num_tokens]                     â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    416          req_pool_indices =              â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ self.req_pool_indices[:bs]                      â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    417          seq_lens = self.seq_lens[:bs]   â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    418          out_cache_loc =                 â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ self.out_cache_loc[:num_tokens]                 â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    419          positions =                     â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ self.positions[:num_tokens]                     â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    420          if self.is_encoder_decoder:     â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    421              encoder_lens =              â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ self.encoder_lens[:bs]                          â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    422          else:                           â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    423              encoder_lens = None         â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    424          mrope_positions =               â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ self.mrope_positions[:, :bs]                    â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    425          self.num_token_non_padded[...]  â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ = num_tokens                                    â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    426                                          â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    427          # pipeline parallelism          â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    428          if self.pp_size > 1:            â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    429              pp_proxy_tensors =          â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ PPProxyTensors(                                 â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    430                  {k: v[:num_tokens] for  â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ k, v in self.pp_proxy_tensors.items()}          â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    431              )                           â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    432                                          â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    433          if self.enable_dp_attention or  â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ self.enable_sp_layernorm:                       â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    434                                          â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ self.global_num_tokens_gpu.copy_(               â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    435                  torch.tensor(           â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    436                      [                   â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    437                          num_tokens //   â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ self.dp_size + (i < bs % self.dp_size)          â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    438                          for i in        â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ range(self.dp_size)                             â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    439                      ],                  â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    440                      dtype=torch.int32,  â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    441                                          â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ device=input_ids.device,                        â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    442                  )                       â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    443              )                           â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    444              global_num_tokens =         â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ self.global_num_tokens_gpu                      â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    445              gathered_buffer =           â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ self.gathered_buffer[:num_tokens]               â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    446          else:                           â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    447              global_num_tokens = None    â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    448              gathered_buffer = None      â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    449                                          â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    450          spec_info =                     â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ self.get_spec_info(num_tokens)                  â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    451          if self.capture_hidden_mode !=  â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ CaptureHiddenMode.FULL:                         â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    452              self.capture_hidden_mode =  â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ (                                               â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    453                                          â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ spec_info.capture_hidden_mode if spec_info else â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ CaptureHiddenMode.NULL                          â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    454              )                           â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    455          if                              â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ self.model_runner.server_args.lora_paths is not â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ None:                                           â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    456              # Currently, if the         â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ lora_path in `lora_paths` is None, the lora     â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ backend will use a                              â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    457              # different logic to handle â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ lora, so we need to set `lora_paths` to a list  â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ of non-None                                     â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    458              # values if lora is         â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ enabled.                                        â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    459              lora_paths =  * bs          â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    460          else:                           â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    461              lora_paths = None           â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    462                                          â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    463          forward_batch = ForwardBatch(   â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    464                                          â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ forward_mode=self.capture_forward_mode,         â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    465              batch_size=bs,              â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    466              input_ids=input_ids,        â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    467                                          â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ req_pool_indices=req_pool_indices,              â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    468              seq_lens=seq_lens,          â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    469                                          â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ req_to_token_pool=self.model_runner.req_to_tokâ€¦ â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    470                                          â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ token_to_kv_pool=self.model_runner.token_to_kvâ€¦ â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    471                                          â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ attn_backend=self.model_runner.attn_backend,    â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    472                                          â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ out_cache_loc=out_cache_loc,                    â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    473                                          â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ seq_lens_sum=seq_lens.sum(),                    â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    474              encoder_lens=encoder_lens,  â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    475              return_logprob=False,       â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    476              positions=positions,        â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    477                                          â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ global_num_tokens_gpu=global_num_tokens,        â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    478                                          â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ gathered_buffer=gathered_buffer,                â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    479                                          â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ mrope_positions=mrope_positions,                â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    480                                          â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ spec_algorithm=self.model_runner.spec_algorithâ€¦ â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    481              spec_info=spec_info,        â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    482                                          â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ capture_hidden_mode=self.capture_hidden_mode,   â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    483              lora_paths=lora_paths,      â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    484                                          â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ num_token_non_padded=self.num_token_non_padded, â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    485                                          â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ global_forward_mode=self.capture_forward_mode,  â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    486          )                               â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    487                                          â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ self.tbo_plugin.capture_one_batch_size(forwardâ€¦ â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ num_tokens=num_tokens)                          â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    488                                          â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    489          if lora_paths is not None:      â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    490                                          â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ self.model_runner.lora_manager.prepare_lora_baâ€¦ â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    491                                          â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    492          # Attention backend             â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    493                                          â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ self.model_runner.attn_backend.init_forward_meâ€¦ â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    494              bs,                         â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    495              num_tokens,                 â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    496              req_pool_indices,           â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    497              seq_lens,                   â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    498              encoder_lens,               â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    499              forward_batch.forward_mode, â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    500              forward_batch.spec_info,    â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    501          )                               â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    502                                          â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    503          # Run and capture               â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    504          def run_once():                 â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    505              # Clean intermediate result â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ cache for DP attention                          â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    506                                          â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ forward_batch.dp_local_start_pos =              â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ forward_batch.dp_local_num_tokens = None        â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    507                                          â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    508              kwargs = {}                 â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    509              if (                        â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    510                  self.pp_size > 1        â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    511                  and "pp_proxy_tensors"  â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ in inspect.signature(forward).parameters        â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    512              ):                          â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    513                                          â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ kwargs["pp_proxy_tensors"] = pp_proxy_tensors   â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    514                                          â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    515                                          â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ logits_output_or_pp_proxy_tensors = forward(    â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    516                  input_ids,              â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    517                                          â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ forward_batch.positions,                        â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    518                  forward_batch,          â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    519                  **kwargs,               â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    520              )                           â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    521              return                      â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ logits_output_or_pp_proxy_tensors               â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    522                                          â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    523          for _ in range(2):              â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    524              torch.cuda.synchronize()    â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    525                                          â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ self.model_runner.tp_group.barrier()            â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    526                                          â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    527              run_once()                  â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    528                                          â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    529          global global_graph_memory_pool â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    530          with torch.cuda.graph(graph,    â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ pool=global_graph_memory_pool, stream=stream):  â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    531              out = run_once()            â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    532                                          â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    533          global_graph_memory_pool =      â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ graph.pool()                                    â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    534          return graph, out               â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    535                                          â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    536      def recapture_if_needed(self,       â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ forward_batch: ForwardBatch):                   â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    537          # If the capture_hidden_mode    â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ changes, we need to recapture the graph         â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    538          hidden_mode_from_spec_info =    â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ getattr(                                        â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    539              forward_batch.spec_info,    â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ "capture_hidden_mode", CaptureHiddenMode.NULL   â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    540          )                               â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    541          if (                            â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    542                                          â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ forward_batch.capture_hidden_mode ==            â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ CaptureHiddenMode.FULL                          â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    543              and                         â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ self.capture_hidden_mode !=                     â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ CaptureHiddenMode.FULL                          â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    544          ):                              â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    545              self.capture_hidden_mode =  â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ CaptureHiddenMode.FULL                          â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    546              self.capture()              â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    547          elif (                          â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    548                                          â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ forward_batch.capture_hidden_mode !=            â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ CaptureHiddenMode.FULL                          â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    549              and                         â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ self.capture_hidden_mode !=                     â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ hidden_mode_from_spec_info                      â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    550          ):                              â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    551              self.capture_hidden_mode =  â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ hidden_mode_from_spec_info                      â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    552              self.capture()              â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    553                                          â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    554      def replay_prepare(                 â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    555          self,                           â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    556          forward_batch: ForwardBatch,    â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    557          pp_proxy_tensors:               â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ Optional[PPProxyTensors] = None,                â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    558      ):                                  â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    559                                          â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ self.recapture_if_needed(forward_batch)         â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    560                                          â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    561          raw_bs =                        â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ forward_batch.batch_size                        â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    562          raw_num_token = raw_bs *        â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ self.num_tokens_per_bs                          â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    563                                          â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    564          # Pad                           â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    565          if self.enable_dp_attention or  â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ self.enable_sp_layernorm:                       â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    566              index = bisect.bisect_left( â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    567                  self.capture_bs,        â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ sum(forward_batch.global_num_tokens_cpu)        â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    568              )                           â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    569          else:                           â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    570              index =                     â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ bisect.bisect_left(self.capture_bs, raw_bs)     â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    571          bs = self.capture_bs            â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    572          if bs != raw_bs:                â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    573              self.seq_lens.fill_(1)      â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    574              self.out_cache_loc.zero_()  â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    575                                          â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    576          # Common inputs                 â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    577                                          â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ self.input_ids[:raw_num_token].copy_(forward_bâ€¦ â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    578                                          â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ self.req_pool_indices[:raw_bs].copy_(forward_bâ€¦ â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    579                                          â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ self.seq_lens[:raw_bs].copy_(forward_batch.seqâ€¦ â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    580                                          â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ self.out_cache_loc[:raw_num_token].copy_(forwaâ€¦ â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    581                                          â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ self.positions[:raw_num_token].copy_(forward_bâ€¦ â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    582          num_token_non_padded =          â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ len(forward_batch.input_ids)                    â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    583          self.num_token_non_padded[...]  â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ = num_token_non_padded                          â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    584          self.tbo_plugin.replay_prepare( â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    585                                          â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ forward_mode=forward_batch.forward_mode,        â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    586              bs=bs,                      â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    587                                          â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ num_token_non_padded=num_token_non_padded,      â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    588          )                               â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    589          if forward_batch.seq_lens_cpu   â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ is not None:                                    â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    590              if bs != raw_bs:            â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    591                                          â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ self.seq_lens_cpu.fill_(1)                      â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    592                                          â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ self.seq_lens_cpu[:raw_bs].copy_(forward_batchâ€¦ â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    593                                          â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    594          if pp_proxy_tensors:            â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    595              for key in                  â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ self.pp_proxy_tensors.keys():                   â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    596                  dim =                   â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ pp_proxy_tensors.shape[0]                       â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    597                                          â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ self.pp_proxy_tensors[:dim].copy_(pp_proxy_tenâ€¦ â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    598                                          â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    599          if self.is_encoder_decoder:     â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    600                                          â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ self.encoder_lens[:raw_bs].copy_(forward_batchâ€¦ â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    601          if                              â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ forward_batch.mrope_positions is not None:      â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    602              self.mrope_positions[:,     â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ :raw_bs].copy_(forward_batch.mrope_positions)   â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    603          if self.enable_dp_attention or  â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ self.enable_sp_layernorm:                       â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    604                                          â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ self.global_num_tokens_gpu.copy_(forward_batchâ€¦ â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    605                                          â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    606          if                              â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ hasattr(forward_batch.spec_info,                â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ "hidden_states"):                               â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    607                                          â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ self.hidden_states[:raw_num_token] =            â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ forward_batch.spec_info.hidden_states           â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    608                                          â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    609          # Attention backend             â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    610                                          â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ self.model_runner.attn_backend.init_forward_meâ€¦ â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    611              bs,                         â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    612              self.req_pool_indices,      â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    613              self.seq_lens,              â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    614              forward_batch.seq_lens_sum  â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ + (bs - raw_bs),                                â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    615              self.encoder_lens,          â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    616              forward_batch.forward_mode, â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    617              forward_batch.spec_info,    â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    618                                          â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ seq_lens_cpu=self.seq_lens_cpu,                 â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    619          )                               â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    620                                          â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    621          # Store fields                  â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    622          self.raw_bs = raw_bs            â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    623          self.raw_num_token =            â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ raw_num_token                                   â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    624          self.bs = bs                    â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    625                                          â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    626      def replay(                         â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    627          self,                           â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    628          forward_batch: ForwardBatch,    â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    629          skip_attn_backend_init: bool =  â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ False,                                          â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    630          pp_proxy_tensors:               â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ Optional[PPProxyTensors] = None,                â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    631      ) -> Union[LogitsProcessorOutput,   â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ PPProxyTensors]:                                â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    632          if not skip_attn_backend_init:  â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    633                                          â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ self.replay_prepare(forward_batch,              â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ pp_proxy_tensors)                               â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    634          else:                           â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    635              # In speculative decoding,  â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ these two fields are still needed.              â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    636              self.input_ids[:            â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ self.raw_num_token].copy_(forward_batch.input_â€¦ â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    637              self.positions[:            â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ self.raw_num_token].copy_(forward_batch.positiâ€¦ â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    638                                          â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    639          # Replay                        â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    640          self.graphs.replay()            â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    641                                          â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    642          output = self.output_buffers    â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    643          if isinstance(output,           â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ LogitsProcessorOutput):                         â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    644              return                      â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ LogitsProcessorOutput(                          â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    645                                          â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ next_token_logits=output.next_token_logits[:    â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ self.raw_num_token],                            â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    646                  hidden_states=(         â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    647                                          â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ output.hidden_states[: self.raw_num_token]      â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    648                      if                  â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ output.hidden_states is not None                â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    649                      else None           â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    650                  ),                      â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    651              )                           â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    652          else:                           â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    653              assert isinstance(output,   â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ PPProxyTensors)                                 â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    654              return PPProxyTensors({k:   â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ v[: self.bs] for k, v in                        â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ output.tensors.items()})                        â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    655                                          â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    656      def get_spec_info(self, num_tokens: â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ int):                                           â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    657          spec_info = None                â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    658          if                              â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ self.model_runner.spec_algorithm.is_eagle():    â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    659              from                        â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ sglang.srt.speculative.eagle_utils import       â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ EagleVerifyInput                                â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    660                                          â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                                                 â”‚   â”‚
â”‚             â”‚ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚
â”‚ Total       â”‚ Input: 233880 Output: 2305                                                                             â”‚
â”‚ Tokens      â”‚                                                                                                        â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Status      â”‚ âœ… Step 13: Completed                                                                                  â”‚
â”‚ Tools       â”‚ ðŸ”§ str_replace_based_edit_tool                                                                         â”‚
â”‚ str_replacâ€¦ â”‚ â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚
â”‚             â”‚ â”‚ {'command': 'view', 'file_text': '',           â”‚ Here's the result of running `cat -n` on        â”‚   â”‚
â”‚             â”‚ â”‚ 'insert_line': 1, 'new_str': '', 'old_str':    â”‚ /home/ubuntu/OmniPerf-Bench/perf-agents-bench/â€¦ â”‚   â”‚
â”‚             â”‚ â”‚ '', 'path':                                    â”‚    100      tbo_split_token_index =             â”‚   â”‚
â”‚             â”‚ â”‚ '/home/ubuntu/OmniPerf-Bench/perf-agents-bencâ€¦ â”‚ compute_split_token_index(                      â”‚   â”‚
â”‚             â”‚ â”‚ 'view_range': [100, 200]}                      â”‚    101                                          â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ split_seq_index=tbo_split_seq_index,            â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    102                                          â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ forward_mode=forward_mode_for_tbo_split,        â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    103          extend_seq_lens=None,           â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    104      )                                   â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    105      return tbo_split_seq_index,         â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ tbo_split_token_index                           â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    106                                          â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    107                                          â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    108  # --------------------------------      â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ Preparation                                     â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ ---------------------------------------         â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    109                                          â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    110                                          â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    111  class TboCudaGraphRunnerPlugin:         â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    112      def __init__(self):                 â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    113          pass  # TODO add logic here     â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    114                                          â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    115      def capture_one_batch_size(self,    â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ batch: ForwardBatch, num_tokens: int):          â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    116          if not                          â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ global_server_args_dict["enable_two_batch_overâ€¦ â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    117              return                      â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    118                                          â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    119          batch.tbo_split_seq_index =     â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ compute_split_seq_index(                        â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    120                                          â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ forward_mode=batch.forward_mode,                â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    121              num_tokens=num_tokens,      â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    122              extend_lens=None,           â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    123          )                               â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    124          # For simplicity, when          â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ two_batch_overlap is enabled, we only capture   â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ CUDA Graph for tbo=true                         â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    125          assert                          â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ batch.tbo_split_seq_index is not None,          â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ f"{num_tokens=}"                                â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    126                                          â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    127                                          â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ TboForwardBatchPreparer.prepare(batch)          â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    128                                          â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    129      def replay_prepare(                 â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    130          self, forward_mode:             â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ ForwardMode, bs: int, num_token_non_padded: int â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    131      ):                                  â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    132          if not                          â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ global_server_args_dict["enable_two_batch_overâ€¦ â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    133              return                      â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    134                                          â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    135          pass  # TODO add logic here     â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    136                                          â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    137                                          â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    138  class TboDPAttentionPreparer:           â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    139      def prepare_all_gather(             â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    140          self, local_batch, deepep_mode, â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ enable_deepep_moe, enable_two_batch_overlap     â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    141      ):                                  â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    142          self.enable_two_batch_overlap = â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ enable_two_batch_overlap                        â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    143                                          â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    144          if local_batch is not None:     â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    145                                          â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ self.local_tbo_split_seq_index =                â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ compute_split_seq_index(                        â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    146                                          â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ forward_mode=local_batch.forward_mode,          â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    147                                          â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ num_tokens=local_batch.input_ids.shape[0],      â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    148                                          â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ extend_lens=local_batch.extend_lens,            â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    149              )                           â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    150              resolved_deepep_mode =      â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ deepep_mode.resolve(local_batch.forward_mode)   â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    151              local_can_run_tbo =         â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ (self.local_tbo_split_seq_index is not None)    â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ and not (                                       â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    152                                          â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ local_batch.forward_mode.is_extend()            â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    153                  and enable_deepep_moe   â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    154                  and                     â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ (resolved_deepep_mode ==                        â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ DeepEPMode.low_latency)                         â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    155              )                           â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    156          else:                           â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    157                                          â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ self.local_tbo_split_seq_index = 0              â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    158              local_can_run_tbo = True    â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    159                                          â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    160          local_forward_mode =            â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ self._compute_local_forward_mode(local_batch)   â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    161                                          â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    162          return local_can_run_tbo,       â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ local_forward_mode                              â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    163                                          â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    164      def compute_output(self,            â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ partial_global_info):                           â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    165          local_can_run_tbo_aggregated =  â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ min(partial_global_info[:, 0, 0].tolist())      â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    166          forward_modes =                 â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ partial_global_info[:, 0, 1].tolist()           â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    167                                          â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    168          global_forward_mode,            â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ forward_mode_agree =                            â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ self._compute_global_forward_mode(              â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    169              forward_modes               â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    170          )                               â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    171                                          â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    172          can_run_tbo = (                 â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    173                                          â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ self.enable_two_batch_overlap                   â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    174              and                         â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ local_can_run_tbo_aggregated                    â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    175              and forward_mode_agree      â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    176          )                               â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    177                                          â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    178          tbo_split_seq_index =           â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ self.local_tbo_split_seq_index if can_run_tbo   â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ else None                                       â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    179          global_forward_mode =           â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ global_forward_mode if can_run_tbo else None    â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    180          return tbo_split_seq_index,     â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ global_forward_mode                             â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    181                                          â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    182      @staticmethod                       â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    183      def                                 â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ _compute_local_forward_mode(local_batch):       â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    184          return (                        â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    185              local_batch.forward_mode if â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ local_batch is not None else ForwardMode.IDLE   â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    186          ).value                         â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    187                                          â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    188      @staticmethod                       â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    189      def                                 â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ _compute_global_forward_mode(forward_modes):    â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    190          converted_forward_modes = [     â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    191              ForwardMode.DECODE.value if â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ x == ForwardMode.IDLE.value else x              â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    192              for x in forward_modes      â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    193          ]                               â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    194          forward_mode_agree =            â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ TboDPAttentionPreparer._is_all_same(            â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    195              converted_forward_modes     â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    196          )                               â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    197          global_forward_mode = (         â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    198                                          â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ ForwardMode(converted_forward_modes[0]) if      â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ forward_mode_agree else None                    â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    199          )                               â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    200          return global_forward_mode,     â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ forward_mode_agree                              â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                                                 â”‚   â”‚
â”‚             â”‚ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚
â”‚ Total       â”‚ Input: 267594 Output: 2781                                                                             â”‚
â”‚ Tokens      â”‚                                                                                                        â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Status      â”‚ âœ… Step 14: Completed                                                                                  â”‚
â”‚ Tools       â”‚ ðŸ”§ bash                                                                                                â”‚
â”‚ bash        â”‚ â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚
â”‚             â”‚ â”‚ {'command': "sed -n '200,620p'                 â”‚         return global_forward_mode,             â”‚   â”‚
â”‚             â”‚ â”‚ /home/ubuntu/OmniPerf-Bench/perf-agents-benchâ€¦ â”‚ forward_mode_agree                              â”‚   â”‚
â”‚             â”‚ â”‚ 'restart': False}                              â”‚                                                 â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚     @staticmethod                               â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚     def _is_all_same(x):                        â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚         return all(value == x[0] for value in   â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ x)                                              â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                                                 â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                                                 â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ class TboForwardBatchPreparer:                  â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚     @classmethod                                â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚     def prepare(cls, batch: ForwardBatch):      â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚         from                                    â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ sglang.srt.layers.attention.tbo_backend import  â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ TboAttnBackend                                  â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                                                 â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚         if batch.tbo_split_seq_index is None:   â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚             return                              â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                                                 â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚         tbo_split_token_index =                 â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ compute_split_token_index(                      â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚             split_seq_index=batch.tbo_split_seâ€¦ â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚             forward_mode=batch.forward_mode,    â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚             extend_seq_lens=batch.extend_seq_lâ€¦ â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚         )                                       â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                                                 â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚         if _tbo_debug:                          â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚             logger.info(                        â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                 f"TboForwardBatchPreparer.prepâ€¦ â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ "                                               â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                 f"tbo_split_seq_index={batch.tâ€¦ â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ "                                               â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                 f"tbo_split_token_index={tbo_sâ€¦ â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ "                                               â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                 f"extend_seq_lens={batch.extenâ€¦ â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚             )                                   â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                                                 â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚         assert isinstance(batch.attn_backend,   â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ TboAttnBackend)                                 â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚         attn_backend_child_a,                   â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ attn_backend_child_b =                          â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ batch.attn_backend.children                     â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                                                 â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚         child_a = cls.filter_batch(             â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚             batch,                              â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚             start_token_index=0,                â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚             end_token_index=tbo_split_token_inâ€¦ â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚             start_seq_index=0,                  â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚             end_seq_index=batch.tbo_split_seq_â€¦ â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚             output_attn_backend=attn_backend_câ€¦ â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚         )                                       â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚         child_b = cls.filter_batch(             â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚             batch,                              â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚             start_token_index=tbo_split_token_â€¦ â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚             end_token_index=batch.input_ids.shâ€¦ â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚             start_seq_index=batch.tbo_split_seâ€¦ â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚             end_seq_index=batch.batch_size,     â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚             output_attn_backend=attn_backend_câ€¦ â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚         )                                       â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                                                 â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚         assert batch.tbo_children is None       â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚         batch.tbo_children =                    â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                                                 â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚     @classmethod                                â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚     def filter_batch(                           â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚         cls,                                    â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚         batch: ForwardBatch,                    â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚         *,                                      â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚         start_token_index: int,                 â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚         end_token_index: int,                   â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚         start_seq_index: int,                   â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚         end_seq_index: int,                     â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚         output_attn_backend: AttentionBackend,  â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚     ):                                          â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚         from sglang.srt.managers.schedule_batch â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ import global_server_args_dict                  â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                                                 â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚         num_tokens = batch.input_ids.shape[0]   â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚         num_seqs = batch.batch_size             â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                                                 â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚         output_dict = dict()                    â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                                                 â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚         for key in [                            â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚             "input_ids",                        â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚             "positions",                        â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚             "out_cache_loc",                    â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚         ]:                                      â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚             old_value = getattr(batch, key)     â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚             assert (                            â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                 old_value.shape[0] ==           â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ num_tokens                                      â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚             ), f"{key=} {old_value=}            â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ {num_tokens=} {batch=}"                         â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚             output_dict = old_value             â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                                                 â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚         for key in [                            â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚             "req_pool_indices",                 â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚             "seq_lens",                         â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚             "seq_lens_cpu",                     â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚             "extend_seq_lens",                  â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚             "extend_prefix_lens",               â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚             "extend_start_loc",                 â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚             "extend_prefix_lens_cpu",           â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚             "extend_seq_lens_cpu",              â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚             "extend_logprob_start_lens_cpu",    â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚             "lora_paths",                       â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚         ]:                                      â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚             old_value = getattr(batch, key)     â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚             if old_value is None:               â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                 continue                        â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚             assert (                            â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                 len(old_value) == num_seqs      â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚             ), f"{key=} {old_value=}            â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ {num_seqs=} {batch=}"                           â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚             output_dict = old_value             â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                                                 â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚         for key in [                            â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚             "forward_mode",                     â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚             "return_logprob",                   â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚             "req_to_token_pool",                â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚             "token_to_kv_pool",                 â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚             "can_run_dp_cuda_graph",            â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚             "global_forward_mode",              â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚             "spec_info",                        â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚             "spec_algorithm",                   â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚             "capture_hidden_mode",              â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚             "padded_static_len",                â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚             "mrope_positions",  # only used by  â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ qwen2-vl, thus not care                         â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚         ]:                                      â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚             output_dict = getattr(batch, key)   â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                                                 â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚         assert (                                â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚             _compute_extend_num_tokens(batch.iâ€¦ â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ batch.forward_mode)                             â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚             == batch.extend_num_tokens          â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚         ), f"{batch=}"                          â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚         extend_num_tokens =                     â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ _compute_extend_num_tokens(                     â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚             output_dict["input_ids"],           â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ output_dict["forward_mode"]                     â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚         )                                       â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                                                 â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚         # TODO improve, e.g. unify w/           â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ `init_raw`                                      â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚         if                                      â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ global_server_args_dict["moe_dense_tp_size"] == â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ 1:                                              â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚             sum_len = end_token_index -         â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ start_token_index                               â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚             gathered_buffer = torch.zeros(      â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                 (sum_len,                       â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ batch.gathered_buffer.shape[1]),                â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                 dtype=batch.gathered_buffer.dtâ€¦ â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                 device=batch.gathered_buffer.dâ€¦ â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚             )                                   â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚         else:                                   â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚             gathered_buffer = None              â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                                                 â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚         output_dict.update(                     â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚             dict(                               â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                 batch_size=end_seq_index -      â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ start_seq_index,                                â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                 seq_lens_sum=(                  â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                     output_dict["seq_lens_cpu"â€¦ â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                     if "seq_lens_cpu" in        â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ output_dict                                     â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                     else None                   â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                 ),                              â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                 extend_num_tokens=extend_num_tâ€¦ â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                 attn_backend=output_attn_backeâ€¦ â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                 tbo_split_seq_index=None,       â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                 tbo_parent_token_range=(start_â€¦ â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ end_token_index),                               â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                 tbo_children=None,              â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                 global_num_tokens_gpu=None,     â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                 global_num_tokens_cpu=None,     â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                 gathered_buffer=gathered_buffeâ€¦ â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                 global_num_tokens_for_logprob_â€¦ â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                 global_num_tokens_for_logprob_â€¦ â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                 sampling_info=None,             â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                 # For logits and logprobs post  â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ processing, thus we do not care                 â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                 temp_scaled_logprobs=False,     â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                 temperature=None,               â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                 top_p_normalized_logprobs=Falsâ€¦ â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                 top_p=None,                     â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                 mm_inputs=None,                 â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                 num_token_non_padded=None,      â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚             )                                   â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚         )                                       â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                                                 â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚         errors = []                             â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚         for field in                            â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ dataclasses.fields(ForwardBatch):               â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚             if getattr(batch, field.name) is    â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ not None and field.name not in output_dict:     â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                 errors.append(                  â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                     f"Field {field.name} has    â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ value, but is not yet supported                 â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ (value={getattr(batch, field.name)}             â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ batch={batch})"                                 â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                 )                               â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚         if len(errors) > 0:                     â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚             raise Exception(f"{len(errors)}     â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ errors happen:\n" + "\n\n".join(errors))        â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                                                 â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚         return ForwardBatch(**output_dict)      â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                                                 â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                                                 â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ def _compute_extend_num_tokens(input_ids,       â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ forward_mode: ForwardMode):                     â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚     if forward_mode.is_extend():                â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚         return input_ids.shape[0]               â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚     elif forward_mode.is_decode() or            â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ forward_mode.is_idle():                         â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚         return None                             â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚     raise NotImplementedError                   â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                                                 â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                                                 â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ # -------------------------------- Execution    â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ ---------------------------------------         â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                                                 â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                                                 â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ def model_forward_maybe_tbo(                    â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚     layers,                                     â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚     enable_tbo: bool,                           â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚     positions: torch.Tensor,                    â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚     forward_batch: ForwardBatch,                â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚     hidden_states: torch.Tensor,                â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚     input_data_scatter_mode: ScatterMode,       â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚     residual: Optional,                         â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚     zero_allocator: Optional[BumpAllocator] =   â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ None,                                           â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ ):                                              â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚     inputs = dict(                              â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚         positions=positions,                    â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚         hidden_states=hidden_states,            â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚         forward_batch=forward_batch,            â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚         residual=residual,                      â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚         zero_allocator=zero_allocator,          â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚     )                                           â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚     layer_input_scatter_mode =                  â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ layers[0].layer_scatter_modes.layer_input_mode  â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚     operations_strategy =                       â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ OperationsStrategy.init_new_tbo(                â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚         layers,                                 â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ forward_batch.global_forward_mode               â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚     )                                           â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚     if enable_tbo:                              â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚         return _model_forward_tbo(              â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚             inputs=inputs,                      â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚             operations_strategy=operations_strâ€¦ â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚             input_data_scatter_mode=input_dataâ€¦ â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚             layer_input_scatter_mode=layer_inpâ€¦ â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚         )                                       â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚     else:                                       â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚         return _model_forward_non_tbo(inputs,   â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ operations_strategy)                            â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                                                 â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                                                 â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ def _model_forward_tbo(                         â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚     inputs,                                     â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚     operations_strategy: OperationsStrategy,    â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚     input_data_scatter_mode: ScatterMode,       â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚     layer_input_scatter_mode: ScatterMode,      â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ ):                                              â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚     inputs_arr =                                â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ _model_forward_tbo_split_inputs(                â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚         **inputs,                               â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚         input_data_scatter_mode=input_data_scaâ€¦ â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚         layer_input_scatter_mode=layer_input_sâ€¦ â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚     )                                           â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚     del inputs                                  â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                                                 â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚     with                                        â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ configure_deep_gemm_num_sms(operations_strategâ€¦ â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚         outputs_arr =                           â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ execute_overlapped_operations(                  â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚             inputs_arr=inputs_arr,              â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚             operations_arr= * 2,                â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚             delta_stages=[0,                    â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ operations_strategy.tbo_delta_stages],          â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚         )                                       â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                                                 â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚     return                                      â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ _model_forward_tbo_merge_outputs(*outputs_arr)  â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                                                 â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                                                 â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ def _model_forward_non_tbo(inputs,              â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ operations_strategy: OperationsStrategy):       â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚     outputs = execute_operations(inputs,        â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ operations_strategy.operations)                 â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚     return outputs["hidden_states"],            â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ outputs["residual"]                             â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                                                 â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                                                 â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ def _model_forward_tbo_split_inputs(            â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚     hidden_states: torch.Tensor,                â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚     residual: torch.Tensor,                     â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚     positions: torch.Tensor,                    â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚     forward_batch: ForwardBatch,                â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚     zero_allocator: Optional[BumpAllocator],    â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚     input_data_scatter_mode: ScatterMode,       â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚     layer_input_scatter_mode: ScatterMode,      â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ ) -> List[Dict]:                                â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚     tbo_splitter_scatter_mode =                 â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ ScatterMode.TP_ATTN_FULL                        â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚     context = CommunicateContext.init_new()     â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                                                 â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚     hidden_states, residual =                   â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ CommunicateSummableTensorPairFn.execute(        â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚         hidden_states_input_mode=input_data_scâ€¦ â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚         residual_input_mode=input_data_scatterâ€¦ â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚         output_mode=tbo_splitter_scatter_mode,  â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚         hidden_states=hidden_states,            â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚         residual=residual,                      â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚         forward_batch=forward_batch,            â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚         context=context,                        â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚     )                                           â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                                                 â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚     inputs_arr =                                â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ _model_forward_tbo_split_inputs_raw(            â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚         hidden_states=hidden_states,            â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚         residual=residual,                      â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚         positions=positions,                    â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚         forward_batch=forward_batch,            â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚         zero_allocator=zero_allocator,          â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚     )                                           â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                                                 â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚     def _post_transform(hidden_states,          â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ residual, forward_batch, **kwargs):             â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚         hidden_states, residual =               â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ CommunicateSummableTensorPairFn.execute(        â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚             hidden_states_input_mode=tbo_splitâ€¦ â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚             residual_input_mode=tbo_splitter_sâ€¦ â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚             output_mode=layer_input_scatter_moâ€¦ â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚             hidden_states=hidden_states,        â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚             residual=residual,                  â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚             forward_batch=forward_batch,        â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚             context=context,                    â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚         )                                       â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚         return dict(                            â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚             hidden_states=hidden_states,        â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚             residual=residual,                  â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚             forward_batch=forward_batch,        â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚             **kwargs,                           â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚         )                                       â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                                                 â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚     return [_post_transform(**inputs) for       â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ inputs in inputs_arr]                           â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                                                 â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                                                 â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ def _model_forward_tbo_split_inputs_raw(        â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚     hidden_states: torch.Tensor,                â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚     residual: torch.Tensor,                     â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚     positions: torch.Tensor,                    â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚     forward_batch: ForwardBatch,                â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚     zero_allocator: Optional[BumpAllocator],    â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ ) -> List[Dict]:                                â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚     return [                                    â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚         dict(                                   â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚             **_model_forward_filter_inputs(     â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                 hidden_states=hidden_states,    â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                 residual=residual,              â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                 positions=positions,            â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                 output_forward_batch=output_foâ€¦ â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                 tbo_subbatch_index=tbo_subbatcâ€¦ â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚             ),                                  â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚             **(                                 â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                 dict(zero_allocator=zero_allocâ€¦ â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                 if zero_allocator is not None   â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                 else {}                         â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚             ),                                  â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚         )                                       â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚         for tbo_subbatch_index,                 â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ output_forward_batch in enumerate(              â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚             forward_batch.tbo_children          â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚         )                                       â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚     ]                                           â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                                                 â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                                                 â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ def _model_forward_filter_inputs(               â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚     hidden_states: torch.Tensor,                â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚     residual: torch.Tensor,                     â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚     positions: torch.Tensor,                    â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚     output_forward_batch: ForwardBatch,         â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚     tbo_subbatch_index: int,                    â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ ) -> Dict:                                      â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚     token_slice =                               â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ slice(*output_forward_batch.tbo_parent_token_râ€¦ â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚     return dict(                                â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚         hidden_states=hidden_states,            â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚         residual=None if residual is None else  â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ residual,                                       â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚         positions=positions,                    â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚         forward_batch=output_forward_batch,     â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚         tbo_subbatch_index=tbo_subbatch_index,  â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚     )                                           â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                                                 â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                                                 â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ def _model_forward_tbo_merge_outputs(output_a,  â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ output_b):                                      â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚     def _handle_key(name):                      â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚         value_a = output_a                      â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚         value_b = output_b                      â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚         assert (value_a is None) == (value_b is â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ None)                                           â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚         if value_a is None:                     â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚             return None                         â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚         return torch.concat(, dim=0)            â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                                                 â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚     return _handle_key("hidden_states"),        â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ _handle_key("residual")                         â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                                                 â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                                                 â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ # -------------------------------- Utilities    â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ and wrappers                                    â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ ---------------------------------------         â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                                                 â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                                                 â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ class MaybeTboDeepEPDispatcher:                 â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚     def __init__(self, **kwargs):               â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚         num_inner_dispatchers = (               â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚             2 if                                â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ global_server_args_dict["enable_two_batch_overâ€¦ â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ else 1                                          â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚         )                                       â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚         self._inners = [                        â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚             DeepEPDispatcher(**kwargs) for _ in â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ range(num_inner_dispatchers)                    â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚         ]                                       â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                                                 â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚     def _execute(self, name,                    â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ tbo_subbatch_index: Optional = None, **kwargs): â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚         return getattr(self._inners,            â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ name)(**kwargs)                                 â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                                                 â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚     def dispatch(self, **kwargs):               â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚         return self._execute("dispatch",        â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ **kwargs)                                       â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                                                 â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚     def dispatch_a(self, **kwargs):             â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚         return self._execute("dispatch_a",      â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ **kwargs)                                       â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                                                 â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚     def dispatch_b(self, **kwargs):             â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚         return self._execute("dispatch_b",      â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ **kwargs)                                       â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                                                 â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚     def combine(self, **kwargs):                â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚         return self._execute("combine",         â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ **kwargs)                                       â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                                                 â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚     def combine_a(self, **kwargs):              â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚         return self._execute("combine_a",       â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ **kwargs)                                       â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                                                 â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚     def combine_b(self, **kwargs):              â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚         return self._execute("combine_b",       â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ **kwargs)                                       â”‚   â”‚
â”‚             â”‚ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚
â”‚ Total       â”‚ Input: 302969 Output: 2858                                                                             â”‚
â”‚ Tokens      â”‚                                                                                                        â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Status      â”‚ âœ… Step 15: Completed                                                                                  â”‚
â”‚ Tools       â”‚ ðŸ”§ str_replace_based_edit_tool                                                                         â”‚
â”‚ str_replacâ€¦ â”‚ â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚
â”‚             â”‚ â”‚ {'command': 'view', 'file_text': '',           â”‚ Here's the result of running `cat -n` on        â”‚   â”‚
â”‚             â”‚ â”‚ 'insert_line': 1, 'new_str': '', 'old_str':    â”‚ /home/ubuntu/OmniPerf-Bench/perf-agents-bench/â€¦ â”‚   â”‚
â”‚             â”‚ â”‚ '', 'path':                                    â”‚      1  # Copyright 2024 SGLang Team            â”‚   â”‚
â”‚             â”‚ â”‚ '/home/ubuntu/OmniPerf-Bench/perf-agents-bencâ€¦ â”‚      2  # Licensed under the Apache License,    â”‚   â”‚
â”‚             â”‚ â”‚ 'view_range': [1, -1]}                         â”‚ Version 2.0 (the "License");                    â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚      3  # you may not use this file except in   â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ compliance with the License.                    â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚      4  # You may obtain a copy of the License  â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ at                                              â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚      5  #                                       â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚      6  #                                       â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ http://www.apache.org/licenses/LICENSE-2.0      â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚      7  #                                       â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚      8  # Unless required by applicable law or  â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ agreed to in writing, software                  â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚      9  # distributed under the License is      â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ distributed on an "AS IS" BASIS,                â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚     10  # WITHOUT WARRANTIES OR CONDITIONS OF   â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ ANY KIND, either express or implied.            â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚     11  # See the License for the specific      â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ language governing permissions and              â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚     12  # limitations under the License.        â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚     13  #                                       â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ ==============================================â€¦ â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚     14                                          â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚     15  import math                             â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚     16  from typing import Callable, Optional   â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚     17                                          â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚     18  import torch                            â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚     19  import torch.nn.functional as F         â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚     20                                          â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚     21  from sglang.srt.managers import         â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ expert_location_dispatch                        â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚     22  from                                    â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ sglang.srt.managers.expert_distribution import  â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ (                                               â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚     23      ExpertDistributionRecorder,         â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚     24                                          â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ get_global_expert_distribution_recorder,        â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚     25  )                                       â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚     26  from                                    â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ sglang.srt.managers.expert_location_dispatch    â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ import (                                        â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚     27      ExpertLocationDispatchInfo,         â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚     28      topk_ids_logical_to_physical,       â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚     29  )                                       â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚     30  from sglang.srt.managers.schedule_batch â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ import global_server_args_dict                  â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚     31  from sglang.srt.utils import            â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ get_compiler_backend, is_cuda, is_hip           â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚     32                                          â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚     33  _is_cuda = is_cuda()                    â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚     34  _is_hip = is_hip()                      â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚     35                                          â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚     36  if _is_cuda:                            â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚     37      from sgl_kernel import              â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ moe_fused_gate                                  â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚     38                                          â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚     39  if _is_cuda or _is_hip:                 â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚     40      from sgl_kernel import topk_softmax â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚     41                                          â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚     42                                          â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚     43  def fused_topk_native(                  â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚     44      hidden_states: torch.Tensor,        â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚     45      gating_output: torch.Tensor,        â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚     46      topk: int,                          â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚     47      renormalize: bool,                  â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚     48  ):                                      â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚     49      assert (                            â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚     50          hidden_states.shape[0] ==       â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ gating_output.shape[0]                          â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚     51      ), f"Number of tokens mismatch,     â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ {hidden_states.shape=} vs                       â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ {gating_output.shape=}"                         â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚     52      M, _ = hidden_states.shape          â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚     53      topk_weights = torch.empty(         â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚     54          M, topk, dtype=torch.float32,   â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ device=hidden_states.device                     â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚     55      )                                   â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚     56      topk_ids = torch.empty(M, topk,     â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ dtype=torch.int32, device=hidden_states.device) â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚     57      topk_weights =                      â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ F.softmax(gating_output.float(), dim=-1)        â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚     58      topk_weights, topk_ids =            â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ torch.topk(topk_weights, topk, dim=-1)          â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚     59      if renormalize:                     â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚     60          topk_weights = topk_weights /   â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ topk_weights.sum(dim=-1, keepdim=True)          â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚     61      return topk_weights, topk_ids       â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚     62                                          â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚     63                                          â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚     64  def fused_topk(                         â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚     65      hidden_states: torch.Tensor,        â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚     66      gating_output: torch.Tensor,        â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚     67      topk: int,                          â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚     68      renormalize: bool,                  â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚     69      expert_location_dispatch_info:      â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ Optional[ExpertLocationDispatchInfo] = None,    â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚     70  ):                                      â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚     71      assert hidden_states.shape[0] ==    â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ gating_output.shape[0], "Number of tokens       â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ mismatch"                                       â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚     72                                          â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚     73      M, _ = hidden_states.shape          â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚     74                                          â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚     75      topk_weights = torch.empty(         â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚     76          M, topk, dtype=torch.float32,   â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ device=hidden_states.device                     â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚     77      )                                   â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚     78      topk_ids = torch.empty(M, topk,     â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ dtype=torch.int32, device=hidden_states.device) â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚     79      token_expert_indicies =             â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ torch.empty(                                    â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚     80          M, topk, dtype=torch.int32,     â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ device=hidden_states.device                     â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚     81      )                                   â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚     82                                          â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚     83      topk_softmax(                       â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚     84          topk_weights,                   â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚     85          topk_ids,                       â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚     86          token_expert_indicies,          â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚     87          gating_output.float(),          â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚     88      )                                   â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚     89      del token_expert_indicies           â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚     90                                          â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚     91      if renormalize:                     â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚     92          topk_weights = topk_weights /   â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ topk_weights.sum(dim=-1, keepdim=True)          â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚     93      topk_ids =                          â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ topk_ids_logical_to_physical(topk_ids,          â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ expert_location_dispatch_info)                  â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚     94      return topk_weights, topk_ids       â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚     95                                          â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚     96                                          â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚     97  # This is used by the Deepseek V2/V3/R1 â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ series models                                   â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚     98  @torch.compile(dynamic=True,            â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ backend=get_compiler_backend())                 â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚     99  def grouped_topk(                       â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    100      hidden_states: torch.Tensor,        â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    101      gating_output: torch.Tensor,        â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    102      topk: int,                          â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    103      renormalize: bool,                  â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    104      num_expert_group: int = 0,          â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    105      topk_group: int = 0,                â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    106      n_share_experts_fusion: int = 0,    â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    107      routed_scaling_factor: Optional =   â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ None,                                           â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    108      num_token_non_padded: Optional =    â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ None,                                           â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    109      expert_location_dispatch_info:      â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ Optional[ExpertLocationDispatchInfo] = None,    â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    110  ):                                      â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    111      assert hidden_states.shape[0] ==    â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ gating_output.shape[0], "Number of tokens       â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ mismatch"                                       â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    112                                          â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    113      scores =                            â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ torch.softmax(gating_output, dim=-1)            â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    114      num_token = scores.shape[0]         â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    115      num_experts = scores.shape[1]       â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    116      group_scores = (                    â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    117          scores.view(num_token,          â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ num_expert_group, -1).max(dim=-1).values        â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    118      )  #                                â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    119      group_idx =                         â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ torch.topk(group_scores, k=topk_group, dim=-1,  â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ sorted=False)[                                  â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    120          1                               â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    121      ]  #                                â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    122      group_mask =                        â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ torch.zeros_like(group_scores)  #               â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    123      group_mask.scatter_(1, group_idx,   â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ 1)  #                                           â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    124      score_mask = (                      â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    125          group_mask.unsqueeze(-1)        â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    126          .expand(num_token,              â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ num_expert_group, scores.shape[-1] //           â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ num_expert_group)                               â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    127          .reshape(num_token, -1)         â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    128      )  #                                â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    129      tmp_scores =                        â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ scores.masked_fill(~score_mask.bool(), 0.0)  #  â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    130      topk_weights, topk_ids =            â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ torch.topk(tmp_scores, k=topk, dim=-1,          â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ sorted=False)                                   â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    131      if n_share_experts_fusion:          â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    132          topk_ids[:, -1] =               â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ torch.randint(                                  â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    133              low=num_experts,            â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    134              high=num_experts +          â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ n_share_experts_fusion,                         â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    135              size=(topk_ids.size(0),),   â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    136              dtype=topk_ids.dtype,       â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    137              device=topk_ids.device,     â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    138          )                               â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    139          topk_weights[:, -1] =           â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ topk_weights[:, :-1].sum(dim=-1) /              â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ routed_scaling_factor                           â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    140                                          â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    141      if renormalize:                     â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    142          topk_weights_sum = (            â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    143              topk_weights.sum(dim=-1,    â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ keepdim=True)                                   â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    144              if n_share_experts_fusion   â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ == 0                                            â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    145              else topk_weights[:,        â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ :-1].sum(dim=-1, keepdim=True)                  â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    146          )                               â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    147          topk_weights = topk_weights /   â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ topk_weights_sum                                â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    148                                          â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    149      topk_weights, topk_ids =            â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ topk_weights.to(torch.float32),                 â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ topk_ids.to(torch.int32)                        â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    150      topk_ids =                          â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ topk_ids_logical_to_physical(topk_ids,          â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ expert_location_dispatch_info)                  â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    151                                          â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ _mask_topk_ids_padded_region(topk_ids,          â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ num_token_non_padded)                           â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    152      return topk_weights, topk_ids       â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    153                                          â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    154                                          â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    155  def biased_grouped_topk_impl(           â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    156      hidden_states: torch.Tensor,        â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    157      gating_output: torch.Tensor,        â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    158      correction_bias: torch.Tensor,      â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    159      topk: int,                          â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    160      renormalize: bool,                  â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    161      num_expert_group: int = 0,          â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    162      topk_group: int = 0,                â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    163      n_share_experts_fusion: int = 0,    â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    164      routed_scaling_factor: Optional =   â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ None,                                           â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    165      num_token_non_padded: Optional =    â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ None,                                           â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    166      expert_location_dispatch_info:      â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ Optional[ExpertLocationDispatchInfo] = None,    â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    167  ):                                      â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    168      assert hidden_states.shape[0] ==    â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ gating_output.shape[0], "Number of tokens       â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ mismatch"                                       â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    169                                          â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    170      scores = gating_output.sigmoid()    â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    171      num_token = scores.shape[0]         â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    172      num_experts = scores.shape[1]       â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    173      scores_for_choice =                 â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ scores.view(num_token, -1) +                    â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ correction_bias.unsqueeze(0)                    â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    174      group_scores = (                    â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    175                                          â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ scores_for_choice.view(num_token,               â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ num_expert_group, -1)                           â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    176          .topk(2, dim=-1)[0]             â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    177          .sum(dim=-1)                    â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    178      )  #                                â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    179      group_idx =                         â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ torch.topk(group_scores, k=topk_group, dim=-1,  â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ sorted=False)[                                  â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    180          1                               â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    181      ]  #                                â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    182      group_mask =                        â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ torch.zeros_like(group_scores)  #               â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    183      group_mask.scatter_(1, group_idx,   â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ 1)  #                                           â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    184      score_mask = (                      â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    185          group_mask.unsqueeze(-1)        â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    186          .expand(num_token,              â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ num_expert_group, scores.shape[-1] //           â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ num_expert_group)                               â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    187          .reshape(num_token, -1)         â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    188      )  #                                â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    189      tmp_scores =                        â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ scores_for_choice.masked_fill(                  â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    190          ~score_mask.bool(),             â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ float("-inf")                                   â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    191      )  #                                â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    192      _, topk_ids =                       â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ torch.topk(tmp_scores, k=topk, dim=-1,          â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ sorted=False)                                   â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    193      topk_weights = scores.gather(1,     â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ topk_ids)                                       â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    194                                          â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    195      if n_share_experts_fusion:          â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    196          topk_ids[:, -1] =               â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ torch.randint(                                  â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    197              low=num_experts,            â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    198              high=num_experts +          â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ n_share_experts_fusion,                         â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    199              size=(topk_ids.size(0),),   â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    200              dtype=topk_ids.dtype,       â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    201              device=topk_ids.device,     â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    202          )                               â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    203          topk_weights[:, -1] =           â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ topk_weights[:, :-1].sum(dim=-1) /              â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ routed_scaling_factor                           â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    204                                          â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    205      if renormalize:                     â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    206          topk_weights_sum = (            â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    207              topk_weights.sum(dim=-1,    â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ keepdim=True)                                   â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    208              if n_share_experts_fusion   â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ == 0                                            â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    209              else topk_weights[:,        â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ :-1].sum(dim=-1, keepdim=True)                  â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    210          )                               â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    211          topk_weights = topk_weights /   â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ topk_weights_sum                                â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    212                                          â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    213      topk_weights, topk_ids =            â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ topk_weights.to(torch.float32),                 â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ topk_ids.to(torch.int32)                        â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    214      topk_ids =                          â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ topk_ids_logical_to_physical(topk_ids,          â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ expert_location_dispatch_info)                  â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    215                                          â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ _mask_topk_ids_padded_region(topk_ids,          â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ num_token_non_padded)                           â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    216      return topk_weights, topk_ids       â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    217                                          â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    218                                          â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    219  def is_power_of_two(n):                 â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    220      return n > 0 and                    â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ math.log2(n).is_integer()                       â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    221                                          â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    222                                          â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    223  def _mask_topk_ids_padded_region(       â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    224      topk_ids: torch.Tensor,             â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    225      num_token_non_padded: Optional =    â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ None,                                           â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    226  ):                                      â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    227      if num_token_non_padded is None:    â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    228          return                          â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    229      indices = torch.arange(0,           â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ topk_ids.shape[0], device=topk_ids.device)      â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    230      topk_ids = -1                       â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    231                                          â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    232                                          â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    233  def biased_grouped_topk(                â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    234      hidden_states: torch.Tensor,        â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    235      gating_output: torch.Tensor,        â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    236      correction_bias: torch.Tensor,      â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    237      topk: int,                          â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    238      renormalize: bool,                  â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    239      num_expert_group: int = 0,          â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    240      topk_group: int = 0,                â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    241      compiled: bool = True,              â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    242      n_share_experts_fusion: int = 0,    â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    243      routed_scaling_factor: Optional =   â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ None,                                           â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    244      num_token_non_padded: Optional =    â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ None,                                           â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    245      expert_location_dispatch_info:      â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ Optional[ExpertLocationDispatchInfo] = None,    â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    246  ):                                      â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    247      assert (                            â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    248          routed_scaling_factor is not    â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ None                                            â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    249      ), "routed_scaling_factor is        â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ required for biased_grouped_topk"               â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    250      # TODO: moe_fused_gate kernel is    â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ not supported for n_share_experts_fusion > 0    â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ now.                                            â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    251      if (                                â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    252          _is_cuda                        â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    253          and gating_output.shape[1] //   â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ num_expert_group                                â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    254          <= 32  # moe_fused_gate kernel  â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ ensure that num_experts/num_expert_group does   â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ not exceed MAX_VPT=32 now. And when kernel can  â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ handle MAX_VPT > 32, we can remove this         â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ assertion.                                      â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    255          and                             â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ is_power_of_two(correction_bias.shape[0])       â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    256      ):                                  â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    257          topk_weights, topk_ids =        â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ moe_fused_gate(                                 â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    258              gating_output,              â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    259              correction_bias,            â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    260              num_expert_group,           â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    261              topk_group,                 â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    262              topk,                       â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    263              n_share_experts_fusion,     â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    264              routed_scaling_factor,      â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    265          )                               â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    266          # TODO merge into kernel for    â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ this branch                                     â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    267          topk_ids =                      â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ topk_ids_logical_to_physical(topk_ids,          â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ expert_location_dispatch_info)                  â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    268          # TODO will fuse this into      â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ kernel, thus use slow manual operation now      â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    269          if num_token_non_padded is      â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ None:                                           â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    270              return topk_weights,        â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ topk_ids                                        â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    271          torch.compile(                  â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    272                                          â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ _mask_topk_ids_padded_region, dynamic=True,     â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ backend=get_compiler_backend()                  â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    273          )(topk_ids,                     â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ num_token_non_padded)                           â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    274          return topk_weights, topk_ids   â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    275      else:                               â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    276          biased_grouped_topk_fn = (      â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    277              torch.compile(              â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    278                                          â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ biased_grouped_topk_impl, dynamic=True,         â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ backend=get_compiler_backend()                  â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    279              )                           â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    280              if compiled                 â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    281              else                        â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ biased_grouped_topk_impl                        â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    282          )                               â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    283          return biased_grouped_topk_fn(  â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    284              hidden_states,              â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    285              gating_output,              â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    286              correction_bias,            â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    287              topk,                       â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    288              renormalize,                â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    289              num_expert_group,           â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    290              topk_group,                 â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    291                                          â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ n_share_experts_fusion=n_share_experts_fusion,  â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    292                                          â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ routed_scaling_factor=routed_scaling_factor,    â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    293                                          â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ num_token_non_padded=num_token_non_padded,      â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    294                                          â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ expert_location_dispatch_info=expert_location_â€¦ â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    295          )                               â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    296                                          â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    297                                          â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    298  def select_experts(                     â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    299      hidden_states: torch.Tensor,        â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    300      router_logits: torch.Tensor,        â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    301      top_k: int,                         â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    302      use_grouped_topk: bool,             â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    303      renormalize: bool,                  â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    304      topk_group: Optional = None,        â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    305      num_expert_group: Optional = None,  â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    306      custom_routing_function:            â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ Optional[Callable] = None,                      â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    307      correction_bias: Optional = None,   â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    308      torch_native: bool = False,         â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    309      routed_scaling_factor: Optional =   â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ None,                                           â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    310      num_token_non_padded: Optional =    â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ None,                                           â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    311      expert_location_dispatch_info:      â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ Optional[ExpertLocationDispatchInfo] = None,    â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    312  ):                                      â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    313      n_share_experts_fusion =            â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ global_server_args_dict["n_share_experts_fusioâ€¦ â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    314                                          â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    315      router_logits, correction_bias = (  â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    316                                          â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ expert_location_dispatch.transform_select_expeâ€¦ â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    317                                          â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ router_logits=router_logits,                    â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    318                                          â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ correction_bias=correction_bias,                â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    319                                          â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ info=expert_location_dispatch_info,             â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    320          )                               â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    321      )                                   â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    322                                          â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    323      # DeepSeek V2/V3/R1 series models   â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ use grouped_top_k                               â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    324      if use_grouped_topk:                â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    325          assert topk_group is not None   â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    326          assert num_expert_group is not  â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ None                                            â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    327          if correction_bias is None:     â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    328              topk_weights, topk_ids =    â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ grouped_topk(                                   â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    329                                          â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ hidden_states=hidden_states,                    â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    330                                          â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ gating_output=router_logits,                    â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    331                  topk=top_k,             â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    332                                          â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ renormalize=renormalize,                        â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    333                                          â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ num_expert_group=num_expert_group,              â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    334                  topk_group=topk_group,  â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    335                                          â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ n_share_experts_fusion=n_share_experts_fusion,  â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    336                                          â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ routed_scaling_factor=routed_scaling_factor,    â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    337                                          â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ num_token_non_padded=num_token_non_padded,      â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    338                                          â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ expert_location_dispatch_info=expert_location_â€¦ â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    339              )                           â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    340          else:                           â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    341              topk_weights, topk_ids =    â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ biased_grouped_topk(                            â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    342                                          â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ hidden_states=hidden_states,                    â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    343                                          â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ gating_output=router_logits,                    â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    344                                          â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ correction_bias=correction_bias,                â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    345                  topk=top_k,             â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    346                                          â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ renormalize=renormalize,                        â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    347                                          â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ num_expert_group=num_expert_group,              â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    348                  topk_group=topk_group,  â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    349                                          â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ n_share_experts_fusion=n_share_experts_fusion,  â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    350                                          â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ routed_scaling_factor=routed_scaling_factor,    â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    351                                          â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ num_token_non_padded=num_token_non_padded,      â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    352                                          â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ expert_location_dispatch_info=expert_location_â€¦ â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    353              )                           â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    354      elif torch_native and               â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ custom_routing_function is None:                â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    355          assert (                        â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    356              num_token_non_padded is     â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ None                                            â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    357          ), "num_token_non_padded is not â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ yet supported in fused_topk_native"             â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    358          assert                          â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ expert_location_dispatch_info is None           â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    359          topk_weights, topk_ids =        â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ fused_topk_native(                              â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    360                                          â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ hidden_states=hidden_states,                    â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    361                                          â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ gating_output=router_logits,                    â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    362              topk=top_k,                 â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    363              renormalize=renormalize,    â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    364          )                               â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    365      elif custom_routing_function is     â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ None:                                           â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    366          assert (                        â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    367              num_token_non_padded is     â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ None                                            â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    368          ), "num_token_non_padded is not â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ yet supported in fused_topk"                    â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    369          # Qwen3MOE uses fused_topk      â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    370          topk_weights, topk_ids =        â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ fused_topk(                                     â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    371                                          â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ hidden_states=hidden_states,                    â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    372                                          â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ gating_output=router_logits,                    â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    373              topk=top_k,                 â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    374              renormalize=renormalize,    â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    375                                          â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ expert_location_dispatch_info=expert_location_â€¦ â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    376          )                               â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    377      else:                               â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    378          assert (                        â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    379              num_token_non_padded is     â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ None                                            â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    380          ), "num_token_non_padded is not â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ yet supported in custom_routing_function"       â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    381          assert                          â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ expert_location_dispatch_info is None           â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    382          topk_weights, topk_ids =        â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ custom_routing_function(                        â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    383                                          â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ hidden_states=hidden_states,                    â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    384                                          â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ gating_output=router_logits,                    â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    385              topk=top_k,                 â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    386              renormalize=renormalize,    â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    387          )                               â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    388                                          â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    389                                          â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ get_global_expert_distribution_recorder().on_sâ€¦ â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    390                                          â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    391      return topk_weights, topk_ids       â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    392                                          â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                                                 â”‚   â”‚
â”‚             â”‚ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚
â”‚ Total       â”‚ Input: 341188 Output: 3848                                                                             â”‚
â”‚ Tokens      â”‚                                                                                                        â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Status      â”‚ âœ… Step 16: Completed                                                                                  â”‚
â”‚ Tools       â”‚ ðŸ”§ str_replace_based_edit_tool                                                                         â”‚
â”‚ str_replacâ€¦ â”‚ â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚
â”‚             â”‚ â”‚ {'command': 'view', 'file_text': '',           â”‚ Here's the result of running `cat -n` on        â”‚   â”‚
â”‚             â”‚ â”‚ 'insert_line': 1, 'new_str': '', 'old_str':    â”‚ /home/ubuntu/OmniPerf-Bench/perf-agents-bench/â€¦ â”‚   â”‚
â”‚             â”‚ â”‚ '', 'path':                                    â”‚      1  # Copyright 2023-2024 SGLang Team       â”‚   â”‚
â”‚             â”‚ â”‚ '/home/ubuntu/OmniPerf-Bench/perf-agents-bencâ€¦ â”‚      2  # Licensed under the Apache License,    â”‚   â”‚
â”‚             â”‚ â”‚ 'view_range': [1, -1]}                         â”‚ Version 2.0 (the "License");                    â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚      3  # you may not use this file except in   â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ compliance with the License.                    â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚      4  # You may obtain a copy of the License  â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ at                                              â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚      5  #                                       â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚      6  #                                       â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ http://www.apache.org/licenses/LICENSE-2.0      â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚      7  #                                       â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚      8  # Unless required by applicable law or  â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ agreed to in writing, software                  â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚      9  # distributed under the License is      â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ distributed on an "AS IS" BASIS,                â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚     10  # WITHOUT WARRANTIES OR CONDITIONS OF   â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ ANY KIND, either express or implied.            â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚     11  # See the License for the specific      â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ language governing permissions and              â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚     12  # limitations under the License.        â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚     13  #                                       â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ ==============================================â€¦ â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚     14  """                                     â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚     15  Store information about a forward       â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ batch.                                          â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚     16                                          â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚     17  The following is the flow of data       â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ structures for a batch:                         â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚     18                                          â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚     19  ScheduleBatch -> ModelWorkerBatch ->    â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ ForwardBatch                                    â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚     20                                          â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚     21  - ScheduleBatch is managed by           â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ `scheduler.py::Scheduler`.                      â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚     22    It contains high-level scheduling     â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ data. Most of the data is on the CPU.           â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚     23  - ModelWorkerBatch is managed by        â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ `tp_worker.py::TpModelWorker`.                  â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚     24    It is a subset of `ScheduleBatch`     â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ that only contains data related to the model    â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ forward on GPU.                                 â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚     25    It will be transformed from CPU       â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ scheduler to GPU model runner.                  â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚     26  - ForwardBatch is managed by            â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ `model_runner.py::ModelRunner`.                 â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚     27    It contains low-level tensor data.    â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ Most of the data consists of GPU tensors.       â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚     28  """                                     â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚     29                                          â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚     30  from __future__ import annotations      â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚     31                                          â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚     32  import dataclasses                      â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚     33  from dataclasses import dataclass       â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚     34  from enum import IntEnum, auto          â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚     35  from typing import TYPE_CHECKING, Dict, â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ List, Optional, Tuple, Union                    â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚     36                                          â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚     37  import torch                            â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚     38  import triton                           â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚     39  import triton.language as tl            â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚     40                                          â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚     41  from sglang.srt.layers.rotary_embedding â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ import MRotaryEmbedding                         â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚     42  from sglang.srt.utils import            â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ flatten_nested_list, get_compiler_backend       â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚     43                                          â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚     44  if TYPE_CHECKING:                       â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚     45      from                                â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ sglang.srt.layers.attention.base_attn_backend   â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ import AttentionBackend                         â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚     46      from                                â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ sglang.srt.managers.schedule_batch import       â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ ModelWorkerBatch, MultimodalInputs              â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚     47      from                                â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ sglang.srt.mem_cache.memory_pool import         â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ KVCache, ReqToTokenPool                         â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚     48      from                                â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ sglang.srt.model_executor.model_runner import   â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ ModelRunner                                     â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚     49      from                                â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ sglang.srt.sampling.sampling_batch_info import  â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ SamplingBatchInfo                               â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚     50      from                                â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ sglang.srt.speculative.eagle_utils import       â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ EagleDraftInput, EagleVerifyInput               â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚     51      from                                â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ sglang.srt.speculative.spec_info import         â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ SpeculativeAlgorithm                            â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚     52                                          â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚     53                                          â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚     54  class ForwardMode(IntEnum):             â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚     55      # Extend a sequence. The KV cache   â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ of the beginning part of the sequence is        â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ already computed (e.g., system prompt).         â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚     56      # It is also called "prefill" in    â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ common terminology.                             â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚     57      EXTEND = auto()                     â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚     58      # Decode one token.                 â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚     59      DECODE = auto()                     â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚     60      # Contains both EXTEND and DECODE   â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ when doing chunked prefill.                     â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚     61      MIXED = auto()                      â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚     62      # No sequence to forward. For data  â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ parallel attention, some workers will be IDLE   â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ if no sequence are allocated.                   â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚     63      IDLE = auto()                       â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚     64                                          â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚     65      # Used in speculative decoding:     â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ verify a batch in the target model.             â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚     66      TARGET_VERIFY = auto()              â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚     67      # Used in speculative decoding:     â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ extend a batch in the draft model.              â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚     68      DRAFT_EXTEND = auto()               â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚     69                                          â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚     70      # A dummy first batch to start the  â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ pipeline for overlap scheduler.                 â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚     71      # It is now used for triggering the â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ sampling_info_done event for the first prefill  â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ batch.                                          â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚     72      DUMMY_FIRST = auto()                â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚     73                                          â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚     74      def is_prefill(self):               â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚     75          return self.is_extend()         â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚     76                                          â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚     77      def is_extend(self):                â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚     78          return (                        â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚     79              self == ForwardMode.EXTEND  â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚     80              or self ==                  â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ ForwardMode.MIXED                               â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚     81              or self ==                  â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ ForwardMode.DRAFT_EXTEND                        â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚     82              or self ==                  â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ ForwardMode.TARGET_VERIFY                       â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚     83          )                               â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚     84                                          â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚     85      def is_decode(self):                â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚     86          return self ==                  â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ ForwardMode.DECODE                              â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚     87                                          â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚     88      def is_mixed(self):                 â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚     89          return self ==                  â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ ForwardMode.MIXED                               â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚     90                                          â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚     91      def is_idle(self):                  â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚     92          return self == ForwardMode.IDLE â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚     93                                          â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚     94      def is_target_verify(self):         â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚     95          return self ==                  â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ ForwardMode.TARGET_VERIFY                       â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚     96                                          â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚     97      def is_draft_extend(self):          â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚     98          return self ==                  â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ ForwardMode.DRAFT_EXTEND                        â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚     99                                          â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    100      def                                 â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ is_extend_or_draft_extend_or_mixed(self):       â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    101          return (                        â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    102              self == ForwardMode.EXTEND  â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    103              or self ==                  â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ ForwardMode.DRAFT_EXTEND                        â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    104              or self ==                  â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ ForwardMode.MIXED                               â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    105          )                               â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    106                                          â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    107      def is_cuda_graph(self):            â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    108          return (                        â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    109              self == ForwardMode.DECODE  â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    110              or self ==                  â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ ForwardMode.TARGET_VERIFY                       â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    111              or self == ForwardMode.IDLE â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    112          )                               â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    113                                          â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    114      def is_dummy_first(self):           â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    115          return self ==                  â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ ForwardMode.DUMMY_FIRST                         â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    116                                          â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    117      def is_decode_or_idle(self):        â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    118          return self ==                  â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ ForwardMode.DECODE or self == ForwardMode.IDLE  â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    119                                          â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    120                                          â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    121  class CaptureHiddenMode(IntEnum):       â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    122      NULL = auto()                       â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    123      # Capture hidden states of all      â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ tokens.                                         â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    124      FULL = auto()                       â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    125      # Capture a hidden state of the     â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ last token.                                     â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    126      LAST = auto()                       â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    127                                          â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    128      def need_capture(self):             â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    129          return self !=                  â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ CaptureHiddenMode.NULL                          â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    130                                          â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    131      def is_full(self):                  â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    132          return self ==                  â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ CaptureHiddenMode.FULL                          â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    133                                          â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    134      def is_last(self):                  â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    135          return self ==                  â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ CaptureHiddenMode.LAST                          â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    136                                          â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    137                                          â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    138  @dataclass                              â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    139  class ForwardBatch:                     â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    140      """Store all inputs of a forward    â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ pass."""                                        â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    141                                          â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    142      # The forward mode                  â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    143      forward_mode: ForwardMode           â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    144      # The batch size                    â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    145      batch_size: int                     â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    146      # The input ids                     â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    147      input_ids: torch.Tensor             â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    148      # The indices of requests in the    â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ req_to_token_pool                               â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    149      req_pool_indices: torch.Tensor      â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    150      # The sequence length               â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    151      seq_lens: torch.Tensor              â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    152      # The indices of output tokens in   â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ the token_to_kv_pool                            â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    153      out_cache_loc: torch.Tensor         â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    154                                          â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    155      # The sum of all sequence lengths   â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    156      seq_lens_sum: int                   â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    157                                          â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    158      # Optional seq_lens on cpu          â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    159      seq_lens_cpu: Optional = None       â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    160                                          â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    161      # For logprob                       â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    162      return_logprob: bool = False        â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    163      top_logprobs_nums: Optional[List] = â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ None                                            â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    164      token_ids_logprobs:                 â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ Optional[List[List]] = None                     â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    165                                          â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    166      # For logits and logprobs post      â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ processing                                      â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    167      temp_scaled_logprobs: bool = False  â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    168      temperature: torch.Tensor = None    â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    169      top_p_normalized_logprobs: bool =   â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ False                                           â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    170      top_p: torch.Tensor = None          â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    171                                          â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    172      # Position information              â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    173      positions: torch.Tensor = None      â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    174                                          â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    175      # For extend                        â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    176      extend_num_tokens: Optional = None  â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    177      extend_seq_lens: Optional = None    â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    178      extend_prefix_lens: Optional = None â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    179      extend_start_loc: Optional = None   â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    180      extend_prefix_lens_cpu:             â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ Optional[List] = None                           â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    181      extend_seq_lens_cpu: Optional[List] â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ = None                                          â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    182      extend_logprob_start_lens_cpu:      â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ Optional[List] = None                           â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    183      extend_input_logprob_token_ids_gpu: â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ Optional = None                                 â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    184                                          â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    185      # For MLA chunked prefix cache used â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ in chunked prefill                              â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    186      # Tell attention backend whether    â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ the kv cache needs to be attended in current    â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ pass                                            â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    187      attn_attend_prefix_cache: Optional  â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ = None                                          â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    188      # Number of prefix cache chunks     â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    189      num_prefix_chunks: Optional = None  â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    190      # Index of current chunk, used by   â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ attention backend                               â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    191      prefix_chunk_idx: Optional = None   â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    192      # Maximum number of tokens in each  â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ chunk per sequence. Computed from maximum chunk â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ capacity                                        â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    193      prefix_chunk_len: Optional = None   â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    194      # Start positions of prefix cache   â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ for each chunk, (num_prefix_chunks, batch_size) â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    195      prefix_chunk_starts: Optional =     â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ None                                            â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    196      # Lengths of prefix cache for each  â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ chunk, (num_prefix_chunks, batch_size)          â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    197      prefix_chunk_seq_lens: Optional =   â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ None                                            â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    198      # Accumulated lengths of prefix     â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ cache for each chunk, (num_prefix_chunks,       â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ batch_size + 1)                                 â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    199      prefix_chunk_cu_seq_lens: Optional  â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ = None                                          â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    200      # Max lengths of prefix cache for   â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ each chunk, (num_prefix_chunks,)                â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    201      prefix_chunk_max_seq_lens:          â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ Optional[List] = None                           â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    202      # Number of tokens in each prefix   â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ cache chunk, (num_prefix_chunks,)               â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    203      prefix_chunk_num_tokens:            â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ Optional[List] = None                           â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    204      # KV Indices for each chunk         â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    205      prefix_chunk_kv_indices:            â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ Optional[List] = None                           â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    206                                          â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    207      # For multimodal                    â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    208      mm_inputs:                          â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ Optional[List[MultimodalInputs]] = None         â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    209                                          â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    210      # Encoder-decoder                   â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    211      encoder_cached: Optional[List] =    â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ None                                            â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    212      encoder_lens: Optional = None       â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    213      encoder_lens_cpu: Optional[List] =  â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ None                                            â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    214      encoder_out_cache_loc: Optional =   â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ None                                            â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    215                                          â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    216      # For LoRA                          â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    217      lora_paths: Optional[List] = None   â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    218                                          â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    219      # For input embeddings              â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    220      input_embeds: Optional = None       â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    221                                          â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    222      # Sampling info                     â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    223      sampling_info: SamplingBatchInfo =  â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ None                                            â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    224                                          â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    225      # Attention backend                 â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    226      req_to_token_pool: ReqToTokenPool = â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ None                                            â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    227      token_to_kv_pool: KVCache = None    â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    228      attn_backend: AttentionBackend =    â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ None                                            â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    229                                          â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    230      # For DP attention                  â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    231      global_num_tokens_cpu:              â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ Optional[List] = None                           â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    232      global_num_tokens_gpu: Optional =   â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ None                                            â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    233      # Has to be None when cuda graph is â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ captured.                                       â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    234      global_num_tokens_for_logprob_cpu:  â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ Optional[List] = None                           â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    235      global_num_tokens_for_logprob_gpu:  â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ Optional = None                                 â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    236      # for extend, local start pos and   â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ num tokens is different in logits processor     â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    237      # this will be computed in          â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ get_dp_local_info                               â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    238      # this will be recomputed in        â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ LogitsMetadata.from_forward_batch               â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    239      dp_local_start_pos: Optional = None â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ # cached info at runtime                        â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    240      dp_local_num_tokens: Optional =     â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ None  # cached info at runtime                  â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    241      gathered_buffer: Optional = None    â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    242      can_run_dp_cuda_graph: bool = False â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    243      global_forward_mode:                â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ Optional[ForwardMode] = None                    â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    244                                          â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    245      # Speculative decoding              â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    246      spec_info:                          â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ Optional[Union[EagleVerifyInput,                â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ EagleDraftInput]] = None                        â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    247      spec_algorithm:                     â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ SpeculativeAlgorithm = None                     â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    248      capture_hidden_mode:                â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ CaptureHiddenMode = None                        â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    249                                          â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    250      # For padding                       â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    251      padded_static_len: int = -1  # -1   â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ if not padded                                   â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    252      num_token_non_padded: Optional =    â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ None  # scalar tensor                           â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    253                                          â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    254      # For Qwen2-VL                      â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    255      mrope_positions: torch.Tensor =     â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ None                                            â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    256                                          â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    257      tbo_split_seq_index: Optional =     â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ None                                            â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    258      tbo_parent_token_range:             â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ Optional[Tuple] = None                          â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    259      tbo_children:                       â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ Optional[List["ForwardBatch"]] = None           â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    260                                          â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    261      @classmethod                        â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    262      def init_new(                       â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    263          cls,                            â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    264          batch: ModelWorkerBatch,        â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    265          model_runner: ModelRunner,      â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    266      ):                                  â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    267          from                            â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ sglang.srt.two_batch_overlap import             â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ TboForwardBatchPreparer                         â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    268                                          â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    269          device = model_runner.device    â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    270                                          â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ extend_input_logprob_token_ids_gpu = None       â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    271          if                              â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ batch.extend_input_logprob_token_ids is not     â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ None:                                           â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    272                                          â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ extend_input_logprob_token_ids_gpu = (          â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    273                                          â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ batch.extend_input_logprob_token_ids.to(device, â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ non_blocking=True)                              â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    274              )                           â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    275          ret = cls(                      â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    276                                          â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ forward_mode=batch.forward_mode,                â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    277                                          â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ batch_size=len(batch.seq_lens),                 â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    278              input_ids=batch.input_ids,  â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    279                                          â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ req_pool_indices=batch.req_pool_indices,        â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    280              seq_lens=batch.seq_lens,    â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    281                                          â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ out_cache_loc=batch.out_cache_loc,              â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    282                                          â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ mm_inputs=batch.multimodal_inputs,              â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    283                                          â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ encoder_cached=batch.encoder_cached,            â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    284                                          â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ encoder_lens=batch.encoder_lens,                â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    285                                          â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ encoder_lens_cpu=batch.encoder_lens_cpu,        â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    286                                          â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ encoder_out_cache_loc=batch.encoder_out_cache_â€¦ â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    287                                          â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ seq_lens_sum=batch.seq_lens_sum,                â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    288                                          â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ return_logprob=batch.return_logprob,            â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    289                                          â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ top_logprobs_nums=batch.top_logprobs_nums,      â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    290                                          â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ token_ids_logprobs=batch.token_ids_logprobs,    â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    291                                          â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ can_run_dp_cuda_graph=batch.can_run_dp_cuda_grâ€¦ â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    292                                          â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ global_forward_mode=batch.global_forward_mode,  â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    293                                          â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ lora_paths=batch.lora_paths,                    â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    294                                          â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ sampling_info=batch.sampling_info,              â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    295                                          â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ req_to_token_pool=model_runner.req_to_token_poâ€¦ â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    296                                          â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ token_to_kv_pool=model_runner.token_to_kv_pool, â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    297                                          â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ attn_backend=model_runner.attn_backend,         â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    298                                          â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ spec_algorithm=batch.spec_algorithm,            â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    299              spec_info=batch.spec_info,  â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    300                                          â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ capture_hidden_mode=batch.capture_hidden_mode,  â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    301                                          â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ input_embeds=batch.input_embeds,                â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    302                                          â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ extend_input_logprob_token_ids_gpu=extend_inpuâ€¦ â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    303                                          â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ num_token_non_padded=torch.tensor(              â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    304                  len(batch.input_ids),   â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ dtype=torch.int32                               â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    305              ).to(device,                â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ non_blocking=True),                             â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    306                                          â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ tbo_split_seq_index=batch.tbo_split_seq_index,  â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    307          )                               â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    308                                          â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    309          # For DP attention              â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    310          if batch.global_num_tokens is   â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ not None:                                       â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    311              ret.global_num_tokens_cpu = â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ batch.global_num_tokens                         â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    312              ret.global_num_tokens_gpu = â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ torch.tensor(                                   â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    313                                          â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ batch.global_num_tokens, dtype=torch.int64      â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    314              ).to(device,                â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ non_blocking=True)                              â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    315                                          â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    316                                          â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ ret.global_num_tokens_for_logprob_cpu =         â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ batch.global_num_tokens_for_logprob             â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    317                                          â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ ret.global_num_tokens_for_logprob_gpu =         â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ torch.tensor(                                   â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    318                                          â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ batch.global_num_tokens_for_logprob,            â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ dtype=torch.int64                               â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    319              ).to(device,                â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ non_blocking=True)                              â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    320                                          â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    321              sum_len =                   â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ sum(batch.global_num_tokens)                    â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    322              ret.gathered_buffer =       â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ torch.zeros(                                    â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    323                  (sum_len,               â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ model_runner.model_config.hidden_size),         â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    324                                          â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ dtype=model_runner.dtype,                       â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    325                  device=device,          â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    326              )                           â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    327          if ret.forward_mode.is_idle():  â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    328              ret.positions =             â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ torch.empty((0,), device=device)                â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    329                                          â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ TboForwardBatchPreparer.prepare(ret)            â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    330              return ret                  â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    331                                          â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    332          # Override the positions with   â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ spec_info                                       â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    333          if (                            â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    334              ret.spec_info is not None   â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    335              and getattr(ret.spec_info,  â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ "positions", None) is not None                  â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    336          ):                              â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    337              ret.positions =             â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ ret.spec_info.positions                         â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    338                                          â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    339          # Get seq_lens_cpu if needed    â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    340          if ret.seq_lens_cpu is None:    â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    341              ret.seq_lens_cpu =          â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ batch.seq_lens_cpu                              â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    342                                          â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    343          # Init position information     â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    344          if                              â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ ret.forward_mode.is_decode():                   â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    345              if ret.positions is None:   â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    346                  ret.positions =         â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ clamp_position(batch.seq_lens)                  â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    347          else:                           â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    348              ret.extend_seq_lens =       â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ torch.tensor(                                   â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    349                  batch.extend_seq_lens,  â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ dtype=torch.int32                               â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    350              ).to(device,                â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ non_blocking=True)                              â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    351              ret.extend_prefix_lens =    â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ torch.tensor(                                   â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    352                                          â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ batch.extend_prefix_lens, dtype=torch.int32     â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    353              ).to(device,                â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ non_blocking=True)                              â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    354              if                          â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ model_runner.server_args.attention_backend !=   â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ "torch_native":                                 â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    355                  ret.extend_num_tokens = â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ batch.extend_num_tokens                         â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    356                  positions,              â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ ret.extend_start_loc = compute_position_triton( â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    357                                          â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ ret.extend_prefix_lens,                         â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    358                                          â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ ret.extend_seq_lens,                            â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    359                                          â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ ret.extend_num_tokens,                          â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    360                  )                       â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    361              else:                       â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    362                  positions,              â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ ret.extend_start_loc = compute_position_torch(  â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    363                                          â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ ret.extend_prefix_lens, ret.extend_seq_lens     â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    364                  )                       â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    365              if ret.positions is None:   â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    366                  ret.positions =         â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ positions                                       â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    367              ret.extend_prefix_lens_cpu  â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ = batch.extend_prefix_lens                      â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    368              ret.extend_seq_lens_cpu =   â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ batch.extend_seq_lens                           â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    369                                          â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ ret.extend_logprob_start_lens_cpu =             â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ batch.extend_logprob_start_lens                 â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    370                                          â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    371          if model_runner.model_is_mrope: â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    372                                          â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ ret._compute_mrope_positions(model_runner,      â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ batch)                                          â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    373                                          â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    374          # Init lora information         â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    375          if                              â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ model_runner.server_args.lora_paths is not      â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ None:                                           â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    376                                          â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ model_runner.lora_manager.prepare_lora_batch(râ€¦ â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    377                                          â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    378                                          â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ TboForwardBatchPreparer.prepare(ret)            â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    379                                          â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    380          return ret                      â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    381                                          â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    382      def merge_mm_inputs(self) ->        â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ Optional[MultimodalInputs]:                     â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    383          """                             â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    384          Merge all multimodal inputs in  â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ the batch into a single MultiModalInputs        â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ object.                                         â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    385                                          â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    386          Returns:                        â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    387              if none, current batch      â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ contains no multimodal input                    â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    388                                          â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    389          """                             â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    390          if not self.mm_inputs or all(x  â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ is None for x in self.mm_inputs):               â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    391              return None                 â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    392          # Filter out None values        â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    393          valid_inputs =                  â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    394                                          â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    395          # TODO: is it expensive?        â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    396          # a workaround to avoid         â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ importing `MultimodalInputs`                    â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    397          merged =                        â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ valid_inputs[0].__class__(mm_items=[])          â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    398                                          â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    399          # Merge remaining inputs        â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    400          for mm_input in valid_inputs:   â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    401              merged.merge(mm_input)      â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    402                                          â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    403          return merged                   â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    404                                          â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    405      def contains_image_inputs(self) ->  â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ bool:                                           â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    406          if self.mm_inputs is None:      â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    407              return False                â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    408          return any(                     â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    409              mm_input is not None and    â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ mm_input.contains_image_inputs()                â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    410              for mm_input in             â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ self.mm_inputs                                  â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    411          )                               â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    412                                          â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    413      def contains_audio_inputs(self) ->  â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ bool:                                           â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    414          if self.mm_inputs is None:      â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    415              return False                â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    416          return any(                     â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    417              mm_input is not None and    â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ mm_input.contains_audio_inputs()                â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    418              for mm_input in             â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ self.mm_inputs                                  â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    419          )                               â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    420                                          â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    421      def contains_mm_inputs(self) ->     â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ bool:                                           â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    422          return                          â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ self.contains_audio_inputs() or se<response     â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ clipped><NOTE>To save on context only part of   â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ this file has been shown to you. You should     â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ retry this tool after you have searched inside  â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ the file with `grep -n` in order to find the    â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ line numbers of what you are looking            â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ for.</NOTE>                                     â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                                                 â”‚   â”‚
â”‚             â”‚ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚
â”‚ Total       â”‚ Input: 385329 Output: 4134                                                                             â”‚
â”‚ Tokens      â”‚                                                                                                        â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Status      â”‚ âœ… Step 17: Completed                                                                                  â”‚
â”‚ Tools       â”‚ ðŸ”§ bash                                                                                                â”‚
â”‚ bash        â”‚ â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚
â”‚             â”‚ â”‚ {'command': 'grep -n                           â”‚                                                 â”‚   â”‚
â”‚             â”‚ â”‚ "fill_\\|zero_\\|zeros(\\|empty(\\|torch\\.fuâ€¦ â”‚                                                 â”‚   â”‚
â”‚             â”‚ â”‚ /home/ubuntu/OmniPerf-Bench/perf-agents-benchâ€¦ â”‚                                                 â”‚   â”‚
â”‚             â”‚ â”‚ | sed -n \'1,200p\'', 'restart': False}        â”‚                                                 â”‚   â”‚
â”‚             â”‚ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚
â”‚ Total       â”‚ Input: 434832 Output: 6477                                                                             â”‚
â”‚ Tokens      â”‚                                                                                                        â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Status      â”‚ âœ… Step 18: Completed                                                                                  â”‚
â”‚ Tools       â”‚ ðŸ”§ bash                                                                                                â”‚
â”‚ bash        â”‚ â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚
â”‚             â”‚ â”‚ {'command': 'grep -n                           â”‚ 327:            gathered_buffer = torch.zeros(  â”‚   â”‚
â”‚             â”‚ â”‚ "fill_\\|zero_\\|zeros(\\|empty("              â”‚ 395:    zero_allocator: Optional[BumpAllocator] â”‚   â”‚
â”‚             â”‚ â”‚ /home/ubuntu/OmniPerf-Bench/perf-agents-benchâ€¦ â”‚ = None,                                         â”‚   â”‚
â”‚             â”‚ â”‚ | sed -n \'1,200p\'', 'restart': False}        â”‚ 402:        zero_allocator=zero_allocator,      â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ 452:    zero_allocator:                         â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ Optional[BumpAllocator],                        â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ 474:        zero_allocator=zero_allocator,      â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ 502:    zero_allocator:                         â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ Optional[BumpAllocator],                        â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ 514:                                            â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ dict(zero_allocator=zero_allocator)             â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ 515:                if zero_allocator is not    â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ None                                            â”‚   â”‚
â”‚             â”‚ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚
â”‚ Total       â”‚ Input: 486734 Output: 6571                                                                             â”‚
â”‚ Tokens      â”‚                                                                                                        â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Status      â”‚ âœ… Step 19: Completed                                                                                  â”‚
â”‚ Tools       â”‚ ðŸ”§ bash                                                                                                â”‚
â”‚ bash        â”‚ â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚
â”‚             â”‚ â”‚ {'command': 'grep -n                           â”‚                                                 â”‚   â”‚
â”‚             â”‚ â”‚ "op_select_experts|select_experts\\(|num_tokeâ€¦ â”‚                                                 â”‚   â”‚
â”‚             â”‚ â”‚ /home/ubuntu/OmniPerf-Bench/perf-agents-benchâ€¦ â”‚                                                 â”‚   â”‚
â”‚             â”‚ â”‚ | sed -n \'1,200p\'', 'restart': False}        â”‚                                                 â”‚   â”‚
â”‚             â”‚ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚
â”‚ Total       â”‚ Input: 538824 Output: 6735                                                                             â”‚
â”‚ Tokens      â”‚                                                                                                        â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Status      â”‚ âœ… Step 20: Completed                                                                                  â”‚
â”‚ Tools       â”‚ ðŸ”§ bash                                                                                                â”‚
â”‚ bash        â”‚ â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚
â”‚             â”‚ â”‚ {'command': 'grep -n                           â”‚ 354:            topk_weights, topk_idx =        â”‚   â”‚
â”‚             â”‚ â”‚ "op_select_experts\\|select_experts(\\|num_toâ€¦ â”‚ select_experts(                                 â”‚   â”‚
â”‚             â”‚ â”‚ /home/ubuntu/OmniPerf-Bench/perf-agents-benchâ€¦ â”‚ 364:                                            â”‚   â”‚
â”‚             â”‚ â”‚ | sed -n \'1,200p\'', 'restart': False}        â”‚ num_token_non_padded=forward_batch.num_token_nâ€¦ â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ 442:    def op_select_experts(self, state):     â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ 447:            state.topk_weights_local,       â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ state.topk_idx_local = select_experts(          â”‚   â”‚
â”‚             â”‚ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚
â”‚ Total       â”‚ Input: 591147 Output: 6835                                                                             â”‚
â”‚ Tokens      â”‚                                                                                                        â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Status      â”‚ âœ… Step 21: Completed                                                                                  â”‚
â”‚ Tools       â”‚ ðŸ”§ str_replace_based_edit_tool                                                                         â”‚
â”‚ str_replacâ€¦ â”‚ â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚
â”‚             â”‚ â”‚ {'command': 'view', 'file_text': '',           â”‚ Here's the result of running `cat -n` on        â”‚   â”‚
â”‚             â”‚ â”‚ 'insert_line': 1, 'new_str': '', 'old_str':    â”‚ /home/ubuntu/OmniPerf-Bench/perf-agents-bench/â€¦ â”‚   â”‚
â”‚             â”‚ â”‚ '', 'path':                                    â”‚    430          else:                           â”‚   â”‚
â”‚             â”‚ â”‚ '/home/ubuntu/OmniPerf-Bench/perf-agents-bencâ€¦ â”‚    431              state.router_logits = None  â”‚   â”‚
â”‚             â”‚ â”‚ 'view_range': [430, 520]}                      â”‚    432                                          â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    433      def op_shared_experts(self, state): â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    434          hidden_states_mlp_input =       â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ state.pop("hidden_states_mlp_input")            â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    435          if (self.n_share_experts_fusion â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ == 0) and is_non_idle_and_non_empty(            â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    436                                          â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ state.forward_batch.forward_mode,               â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ hidden_states_mlp_input                         â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    437          ):                              â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    438              state.shared_output =       â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ self.shared_experts(hidden_states_mlp_input)    â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    439          else:                           â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    440              state.shared_output = None  â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    441                                          â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    442      def op_select_experts(self, state): â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    443          router_logits =                 â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ state.pop("router_logits")                      â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    444          hidden_states =                 â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ state.hidden_states_mlp_input                   â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    445                                          â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    446          if router_logits is not None:   â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    447              state.topk_weights_local,   â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ state.topk_idx_local = select_experts(          â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    448                                          â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ hidden_states=hidden_states,                    â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    449                                          â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ router_logits=router_logits,                    â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    450                  top_k=self.top_k,       â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    451                  use_grouped_topk=True,  â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    452                                          â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ renormalize=self.renormalize,                   â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    453                                          â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ topk_group=self.topk_group,                     â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    454                                          â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ num_expert_group=self.num_expert_group,         â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    455                                          â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ correction_bias=self.correction_bias,           â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    456                                          â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ routed_scaling_factor=self.routed_scaling_factâ€¦ â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    457                                          â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ expert_location_dispatch_info=ExpertLocationDiâ€¦ â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    458                                          â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ layer_id=self.layer_id,                         â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    459                  ),                      â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    460              )                           â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    461          else:                           â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    462              state.topk_idx_local =      â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ torch.full(                                     â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    463                  (0, self.top_k), -1,    â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ dtype=torch.int, device=hidden_states.device    â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    464              )                           â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    465              state.topk_weights_local =  â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ torch.empty(                                    â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    466                  (0, self.top_k),        â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ dtype=torch.float32,                            â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ device=hidden_states.device                     â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    467              )                           â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    468                                          â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    469      def op_dispatch_a(self, state):     â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    470          if self.ep_size > 1:            â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    471              # TODO(ch-wan): allow users â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ to set num_max_dispatch_tokens_per_rank value   â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    472                                          â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ self.deepep_dispatcher.dispatch_a(              â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    473                                          â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ hidden_states=state.hidden_states_mlp_input,    â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    474                                          â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ topk_idx=state.pop("topk_idx_local"),           â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    475                                          â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ topk_weights=state.pop("topk_weights_local"),   â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    476                                          â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ forward_mode=state.forward_batch.forward_mode,  â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    477                                          â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ tbo_subbatch_index=state.get("tbo_subbatch_indâ€¦ â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    478              )                           â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    479                                          â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    480      def op_dispatch_b(self, state):     â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    481          if self.ep_size > 1:            â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    482              with                        â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ get_global_expert_distribution_recorder().withâ€¦ â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    483                  self.layer_id           â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    484              ):                          â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    485                  (                       â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    486                                          â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ state.hidden_states_experts_input,              â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    487                                          â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ state.topk_idx_dispatched,                      â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    488                                          â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ state.topk_weights_dispatched,                  â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    489                                          â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ state.reorder_topk_ids,                         â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    490                                          â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ state.num_recv_tokens_per_expert,               â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    491                      state.seg_indptr,   â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    492                      state.masked_m,     â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    493                      state.expected_m,   â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    494                  ) =                     â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ self.deepep_dispatcher.dispatch_b(              â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    495                                          â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ tbo_subbatch_index=state.get("tbo_subbatch_indâ€¦ â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    496                  )                       â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    497                                          â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    498      def op_experts(self, state):        â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    499                                          â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ state.hidden_states_experts_output =            â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ self.experts(                                   â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    500                                          â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ hidden_states=state.pop("hidden_states_expertsâ€¦ â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    501                                          â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ topk_idx=state.topk_idx_dispatched,             â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    502                                          â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ topk_weights=state.topk_weights_dispatched,     â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    503                                          â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ reorder_topk_ids=state.pop("reorder_topk_ids"), â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    504                                          â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ seg_indptr=state.pop("seg_indptr"),             â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    505                                          â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ masked_m=state.pop("masked_m"),                 â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    506                                          â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ expected_m=state.pop("expected_m"),             â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    507                                          â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ num_recv_tokens_per_expert=state.pop("num_recvâ€¦ â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    508                                          â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ forward_mode=state.forward_batch.forward_mode,  â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    509          )                               â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    510                                          â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    511      def op_combine_a(self, state):      â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    512          if self.ep_size > 1:            â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    513                                          â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ self.deepep_dispatcher.combine_a(               â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    514                                          â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ hidden_states=state.pop("hidden_states_expertsâ€¦ â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    515                                          â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ topk_idx=state.pop("topk_idx_dispatched"),      â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    516                                          â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ topk_weights=state.pop("topk_weights_dispatcheâ€¦ â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    517                                          â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ forward_mode=state.forward_batch.forward_mode,  â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    518                                          â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ tbo_subbatch_index=state.get("tbo_subbatch_indâ€¦ â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    519              )                           â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    520                                          â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                                                 â”‚   â”‚
â”‚             â”‚ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚
â”‚ Total       â”‚ Input: 643644 Output: 6927                                                                             â”‚
â”‚ Tokens      â”‚                                                                                                        â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Status      â”‚ âœ… Step 22: Completed                                                                                  â”‚
â”‚ Tools       â”‚ ðŸ”§ str_replace_based_edit_tool                                                                         â”‚
â”‚ str_replacâ€¦ â”‚ â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚
â”‚             â”‚ â”‚ {'command': 'view', 'file_text': '',           â”‚ Here's the result of running `cat -n` on        â”‚   â”‚
â”‚             â”‚ â”‚ 'insert_line': 1, 'new_str': '', 'old_str':    â”‚ /home/ubuntu/OmniPerf-Bench/perf-agents-bench/â€¦ â”‚   â”‚
â”‚             â”‚ â”‚ '', 'path':                                    â”‚      1  import logging                          â”‚   â”‚
â”‚             â”‚ â”‚ '/home/ubuntu/OmniPerf-Bench/perf-agents-bencâ€¦ â”‚      2  from dataclasses import dataclass       â”‚   â”‚
â”‚             â”‚ â”‚ 'view_range': [1, -1]}                         â”‚      3                                          â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚      4  from                                    â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ sglang.srt.layers.quantization.deep_gemm import â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ _ENABLE_JIT_DEEPGEMM                            â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚      5  from                                    â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ sglang.srt.managers.expert_distribution import  â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ (                                               â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚      6                                          â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ get_global_expert_distribution_recorder,        â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚      7  )                                       â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚      8  from sglang.srt.managers.schedule_batch â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ import global_server_args_dict                  â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚      9  from sglang.srt.utils import            â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ DeepEPMode, load_json_config                    â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚     10                                          â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚     11  try:                                    â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚     12      from deep_ep import Buffer, Config  â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚     13                                          â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚     14      from                                â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ sglang.srt.layers.quantization.fp8_kernel       â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ import (                                        â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚     15                                          â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ sglang_per_token_group_quant_fp8,               â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚     16      )                                   â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚     17                                          â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚     18      use_deepep = True                   â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚     19  except ImportError:                     â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚     20      use_deepep = False                  â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚     21                                          â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚     22  from enum import Enum, IntEnum, auto    â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚     23  from typing import Optional, Tuple,     â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ Union                                           â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚     24                                          â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚     25  import torch                            â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚     26  import torch.distributed as dist        â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚     27                                          â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚     28  from                                    â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ sglang.srt.layers.moe.ep_moe.kernels import (   â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚     29      deepep_permute_triton_kernel,       â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚     30      deepep_post_reorder_triton_kernel,  â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚     31      deepep_run_moe_deep_preprocess,     â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚     32  )                                       â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚     33  from                                    â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ sglang.srt.model_executor.forward_batch_info    â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ import ForwardMode                              â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚     34                                          â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚     35  logger = logging.getLogger(__name__)    â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚     36                                          â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚     37                                          â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚     38  class DeepEPDispatchMode(IntEnum):      â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚     39      NORMAL = auto()                     â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚     40      LOW_LATENCY = auto()                â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚     41                                          â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚     42                                          â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚     43  class DeepEPBuffer:                     â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚     44      _buffer = None                      â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚     45      _dispatch_mode:                     â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ Optional[DeepEPDispatchMode] = None             â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚     46      _hidden_size: Optional = None       â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚     47      _num_max_dispatch_tokens_per_rank:  â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ Optional = None                                 â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚     48      _num_experts: Optional = None       â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚     49                                          â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚     50      @classmethod                        â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚     51      def get_deepep_buffer(              â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚     52          cls,                            â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚     53          group: dist.ProcessGroup,       â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚     54          hidden_size: int,               â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚     55          param_bytes: int,               â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚     56          deepep_mode: DeepEPMode,        â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚     57                                          â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ num_max_dispatch_tokens_per_rank: int = None,   â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚     58          num_experts: int = None,        â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚     59      ):                                  â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚     60          if cls._buffer is not None:     â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚     61              return cls._buffer          â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚     62                                          â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚     63          cls._hidden_size = hidden_size  â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚     64                                          â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ cls._num_max_dispatch_tokens_per_rank =         â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ num_max_dispatch_tokens_per_rank                â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚     65          cls._num_experts = num_experts  â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚     66                                          â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚     67          num_nvl_bytes, num_rdma_bytes = â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ 0, 0                                            â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚     68          if deepep_mode.enable_normal(): â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚     69              hidden_bytes = hidden_size  â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ * param_bytes                                   â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚     70              for config in (             â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚     71                                          â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ DeepEPConfig.get_instance().normal_dispatch_coâ€¦ â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚     72                  or                      â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ Buffer.get_dispatch_config(group.size()),       â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚     73                                          â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ DeepEPConfig.get_instance().normal_combine_conâ€¦ â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚     74                  or                      â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ Buffer.get_combine_config(group.size()),        â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚     75              ):                          â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚     76                  num_nvl_bytes = max(    â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚     77                                          â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ config.get_nvl_buffer_size_hint(hidden_bytes,   â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ group.size()),                                  â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚     78                      num_nvl_bytes,      â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚     79                  )                       â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚     80                  num_rdma_bytes = max(   â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚     81                                          â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ config.get_rdma_buffer_size_hint(hidden_bytes,  â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ group.size()),                                  â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚     82                      num_rdma_bytes,     â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚     83                  )                       â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚     84          if                              â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ deepep_mode.enable_low_latency():               â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚     85              assert                      â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ num_max_dispatch_tokens_per_rank is not None    â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚     86              assert num_experts is not   â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ None and num_experts % group.size() == 0        â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚     87              num_rdma_bytes = max(       â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚     88                                          â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ Buffer.get_low_latency_rdma_size_hint(          â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚     89                                          â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ num_max_dispatch_tokens_per_rank,               â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚     90                      hidden_size,        â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚     91                      group.size(),       â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚     92                      num_experts,        â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚     93                  ),                      â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚     94                  num_rdma_bytes,         â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚     95              )                           â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚     96                                          â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚     97          if deepep_mode ==               â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ DeepEPMode.normal:                              â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚     98              num_qps_per_rank =          â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ DeepEPConfig.get_instance().num_sms // 2        â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚     99          elif deepep_mode in             â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ [DeepEPMode.low_latency, DeepEPMode.auto]:      â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    100              num_qps_per_rank =          â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ num_experts // group.size()                     â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    101          else:                           â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    102              raise NotImplementedError   â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    103                                          â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    104          cls._buffer = Buffer(           â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    105              group,                      â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    106              num_nvl_bytes,              â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    107              num_rdma_bytes,             â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    108                                          â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ low_latency_mode=deepep_mode.enable_low_latencâ€¦ â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    109                                          â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ num_qps_per_rank=num_qps_per_rank,              â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    110          )                               â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    111          return cls._buffer              â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    112                                          â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    113      @classmethod                        â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    114      def clean_buffer(cls):              â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    115          if not                          â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ cls._buffer.low_latency_mode:                   â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    116              return                      â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    117                                          â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ cls._buffer.clean_low_latency_buffer(           â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    118                                          â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ cls._num_max_dispatch_tokens_per_rank,          â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    119              cls._hidden_size,           â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    120              cls._num_experts,           â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    121          )                               â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    122                                          â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    123      @classmethod                        â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    124      def                                 â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ set_dispatch_mode_as_normal(cls):               â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    125          cls._dispatch_mode =            â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ DeepEPDispatchMode.NORMAL                       â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    126                                          â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    127      @classmethod                        â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    128      def                                 â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ set_dispatch_mode_as_low_latency(cls):          â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    129          if cls._dispatch_mode ==        â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ DeepEPDispatchMode.NORMAL:                      â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    130              cls.clean_buffer()          â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    131          cls._dispatch_mode =            â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ DeepEPDispatchMode.LOW_LATENCY                  â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    132                                          â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    133                                          â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    134  class DeepEPConfig:                     â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    135      _instance = None                    â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    136                                          â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    137      def __init__(self):                 â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    138          config_str =                    â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ global_server_args_dict["deepep_config"]        â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    139          if config_str:                  â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    140              config_parsed =             â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ load_json_config(config_str)                    â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    141              if                          â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ torch.distributed.get_rank() == 0:              â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    142                  logger.info(f"Use       â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ DeepEP Config: {config_parsed}")                â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    143              config_dispatch =           â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ config_parsed["normal_dispatch"]                â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    144              config_combine =            â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ config_parsed["normal_combine"]                 â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    145                                          â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    146              self.normal_dispatch_config â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ = Config(**config_dispatch)                     â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    147              self.normal_combine_config  â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ = Config(**config_combine)                      â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    148                                          â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    149              assert                      â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ config_dispatch["num_sms"] ==                   â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ config_combine["num_sms"]                       â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    150              self.num_sms =              â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ config_dispatch["num_sms"]                      â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    151          else:                           â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    152              self.normal_dispatch_config â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ = None                                          â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    153              self.normal_combine_config  â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ = None                                          â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    154              self.num_sms =              â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ Buffer.num_sms                                  â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    155                                          â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    156      @classmethod                        â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    157      def get_instance(cls):              â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    158          if cls._instance is None:       â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    159              cls._instance =             â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ DeepEPConfig()                                  â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    160          return cls._instance            â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    161                                          â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    162                                          â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    163  class _DeepEPDispatcherImplBase:        â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    164      def __init__(                       â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    165          self,                           â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    166          group:                          â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ torch.distributed.ProcessGroup,                 â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    167          router_topk: int,               â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    168          permute_fusion: bool,           â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    169          num_experts: int,               â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    170          num_local_experts: int,         â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    171          hidden_size: int,               â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    172          params_dtype: torch.dtype,      â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    173          deepep_mode: DeepEPMode,        â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    174      ):                                  â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    175          if not use_deepep:              â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    176              raise ImportError(          â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    177                  "DeepEP is not          â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ installed. Please install DeepEP package from " â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    178                                          â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ "https://github.com/deepseek-ai/deepep."        â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    179              )                           â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    180                                          â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    181          self.group = group              â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    182          self.router_topk = router_topk  â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    183          self.permute_fusion =           â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ permute_fusion                                  â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    184          self.num_experts = num_experts  â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    185          self.num_local_experts =        â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ num_local_experts                               â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    186          self.hidden_size = hidden_size  â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    187          self.params_dtype =             â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ params_dtype                                    â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    188          self.deepep_mode = deepep_mode  â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    189                                          â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    190          self.params_bytes = 2           â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    191                                          â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ self.num_max_dispatch_tokens_per_rank = 128     â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    192                                          â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    193          self.handle = None              â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    194                                          â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    195      def dispatch_a(                     â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    196          self,                           â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    197          hidden_states: torch.Tensor,    â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    198          topk_idx: torch.Tensor,         â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    199          topk_weights: torch.Tensor,     â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    200      ):                                  â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    201          raise NotImplementedError       â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    202                                          â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    203      def dispatch_b(self, *args,         â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ **kwargs):                                      â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    204          raise NotImplementedError       â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    205                                          â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    206      def combine_a(                      â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    207          self,                           â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    208          hidden_states: torch.Tensor,    â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    209          topk_idx: torch.Tensor,         â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    210          topk_weights: torch.Tensor,     â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    211      ):                                  â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    212          raise NotImplementedError       â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    213                                          â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    214      def combine_b(self, *args,          â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ **kwargs):                                      â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    215          raise NotImplementedError       â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    216                                          â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    217      def _get_buffer(self):              â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    218          raise NotImplementedError       â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    219                                          â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    220                                          â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    221  class                                   â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ _DeepEPDispatcherImplNormal(_DeepEPDispatcherIâ€¦ â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    222      def __init__(self, async_finish:    â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ bool, **kwargs):                                â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    223          super().__init__(**kwargs)      â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    224                                          â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    225          self.async_finish =             â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ async_finish                                    â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    226          self.src2dst = None             â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    227                                          â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    228      def dispatch_a(                     â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    229          self,                           â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    230          hidden_states: torch.Tensor,    â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    231          topk_idx: torch.Tensor,         â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    232          topk_weights: torch.Tensor,     â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    233      ):                                  â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    234          topk_idx =                      â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ topk_idx.to(torch.int64)                        â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    235          if _ENABLE_JIT_DEEPGEMM:        â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    236              # TODO hard code 128 block  â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ quant,use fp8 communication                     â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    237              hidden_states =             â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ sglang_per_token_group_quant_fp8(hidden_states, â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ 128)                                            â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    238          previous_event =                â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ Buffer.capture() if self.async_finish else None â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    239          return hidden_states, topk_idx, â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ topk_weights, previous_event                    â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    240                                          â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    241      def dispatch_b(self, hidden_states, â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ topk_idx, topk_weights, previous_event):        â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    242          if _ENABLE_JIT_DEEPGEMM:        â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    243              (                           â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    244                  hidden_states,          â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    245                  topk_idx,               â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    246                  topk_weights,           â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    247                                          â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ num_recv_tokens_per_expert_list,                â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    248                  event,                  â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    249              ) = self._dispatch_core(    â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    250                  hidden_states,          â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ topk_idx, topk_weights, previous_event          â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    251              )                           â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    252              event.current_stream_wait() â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ if self.async_finish else ()                    â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    253              return (                    â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    254                  hidden_states,          â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    255                  topk_idx,               â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    256                  topk_weights,           â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    257                  None,                   â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    258                                          â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ num_recv_tokens_per_expert_list,                â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    259                  None,                   â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    260                  None,                   â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    261                  None,                   â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    262              )                           â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    263          else:                           â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    264              (                           â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    265                  hidden_states,          â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    266                  topk_idx,               â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    267                  topk_weights,           â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    268                                          â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ num_recv_tokens_per_expert_list,                â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    269                  event,                  â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    270              ) = self._dispatch_core(    â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    271                  hidden_states,          â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ topk_idx, topk_weights, previous_event          â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    272              )                           â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    273              event.current_stream_wait() â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ if self.async_finish else ()                    â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    274              if hidden_states.shape[0] > â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ 0:                                              â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    275                  reorder_topk_ids,       â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ seg_indptr, hidden_states =                     â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ self._deepep_permute(                           â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    276                      hidden_states,      â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ topk_idx, fp8_dtype=hidden_states.dtype         â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    277                  )                       â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    278              else:                       â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    279                  reorder_topk_ids =      â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ torch.empty(                                    â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    280                      (0,),               â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ device=hidden_states.device, dtype=torch.int64  â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    281                  )                       â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    282                  seg_indptr =            â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ torch.zeros(                                    â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    283                      (self.num_experts + â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ 1,),                                            â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    284                                          â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ device=hidden_states.device,                    â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    285                      dtype=torch.int64,  â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    286                  )                       â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    287                                          â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    288              masked_m = expected_m =     â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ None                                            â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    289              return (                    â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    290                  hidden_states,          â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    291                  topk_idx,               â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    292                  topk_weights,           â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    293                  reorder_topk_ids,       â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    294                  None,                   â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    295                  seg_indptr,             â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    296                  masked_m,               â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    297                  expected_m,             â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    298              )                           â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    299                                          â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    300      def _dispatch_core(                 â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    301          self,                           â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    302          x: Union[torch.Tensor, Tuple],  â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    303          topk_idx: torch.Tensor,         â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    304          topk_weights: torch.Tensor,     â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    305          previous_event,                 â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    306      ):                                  â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    307          buffer = self._get_buffer()     â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    308          (                               â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    309              num_tokens_per_rank,        â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    310              num_tokens_per_rdma_rank,   â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    311              num_tokens_per_expert,      â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    312              is_token_in_rank,           â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    313              previous_event,             â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    314          ) = buffer.get_dispatch_layout( â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    315              topk_idx,                   â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    316              self.num_experts,           â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    317                                          â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ previous_event=previous_event,                  â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    318                                          â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ async_finish=self.async_finish,                 â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    319                                          â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ allocate_on_comm_stream=previous_event is not   â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ None,                                           â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    320          )                               â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    321                                          â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    322          # FIXME: `handle` should be     â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ transmitted with tokens from dispatch to        â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ combine.                                        â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    323          # However, doing this would     â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ incur an unknown synchronization error, but     â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ keeping                                         â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    324          # `handle` as a member variable â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ works.                                          â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    325                                          â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    326          (                               â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    327              recv_x,                     â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    328              recv_topk_idx,              â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    329              recv_topk_weights,          â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    330                                          â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ num_recv_tokens_per_expert_list,                â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    331              self.handle,                â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    332              event,                      â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    333          ) = buffer.dispatch(            â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    334              x,                          â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    335              topk_idx=topk_idx,          â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    336              topk_weights=topk_weights,  â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    337                                          â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ num_tokens_per_rank=num_tokens_per_rank,        â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    338                                          â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ num_tokens_per_rdma_rank=num_tokens_per_rdma_râ€¦ â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    339                                          â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ is_token_in_rank=is_token_in_rank,              â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    340                                          â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ num_tokens_per_expert=num_tokens_per_expert,    â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    341                                          â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ previous_event=previous_event,                  â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    342                                          â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ async_finish=self.async_finish,                 â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    343                                          â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ allocate_on_comm_stream=(previous_event is not  â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ None) and self.async_finish,                    â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    344              expert_alignment=128 if     â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ _ENABLE_JIT_DEEPGEMM else 1,                    â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    345                                          â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ config=DeepEPConfig.get_instance().normal_dispâ€¦ â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    346          )                               â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    347                                          â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    348                                          â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ get_global_expert_distribution_recorder().on_dâ€¦ â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    349                                          â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ num_recv_tokens_per_expert_list,                â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    350                                          â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ num_tokens_per_rank=num_tokens_per_rank,        â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    351                                          â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ num_tokens_per_rdma_rank=num_tokens_per_rdma_râ€¦ â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    352                                          â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ num_tokens_per_expert=num_tokens_per_expert,    â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    353          )                               â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    354                                          â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    355          return (                        â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    356              recv_x,                     â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    357              recv_topk_idx,              â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    358              recv_topk_weights,          â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    359                                          â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ num_recv_tokens_per_expert_list,                â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    360              event,                      â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    361          )                               â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    362                                          â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    363      def _deepep_permute(                â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    364          self,                           â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    365          hidden_states: torch.Tensor,    â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    366          topk_idx: torch.Tensor,         â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    367          fp8_dtype: Optional = None,     â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    368          use_fp8_w8a8: bool = False,     â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    369          use_block_quant: bool = False,  â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    370      ):                                  â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    371          """                             â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    372          Copy from Megatron-Core         â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ token_dispatcher MoEFlexTokenDispatcher         â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    373                                          â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ https://github.com/NVIDIA/Megatron-LM/blob/maiâ€¦ â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    374          """                             â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    375                                          â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    376          reorder_topk_ids, self.src2dst, â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ seg_indptr = deepep_run_moe_deep_preprocess(    â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    377              topk_idx, self.num_experts  â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    378          )                               â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    379          num_total_tokens =              â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ reorder_topk_ids.numel()                        â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    380          gateup_input = torch.empty(     â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    381              (int(num_total_tokens),     â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ hidden_states.shape[1]),                        â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    382                                          â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ device=hidden_states.device,                    â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    383              dtype=(                     â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    384                  fp8_dtype               â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    385                  if (use_fp8_w8a8 and    â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ not use_block_quant)                            â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    386                  else                    â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ hidden_states.dtype                             â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    387              ),                          â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    388          )                               â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    389          # PreReorder                    â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    390                                          â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ deepep_permute_triton_kernel[(hidden_states.shâ€¦ â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    391              hidden_states,              â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    392              gateup_input,               â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    393              self.src2dst,               â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    394              topk_idx,                   â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    395              None,                       â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    396              self.router_topk,           â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    397              hidden_states.shape[1],     â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    398              BLOCK_SIZE=512,             â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    399          )                               â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    400          return reorder_topk_ids,        â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ seg_indptr, gateup_input                        â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    401                                          â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    402      def combine_a(                      â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    403          self,                           â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    404          hidden_states: torch.Tensor,    â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    405          topk_idx: torch.Tensor,         â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    406          topk_weights: torch.Tensor,     â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    407      ):                                  â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    408          if _ENABLE_JIT_DEEPGEMM:        â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    409              output = hidden_states      â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    410          else:                           â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    411              if hidden_states.shape[0] > â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ 0:                                              â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    412                  num_tokens =            â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ self.src2dst.shape[0] // self.router_topk       â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    413                  output = torch.empty(   â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    414                      (num_tokens,        â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ hidden_states.shape[1]),                        â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    415                                          â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ device=hidden_states.device,                    â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    416                                          â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ dtype=hidden_states.dtype,                      â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    417                  )                       â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    418                                          â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ deepep_post_reorder_triton_kernel[(num_tokens,â€¦ â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    419                      hidden_states,      â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    420                      output,             â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    421                      self.src2dst,       â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    422                      topk_idx,           â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    423                      topk_weights,       â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    424                      self.router_topk,   â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    425                                          â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ hidden_states.shape[1],                         â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    426                      BLOCK_SIZE=512,     â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    427                  )                       â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    428              else:                       â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    429                  output = torch.zeros(   â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    430                      (0,                 â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ hidden_states.shape[1]),                        â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    431                                          â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ device=hidden_states.device,                    â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    432                                          â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ dtype=hidden_states.dtype,                      â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    433                  )                       â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    434          previous_event =                â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ Buffer.capture() if self.async_finish else None â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    435          return output, previous_event   â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    436                                          â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    437      def combine_b(self, output,         â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ previous_event):                                â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    438          hidden_states, event =          â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ self._combine_core(output, previous_event)      â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    439          event.current_stream_wait() if  â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ self.async_finish else ()                       â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    440          self.handle = None              â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    441          self.src2dst = None             â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    442          return hidden_states            â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    443                                          â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    444      def _combine_core(self, x:          â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ torch.Tensor, previous_event):                  â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    445          buffer = self._get_buffer()     â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    446          combined_x, _, event =          â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ buffer.combine(                                 â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    447              x,                          â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    448              self.handle,                â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    449                                          â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ async_finish=self.async_finish,                 â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    450                                          â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ previous_event=previous_event,                  â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    451                                          â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ allocate_on_comm_stream=previous_event is not   â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ None,                                           â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    452                                          â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ config=DeepEPConfig.get_instance().normal_combâ€¦ â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    453          )                               â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    454          return combined_x, event        â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    455                                          â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    456      def _get_buffer(self):              â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    457                                          â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ DeepEPBuffer.set_dispatch_mode_as_normal()      â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    458                                          â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    459          return                          â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ DeepEPBuffer.get_deepep_buffer(                 â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    460              self.group,                 â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    461              self.hidden_size,           â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    462              self.params_bytes,          â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    463              self.deepep_mode,           â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    464                                          â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ self.num_max_dispatch_tokens_per_rank,          â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    465              self.num_experts,           â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    466          )                               â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    467                                          â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    468                                          â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    469  class                                   â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ _DeepEPDispatcherImplLowLatency(_DeepEPDispatcâ€¦ â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    470      def __init__(self,                  â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ return_recv_hook: bool, **kwargs):              â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    471          super().__init__(**kwargs)      â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    472                                          â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    473          """                             â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    474                                          â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ num_max_dispatch_tokens_per_rank: the actual    â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ batch size in the decoding engine should be     â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ less than 256                                   â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    475                                          â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ https://github.com/deepseek-ai/DeepEP?tab=readâ€¦ â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    476          """                             â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    477          self.return_recv_hook =         â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ return_recv_hook                                â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    478                                          â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    479      def dispatch_a(                     â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    480          self,                           â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    481          hidden_states: torch.Tensor,    â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    482          topk_idx: torch.Tensor,         â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    483          topk_weights: torch.Tensor,     â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    484      ):                                  â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    485          buffer = self._get_buffer()     â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    486          topk_idx =                      â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ topk_idx.to(torch.int64)                        â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    487          expected<response               â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ clipped><NOTE>To save on context only part of   â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ this file has been shown to you. You should     â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ retry this tool after you have searched inside  â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ the file with `grep -n` in order to find the    â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ line numbers of what you are looking            â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ for.</NOTE>                                     â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                                                 â”‚   â”‚
â”‚             â”‚ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚
â”‚ Total       â”‚ Input: 697384 Output: 7858                                                                             â”‚
â”‚ Tokens      â”‚                                                                                                        â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Status      â”‚ âœ… Step 23: Completed                                                                                  â”‚
â”‚ LLM         â”‚ ðŸ’¬ I'll create the benchmark script in .bench_scratch to time relevant operations, then run it from    â”‚
â”‚ Response    â”‚ the repo root. This involves creating a new directory/file and executing the script.                   â”‚
â”‚ Tools       â”‚ ðŸ”§ bash                                                                                                â”‚
â”‚ bash        â”‚ â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚
â”‚             â”‚ â”‚ {'command': 'mkdir -p                          â”‚ Python 3.12.3                                   â”‚   â”‚
â”‚             â”‚ â”‚ /home/ubuntu/OmniPerf-Bench/perf-agents-benchâ€¦ â”‚                                                 â”‚   â”‚
â”‚             â”‚ â”‚ && python3 -V', 'restart': False}              â”‚                                                 â”‚   â”‚
â”‚             â”‚ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚
â”‚ Total       â”‚ Input: 757150 Output: 11041                                                                            â”‚
â”‚ Tokens      â”‚                                                                                                        â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Status      â”‚ âœ… Step 24: Completed                                                                                  â”‚
â”‚ Error       â”‚ âŒ Expecting ',' delimiter: line 107 column 912 (char 13714)                                           â”‚
â”‚ Total       â”‚ Input: 757150 Output: 11041                                                                            â”‚
â”‚ Tokens      â”‚                                                                                                        â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

============================================================
[1mLakeview Summary[0m
============================================================
â•­â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ Step 1 (Lakeview) â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•®
â”‚ [ðŸ‘ï¸EXAMINE_CODE] The agent [1m<task>The agent is listing repository files.[0m       â”‚
â”‚ [3mThe agent runs ls on the sglang_core worktree to locate files using [0m         â”‚
â”‚ [3mtorch.zeros/fill_ for optimization, planning to open and modify those [0m       â”‚
â”‚ [3mtargets next.[0m                                                                â”‚
â•°â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•¯
â•­â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ Step 2 (Lakeview) â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•®
â”‚ [ðŸ‘ï¸EXAMINE_CODE] The agent [1m<task>The agent is examining source code.[0m          â”‚
â”‚ [3mIt greps for torch.zeros or fill_ in moe_align_block_size.py and its [0m        â”‚
â”‚ [3mbenchmark to find candidate lines for optimization.[0m                          â”‚
â•°â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•¯
â•­â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ Step 3 (Lakeview) â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•®
â”‚ [ðŸ‘ï¸EXAMINE_CODE] The agent [1m<task>The agent is examining source code.[0m          â”‚
â”‚ [3mIt views python/sglang/srt/models/deepseek_v2.py using the edit tool to [0m     â”‚
â”‚ [3minspect the fileâ€™s contents before planning modifications related to the [0m    â”‚
â”‚ [3missue.[0m                                                                       â”‚
â•°â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•¯
â•­â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ Step 4 (Lakeview) â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•®
â”‚ [ðŸ‘ï¸EXAMINE_CODE] The agent [1m<task>The agent is examining source code.[0m          â”‚
â”‚ [3mIt greps deepseek_v2.py for torch.zeros, fill_, torch.empty, cuda.graph, [0m    â”‚
â”‚ [3mtwo_batch_overlap, and TboCudaGraphRunnerPlugin to locate relevant code for [0m â”‚
â”‚ [3mthe suspected performance bug.[0m                                               â”‚
â•°â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•¯
â•­â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ Step 5 (Lakeview) â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•®
â”‚ [ðŸ‘ï¸EXAMINE_CODE] The agent [1m<task>The agent is examining source code.[0m          â”‚
â”‚ [3mIt views sglang/srt/two_batch_overlap.py using the edit tool to inspect its [0m â”‚
â”‚ [3mcontents and locate relevant sections for the suspected performance issue.[0m   â”‚
â•°â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•¯
â•­â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ Step 6 (Lakeview) â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•®
â”‚ [ðŸ‘ï¸EXAMINE_CODE] The agent [1m<task>The agent is examining source code.[0m          â”‚
â”‚ [3mIt greps two_batch_overlap.py for torch.zeros, fill_, empty, cuda.graph, [0m    â”‚
â”‚ [3mnum_token_non_padded, tbo_children, TboCudaGraphRunnerPlugin, [0m               â”‚
â”‚ [3mgathered_buffer, and BumpAllocator to identify candidate code areas for [0m     â”‚
â”‚ [3moptimization changes.[0m                                                        â”‚
â•°â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•¯
â•­â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ Step 7 (Lakeview) â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•®
â”‚ [ðŸ‘ï¸EXAMINE_CODE] The agent [1m<task>The agent is examining source code.[0m          â”‚
â”‚ [3mIt recursively greps the Python tree for "gathered_buffer" to locate [0m        â”‚
â”‚ [3mdefinitions/usages relevant to two-batch overlap and buffer allocation [0m      â”‚
â”‚ [3moptimizations.[0m                                                               â”‚
â•°â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•¯

Unexpected error: list index out of range
Traceback [1m([0mmost recent call last[1m)[0m:
  File "/home/ubuntu/OmniPerf-Bench/third-party/trae-agent/trae_agent/cli.py", line [1m212[0m, in run
    _ = [1masyncio.run[0m[1m([0m[1magent.run[0m[1m([0mtask, task_args[1m)[0m[1m)[0m
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/lib/python3.12/asyncio/runners.py", line [1m194[0m, in run
    return [1mrunner.run[0m[1m([0mmain[1m)[0m
           ^^^^^^^^^^^^^^^^
  File "/usr/lib/python3.12/asyncio/runners.py", line [1m118[0m, in run
    return [1mself._loop.run_until_complete[0m[1m([0mtask[1m)[0m
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/lib/python3.12/asyncio/base_events.py", line [1m687[0m, in run_until_complete
    return [1mfuture.result[0m[1m([0m[1m)[0m
           ^^^^^^^^^^^^^^^
  File "/home/ubuntu/OmniPerf-Bench/third-party/trae-agent/trae_agent/agent/agent.py", line [1m93[0m, in run
    await cli_console_task
  File "/home/ubuntu/OmniPerf-Bench/third-party/trae-agent/trae_agent/utils/cli/simple_console.py", line [1m119[0m, in start
    await [1mself._print_lakeview_summary[0m[1m([0m[1m)[0m
  File "/home/ubuntu/OmniPerf-Bench/third-party/trae-agent/trae_agent/utils/cli/simple_console.py", line [1m154[0m, in _print_lakeview_summary
    lake_view_panel = await step.lake_view_panel_generator
                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/OmniPerf-Bench/third-party/trae-agent/trae_agent/utils/cli/simple_console.py", line [1m306[0m, in _create_lakeview_step_display
    lake_view_step = await [1mself.lake_view.create_lakeview_step[0m[1m([0magent_step[1m)[0m
                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/OmniPerf-Bench/third-party/trae-agent/trae_agent/utils/lake_view.py", line [1m205[0m, in create_lakeview_step
    tags = await [1mself.extract_tag_in_step[0m[1m([0mthis_step_str[1m)[0m
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/OmniPerf-Bench/third-party/trae-agent/trae_agent/utils/lake_view.py", line [1m169[0m, in extract_tag_in_step
    tags: list = [1m[[0m[1mtag.strip[0m[1m([0m[1m)[0m for tag in matched_tags[1m[[0m[1m0[0m[1m][0m[1m.split[0m[1m([0m","[1m)[0m[1m][0m
                                              ~~~~~~~~~~~~^^^
IndexError: list index out of range

Trajectory saved to: /home/ubuntu/OmniPerf-Bench/perf-agents-bench/state/runs/sglang_core-ae58875a/sglang_019_31589e17/trajectory.json