{"instance_id": "/home/ubuntu/OmniPerf-Bench/sglan__repo-cd68723", "model_patch": "diff --git a/python/sglang/srt/managers/router/model_runner.py b/python/sglang/srt/managers/router/model_runner.py\nindex c85ec534d..fdd215bff 100644\n--- a/python/sglang/srt/managers/router/model_runner.py\n+++ b/python/sglang/srt/managers/router/model_runner.py\n@@ -86,9 +86,10 @@ class InputMetadata:\n     decode_wrapper = None\n \n     def init_flashinfer_args(self, tp_size):\n-        self.kv_indptr = torch.zeros(\n+        self.kv_indptr = torch.empty(\n             (self.batch_size + 1,), dtype=torch.int32, device=\"cuda\"\n         )\n+        self.kv_indptr[0] = 0\n         self.kv_indptr[1:] = torch.cumsum(self.seq_lens, dim=0)\n         self.kv_indices = torch.cat(\n             [\n@@ -112,9 +113,10 @@ class InputMetadata:\n             self.forward_mode == ForwardMode.PREFILL\n             or self.forward_mode == ForwardMode.EXTEND\n         ):\n-            self.qo_indptr = torch.zeros(\n+            self.qo_indptr = torch.empty(\n                 (self.batch_size + 1,), dtype=torch.int32, device=\"cuda\"\n             )\n+            self.qo_indptr[0] = 0\n             self.qo_indptr[1:] = torch.cumsum(self.extend_seq_lens, dim=0)\n             self.prefill_wrapper = BatchPrefillWithPagedKVCacheWrapper()\n             self.prefill_wrapper.begin_forward(\n@@ -139,7 +141,8 @@ class InputMetadata:\n \n     def init_extend_args(self):\n         self.extend_seq_lens = self.seq_lens - self.prefix_lens\n-        self.extend_start_loc = torch.zeros_like(self.seq_lens)\n+        self.extend_start_loc = torch.empty_like(self.seq_lens)\n+        self.extend_start_loc[0] = 0\n         self.extend_start_loc[1:] = torch.cumsum(self.extend_seq_lens[:-1], dim=0)\n         self.max_extend_len = int(torch.max(self.extend_seq_lens))\n \n@@ -159,7 +162,8 @@ class InputMetadata:\n         return_logprob=False,\n     ):\n         batch_size = len(req_pool_indices)\n-        start_loc = torch.zeros((batch_size,), dtype=torch.int32, device=\"cuda\")\n+        start_loc = torch.empty((batch_size,), dtype=torch.int32, device=\"cuda\")\n+        start_loc[0] = 0\n         start_loc[1:] = torch.cumsum(seq_lens[:-1], dim=0)\n         total_num_tokens = int(torch.sum(seq_lens))\n         max_seq_len = int(torch.max(seq_lens))\n", "model_name_or_path": "gpt-5-2025-08-07"}
