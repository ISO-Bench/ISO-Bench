Changed working directory to: /home/ubuntu/OmniPerf-Bench/perf-agents-bench/.work/worktrees/sglang_core/sglang_033_73b13e69
Initialising MCP tools...
â•­â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ Task Details â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•®
â”‚ [1mTask:[0m I've uploaded a python code repository in the directory /home/ubuntu/OmniPerf-Bench/perf-agents-bench/.work/worktrees/sglang_core/sglang_033_73b13e69.     â”‚
â”‚ Consider the following test script showing an example usage of the repository:                                                                                   â”‚
â”‚                                                                                                                                                                  â”‚
â”‚ <test_script>                                                                                                                                                    â”‚
â”‚ # This is a performance optimization task                                                                                                                        â”‚
â”‚ # The specific operations to optimize are in the files listed below                                                                                              â”‚
â”‚ # Focus on performance improvements in the target functions                                                                                                      â”‚
â”‚                                                                                                                                                                  â”‚
â”‚ </test_script>                                                                                                                                                   â”‚
â”‚                                                                                                                                                                  â”‚
â”‚ Can you help me implement the necessary changes to the repository so that the runtime of the <test_script> is optimized?                                         â”‚
â”‚                                                                                                                                                                  â”‚
â”‚ Basic guidelines:                                                                                                                                                â”‚
â”‚ 1. Your task is to make changes to non-test files in the /home/ubuntu/OmniPerf-Bench/perf-agents-bench/.work/worktrees/sglang_core/sglang_033_73b13e69 directory â”‚
â”‚ to improve the performance of the <test_script>.                                                                                                                 â”‚
â”‚ 2. Make changes while ensuring the repository is functionally equivalent to the original.                                                                        â”‚
â”‚ 3. Do not overoptimize for just the specific inputs in <test_script>. Make general performance improvements for the usage scenario shown.                        â”‚
â”‚ 4. You may need to rebuild the repo for your changes to take effect before testing. Some rebuilds may take time to run, so be patient with running them.         â”‚
â”‚                                                                                                                                                                  â”‚
â”‚ Follow these steps to improve performance:                                                                                                                       â”‚
â”‚ 1. As a first step, explore the repository structure.                                                                                                            â”‚
â”‚ 2. Create a script ONLY inside /home/ubuntu/OmniPerf-Bench/perf-agents-bench/.work/worktrees/sglang_core/sglang_033_73b13e69/.bench_scratch (e.g.,               â”‚
â”‚ /home/ubuntu/OmniPerf-Bench/perf-agents-bench/.work/worktrees/sglang_core/sglang_033_73b13e69/.bench_scratch/test_opt.py) to reproduce and time the example,     â”‚
â”‚ then execute it with python <filename.py> from the repo root.                                                                                                    â”‚
â”‚ 3. Edit the source code of the repository to improve performance.                                                                                                â”‚
â”‚ 4. Rebuild and rerun your script to confirm that performance has improved.                                                                                       â”‚
â”‚                                                                                                                                                                  â”‚
â”‚ Here is an example of the kind of optimizations that have been shown to improve performance in this codebase:                                                    â”‚
â”‚                                                                                                                                                                  â”‚
â”‚ <example_optimization_diff>                                                                                                                                      â”‚
â”‚ diff --git a/python/sglang/srt/managers/scheduler.py b/python/sglang/srt/managers/scheduler.py                                                                   â”‚
â”‚ index 8253a303b..14ed362cf 100644                                                                                                                                â”‚
â”‚ --- a/python/sglang/srt/managers/scheduler.py                                                                                                                    â”‚
â”‚ +++ b/python/sglang/srt/managers/scheduler.py                                                                                                                    â”‚
â”‚ @@ -1399,29 +1399,6 @@ class Scheduler(                                                                                                                          â”‚
â”‚              self.metrics_collector.log_stats(self.stats)                                                                                                        â”‚
â”‚          self._publish_kv_events()                                                                                                                               â”‚
â”‚                                                                                                                                                                  â”‚
â”‚ -    def coordinate_spec_dp_attn_batch(self, new_batch: Optional[ScheduleBatch]):                                                                                â”‚
â”‚ -        """Coordinate the DP attention batch."""                                                                                                                â”‚
â”‚ -                                                                                                                                                                â”‚
â”‚ -        local_info = torch.tensor(                                                                                                                              â”‚
â”‚ -            [                                                                                                                                                   â”‚
â”‚ -                (new_batch is not None),                                                                                                                        â”‚
â”‚ -            ],                                                                                                                                                  â”‚
â”‚ -            dtype=torch.int64,                                                                                                                                  â”‚
â”‚ -        )                                                                                                                                                       â”‚
â”‚ -        global_info = torch.empty(                                                                                                                              â”‚
â”‚ -            (self.server_args.dp_size, self.attn_tp_size, 1),                                                                                                   â”‚
â”‚ -            dtype=torch.int64,                                                                                                                                  â”‚
â”‚ -        )                                                                                                                                                       â”‚
â”‚ -        torch.distributed.all_gather_into_tensor(                                                                                                               â”‚
â”‚ -            global_info.flatten(),                                                                                                                              â”‚
â”‚ -            local_info,                                                                                                                                         â”‚
â”‚ -            group=self.tp_cpu_group,                                                                                                                            â”‚
â”‚ -        )                                                                                                                                                       â”‚
â”‚ -        any_new_batch = any(                                                                                                                                    â”‚
â”‚ -            global_info[:, 0, 0].tolist()                                                                                                                       â”‚
â”‚ -        )  # Any DP worker has forward batch                                                                                                                    â”‚
â”‚ -        return any_new_batch                                                                                                                                    â”‚
â”‚ </example_optimization_diff>                                                                                                                                     â”‚
â”‚                                                                                                                                                                  â”‚
â”‚ IMPORTANT: The above diff is an EXAMPLE of optimizations that were successful in a different context.                                                            â”‚
â”‚ These changes have NOT been applied to your codebase yet.                                                                                                        â”‚
â”‚ Your task is to:                                                                                                                                                 â”‚
â”‚ 1. Understand the optimization pattern shown (e.g., torch.zeros â†’ torch.empty)                                                                                   â”‚
â”‚ 2. Look at the CURRENT code in the target files                                                                                                                  â”‚
â”‚ 3. Find places where you can apply SIMILAR optimizations                                                                                                         â”‚
â”‚ 4. MAKE THE CHANGES yourself using str_replace_editor                                                                                                            â”‚
â”‚                                                                                                                                                                  â”‚
â”‚ The codebase you're working with is at the BASE commit - it does NOT have these optimizations yet.                                                               â”‚
â”‚ You need to IMPLEMENT similar optimizations yourself.                                                                                                            â”‚
â”‚                                                                                                                                                                  â”‚
â”‚ HERE'S WHAT YOU NEED TO DO:                                                                                                                                      â”‚
â”‚ 1. The files CURRENTLY contain torch.zeros() calls that need optimization                                                                                        â”‚
â”‚ 2. You need to CHANGE torch.zeros to torch.empty where appropriate                                                                                               â”‚
â”‚ 3. You need to REMOVE .fill_() operations that are unnecessary                                                                                                   â”‚
â”‚ 4. These are NEW changes you're making - not already in the code                                                                                                 â”‚
â”‚                                                                                                                                                                  â”‚
â”‚ START WITH THIS COMMAND to see what needs changing:                                                                                                              â”‚
â”‚ ```bash                                                                                                                                                          â”‚
â”‚ grep -n 'torch.zeros\|fill_' vllm/model_executor/layers/fused_moe/moe_align_block_size.py benchmarks/kernels/benchmark_moe_align_block_size.py                   â”‚
â”‚ ```                                                                                                                                                              â”‚
â”‚                                                                                                                                                                  â”‚
â”‚ CRITICAL: You MUST make actual code changes. Look for patterns like:                                                                                             â”‚
â”‚ - Analyze the target files for performance bottlenecks                                                                                                           â”‚
â”‚ - Look for unnecessary memory allocations or initializations                                                                                                     â”‚
â”‚ - Consider more efficient algorithms or data structures                                                                                                          â”‚
â”‚                                                                                                                                                                  â”‚
â”‚ Target files to optimize:                                                                                                                                        â”‚
â”‚ - python/sglang/srt/managers/scheduler.py                                                                                                                        â”‚
â”‚                                                                                                                                                                  â”‚
â”‚ IMPORTANT: You MUST make actual code changes to at least one file.                                                                                               â”‚
â”‚ The task will fail if no files are modified.                                                                                                                     â”‚
â”‚                                                                                                                                                                  â”‚
â”‚ ## Constraints                                                                                                                                                   â”‚
â”‚ - No public API breakage                                                                                                                                         â”‚
â”‚ - All TestPack checks must pass                                                                                                                                  â”‚
â”‚                                                                                                                                                                  â”‚
â”‚ ## Target Files (ONLY modify these)                                                                                                                              â”‚
â”‚ - `python/sglang/srt/managers/scheduler.py`                                                                                                                      â”‚
â”‚                                                                                                                                                                  â”‚
â”‚ ## SPECIFIC OPTIMIZATION TARGETS:                                                                                                                                â”‚
â”‚ Based on the human commit analysis, focus on these areas:                                                                                                        â”‚
â”‚ - Memory allocation patterns (torch.zeros vs torch.empty)                                                                                                        â”‚
â”‚ - Tensor initialization strategies                                                                                                                               â”‚
â”‚ - Kernel parameter optimization                                                                                                                                  â”‚
â”‚ - Buffer reuse and caching                                                                                                                                       â”‚
â”‚                                                                                                                                                                  â”‚
â”‚ ### Human Developer's Approach:                                                                                                                                  â”‚
â”‚ ```                                                                                                                                                              â”‚
â”‚ Optimize DP attn scheduling for speculative decoding (#7285)                                                                                                     â”‚
â”‚ ```                                                                                                                                                              â”‚
â”‚                                                                                                                                                                  â”‚
â”‚ ### Files Modified (statistics):                                                                                                                                 â”‚
â”‚ ```                                                                                                                                                              â”‚
â”‚ python/sglang/srt/managers/scheduler.py | 44 +++++++++------------------------                                                                                   â”‚
â”‚  1 file changed, 12 insertions(+), 32 deletions(-)                                                                                                               â”‚
â”‚ ```                                                                                                                                                              â”‚
â”‚                                                                                                                                                                  â”‚
â”‚ ## IMMEDIATE ACTION REQUIREMENTS:                                                                                                                                â”‚
â”‚ 1. Start editing files by iteration 3                                                                                                                            â”‚
â”‚ 2. Create and run /home/ubuntu/OmniPerf-Bench/perf-agents-bench/.work/worktrees/sglang_core/sglang_033_73b13e69/.bench_scratch/test_opt.py before and after      â”‚
â”‚ edits (do not create timing scripts outside /home/ubuntu/OmniPerf-Bench/perf-agents-bench/.work/worktrees/sglang_core/sglang_033_73b13e69/.bench_scratch)        â”‚
â”‚ 3. Make at least 3 concrete optimizations                                                                                                                        â”‚
â”‚ 4. Commit changes by iteration 8                                                                                                                                 â”‚
â”‚ 5. Use finish command by iteration 10                                                                                                                            â”‚
â”‚                                                                                                                                                                  â”‚
â”‚ ## TASK COMPLETION COMMAND:                                                                                                                                      â”‚
â”‚ When you have made optimizations:                                                                                                                                â”‚
â”‚ ```bash                                                                                                                                                          â”‚
â”‚ git add -A                                                                                                                                                       â”‚
â”‚ git reset -q .bench_scratch || true                                                                                                                              â”‚
â”‚ git commit -m 'Optimize MoE align sum kernels performance'                                                                                                       â”‚
â”‚ git diff $(git merge-base HEAD origin/HEAD || git rev-parse HEAD~1) -- . ':(exclude).bench_scratch' >                                                            â”‚
â”‚ /home/ubuntu/OmniPerf-Bench/perf-agents-bench/.work/worktrees/sglang_core/sglang_033_73b13e69/model_patch.diff || true                                           â”‚
â”‚ finish                                                                                                                                                           â”‚
â”‚ ```                                                                                                                                                              â”‚
â”‚                                                                                                                                                                  â”‚
â”‚ START IMPLEMENTING IMMEDIATELY. NO MORE ANALYSIS.                                                                                                                â”‚
â”‚                                                                                                                                                                  â”‚
â”‚ [1mModel Provider:[0m openai                                                                                                                                           â”‚
â”‚ [1mModel:[0m gpt-5-2025-08-07                                                                                                                                          â”‚
â”‚ [1mMax Steps:[0m 120                                                                                                                                                   â”‚
â”‚ [1mTrajectory File:[0m /home/ubuntu/OmniPerf-Bench/perf-agents-bench/state/runs/sglang_core-ae58875a/sglang_033_73b13e69/trajectory.json                               â”‚
â”‚ [1mTools:[0m bash, str_replace_based_edit_tool, sequentialthinking, task_done                                                                                          â”‚
â”‚ [1mProject_path:[0m /home/ubuntu/OmniPerf-Bench/perf-agents-bench/.work/worktrees/sglang_core/sglang_033_73b13e69                                                      â”‚
â”‚ [1mIssue:[0m I've uploaded a python code repository in the directory /home/ubuntu/OmniPerf-Bench/perf-agents-bench/.work/worktrees/sglang_core/sglang_033_73b13e69.    â”‚
â”‚ Consider the following test script showing an example usage of the repository:                                                                                   â”‚
â”‚                                                                                                                                                                  â”‚
â”‚ <test_script>                                                                                                                                                    â”‚
â”‚ # This is a performance optimization task                                                                                                                        â”‚
â”‚ # The specific operations to optimize are in the files listed below                                                                                              â”‚
â”‚ # Focus on performance improvements in the target functions                                                                                                      â”‚
â”‚                                                                                                                                                                  â”‚
â”‚ </test_script>                                                                                                                                                   â”‚
â”‚                                                                                                                                                                  â”‚
â”‚ Can you help me implement the necessary changes to the repository so that the runtime of the <test_script> is optimized?                                         â”‚
â”‚                                                                                                                                                                  â”‚
â”‚ Basic guidelines:                                                                                                                                                â”‚
â”‚ 1. Your task is to make changes to non-test files in the /home/ubuntu/OmniPerf-Bench/perf-agents-bench/.work/worktrees/sglang_core/sglang_033_73b13e69 directory â”‚
â”‚ to improve the performance of the <test_script>.                                                                                                                 â”‚
â”‚ 2. Make changes while ensuring the repository is functionally equivalent to the original.                                                                        â”‚
â”‚ 3. Do not overoptimize for just the specific inputs in <test_script>. Make general performance improvements for the usage scenario shown.                        â”‚
â”‚ 4. You may need to rebuild the repo for your changes to take effect before testing. Some rebuilds may take time to run, so be patient with running them.         â”‚
â”‚                                                                                                                                                                  â”‚
â”‚ Follow these steps to improve performance:                                                                                                                       â”‚
â”‚ 1. As a first step, explore the repository structure.                                                                                                            â”‚
â”‚ 2. Create a script ONLY inside /home/ubuntu/OmniPerf-Bench/perf-agents-bench/.work/worktrees/sglang_core/sglang_033_73b13e69/.bench_scratch (e.g.,               â”‚
â”‚ /home/ubuntu/OmniPerf-Bench/perf-agents-bench/.work/worktrees/sglang_core/sglang_033_73b13e69/.bench_scratch/test_opt.py) to reproduce and time the example,     â”‚
â”‚ then execute it with python <filename.py> from the repo root.                                                                                                    â”‚
â”‚ 3. Edit the source code of the repository to improve performance.                                                                                                â”‚
â”‚ 4. Rebuild and rerun your script to confirm that performance has improved.                                                                                       â”‚
â”‚                                                                                                                                                                  â”‚
â”‚ Here is an example of the kind of optimizations that have been shown to improve performance in this codebase:                                                    â”‚
â”‚                                                                                                                                                                  â”‚
â”‚ <example_optimization_diff>                                                                                                                                      â”‚
â”‚ diff --git a/python/sglang/srt/managers/scheduler.py b/python/sglang/srt/managers/scheduler.py                                                                   â”‚
â”‚ index 8253a303b..14ed362cf 100644                                                                                                                                â”‚
â”‚ --- a/python/sglang/srt/managers/scheduler.py                                                                                                                    â”‚
â”‚ +++ b/python/sglang/srt/managers/scheduler.py                                                                                                                    â”‚
â”‚ @@ -1399,29 +1399,6 @@ class Scheduler(                                                                                                                          â”‚
â”‚              self.metrics_collector.log_stats(self.stats)                                                                                                        â”‚
â”‚          self._publish_kv_events()                                                                                                                               â”‚
â”‚                                                                                                                                                                  â”‚
â”‚ -    def coordinate_spec_dp_attn_batch(self, new_batch: Optional[ScheduleBatch]):                                                                                â”‚
â”‚ -        """Coordinate the DP attention batch."""                                                                                                                â”‚
â”‚ -                                                                                                                                                                â”‚
â”‚ -        local_info = torch.tensor(                                                                                                                              â”‚
â”‚ -            [                                                                                                                                                   â”‚
â”‚ -                (new_batch is not None),                                                                                                                        â”‚
â”‚ -            ],                                                                                                                                                  â”‚
â”‚ -            dtype=torch.int64,                                                                                                                                  â”‚
â”‚ -        )                                                                                                                                                       â”‚
â”‚ -        global_info = torch.empty(                                                                                                                              â”‚
â”‚ -            (self.server_args.dp_size, self.attn_tp_size, 1),                                                                                                   â”‚
â”‚ -            dtype=torch.int64,                                                                                                                                  â”‚
â”‚ -        )                                                                                                                                                       â”‚
â”‚ -        torch.distributed.all_gather_into_tensor(                                                                                                               â”‚
â”‚ -            global_info.flatten(),                                                                                                                              â”‚
â”‚ -            local_info,                                                                                                                                         â”‚
â”‚ -            group=self.tp_cpu_group,                                                                                                                            â”‚
â”‚ -        )                                                                                                                                                       â”‚
â”‚ -        any_new_batch = any(                                                                                                                                    â”‚
â”‚ -            global_info[:, 0, 0].tolist()                                                                                                                       â”‚
â”‚ -        )  # Any DP worker has forward batch                                                                                                                    â”‚
â”‚ -        return any_new_batch                                                                                                                                    â”‚
â”‚ </example_optimization_diff>                                                                                                                                     â”‚
â”‚                                                                                                                                                                  â”‚
â”‚ IMPORTANT: The above diff is an EXAMPLE of optimizations that were successful in a different context.                                                            â”‚
â”‚ These changes have NOT been applied to your codebase yet.                                                                                                        â”‚
â”‚ Your task is to:                                                                                                                                                 â”‚
â”‚ 1. Understand the optimization pattern shown (e.g., torch.zeros â†’ torch.empty)                                                                                   â”‚
â”‚ 2. Look at the CURRENT code in the target files                                                                                                                  â”‚
â”‚ 3. Find places where you can apply SIMILAR optimizations                                                                                                         â”‚
â”‚ 4. MAKE THE CHANGES yourself using str_replace_editor                                                                                                            â”‚
â”‚                                                                                                                                                                  â”‚
â”‚ The codebase you're working with is at the BASE commit - it does NOT have these optimizations yet.                                                               â”‚
â”‚ You need to IMPLEMENT similar optimizations yourself.                                                                                                            â”‚
â”‚                                                                                                                                                                  â”‚
â”‚ HERE'S WHAT YOU NEED TO DO:                                                                                                                                      â”‚
â”‚ 1. The files CURRENTLY contain torch.zeros() calls that need optimization                                                                                        â”‚
â”‚ 2. You need to CHANGE torch.zeros to torch.empty where appropriate                                                                                               â”‚
â”‚ 3. You need to REMOVE .fill_() operations that are unnecessary                                                                                                   â”‚
â”‚ 4. These are NEW changes you're making - not already in the code                                                                                                 â”‚
â”‚                                                                                                                                                                  â”‚
â”‚ START WITH THIS COMMAND to see what needs changing:                                                                                                              â”‚
â”‚ ```bash                                                                                                                                                          â”‚
â”‚ grep -n 'torch.zeros\|fill_' vllm/model_executor/layers/fused_moe/moe_align_block_size.py benchmarks/kernels/benchmark_moe_align_block_size.py                   â”‚
â”‚ ```                                                                                                                                                              â”‚
â”‚                                                                                                                                                                  â”‚
â”‚ CRITICAL: You MUST make actual code changes. Look for patterns like:                                                                                             â”‚
â”‚ - Analyze the target files for performance bottlenecks                                                                                                           â”‚
â”‚ - Look for unnecessary memory allocations or initializations                                                                                                     â”‚
â”‚ - Consider more efficient algorithms or data structures                                                                                                          â”‚
â”‚                                                                                                                                                                  â”‚
â”‚ Target files to optimize:                                                                                                                                        â”‚
â”‚ - python/sglang/srt/managers/scheduler.py                                                                                                                        â”‚
â”‚                                                                                                                                                                  â”‚
â”‚ IMPORTANT: You MUST make actual code changes to at least one file.                                                                                               â”‚
â”‚ The task will fail if no files are modified.                                                                                                                     â”‚
â”‚                                                                                                                                                                  â”‚
â”‚ ## Constraints                                                                                                                                                   â”‚
â”‚ - No public API breakage                                                                                                                                         â”‚
â”‚ - All TestPack checks must pass                                                                                                                                  â”‚
â”‚                                                                                                                                                                  â”‚
â”‚ ## Target Files (ONLY modify these)                                                                                                                              â”‚
â”‚ - `python/sglang/srt/managers/scheduler.py`                                                                                                                      â”‚
â”‚                                                                                                                                                                  â”‚
â”‚ ## SPECIFIC OPTIMIZATION TARGETS:                                                                                                                                â”‚
â”‚ Based on the human commit analysis, focus on these areas:                                                                                                        â”‚
â”‚ - Memory allocation patterns (torch.zeros vs torch.empty)                                                                                                        â”‚
â”‚ - Tensor initialization strategies                                                                                                                               â”‚
â”‚ - Kernel parameter optimization                                                                                                                                  â”‚
â”‚ - Buffer reuse and caching                                                                                                                                       â”‚
â”‚                                                                                                                                                                  â”‚
â”‚ ### Human Developer's Approach:                                                                                                                                  â”‚
â”‚ ```                                                                                                                                                              â”‚
â”‚ Optimize DP attn scheduling for speculative decoding (#7285)                                                                                                     â”‚
â”‚ ```                                                                                                                                                              â”‚
â”‚                                                                                                                                                                  â”‚
â”‚ ### Files Modified (statistics):                                                                                                                                 â”‚
â”‚ ```                                                                                                                                                              â”‚
â”‚ python/sglang/srt/managers/scheduler.py | 44 +++++++++------------------------                                                                                   â”‚
â”‚  1 file changed, 12 insertions(+), 32 deletions(-)                                                                                                               â”‚
â”‚ ```                                                                                                                                                              â”‚
â”‚                                                                                                                                                                  â”‚
â”‚ ## IMMEDIATE ACTION REQUIREMENTS:                                                                                                                                â”‚
â”‚ 1. Start editing files by iteration 3                                                                                                                            â”‚
â”‚ 2. Create and run /home/ubuntu/OmniPerf-Bench/perf-agents-bench/.work/worktrees/sglang_core/sglang_033_73b13e69/.bench_scratch/test_opt.py before and after      â”‚
â”‚ edits (do not create timing scripts outside /home/ubuntu/OmniPerf-Bench/perf-agents-bench/.work/worktrees/sglang_core/sglang_033_73b13e69/.bench_scratch)        â”‚
â”‚ 3. Make at least 3 concrete optimizations                                                                                                                        â”‚
â”‚ 4. Commit changes by iteration 8                                                                                                                                 â”‚
â”‚ 5. Use finish command by iteration 10                                                                                                                            â”‚
â”‚                                                                                                                                                                  â”‚
â”‚ ## TASK COMPLETION COMMAND:                                                                                                                                      â”‚
â”‚ When you have made optimizations:                                                                                                                                â”‚
â”‚ ```bash                                                                                                                                                          â”‚
â”‚ git add -A                                                                                                                                                       â”‚
â”‚ git reset -q .bench_scratch || true                                                                                                                              â”‚
â”‚ git commit -m 'Optimize MoE align sum kernels performance'                                                                                                       â”‚
â”‚ git diff $(git merge-base HEAD origin/HEAD || git rev-parse HEAD~1) -- . ':(exclude).bench_scratch' >                                                            â”‚
â”‚ /home/ubuntu/OmniPerf-Bench/perf-agents-bench/.work/worktrees/sglang_core/sglang_033_73b13e69/model_patch.diff || true                                           â”‚
â”‚ finish                                                                                                                                                           â”‚
â”‚ ```                                                                                                                                                              â”‚
â”‚                                                                                                                                                                  â”‚
â”‚ START IMPLEMENTING IMMEDIATELY. NO MORE ANALYSIS.                                                                                                                â”‚
â”‚                                                                                                                                                                  â”‚
â”‚ [1mMust_patch:[0m true                                                                                                                                                 â”‚
â”‚ [1mPatch_path:[0m /home/ubuntu/OmniPerf-Bench/perf-agents-bench/state/runs/sglang_core-ae58875a/sglang_033_73b13e69/model_patch.diff                                   â”‚
â•°â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•¯
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Status      â”‚ âœ… Step 1: Completed                                                                                   â”‚
â”‚ LLM         â”‚ ðŸ’¬ I'll start by exploring the repository structure and locating the target file. Then I'll create a   â”‚
â”‚ Response    â”‚ benchmarking script under .bench_scratch to time relevant operations before changes. Next, I'll        â”‚
â”‚             â”‚ examine python/sglang/srt/managers/scheduler.py for torch.zeros and fill_ patterns to apply            â”‚
â”‚             â”‚ optimizations. Finally, I'll run the benchmark and proceed with commits.                               â”‚
â”‚             â”‚                                                                                                        â”‚
â”‚             â”‚ I will run a safe listing of the repository and view the target file to plan edits.                    â”‚
â”‚ Tools       â”‚ ðŸ”§ bash                                                                                                â”‚
â”‚ bash        â”‚ â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚
â”‚             â”‚ â”‚ {'command': 'ls -la                            â”‚ total 112                                       â”‚   â”‚
â”‚             â”‚ â”‚ /home/ubuntu/OmniPerf-Bench/perf-agents-benchâ€¦ â”‚ drwxrwxr-x 16 ubuntu ubuntu  4096 Nov 15 05:15  â”‚   â”‚
â”‚             â”‚ â”‚ | head -200', 'restart': False}                â”‚ .                                               â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ drwxrwxr-x 36 ubuntu ubuntu  4096 Nov 15 05:15  â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ ..                                              â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ -rw-rw-r--  1 ubuntu ubuntu    35 Nov 15 05:15  â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ .clang-format-ignore                            â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ drwxrwxr-x  2 ubuntu ubuntu  4096 Nov 15 05:15  â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ .devcontainer                                   â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ -rw-rw-r--  1 ubuntu ubuntu   341 Nov 15 05:15  â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ .editorconfig                                   â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ -rw-rw-r--  1 ubuntu ubuntu   113 Nov 15 05:15  â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ .git                                            â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ drwxrwxr-x  4 ubuntu ubuntu  4096 Nov 15 05:15  â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ .github                                         â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ -rw-rw-r--  1 ubuntu ubuntu  3706 Nov 15 05:15  â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ .gitignore                                      â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ -rw-rw-r--  1 ubuntu ubuntu     0 Nov 15 05:15  â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ .gitmodules                                     â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ -rw-rw-r--  1 ubuntu ubuntu    50 Nov 15 05:15  â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ .isort.cfg                                      â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ -rw-rw-r--  1 ubuntu ubuntu  1568 Nov 15 05:15  â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ .pre-commit-config.yaml                         â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ drwxrwxr-x  3 ubuntu ubuntu  4096 Nov 15 05:15  â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ 3rdparty                                        â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ -rw-rw-r--  1 ubuntu ubuntu 11346 Nov 15 05:15  â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ LICENSE                                         â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ -rw-rw-r--  1 ubuntu ubuntu  1760 Nov 15 05:15  â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ Makefile                                        â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ -rw-rw-r--  1 ubuntu ubuntu  6897 Nov 15 05:15  â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ README.md                                       â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ drwxrwxr-x  2 ubuntu ubuntu  4096 Nov 15 05:15  â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ assets                                          â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ drwxrwxr-x 33 ubuntu ubuntu  4096 Nov 15 05:15  â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ benchmark                                       â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ drwxrwxr-x  2 ubuntu ubuntu  4096 Nov 15 05:15  â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ docker                                          â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ drwxrwxr-x  9 ubuntu ubuntu  4096 Nov 15 05:15  â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ docs                                            â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ drwxrwxr-x  6 ubuntu ubuntu  4096 Nov 15 05:15  â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ examples                                        â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ drwxrwxr-x  3 ubuntu ubuntu  4096 Nov 15 05:15  â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ python                                          â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ drwxrwxr-x  4 ubuntu ubuntu  4096 Nov 15 05:15  â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ scripts                                         â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ drwxrwxr-x  8 ubuntu ubuntu  4096 Nov 15 05:15  â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ sgl-kernel                                      â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ drwxrwxr-x  4 ubuntu ubuntu  4096 Nov 15 05:15  â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ sgl-pdlb                                        â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ drwxrwxr-x  7 ubuntu ubuntu  4096 Nov 15 05:15  â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ sgl-router                                      â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ drwxrwxr-x  4 ubuntu ubuntu  4096 Nov 15 05:15  â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ test                                            â”‚   â”‚
â”‚             â”‚ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚
â”‚ Total       â”‚ Input: 8006 Output: 1375                                                                               â”‚
â”‚ Tokens      â”‚                                                                                                        â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Status      â”‚ âœ… Step 2: Completed                                                                                   â”‚
â”‚ Tools       â”‚ ðŸ”§ str_replace_based_edit_tool                                                                         â”‚
â”‚ str_replacâ€¦ â”‚ â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚
â”‚             â”‚ â”‚ {'command': 'view', 'file_text': '',           â”‚ Here's the result of running `cat -n` on        â”‚   â”‚
â”‚             â”‚ â”‚ 'insert_line': 1, 'new_str': '', 'old_str':    â”‚ /home/ubuntu/OmniPerf-Bench/perf-agents-bench/â€¦ â”‚   â”‚
â”‚             â”‚ â”‚ '', 'path':                                    â”‚      1  # Copyright 2023-2024 SGLang Team       â”‚   â”‚
â”‚             â”‚ â”‚ '/home/ubuntu/OmniPerf-Bench/perf-agents-bencâ€¦ â”‚      2  # Licensed under the Apache License,    â”‚   â”‚
â”‚             â”‚ â”‚ 'view_range': [1, -1]}                         â”‚ Version 2.0 (the "License");                    â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚      3  # you may not use this file except in   â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ compliance with the License.                    â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚      4  # You may obtain a copy of the License  â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ at                                              â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚      5  #                                       â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚      6  #                                       â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ http://www.apache.org/licenses/LICENSE-2.0      â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚      7  #                                       â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚      8  # Unless required by applicable law or  â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ agreed to in writing, software                  â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚      9  # distributed under the License is      â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ distributed on an "AS IS" BASIS,                â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚     10  # WITHOUT WARRANTIES OR CONDITIONS OF   â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ ANY KIND, either express or implied.            â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚     11  # See the License for the specific      â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ language governing permissions and              â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚     12  # limitations under the License.        â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚     13  #                                       â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ ==============================================â€¦ â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚     14  """A scheduler that manages a tensor    â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ parallel GPU worker."""                         â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚     15                                          â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚     16  import faulthandler                     â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚     17  import logging                          â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚     18  import os                               â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚     19  import signal                           â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚     20  import sys                              â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚     21  import threading                        â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚     22  import time                             â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚     23  from collections import defaultdict,    â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ deque                                           â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚     24  from concurrent import futures          â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚     25  from dataclasses import dataclass       â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚     26  from http import HTTPStatus             â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚     27  from pathlib import Path                â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚     28  from types import SimpleNamespace       â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚     29  from typing import Dict, List,          â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ Optional, Tuple, Union                          â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚     30                                          â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚     31  import psutil                           â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚     32  import setproctitle                     â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚     33  import torch                            â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚     34  import zmq                              â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚     35  from torch.distributed import barrier   â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚     36                                          â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚     37  from sglang.global_config import        â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ global_config                                   â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚     38  from sglang.srt.configs.model_config    â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ import ModelConfig                              â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚     39  from sglang.srt.constants import        â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ GPU_MEMORY_TYPE_KV_CACHE,                       â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ GPU_MEMORY_TYPE_WEIGHTS                         â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚     40  from                                    â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ sglang.srt.constrained.base_grammar_backend     â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ import (                                        â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚     41      INVALID_GRAMMAR_OBJ,                â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚     42      create_grammar_backend,             â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚     43  )                                       â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚     44  from sglang.srt.disaggregation.decode   â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ import (                                        â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚     45      DecodePreallocQueue,                â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚     46      DecodeTransferQueue,                â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚     47      SchedulerDisaggregationDecodeMixin, â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚     48  )                                       â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚     49  from                                    â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ sglang.srt.disaggregation.kv_events import      â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ EventPublisherFactory, KVEventBatch             â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚     50  from sglang.srt.disaggregation.prefill  â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ import (                                        â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚     51      PrefillBootstrapQueue,              â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚     52                                          â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ SchedulerDisaggregationPrefillMixin,            â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚     53  )                                       â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚     54  from sglang.srt.disaggregation.utils    â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ import (                                        â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚     55      DisaggregationMode,                 â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚     56      MetadataBuffers,                    â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚     57      ReqToMetadataIdxAllocator,          â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚     58      TransferBackend,                    â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚     59      prepare_abort,                      â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚     60  )                                       â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚     61  from sglang.srt.distributed import      â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ get_pp_group, get_world_group                   â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚     62  from sglang.srt.hf_transformers_utils   â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ import (                                        â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚     63      get_processor,                      â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚     64      get_tokenizer,                      â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚     65      get_tokenizer_from_processor,       â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚     66  )                                       â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚     67  from sglang.srt.layers.dp_attention     â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ import compute_dp_attention_world_info          â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚     68  from sglang.srt.layers.logits_processor â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ import LogitsProcessorOutput                    â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚     69  from                                    â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ sglang.srt.managers.expert_distribution import  â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ (                                               â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚     70                                          â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ get_global_expert_distribution_recorder,        â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚     71  )                                       â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚     72  from sglang.srt.managers.io_struct      â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ import (                                        â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚     73      AbortReq,                           â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚     74      CloseSessionReqInput,               â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚     75      ExpertDistributionReq,              â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚     76      ExpertDistributionReqOutput,        â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚     77      FlushCacheReqInput,                 â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚     78      FlushCacheReqOutput,                â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚     79      GetInternalStateReq,                â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚     80      GetInternalStateReqOutput,          â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚     81      GetWeightsByNameReqInput,           â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚     82      GetWeightsByNameReqOutput,          â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚     83      HealthCheckOutput,                  â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚     84      InitWeightsUpdateGroupReqInput,     â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚     85      InitWeightsUpdateGroupReqOutput,    â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚     86      OpenSessionReqInput,                â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚     87      OpenSessionReqOutput,               â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚     88      ProfileReq,                         â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚     89      ProfileReqOutput,                   â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚     90      ProfileReqType,                     â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚     91      ReleaseMemoryOccupationReqInput,    â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚     92      ReleaseMemoryOccupationReqOutput,   â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚     93      ResumeMemoryOccupationReqInput,     â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚     94      ResumeMemoryOccupationReqOutput,    â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚     95      RpcReqInput,                        â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚     96      RpcReqOutput,                       â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚     97      SetInternalStateReq,                â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚     98      SetInternalStateReqOutput,          â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚     99      SlowDownReqInput,                   â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    100      SlowDownReqOutput,                  â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    101      TokenizedEmbeddingReqInput,         â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    102      TokenizedGenerateReqInput,          â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    103      UpdateWeightFromDiskReqInput,       â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    104      UpdateWeightFromDiskReqOutput,      â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    105                                          â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ UpdateWeightsFromDistributedReqInput,           â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    106                                          â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ UpdateWeightsFromDistributedReqOutput,          â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    107      UpdateWeightsFromTensorReqInput,    â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    108      UpdateWeightsFromTensorReqOutput,   â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    109  )                                       â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    110  from sglang.srt.managers.mm_utils       â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ import init_embedding_cache                     â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    111  from sglang.srt.managers.schedule_batch â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ import (                                        â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    112      FINISH_ABORT,                       â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    113      MultimodalInputs,                   â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    114      Req,                                â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    115      ScheduleBatch,                      â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    116      global_server_args_dict,            â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    117  )                                       â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    118  from                                    â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ sglang.srt.managers.schedule_policy import (    â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    119      AddReqResult,                       â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    120      PrefillAdder,                       â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    121      SchedulePolicy,                     â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    122  )                                       â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    123  from                                    â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ sglang.srt.managers.scheduler_output_processorâ€¦ â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ import (                                        â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    124      SchedulerOutputProcessorMixin,      â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    125  )                                       â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    126  from                                    â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ sglang.srt.managers.session_controller import   â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ Session                                         â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    127  from sglang.srt.managers.tp_worker      â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ import TpModelWorker                            â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    128  from                                    â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ sglang.srt.managers.tp_worker_overlap_thread    â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ import TpModelWorkerClient                      â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    129  from sglang.srt.managers.utils import   â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ validate_input_length                           â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    130  from sglang.srt.mem_cache.chunk_cache   â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ import ChunkCache                               â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    131  from sglang.srt.mem_cache.hiradix_cache â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ import HiRadixCache                             â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    132  from sglang.srt.mem_cache.radix_cache   â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ import RadixCache                               â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    133  from sglang.srt.metrics.collector       â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ import SchedulerMetricsCollector,               â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ SchedulerStats                                  â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    134  from                                    â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ sglang.srt.model_executor.forward_batch_info    â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ import ForwardMode, PPProxyTensors              â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    135  from sglang.srt.reasoning_parser import â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ ReasoningParser                                 â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    136  from sglang.srt.server_args import      â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ PortArgs, ServerArgs                            â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    137  from sglang.srt.speculative.spec_info   â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ import SpeculativeAlgorithm                     â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    138  from                                    â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ sglang.srt.torch_memory_saver_adapter import    â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ TorchMemorySaverAdapter                         â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    139  from sglang.srt.two_batch_overlap       â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ import TboDPAttentionPreparer                   â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    140  from sglang.srt.utils import (          â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    141      DeepEPMode,                         â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    142      DynamicGradMode,                    â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    143      broadcast_pyobj,                    â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    144      configure_logger,                   â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    145      disable_request_logging,            â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    146      get_available_gpu_memory,           â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    147      get_bool_env_var,                   â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    148      get_zmq_socket,                     â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    149      kill_itself_when_parent_died,       â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    150      point_to_point_pyobj,               â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    151      pyspy_dump_schedulers,              â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    152      require_mlp_sync,                   â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    153      require_mlp_tp_gather,              â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    154      set_gpu_proc_affinity,              â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    155      set_random_seed,                    â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    156      suppress_other_loggers,             â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    157  )                                       â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    158  from sglang.utils import                â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ TypeBasedDispatcher, get_exception_traceback    â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    159                                          â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    160  logger = logging.getLogger(__name__)    â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    161                                          â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    162  # Test retract decode for debugging     â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ purposes                                        â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    163  TEST_RETRACT =                          â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ get_bool_env_var("SGLANG_TEST_RETRACT")         â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    164  RECORD_STEP_TIME =                      â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ get_bool_env_var("SGLANG_RECORD_STEP_TIME")     â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    165  GRAMMAR_TIMEOUT =                       â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ float(os.environ.get("SGLANG_GRAMMAR_TIMEOUT",  â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ 300))                                           â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    166                                          â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    167                                          â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    168  @dataclass                              â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    169  class GenerationBatchResult:            â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    170      logits_output:                      â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ Optional[LogitsProcessorOutput]                 â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    171      pp_hidden_states_proxy_tensors:     â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ Optional                                        â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    172      next_token_ids: Optional[List]      â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    173      extend_input_len_per_req: List      â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    174      extend_logprob_start_len_per_req:   â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ List                                            â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    175      bid: int                            â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    176      can_run_cuda_graph: bool            â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    177                                          â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    178                                          â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    179  @dataclass                              â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    180  class EmbeddingBatchResult:             â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    181      embeddings: torch.Tensor            â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    182      bid: int                            â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    183                                          â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    184                                          â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    185  class IdleSleeper:                      â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    186      """                                 â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    187      In setups which have long           â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ inactivity periods it is desirable to reduce    â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    188      system power consumption when       â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ sglang does nothing. This would lead not only   â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    189      to power savings, but also to more  â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ CPU thermal headroom when a request             â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    190      eventually comes. This is important â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ in cases when multiple GPUs are connected       â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    191      as each GPU would otherwise pin one â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ thread at 100% CPU usage.                       â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    192                                          â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    193      The simplest solution is to use     â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ zmq.Poller on all sockets that may receive      â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    194      data that needs handling            â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ immediately.                                    â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    195      """                                 â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    196                                          â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    197      def __init__(self, sockets):        â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    198          self.poller = zmq.Poller()      â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    199          for s in sockets:               â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    200              self.poller.register(s,     â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ zmq.POLLIN)                                     â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    201                                          â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    202      def maybe_sleep(self):              â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    203          self.poller.poll(1000)          â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    204                                          â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    205                                          â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    206  class Scheduler(                        â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    207      SchedulerOutputProcessorMixin,      â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    208      SchedulerDisaggregationDecodeMixin, â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    209                                          â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ SchedulerDisaggregationPrefillMixin,            â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    210  ):                                      â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    211      """A scheduler that manages a       â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ tensor parallel GPU worker."""                  â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    212                                          â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    213      def __init__(                       â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    214          self,                           â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    215          server_args: ServerArgs,        â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    216          port_args: PortArgs,            â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    217          gpu_id: int,                    â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    218          tp_rank: int,                   â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    219          pp_rank: int,                   â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    220          dp_rank: Optional,              â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    221      ):                                  â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    222          # Parse args                    â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    223          self.server_args = server_args  â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    224          self.tp_rank = tp_rank          â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    225          self.pp_rank = pp_rank          â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    226          self.tp_size =                  â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ server_args.tp_size                             â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    227          self.pp_size =                  â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ server_args.pp_size                             â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    228          self.dp_size =                  â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ server_args.dp_size                             â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    229          self.schedule_policy =          â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ server_args.schedule_policy                     â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    230          self.lora_paths =               â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ server_args.lora_paths                          â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    231          self.max_loras_per_batch =      â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ server_args.max_loras_per_batch                 â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    232          self.enable_overlap = not       â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ server_args.disable_overlap_schedule            â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    233          self.skip_tokenizer_init =      â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ server_args.skip_tokenizer_init                 â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    234          self.enable_metrics =           â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ server_args.enable_metrics                      â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    235          self.enable_kv_cache_events =   â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ server_args.kv_events_config is not None        â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    236          self.stream_interval =          â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ server_args.stream_interval                     â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    237          self.spec_algorithm =           â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ SpeculativeAlgorithm.from_string(               â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    238                                          â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ server_args.speculative_algorithm               â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    239          )                               â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    240          self.gpu_id = gpu_id            â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    241          self.enable_hierarchical_cache  â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ = server_args.enable_hierarchical_cache         â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    242          self.page_size =                â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ server_args.page_size                           â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    243          self.dp_size =                  â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ server_args.dp_size                             â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    244          self.attn_tp_rank,              â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ self.attn_tp_size, self.attn_dp_rank = (        â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    245                                          â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ compute_dp_attention_world_info(                â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    246                                          â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ server_args.enable_dp_attention,                â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    247                  self.tp_rank,           â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    248                  self.tp_size,           â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    249                  self.dp_size,           â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    250              )                           â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    251          )                               â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    252                                          â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    253          # Init inter-process            â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ communication                                   â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    254          context = zmq.Context(2)        â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    255          self.idle_sleeper = None        â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    256                                          â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    257          if self.pp_rank == 0 and        â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ self.attn_tp_rank == 0:                         â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    258              self.recv_from_tokenizer =  â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ get_zmq_socket(                                 â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    259                  context, zmq.PULL,      â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ port_args.scheduler_input_ipc_name, False       â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    260              )                           â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    261              self.send_to_tokenizer =    â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ get_zmq_socket(                                 â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    262                  context, zmq.PUSH,      â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ port_args.tokenizer_ipc_name, False             â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    263              )                           â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    264                                          â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    265              if                          â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ server_args.skip_tokenizer_init:                â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    266                  # Directly send to the  â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ TokenizerManager                                â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    267                                          â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ self.send_to_detokenizer = get_zmq_socket(      â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    268                      context, zmq.PUSH,  â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ port_args.tokenizer_ipc_name, False             â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    269                  )                       â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    270              else:                       â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    271                  # Send to the           â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ DetokenizerManager                              â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    272                                          â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ self.send_to_detokenizer = get_zmq_socket(      â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    273                      context, zmq.PUSH,  â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ port_args.detokenizer_ipc_name, False           â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    274                  )                       â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    275                                          â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    276              self.recv_from_rpc =        â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ get_zmq_socket(                                 â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    277                  context, zmq.DEALER,    â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ port_args.rpc_ipc_name, False                   â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    278              )                           â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    279              if                          â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ self.server_args.sleep_on_idle:                 â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    280                  self.idle_sleeper =     â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ IdleSleeper(                                    â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    281                      [                   â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    282                                          â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ self.recv_from_tokenizer,                       â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    283                                          â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ self.recv_from_rpc,                             â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    284                      ]                   â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    285                  )                       â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    286          else:                           â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    287              self.recv_from_tokenizer =  â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ None                                            â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    288              self.recv_from_rpc = None   â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    289              self.send_to_tokenizer =    â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ SimpleNamespace(send_pyobj=lambda x: None)      â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    290              self.send_to_detokenizer =  â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ SimpleNamespace(send_pyobj=lambda x: None)      â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    291                                          â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    292          # Init tokenizer                â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    293          self.init_tokenizer()           â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    294                                          â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    295          # Set reasoning_parser and      â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ think_end_id if --reasoning_parser is enabled   â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    296          if                              â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ self.server_args.reasoning_parser and           â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ self.tokenizer:                                 â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    297              reasoning_parser =          â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ ReasoningParser(                                â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    298                                          â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ model_type=self.server_args.reasoning_parser,   â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ stream_reasoning=False                          â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    299              )                           â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    300              self.tokenizer.think_end_id â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ = self.tokenizer.encode(                        â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    301                                          â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ reasoning_parser.detector.think_end_token,      â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ add_special_tokens=False                        â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    302              )[0]                        â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    303                                          â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    304          # Check whether overlap can be  â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ enabled                                         â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    305          if not self.is_generation:      â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    306              self.enable_overlap = False â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    307              logger.info("Overlap        â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ scheduler is disabled for embedding models.")   â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    308                                          â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    309          # Launch a tensor parallel      â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ worker                                          â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    310          if self.enable_overlap:         â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    311              TpWorkerClass =             â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ TpModelWorkerClient                             â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    312          else:                           â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    313              TpWorkerClass =             â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ TpModelWorker                                   â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    314                                          â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    315          self.tp_worker = TpWorkerClass( â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    316              server_args=server_args,    â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    317              gpu_id=gpu_id,              â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    318              tp_rank=tp_rank,            â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    319              pp_rank=pp_rank,            â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    320              dp_rank=dp_rank,            â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    321                                          â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ nccl_port=port_args.nccl_port,                  â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    322          )                               â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    323                                          â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    324          # Launch a draft worker for     â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ speculative decoding                            â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    325          if                              â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ self.spec_algorithm.is_eagle():                 â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    326              from                        â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ sglang.srt.speculative.eagle_worker import      â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ EAGLEWorker                                     â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    327                                          â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    328              self.draft_worker =         â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ EAGLEWorker(                                    â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    329                  gpu_id=gpu_id,          â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    330                  tp_rank=tp_rank,        â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    331                                          â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ server_args=server_args,                        â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    332                                          â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ nccl_port=port_args.nccl_port,                  â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    333                                          â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ target_worker=self.tp_worker,                   â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    334                  dp_rank=dp_rank,        â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    335              )                           â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    336          else:                           â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    337              self.draft_worker = None    â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    338                                          â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    339          # Get token and memory info     â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ from the model worker                           â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    340          (                               â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    341              self.max_total_num_tokens,  â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    342              self.max_prefill_tokens,    â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    343              self.max_running_requests,  â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    344              self.max_req_len,           â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    345              self.max_req_input_len,     â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    346              self.random_seed,           â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    347              self.device,                â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    348                                          â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ worker_global_server_args_dict,                 â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    349              _,                          â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    350              _,                          â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    351              _,                          â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    352          ) =                             â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ self.tp_worker.get_worker_info()                â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    353          if                              â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ global_server_args_dict["max_micro_batch_size"] â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ is None:                                        â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    354                                          â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ global_server_args_dict["max_micro_batch_size"] â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ = max(                                          â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    355                                          â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ self.max_running_requests //                    â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ server_args.pp_size, 1                          â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    356              )                           â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    357                                          â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    358          self.tp_group =                 â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ self.tp_worker.get_tp_group()                   â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    359          self.tp_cpu_group =             â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ self.tp_group.cpu_group                         â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    360          self.attn_tp_group =            â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ self.tp_worker.get_attention_tp_group()         â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    361          self.attn_tp_cpu_group =        â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ self.tp_worker.get_attention_tp_cpu_group()     â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    362          self.pp_group = get_pp_group()  â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    363          self.world_group =              â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ get_world_group()                               â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    364                                          â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    365          self.pad_input_ids_func =       â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ self.tp_worker.get_pad_input_ids_func()         â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    366                                          â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ global_server_args_dict.update(worker_global_sâ€¦ â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    367                                          â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ set_random_seed(self.random_seed)               â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    368                                          â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    369          # Print debug info              â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    370          if tp_rank == 0:                â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    371              avail_mem =                 â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ get_available_gpu_memory(                       â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    372                  self.device,            â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ self.gpu_id, empty_cache=False                  â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    373              )                           â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    374              logger.info(                â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    375                                          â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ f"max_total_num_tokens={self.max_total_num_tokâ€¦ â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ "                                               â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    376                                          â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ f"chunked_prefill_size={server_args.chunked_prâ€¦ â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ "                                               â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    377                                          â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ f"max_prefill_tokens={self.max_prefill_tokens}, â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ "                                               â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    378                                          â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ f"max_running_requests={self.max_running_requeâ€¦ â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ "                                               â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    379                                          â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ f"context_len={self.model_config.context_len},  â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ "                                               â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    380                                          â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ f"available_gpu_mem={avail_mem:.2f} GB"         â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    381              )                           â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    382                                          â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    383          # Init memory pool and cache    â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    384                                          â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ self.init_memory_pool_and_cache()               â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    385                                          â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    386          # Init running status           â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    387          self.waiting_queue: List[Req] = â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ []                                              â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    388          # The running decoding batch    â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ for continuous batching                         â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    389          self.running_batch:             â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ ScheduleBatch = ScheduleBatch(reqs=[],          â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ batch_is_full=False)                            â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    390          # The current forward batch     â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    391          self.cur_batch:                 â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ Optional[ScheduleBatch] = None                  â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    392          # The last forward batch        â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    393          self.last_batch:                â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ Optional[ScheduleBatch] = None                  â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    394          self.forward_ct = 0             â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    395          self.forward_ct_decode = 0      â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    396          self.num_generated_tokens = 0   â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    397          self.last_prefill_tokens = 0    â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    398          self.last_decode_stats_tic =    â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ time.perf_counter()                             â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    399          self.last_prefill_stats_tic =   â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ time.perf_counter()                             â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    400          self.return_health_check_ct = 0 â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    401          self.current_stream =           â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ torch.get_device_module(self.device).current_sâ€¦ â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    402          if self.device == "cpu":        â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    403                                          â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ self.current_stream.synchronize = lambda: None  â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ # No-op for CPU                                 â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    404          self.forward_sleep_time = None  â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    405                                          â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    406          # Init session info             â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    407          self.sessions: Dict = {}        â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    408                                          â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    409          # Init chunked prefill          â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    410          self.chunked_prefill_size =     â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ server_args.chunked_prefill_size                â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    411          if self.chunked_prefill_size <= â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ 0:  # -1 means disable                          â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    412              self.chunked_prefill_size = â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ None                                            â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    413          self.chunked_req = None         â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    414          self.is_mixed_chunk = (         â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    415              self.chunked_prefill_size   â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ is not None and server_args.enable_mixed_chunk  â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    416          )                               â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    417                                          â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    418          # Init the grammar backend for  â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ constrained generation                          â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    419          self.grammar_queue: List[Req] = â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ []                                              â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    420          if not                          â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ server_args.skip_tokenizer_init:                â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    421              self.grammar_backend =      â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ create_grammar_backend(                         â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    422                  server_args,            â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ self.tokenizer, self.model_config.vocab_size    â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    423              )                           â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    424          else:                           â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    425              self.grammar_backend = None â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    426                                          â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    427          # Init schedule policy and new  â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ token estimation                                â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    428          self.policy = SchedulePolicy(   â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    429              self.schedule_policy,       â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    430              self.tree_cache,            â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    431                                          â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ self.enable_hierarchical_cache,                 â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    432          )                               â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    433          assert (                        â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    434                                          â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ server_args.schedule_conservativeness >= 0      â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    435          ), "Invalid                     â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ schedule_conservativeness"                      â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    436          self.init_new_token_ratio =     â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ min(                                            â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    437                                          â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ global_config.default_init_new_token_ratio      â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    438              *                           â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ server_args.schedule_conservativeness,          â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    439              1.0,                        â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    440          )                               â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    441          self.min_new_token_ratio = min( â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    442              self.init_new_token_ratio   â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    443              *                           â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ global_config.default_min_new_token_ratio_factâ€¦ â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    444              1.0,                        â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    445       <response clipped><NOTE>To save on â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ context only part of this file has been shown   â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ to you. You should retry this tool after you    â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ have searched inside the file with `grep -n` in â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ order to find the line numbers of what you are  â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ looking for.</NOTE>                             â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                                                 â”‚   â”‚
â”‚             â”‚ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚
â”‚ Total       â”‚ Input: 18072 Output: 1468                                                                              â”‚
â”‚ Tokens      â”‚                                                                                                        â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Status      â”‚ âœ… Step 3: Completed                                                                                   â”‚
â”‚ Tools       â”‚ ðŸ”§ bash                                                                                                â”‚
â”‚ bash        â”‚ â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚
â”‚             â”‚ â”‚ {'command': 'grep -n                           â”‚                                                 â”‚   â”‚
â”‚             â”‚ â”‚ "torch\\.zeros\\|fill_\\|torch\\.empty\\|tensâ€¦ â”‚                                                 â”‚   â”‚
â”‚             â”‚ â”‚ -n                                             â”‚                                                 â”‚   â”‚
â”‚             â”‚ â”‚ /home/ubuntu/OmniPerf-Bench/perf-agents-benchâ€¦ â”‚                                                 â”‚   â”‚
â”‚             â”‚ â”‚ | head -200', 'restart': False}                â”‚                                                 â”‚   â”‚
â”‚             â”‚ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚
â”‚ Total       â”‚ Input: 33260 Output: 1564                                                                              â”‚
â”‚ Tokens      â”‚                                                                                                        â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Status      â”‚ âœ… Step 4: Completed                                                                                   â”‚
â”‚ Tools       â”‚ ðŸ”§ bash                                                                                                â”‚
â”‚ bash        â”‚ â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚
â”‚             â”‚ â”‚ {'command': 'grep -nE                          â”‚ 342:            self.max_prefill_tokens,        â”‚   â”‚
â”‚             â”‚ â”‚ "torch\\.zeros|fill_|torch\\.empty|tensor\\("  â”‚ 376:                                            â”‚   â”‚
â”‚             â”‚ â”‚ /home/ubuntu/OmniPerf-Bench/perf-agents-benchâ€¦ â”‚ f"chunked_prefill_size={server_args.chunked_prâ€¦ â”‚   â”‚
â”‚             â”‚ â”‚ | sed -n \'1,200p\'', 'restart': False}        â”‚ "                                               â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ 377:                                            â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ f"max_prefill_tokens={self.max_prefill_tokens}, â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ "                                               â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ 397:        self.last_prefill_tokens = 0        â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ 399:        self.last_prefill_stats_tic =       â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ time.perf_counter()                             â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ 410:        self.chunked_prefill_size =         â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ server_args.chunked_prefill_size                â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ 411:        if self.chunked_prefill_size <= 0:  â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ # -1 means disable                              â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ 412:            self.chunked_prefill_size =     â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ None                                            â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ 415:            self.chunked_prefill_size is    â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ not None and server_args.enable_mixed_chunk     â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ 466:        self.profiler_target_prefill_ct:    â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ Optional = None                                 â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ 468:        self.profiler_prefill_ct: Optional  â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ = None                                          â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ 550:                                            â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ server_args.chunked_prefill_size is not None    â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ 672:                                            â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ prefill_pp_size=self.server_args.disaggregatioâ€¦ â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ 693:                                            â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ self.disagg_prefill_bootstrap_queue =           â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ PrefillBootstrapQueue(                          â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ 716:                                            â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ self.disagg_prefill_inflight_queue: List[Req] = â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ []                                              â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ 1174:                                           â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ self.disagg_prefill_bootstrap_queue.add(        â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ 1184:                                           â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ self.disagg_prefill_bootstrap_queue.extend(     â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ 1239:    def log_prefill_stats(                 â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ 1245:        gap_latency = time.perf_counter()  â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ - self.last_prefill_stats_tic                   â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ 1246:        self.last_prefill_stats_tic =      â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ time.perf_counter()                             â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ 1247:        self.last_input_throughput =       â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ self.last_prefill_tokens / gap_latency          â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ 1248:        self.last_prefill_tokens =         â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ adder.log_input_tokens                          â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ 1265:            f += f"#unbootstrapped-req:    â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ {len(self.disagg_prefill_bootstrap_queue.queueâ€¦ â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ "                                               â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ 1267:            f += f"#transferring-req:      â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ {len(self.disagg_prefill_inflight_queue)}, "    â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ 1405:        local_info = torch.tensor(         â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ 1411:        global_info = torch.empty(         â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ 1415:                                           â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ torch.distributed.all_gather_into_tensor(       â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ 1521:            self.max_prefill_tokens,       â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ 1522:            self.chunked_prefill_size,     â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ 1595:            self.log_prefill_stats(adder,  â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ can_run_list, running_bs)                       â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ 1845:        local_info = torch.tensor(         â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ 1860:        global_info = torch.empty(         â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ 1864:                                           â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ torch.distributed.all_gather_into_tensor(       â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ 1950:            tensor = torch.tensor(,        â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ dtype=torch.int32)                              â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ 2074:                for req in                 â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ self.disagg_prefill_bootstrap_queue.queue       â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ 2243:    def update_weights_from_tensor(self,   â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ recv_req: UpdateWeightsFromTensorReqInput):     â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ 2245:        success, message =                 â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ self.tp_worker.update_weights_from_tensor(recvâ€¦ â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ 2361:                                           â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ self.profiler_target_prefill_ct = num_steps     â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ 2363:                self.profiler_prefill_ct = â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ 0                                               â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ 2506:                if                         â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ self.profiler_prefill_ct == 0:                  â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ 2508:                self.profiler_prefill_ct   â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ += 1                                            â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ 2509:                if                         â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ self.profiler_prefill_ct >                      â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ self.profiler_target_prefill_ct:                â”‚   â”‚
â”‚             â”‚ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚
â”‚ Total       â”‚ Input: 48594 Output: 1661                                                                              â”‚
â”‚ Tokens      â”‚                                                                                                        â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Status      â”‚ âœ… Step 5: Completed                                                                                   â”‚
â”‚ Tools       â”‚ ðŸ”§ str_replace_based_edit_tool                                                                         â”‚
â”‚ str_replacâ€¦ â”‚ â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚
â”‚             â”‚ â”‚ {'command': 'view', 'file_text': '',           â”‚ Here's the result of running `cat -n` on        â”‚   â”‚
â”‚             â”‚ â”‚ 'insert_line': 1, 'new_str': '', 'old_str':    â”‚ /home/ubuntu/OmniPerf-Bench/perf-agents-bench/â€¦ â”‚   â”‚
â”‚             â”‚ â”‚ '', 'path':                                    â”‚   1375              msg = (                     â”‚   â”‚
â”‚             â”‚ â”‚ '/home/ubuntu/OmniPerf-Bench/perf-agents-bencâ€¦ â”‚   1376                  "req_to_token_pool      â”‚   â”‚
â”‚             â”‚ â”‚ 'view_range': [1375, 1430]}                    â”‚ memory leak detected!"                          â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚   1377                                          â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ f"available_size={len(self.req_to_token_pool.fâ€¦ â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ "                                               â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚   1378                                          â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ f"total_size={self.req_to_token_pool.size}\n"   â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚   1379              )                           â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚   1380              raise ValueError(msg)       â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚   1381                                          â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚   1382          if (                            â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚   1383              self.enable_metrics         â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚   1384              and self.attn_tp_rank == 0  â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚   1385              and time.perf_counter() >   â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ self.metrics_collector.last_log_time + 30       â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚   1386          ):                              â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚   1387              # During idle time, also    â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ collect metrics every 30 seconds.               â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚   1388              num_used =                  â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ self.max_total_num_tokens - (                   â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚   1389                                          â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ self.token_to_kv_pool_allocator.available_sizeâ€¦ â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚   1390                  +                       â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ self.tree_cache.evictable_size()                â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚   1391              )                           â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚   1392              num_running_reqs =          â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ len(self.running_batch.reqs)                    â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚   1393              self.stats.num_running_reqs â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ = num_running_reqs                              â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚   1394              self.stats.num_used_tokens  â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ = num_used                                      â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚   1395              self.stats.token_usage =    â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ num_used / self.max_total_num_tokens            â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚   1396              self.stats.gen_throughput = â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ 0                                               â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚   1397              self.stats.num_queue_reqs = â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ len(self.waiting_queue)                         â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚   1398                                          â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ self.stats.num_grammar_queue_reqs =             â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ len(self.grammar_queue)                         â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚   1399                                          â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ self.metrics_collector.log_stats(self.stats)    â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚   1400          self._publish_kv_events()       â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚   1401                                          â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚   1402      def                                 â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ coordinate_spec_dp_attn_batch(self, new_batch:  â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ Optional[ScheduleBatch]):                       â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚   1403          """Coordinate the DP attention  â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ batch."""                                       â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚   1404                                          â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚   1405          local_info = torch.tensor(      â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚   1406              [                           â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚   1407                  (new_batch is not       â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ None),                                          â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚   1408              ],                          â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚   1409              dtype=torch.int64,          â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚   1410          )                               â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚   1411          global_info = torch.empty(      â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚   1412              (self.server_args.dp_size,  â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ self.attn_tp_size, 1),                          â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚   1413              dtype=torch.int64,          â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚   1414          )                               â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚   1415                                          â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ torch.distributed.all_gather_into_tensor(       â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚   1416              global_info.flatten(),      â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚   1417              local_info,                 â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚   1418              group=self.tp_cpu_group,    â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚   1419          )                               â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚   1420          any_new_batch = any(            â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚   1421              global_info[:, 0,           â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ 0].tolist()                                     â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚   1422          )  # Any DP worker has forward  â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ batch                                           â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚   1423          return any_new_batch            â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚   1424                                          â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚   1425      def get_next_batch_to_run(self) ->  â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ Optional[ScheduleBatch]:                        â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚   1426          # Merge the prefill batch into  â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ the running batch                               â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚   1427          chunked_req_to_exclude = set()  â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚   1428          if self.chunked_req:            â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚   1429              # Move the chunked request  â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ out of the batch so that we can merge           â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚   1430              # only finished requests to â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ running_batch.                                  â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                                                 â”‚   â”‚
â”‚             â”‚ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚
â”‚ Total       â”‚ Input: 64757 Output: 1820                                                                              â”‚
â”‚ Tokens      â”‚                                                                                                        â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Status      â”‚ âœ… Step 6: Completed                                                                                   â”‚
â”‚ Tools       â”‚ ðŸ”§ str_replace_based_edit_tool                                                                         â”‚
â”‚ str_replacâ€¦ â”‚ â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚
â”‚             â”‚ â”‚ {'command': 'view', 'file_text': '',           â”‚ Here's the result of running `cat -n` on        â”‚   â”‚
â”‚             â”‚ â”‚ 'insert_line': 1, 'new_str': '', 'old_str':    â”‚ /home/ubuntu/OmniPerf-Bench/perf-agents-bench/â€¦ â”‚   â”‚
â”‚             â”‚ â”‚ '', 'path':                                    â”‚   1825                      )                   â”‚   â”‚
â”‚             â”‚ â”‚ '/home/ubuntu/OmniPerf-Bench/perf-agents-bencâ€¦ â”‚   1826                  ]                       â”‚   â”‚
â”‚             â”‚ â”‚ 'view_range': [1825, 1875]}                    â”‚   1827              )                           â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚   1828                                          â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚   1829          if local_batch is None or       â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ local_batch.forward_mode.is_decode_or_idle():   â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚   1830              can_cuda_graph = 1          â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚   1831          else:                           â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚   1832              can_cuda_graph = 0          â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚   1833                                          â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚   1834          if not                          â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ spec_algorithm.is_none():                       â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚   1835              # TODO(sang): Support cuda  â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ graph when idle batch is there.                 â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚   1836              if local_batch is None or   â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ local_batch.forward_mode.is_idle():             â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚   1837                  can_cuda_graph = 0      â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚   1838                                          â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚   1839          is_extend_in_batch = (          â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚   1840                                          â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ local_batch.forward_mode.is_extend() if         â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ local_batch else False                          â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚   1841          )                               â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚   1842                                          â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚   1843          tbo_preparer =                  â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ TboDPAttentionPreparer()                        â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚   1844                                          â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚   1845          local_info = torch.tensor(      â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚   1846              [                           â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚   1847                  num_tokens,             â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚   1848                  can_cuda_graph,         â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚   1849                  num_tokens_for_logprob, â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚   1850                  is_extend_in_batch,     â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚   1851                                          â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ *tbo_preparer.prepare_all_gather(               â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚   1852                      local_batch,        â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚   1853                      deepep_mode,        â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚   1854                      enable_deepep_moe,  â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚   1855                                          â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ enable_two_batch_overlap,                       â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚   1856                  ),                      â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚   1857              ],                          â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚   1858              dtype=torch.int64,          â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚   1859          )                               â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚   1860          global_info = torch.empty(      â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚   1861              (dp_size, attn_tp_size, 6), â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚   1862              dtype=torch.int64,          â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚   1863          )                               â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚   1864                                          â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ torch.distributed.all_gather_into_tensor(       â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚   1865              global_info.flatten(),      â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚   1866              local_info,                 â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚   1867              group=tp_cpu_group,         â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚   1868          )                               â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚   1869          global_num_tokens =             â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ global_info[:, 0, 0].tolist()                   â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚   1870          can_cuda_graph =                â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ min(global_info[:, 0, 1].tolist())              â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚   1871          global_num_tokens_for_logprob = â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ global_info[:, 0, 2].tolist()                   â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚   1872          is_extend_in_batch =            â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ global_info[:, 0, 3].tolist()                   â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚   1873                                          â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚   1874          tbo_split_seq_index,            â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ global_forward_mode =                           â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ tbo_preparer.compute_output(                    â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚   1875              global_info[:, :, 4:6]      â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                                                 â”‚   â”‚
â”‚             â”‚ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚
â”‚ Total       â”‚ Input: 81872 Output: 2363                                                                              â”‚
â”‚ Tokens      â”‚                                                                                                        â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Status      â”‚ âœ… Step 7: Completed                                                                                   â”‚
â”‚ Tools       â”‚ ðŸ”§ str_replace_based_edit_tool                                                                         â”‚
â”‚ str_replacâ€¦ â”‚ â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚
â”‚             â”‚ â”‚ {'command': 'view', 'file_text': '',           â”‚ Here's the result of running `cat -n` on        â”‚   â”‚
â”‚             â”‚ â”‚ 'insert_line': 1, 'new_str': '', 'old_str':    â”‚ /home/ubuntu/OmniPerf-Bench/perf-agents-bench/â€¦ â”‚   â”‚
â”‚             â”‚ â”‚ '', 'path':                                    â”‚   1870          can_cuda_graph =                â”‚   â”‚
â”‚             â”‚ â”‚ '/home/ubuntu/OmniPerf-Bench/perf-agents-bencâ€¦ â”‚ min(global_info[:, 0, 1].tolist())              â”‚   â”‚
â”‚             â”‚ â”‚ 'view_range': [1870, 1905]}                    â”‚   1871          global_num_tokens_for_logprob = â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ global_info[:, 0, 2].tolist()                   â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚   1872          is_extend_in_batch =            â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ global_info[:, 0, 3].tolist()                   â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚   1873                                          â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚   1874          tbo_split_seq_index,            â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ global_forward_mode =                           â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ tbo_preparer.compute_output(                    â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚   1875              global_info[:, :, 4:6]      â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚   1876          )                               â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚   1877                                          â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚   1878          if local_batch is None and      â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ max(global_num_tokens) > 0:                     â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚   1879              local_batch =               â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ get_idle_batch()                                â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚   1880                                          â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚   1881          if local_batch is not None:     â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚   1882              # TODO: handle the case     â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ when moe_dense_tp_size != 1                     â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚   1883              if not                      â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ require_mlp_tp_gather:                          â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚   1884                                          â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ local_batch.global_num_tokens =                 â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚   1885                                          â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ local_batch.global_num_tokens_for_logprob =     â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚   1886              else:                       â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚   1887                                          â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ local_batch.global_num_tokens =                 â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ global_num_tokens                               â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚   1888                                          â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ local_batch.global_num_tokens_for_logprob = (   â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚   1889                                          â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ global_num_tokens_for_logprob                   â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚   1890                  )                       â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚   1891                                          â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ local_batch.is_extend_in_batch =                â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ any(is_extend_in_batch)                         â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚   1892                                          â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ local_batch.tbo_split_seq_index =               â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ tbo_split_seq_index                             â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚   1893                                          â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ local_batch.global_forward_mode =               â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ global_forward_mode                             â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚   1894                                          â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚   1895              # Check forward mode for    â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ cuda graph                                      â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚   1896              if not disable_cuda_graph:  â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚   1897                                          â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ local_batch.can_run_dp_cuda_graph =             â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ can_cuda_graph                                  â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚   1898                                          â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚   1899          # TODO(ch-wan): refactor:       â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ any(is_extend_in_batch) now is a part of        â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ local_batch. Remove it from here.               â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚   1900          return local_batch,             â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ any(is_extend_in_batch)                         â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚   1901                                          â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚   1902      def get_idle_batch(self):           â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚   1903          idle_batch =                    â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ ScheduleBatch.init_new(                         â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚   1904              [],                         â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚   1905              self.req_to_token_pool,     â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                                                 â”‚   â”‚
â”‚             â”‚ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚
â”‚ Total       â”‚ Input: 100184 Output: 3290                                                                             â”‚
â”‚ Tokens      â”‚                                                                                                        â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Status      â”‚ âœ… Step 8: Completed                                                                                   â”‚
â”‚ Tools       â”‚ ðŸ”§ str_replace_based_edit_tool                                                                         â”‚
â”‚ str_replacâ€¦ â”‚ â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚
â”‚             â”‚ â”‚ {'command': 'view', 'file_text': '',           â”‚ Here's the result of running `cat -n` on        â”‚   â”‚
â”‚             â”‚ â”‚ 'insert_line': 1, 'new_str': '', 'old_str':    â”‚ /home/ubuntu/OmniPerf-Bench/perf-agents-bench/â€¦ â”‚   â”‚
â”‚             â”‚ â”‚ '', 'path':                                    â”‚   1760      ):                                  â”‚   â”‚
â”‚             â”‚ â”‚ '/home/ubuntu/OmniPerf-Bench/perf-agents-bencâ€¦ â”‚   1761          if                              â”‚   â”‚
â”‚             â”‚ â”‚ 'view_range': [1760, 1855]}                    â”‚ batch.forward_mode.is_decode():                 â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚   1762                                          â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ self.process_batch_result_decode(batch, result, â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ launch_done)                                    â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚   1763          elif                            â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ batch.forward_mode.is_extend():                 â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚   1764                                          â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ self.process_batch_result_prefill(batch,        â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ result, launch_done)                            â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚   1765          elif                            â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ batch.forward_mode.is_idle():                   â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚   1766              if self.enable_overlap:     â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚   1767                                          â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ self.tp_worker.resolve_last_batch_result(launcâ€¦ â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚   1768                                          â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ self.set_next_batch_sampling_info_done(batch)   â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚   1769          elif                            â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ batch.forward_mode.is_dummy_first():            â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚   1770                                          â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ self.set_next_batch_sampling_info_done(batch)   â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚   1771                                          â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚   1772          if self.return_health_check_ct: â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚   1773              # Return some signal for    â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ the health check.                               â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚   1774              # This is used to prevent   â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ the health check signal being blocked by long   â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ context prefill.                                â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚   1775              # However, one minor issue  â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ is that this code path does not check the       â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ status of detokenizer manager.                  â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚   1776              self.return_health_check_ct â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ -= 1                                            â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚   1777                                          â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ self.send_to_tokenizer.send_pyobj(HealthCheckOâ€¦ â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚   1778                                          â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚   1779      def prepare_mlp_sync_batch(self,    â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ local_batch: ScheduleBatch):                    â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚   1780          return                          â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ self.prepare_mlp_sync_batch_raw(                â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚   1781              local_batch,                â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚   1782                                          â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ dp_size=self.server_args.dp_size,               â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚   1783                                          â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ attn_tp_size=self.attn_tp_size,                 â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚   1784                                          â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ tp_cpu_group=self.tp_cpu_group,                 â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚   1785                                          â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ get_idle_batch=self.get_idle_batch,             â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚   1786                                          â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ disable_cuda_graph=self.server_args.disable_cuâ€¦ â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚   1787                                          â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ spec_algorithm=self.spec_algorithm,             â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚   1788                                          â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ speculative_num_draft_tokens=self.server_args.â€¦ â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚   1789                                          â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ enable_two_batch_overlap=self.server_args.enabâ€¦ â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚   1790                                          â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ enable_deepep_moe=self.server_args.enable_deepâ€¦ â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚   1791              deepep_mode=DeepEPMode,     â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚   1792                                          â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ require_mlp_tp_gather=require_mlp_tp_gather(seâ€¦ â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚   1793          )                               â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚   1794                                          â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚   1795      @staticmethod                       â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚   1796      def prepare_mlp_sync_batch_raw(     â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚   1797          local_batch: ScheduleBatch,     â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚   1798          dp_size,                        â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚   1799          attn_tp_size: int,              â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚   1800          tp_cpu_group,                   â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚   1801          get_idle_batch,                 â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚   1802          disable_cuda_graph: bool,       â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚   1803          spec_algorithm,                 â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚   1804          speculative_num_draft_tokens,   â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚   1805          enable_two_batch_overlap: bool, â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚   1806          enable_deepep_moe: bool,        â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚   1807          deepep_mode: DeepEPMode,        â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚   1808          require_mlp_tp_gather: bool,    â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚   1809      ):                                  â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚   1810          # Check if other DP workers     â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ have running batches                            â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚   1811          if local_batch is None:         â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚   1812              num_tokens = 0              â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚   1813              num_tokens_for_logprob = 0  â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚   1814          elif                            â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ local_batch.forward_mode.is_decode():           â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚   1815              num_tokens =                â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ local_batch.batch_size()                        â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚   1816              num_tokens_for_logprob =    â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ num_tokens                                      â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚   1817          else:                           â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚   1818              num_tokens =                â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ local_batch.extend_num_tokens                   â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚   1819              num_tokens_for_logprob =    â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ sum(                                            â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚   1820                  [                       â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚   1821                      # We should have at â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ least 1 token for sample in every case.         â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚   1822                      max(extend_len -    â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ logprob_start_len, 1)                           â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚   1823                      for                 â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ logprob_start_len, extend_len in zip(           â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚   1824                                          â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ local_batch.extend_logprob_start_lens,          â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ local_batch.extend_lens                         â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚   1825                      )                   â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚   1826                  ]                       â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚   1827              )                           â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚   1828                                          â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚   1829          if local_batch is None or       â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ local_batch.forward_mode.is_decode_or_idle():   â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚   1830              can_cuda_graph = 1          â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚   1831          else:                           â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚   1832              can_cuda_graph = 0          â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚   1833                                          â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚   1834          if not                          â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ spec_algorithm.is_none():                       â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚   1835              # TODO(sang): Support cuda  â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ graph when idle batch is there.                 â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚   1836              if local_batch is None or   â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ local_batch.forward_mode.is_idle():             â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚   1837                  can_cuda_graph = 0      â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚   1838                                          â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚   1839          is_extend_in_batch = (          â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚   1840                                          â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ local_batch.forward_mode.is_extend() if         â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ local_batch else False                          â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚   1841          )                               â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚   1842                                          â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚   1843          tbo_preparer =                  â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ TboDPAttentionPreparer()                        â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚   1844                                          â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚   1845          local_info = torch.tensor(      â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚   1846              [                           â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚   1847                  num_tokens,             â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚   1848                  can_cuda_graph,         â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚   1849                  num_tokens_for_logprob, â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚   1850                  is_extend_in_batch,     â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚   1851                                          â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ *tbo_preparer.prepare_all_gather(               â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚   1852                      local_batch,        â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚   1853                      deepep_mode,        â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚   1854                      enable_deepep_moe,  â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚   1855                                          â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ enable_two_batch_overlap,                       â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                                                 â”‚   â”‚
â”‚             â”‚ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚
â”‚ Total       â”‚ Input: 120029 Output: 4665                                                                             â”‚
â”‚ Tokens      â”‚                                                                                                        â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Status      â”‚ âœ… Step 9: Completed                                                                                   â”‚
â”‚ Tools       â”‚ ðŸ”§ str_replace_based_edit_tool                                                                         â”‚
â”‚ str_replacâ€¦ â”‚ â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚
â”‚             â”‚ â”‚ {'command': 'view', 'file_text': '',           â”‚ Here's the result of running `cat -n` on        â”‚   â”‚
â”‚             â”‚ â”‚ 'insert_line': 1, 'new_str': '', 'old_str':    â”‚ /home/ubuntu/OmniPerf-Bench/perf-agents-bench/â€¦ â”‚   â”‚
â”‚             â”‚ â”‚ '', 'path':                                    â”‚   1850                  is_extend_in_batch,     â”‚   â”‚
â”‚             â”‚ â”‚ '/home/ubuntu/OmniPerf-Bench/perf-agents-bencâ€¦ â”‚   1851                                          â”‚   â”‚
â”‚             â”‚ â”‚ 'view_range': [1850, 1905]}                    â”‚ *tbo_preparer.prepare_all_gather(               â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚   1852                      local_batch,        â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚   1853                      deepep_mode,        â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚   1854                      enable_deepep_moe,  â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚   1855                                          â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ enable_two_batch_overlap,                       â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚   1856                  ),                      â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚   1857              ],                          â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚   1858              dtype=torch.int64,          â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚   1859          )                               â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚   1860          global_info = torch.empty(      â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚   1861              (dp_size, attn_tp_size, 6), â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚   1862              dtype=torch.int64,          â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚   1863          )                               â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚   1864                                          â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ torch.distributed.all_gather_into_tensor(       â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚   1865              global_info.flatten(),      â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚   1866              local_info,                 â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚   1867              group=tp_cpu_group,         â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚   1868          )                               â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚   1869          global_num_tokens =             â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ global_info[:, 0, 0].tolist()                   â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚   1870          can_cuda_graph =                â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ min(global_info[:, 0, 1].tolist())              â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚   1871          global_num_tokens_for_logprob = â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ global_info[:, 0, 2].tolist()                   â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚   1872          is_extend_in_batch =            â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ global_info[:, 0, 3].tolist()                   â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚   1873                                          â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚   1874          tbo_split_seq_index,            â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ global_forward_mode =                           â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ tbo_preparer.compute_output(                    â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚   1875              global_info[:, :, 4:6]      â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚   1876          )                               â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚   1877                                          â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚   1878          if local_batch is None and      â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ max(global_num_tokens) > 0:                     â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚   1879              local_batch =               â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ get_idle_batch()                                â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚   1880                                          â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚   1881          if local_batch is not None:     â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚   1882              # TODO: handle the case     â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ when moe_dense_tp_size != 1                     â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚   1883              if not                      â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ require_mlp_tp_gather:                          â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚   1884                                          â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ local_batch.global_num_tokens =                 â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚   1885                                          â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ local_batch.global_num_tokens_for_logprob =     â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚   1886              else:                       â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚   1887                                          â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ local_batch.global_num_tokens =                 â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ global_num_tokens                               â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚   1888                                          â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ local_batch.global_num_tokens_for_logprob = (   â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚   1889                                          â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ global_num_tokens_for_logprob                   â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚   1890                  )                       â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚   1891                                          â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ local_batch.is_extend_in_batch =                â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ any(is_extend_in_batch)                         â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚   1892                                          â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ local_batch.tbo_split_seq_index =               â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ tbo_split_seq_index                             â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚   1893                                          â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ local_batch.global_forward_mode =               â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ global_forward_mode                             â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚   1894                                          â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚   1895              # Check forward mode for    â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ cuda graph                                      â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚   1896              if not disable_cuda_graph:  â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚   1897                                          â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ local_batch.can_run_dp_cuda_graph =             â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ can_cuda_graph                                  â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚   1898                                          â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚   1899          # TODO(ch-wan): refactor:       â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ any(is_extend_in_batch) now is a part of        â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ local_batch. Remove it from here.               â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚   1900          return local_batch,             â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ any(is_extend_in_batch)                         â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚   1901                                          â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚   1902      def get_idle_batch(self):           â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚   1903          idle_batch =                    â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ ScheduleBatch.init_new(                         â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚   1904              [],                         â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚   1905              self.req_to_token_pool,     â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                                                 â”‚   â”‚
â”‚             â”‚ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚
â”‚ Total       â”‚ Input: 142502 Output: 4760                                                                             â”‚
â”‚ Tokens      â”‚                                                                                                        â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
OpenAI API call failed: Error code: 429 - {'error': {'message': 'You exceeded your current quota, please check your plan and billing details. For more information on this error, read the docs: https://platform.openai.com/docs/guides/error-codes/api-errors.', 'type': 'insufficient_quota', 'param': None, 'code': 'insufficient_quota'}}. Will sleep for 19 seconds and will retry.
Traceback (most recent call last):
  File "/home/ubuntu/OmniPerf-Bench/third-party/trae-agent/trae_agent/utils/llm_clients/retry_utils.py", line 36, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/OmniPerf-Bench/third-party/trae-agent/trae_agent/utils/llm_clients/openai_client.py", line 86, in _create_openai_response
    response = self.client.responses.create(**api_params)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/OmniPerf-Bench/bench-env/lib/python3.12/site-packages/openai/resources/responses/responses.py", line 795, in create
    return self._post(
           ^^^^^^^^^^^
  File "/home/ubuntu/OmniPerf-Bench/bench-env/lib/python3.12/site-packages/openai/_base_client.py", line 1259, in post
    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))
                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/OmniPerf-Bench/bench-env/lib/python3.12/site-packages/openai/_base_client.py", line 1047, in request
    raise self._make_status_error_from_response(err.response) from None
openai.RateLimitError: Error code: 429 - {'error': {'message': 'You exceeded your current quota, please check your plan and billing details. For more information on this error, read the docs: https://platform.openai.com/docs/guides/error-codes/api-errors.', 'type': 'insufficient_quota', 'param': None, 'code': 'insufficient_quota'}}

OpenAI API call failed: Error code: 429 - {'error': {'message': 'You exceeded your current quota, please check your plan and billing details. For more information on this error, read the docs: https://platform.openai.com/docs/guides/error-codes/api-errors.', 'type': 'insufficient_quota', 'param': None, 'code': 'insufficient_quota'}}. Will sleep for 3 seconds and will retry.
Traceback (most recent call last):
  File "/home/ubuntu/OmniPerf-Bench/third-party/trae-agent/trae_agent/utils/llm_clients/retry_utils.py", line 36, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/OmniPerf-Bench/third-party/trae-agent/trae_agent/utils/llm_clients/openai_client.py", line 86, in _create_openai_response
    response = self.client.responses.create(**api_params)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/OmniPerf-Bench/bench-env/lib/python3.12/site-packages/openai/resources/responses/responses.py", line 795, in create
    return self._post(
           ^^^^^^^^^^^
  File "/home/ubuntu/OmniPerf-Bench/bench-env/lib/python3.12/site-packages/openai/_base_client.py", line 1259, in post
    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))
                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/OmniPerf-Bench/bench-env/lib/python3.12/site-packages/openai/_base_client.py", line 1047, in request
    raise self._make_status_error_from_response(err.response) from None
openai.RateLimitError: Error code: 429 - {'error': {'message': 'You exceeded your current quota, please check your plan and billing details. For more information on this error, read the docs: https://platform.openai.com/docs/guides/error-codes/api-errors.', 'type': 'insufficient_quota', 'param': None, 'code': 'insufficient_quota'}}

OpenAI API call failed: Error code: 429 - {'error': {'message': 'You exceeded your current quota, please check your plan and billing details. For more information on this error, read the docs: https://platform.openai.com/docs/guides/error-codes/api-errors.', 'type': 'insufficient_quota', 'param': None, 'code': 'insufficient_quota'}}. Will sleep for 24 seconds and will retry.
Traceback (most recent call last):
  File "/home/ubuntu/OmniPerf-Bench/third-party/trae-agent/trae_agent/utils/llm_clients/retry_utils.py", line 36, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/OmniPerf-Bench/third-party/trae-agent/trae_agent/utils/llm_clients/openai_client.py", line 86, in _create_openai_response
    response = self.client.responses.create(**api_params)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/OmniPerf-Bench/bench-env/lib/python3.12/site-packages/openai/resources/responses/responses.py", line 795, in create
    return self._post(
           ^^^^^^^^^^^
  File "/home/ubuntu/OmniPerf-Bench/bench-env/lib/python3.12/site-packages/openai/_base_client.py", line 1259, in post
    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))
                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/OmniPerf-Bench/bench-env/lib/python3.12/site-packages/openai/_base_client.py", line 1047, in request
    raise self._make_status_error_from_response(err.response) from None
openai.RateLimitError: Error code: 429 - {'error': {'message': 'You exceeded your current quota, please check your plan and billing details. For more information on this error, read the docs: https://platform.openai.com/docs/guides/error-codes/api-errors.', 'type': 'insufficient_quota', 'param': None, 'code': 'insufficient_quota'}}

OpenAI API call failed: Error code: 429 - {'error': {'message': 'You exceeded your current quota, please check your plan and billing details. For more information on this error, read the docs: https://platform.openai.com/docs/guides/error-codes/api-errors.', 'type': 'insufficient_quota', 'param': None, 'code': 'insufficient_quota'}}. Will sleep for 30 seconds and will retry.
Traceback (most recent call last):
  File "/home/ubuntu/OmniPerf-Bench/third-party/trae-agent/trae_agent/utils/llm_clients/retry_utils.py", line 36, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/OmniPerf-Bench/third-party/trae-agent/trae_agent/utils/llm_clients/openai_client.py", line 86, in _create_openai_response
    response = self.client.responses.create(**api_params)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/OmniPerf-Bench/bench-env/lib/python3.12/site-packages/openai/resources/responses/responses.py", line 795, in create
    return self._post(
           ^^^^^^^^^^^
  File "/home/ubuntu/OmniPerf-Bench/bench-env/lib/python3.12/site-packages/openai/_base_client.py", line 1259, in post
    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))
                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/OmniPerf-Bench/bench-env/lib/python3.12/site-packages/openai/_base_client.py", line 1047, in request
    raise self._make_status_error_from_response(err.response) from None
openai.RateLimitError: Error code: 429 - {'error': {'message': 'You exceeded your current quota, please check your plan and billing details. For more information on this error, read the docs: https://platform.openai.com/docs/guides/error-codes/api-errors.', 'type': 'insufficient_quota', 'param': None, 'code': 'insufficient_quota'}}

OpenAI API call failed: Error code: 429 - {'error': {'message': 'You exceeded your current quota, please check your plan and billing details. For more information on this error, read the docs: https://platform.openai.com/docs/guides/error-codes/api-errors.', 'type': 'insufficient_quota', 'param': None, 'code': 'insufficient_quota'}}. Will sleep for 3 seconds and will retry.
Traceback (most recent call last):
  File "/home/ubuntu/OmniPerf-Bench/third-party/trae-agent/trae_agent/utils/llm_clients/retry_utils.py", line 36, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/OmniPerf-Bench/third-party/trae-agent/trae_agent/utils/llm_clients/openai_client.py", line 86, in _create_openai_response
    response = self.client.responses.create(**api_params)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/OmniPerf-Bench/bench-env/lib/python3.12/site-packages/openai/resources/responses/responses.py", line 795, in create
    return self._post(
           ^^^^^^^^^^^
  File "/home/ubuntu/OmniPerf-Bench/bench-env/lib/python3.12/site-packages/openai/_base_client.py", line 1259, in post
    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))
                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/OmniPerf-Bench/bench-env/lib/python3.12/site-packages/openai/_base_client.py", line 1047, in request
    raise self._make_status_error_from_response(err.response) from None
openai.RateLimitError: Error code: 429 - {'error': {'message': 'You exceeded your current quota, please check your plan and billing details. For more information on this error, read the docs: https://platform.openai.com/docs/guides/error-codes/api-errors.', 'type': 'insufficient_quota', 'param': None, 'code': 'insufficient_quota'}}

OpenAI API call failed: Error code: 429 - {'error': {'message': 'You exceeded your current quota, please check your plan and billing details. For more information on this error, read the docs: https://platform.openai.com/docs/guides/error-codes/api-errors.', 'type': 'insufficient_quota', 'param': None, 'code': 'insufficient_quota'}}. Will sleep for 19 seconds and will retry.
Traceback (most recent call last):
  File "/home/ubuntu/OmniPerf-Bench/third-party/trae-agent/trae_agent/utils/llm_clients/retry_utils.py", line 36, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/OmniPerf-Bench/third-party/trae-agent/trae_agent/utils/llm_clients/openai_client.py", line 86, in _create_openai_response
    response = self.client.responses.create(**api_params)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/OmniPerf-Bench/bench-env/lib/python3.12/site-packages/openai/resources/responses/responses.py", line 795, in create
    return self._post(
           ^^^^^^^^^^^
  File "/home/ubuntu/OmniPerf-Bench/bench-env/lib/python3.12/site-packages/openai/_base_client.py", line 1259, in post
    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))
                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/OmniPerf-Bench/bench-env/lib/python3.12/site-packages/openai/_base_client.py", line 1047, in request
    raise self._make_status_error_from_response(err.response) from None
openai.RateLimitError: Error code: 429 - {'error': {'message': 'You exceeded your current quota, please check your plan and billing details. For more information on this error, read the docs: https://platform.openai.com/docs/guides/error-codes/api-errors.', 'type': 'insufficient_quota', 'param': None, 'code': 'insufficient_quota'}}

OpenAI API call failed: Error code: 429 - {'error': {'message': 'You exceeded your current quota, please check your plan and billing details. For more information on this error, read the docs: https://platform.openai.com/docs/guides/error-codes/api-errors.', 'type': 'insufficient_quota', 'param': None, 'code': 'insufficient_quota'}}. Will sleep for 12 seconds and will retry.
Traceback (most recent call last):
  File "/home/ubuntu/OmniPerf-Bench/third-party/trae-agent/trae_agent/utils/llm_clients/retry_utils.py", line 36, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/OmniPerf-Bench/third-party/trae-agent/trae_agent/utils/llm_clients/openai_client.py", line 86, in _create_openai_response
    response = self.client.responses.create(**api_params)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/OmniPerf-Bench/bench-env/lib/python3.12/site-packages/openai/resources/responses/responses.py", line 795, in create
    return self._post(
           ^^^^^^^^^^^
  File "/home/ubuntu/OmniPerf-Bench/bench-env/lib/python3.12/site-packages/openai/_base_client.py", line 1259, in post
    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))
                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/OmniPerf-Bench/bench-env/lib/python3.12/site-packages/openai/_base_client.py", line 1047, in request
    raise self._make_status_error_from_response(err.response) from None
openai.RateLimitError: Error code: 429 - {'error': {'message': 'You exceeded your current quota, please check your plan and billing details. For more information on this error, read the docs: https://platform.openai.com/docs/guides/error-codes/api-errors.', 'type': 'insufficient_quota', 'param': None, 'code': 'insufficient_quota'}}

OpenAI API call failed: Error code: 429 - {'error': {'message': 'You exceeded your current quota, please check your plan and billing details. For more information on this error, read the docs: https://platform.openai.com/docs/guides/error-codes/api-errors.', 'type': 'insufficient_quota', 'param': None, 'code': 'insufficient_quota'}}. Will sleep for 23 seconds and will retry.
Traceback (most recent call last):
  File "/home/ubuntu/OmniPerf-Bench/third-party/trae-agent/trae_agent/utils/llm_clients/retry_utils.py", line 36, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/OmniPerf-Bench/third-party/trae-agent/trae_agent/utils/llm_clients/openai_client.py", line 86, in _create_openai_response
    response = self.client.responses.create(**api_params)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/OmniPerf-Bench/bench-env/lib/python3.12/site-packages/openai/resources/responses/responses.py", line 795, in create
    return self._post(
           ^^^^^^^^^^^
  File "/home/ubuntu/OmniPerf-Bench/bench-env/lib/python3.12/site-packages/openai/_base_client.py", line 1259, in post
    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))
                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/OmniPerf-Bench/bench-env/lib/python3.12/site-packages/openai/_base_client.py", line 1047, in request
    raise self._make_status_error_from_response(err.response) from None
openai.RateLimitError: Error code: 429 - {'error': {'message': 'You exceeded your current quota, please check your plan and billing details. For more information on this error, read the docs: https://platform.openai.com/docs/guides/error-codes/api-errors.', 'type': 'insufficient_quota', 'param': None, 'code': 'insufficient_quota'}}

OpenAI API call failed: Error code: 429 - {'error': {'message': 'You exceeded your current quota, please check your plan and billing details. For more information on this error, read the docs: https://platform.openai.com/docs/guides/error-codes/api-errors.', 'type': 'insufficient_quota', 'param': None, 'code': 'insufficient_quota'}}. Will sleep for 22 seconds and will retry.
Traceback (most recent call last):
  File "/home/ubuntu/OmniPerf-Bench/third-party/trae-agent/trae_agent/utils/llm_clients/retry_utils.py", line 36, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/OmniPerf-Bench/third-party/trae-agent/trae_agent/utils/llm_clients/openai_client.py", line 86, in _create_openai_response
    response = self.client.responses.create(**api_params)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/OmniPerf-Bench/bench-env/lib/python3.12/site-packages/openai/resources/responses/responses.py", line 795, in create
    return self._post(
           ^^^^^^^^^^^
  File "/home/ubuntu/OmniPerf-Bench/bench-env/lib/python3.12/site-packages/openai/_base_client.py", line 1259, in post
    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))
                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/OmniPerf-Bench/bench-env/lib/python3.12/site-packages/openai/_base_client.py", line 1047, in request
    raise self._make_status_error_from_response(err.response) from None
openai.RateLimitError: Error code: 429 - {'error': {'message': 'You exceeded your current quota, please check your plan and billing details. For more information on this error, read the docs: https://platform.openai.com/docs/guides/error-codes/api-errors.', 'type': 'insufficient_quota', 'param': None, 'code': 'insufficient_quota'}}

OpenAI API call failed: Error code: 429 - {'error': {'message': 'You exceeded your current quota, please check your plan and billing details. For more information on this error, read the docs: https://platform.openai.com/docs/guides/error-codes/api-errors.', 'type': 'insufficient_quota', 'param': None, 'code': 'insufficient_quota'}}. Will sleep for 23 seconds and will retry.
Traceback (most recent call last):
  File "/home/ubuntu/OmniPerf-Bench/third-party/trae-agent/trae_agent/utils/llm_clients/retry_utils.py", line 36, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/OmniPerf-Bench/third-party/trae-agent/trae_agent/utils/llm_clients/openai_client.py", line 86, in _create_openai_response
    response = self.client.responses.create(**api_params)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/OmniPerf-Bench/bench-env/lib/python3.12/site-packages/openai/resources/responses/responses.py", line 795, in create
    return self._post(
           ^^^^^^^^^^^
  File "/home/ubuntu/OmniPerf-Bench/bench-env/lib/python3.12/site-packages/openai/_base_client.py", line 1259, in post
    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))
                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/OmniPerf-Bench/bench-env/lib/python3.12/site-packages/openai/_base_client.py", line 1047, in request
    raise self._make_status_error_from_response(err.response) from None
openai.RateLimitError: Error code: 429 - {'error': {'message': 'You exceeded your current quota, please check your plan and billing details. For more information on this error, read the docs: https://platform.openai.com/docs/guides/error-codes/api-errors.', 'type': 'insufficient_quota', 'param': None, 'code': 'insufficient_quota'}}

OpenAI API call failed: Error code: 429 - {'error': {'message': 'You exceeded your current quota, please check your plan and billing details. For more information on this error, read the docs: https://platform.openai.com/docs/guides/error-codes/api-errors.', 'type': 'insufficient_quota', 'param': None, 'code': 'insufficient_quota'}}. Will sleep for 29 seconds and will retry.
Traceback (most recent call last):
  File "/home/ubuntu/OmniPerf-Bench/third-party/trae-agent/trae_agent/utils/llm_clients/retry_utils.py", line 36, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/OmniPerf-Bench/third-party/trae-agent/trae_agent/utils/llm_clients/openai_client.py", line 86, in _create_openai_response
    response = self.client.responses.create(**api_params)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/OmniPerf-Bench/bench-env/lib/python3.12/site-packages/openai/resources/responses/responses.py", line 795, in create
    return self._post(
           ^^^^^^^^^^^
  File "/home/ubuntu/OmniPerf-Bench/bench-env/lib/python3.12/site-packages/openai/_base_client.py", line 1259, in post
    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))
                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/OmniPerf-Bench/bench-env/lib/python3.12/site-packages/openai/_base_client.py", line 1047, in request
    raise self._make_status_error_from_response(err.response) from None
openai.RateLimitError: Error code: 429 - {'error': {'message': 'You exceeded your current quota, please check your plan and billing details. For more information on this error, read the docs: https://platform.openai.com/docs/guides/error-codes/api-errors.', 'type': 'insufficient_quota', 'param': None, 'code': 'insufficient_quota'}}

OpenAI API call failed: Error code: 429 - {'error': {'message': 'You exceeded your current quota, please check your plan and billing details. For more information on this error, read the docs: https://platform.openai.com/docs/guides/error-codes/api-errors.', 'type': 'insufficient_quota', 'param': None, 'code': 'insufficient_quota'}}. Will sleep for 15 seconds and will retry.
Traceback (most recent call last):
  File "/home/ubuntu/OmniPerf-Bench/third-party/trae-agent/trae_agent/utils/llm_clients/retry_utils.py", line 36, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/OmniPerf-Bench/third-party/trae-agent/trae_agent/utils/llm_clients/openai_client.py", line 86, in _create_openai_response
    response = self.client.responses.create(**api_params)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/OmniPerf-Bench/bench-env/lib/python3.12/site-packages/openai/resources/responses/responses.py", line 795, in create
    return self._post(
           ^^^^^^^^^^^
  File "/home/ubuntu/OmniPerf-Bench/bench-env/lib/python3.12/site-packages/openai/_base_client.py", line 1259, in post
    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))
                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/OmniPerf-Bench/bench-env/lib/python3.12/site-packages/openai/_base_client.py", line 1047, in request
    raise self._make_status_error_from_response(err.response) from None
openai.RateLimitError: Error code: 429 - {'error': {'message': 'You exceeded your current quota, please check your plan and billing details. For more information on this error, read the docs: https://platform.openai.com/docs/guides/error-codes/api-errors.', 'type': 'insufficient_quota', 'param': None, 'code': 'insufficient_quota'}}

OpenAI API call failed: Error code: 429 - {'error': {'message': 'You exceeded your current quota, please check your plan and billing details. For more information on this error, read the docs: https://platform.openai.com/docs/guides/error-codes/api-errors.', 'type': 'insufficient_quota', 'param': None, 'code': 'insufficient_quota'}}. Will sleep for 5 seconds and will retry.
Traceback (most recent call last):
  File "/home/ubuntu/OmniPerf-Bench/third-party/trae-agent/trae_agent/utils/llm_clients/retry_utils.py", line 36, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/OmniPerf-Bench/third-party/trae-agent/trae_agent/utils/llm_clients/openai_client.py", line 86, in _create_openai_response
    response = self.client.responses.create(**api_params)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/OmniPerf-Bench/bench-env/lib/python3.12/site-packages/openai/resources/responses/responses.py", line 795, in create
    return self._post(
           ^^^^^^^^^^^
  File "/home/ubuntu/OmniPerf-Bench/bench-env/lib/python3.12/site-packages/openai/_base_client.py", line 1259, in post
    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))
                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/OmniPerf-Bench/bench-env/lib/python3.12/site-packages/openai/_base_client.py", line 1047, in request
    raise self._make_status_error_from_response(err.response) from None
openai.RateLimitError: Error code: 429 - {'error': {'message': 'You exceeded your current quota, please check your plan and billing details. For more information on this error, read the docs: https://platform.openai.com/docs/guides/error-codes/api-errors.', 'type': 'insufficient_quota', 'param': None, 'code': 'insufficient_quota'}}

OpenAI API call failed: Error code: 429 - {'error': {'message': 'You exceeded your current quota, please check your plan and billing details. For more information on this error, read the docs: https://platform.openai.com/docs/guides/error-codes/api-errors.', 'type': 'insufficient_quota', 'param': None, 'code': 'insufficient_quota'}}. Will sleep for 11 seconds and will retry.
Traceback (most recent call last):
  File "/home/ubuntu/OmniPerf-Bench/third-party/trae-agent/trae_agent/utils/llm_clients/retry_utils.py", line 36, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/OmniPerf-Bench/third-party/trae-agent/trae_agent/utils/llm_clients/openai_client.py", line 86, in _create_openai_response
    response = self.client.responses.create(**api_params)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/OmniPerf-Bench/bench-env/lib/python3.12/site-packages/openai/resources/responses/responses.py", line 795, in create
    return self._post(
           ^^^^^^^^^^^
  File "/home/ubuntu/OmniPerf-Bench/bench-env/lib/python3.12/site-packages/openai/_base_client.py", line 1259, in post
    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))
                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/OmniPerf-Bench/bench-env/lib/python3.12/site-packages/openai/_base_client.py", line 1047, in request
    raise self._make_status_error_from_response(err.response) from None
openai.RateLimitError: Error code: 429 - {'error': {'message': 'You exceeded your current quota, please check your plan and billing details. For more information on this error, read the docs: https://platform.openai.com/docs/guides/error-codes/api-errors.', 'type': 'insufficient_quota', 'param': None, 'code': 'insufficient_quota'}}

OpenAI API call failed: Error code: 429 - {'error': {'message': 'You exceeded your current quota, please check your plan and billing details. For more information on this error, read the docs: https://platform.openai.com/docs/guides/error-codes/api-errors.', 'type': 'insufficient_quota', 'param': None, 'code': 'insufficient_quota'}}. Will sleep for 14 seconds and will retry.
Traceback (most recent call last):
  File "/home/ubuntu/OmniPerf-Bench/third-party/trae-agent/trae_agent/utils/llm_clients/retry_utils.py", line 36, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/OmniPerf-Bench/third-party/trae-agent/trae_agent/utils/llm_clients/openai_client.py", line 86, in _create_openai_response
    response = self.client.responses.create(**api_params)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/OmniPerf-Bench/bench-env/lib/python3.12/site-packages/openai/resources/responses/responses.py", line 795, in create
    return self._post(
           ^^^^^^^^^^^
  File "/home/ubuntu/OmniPerf-Bench/bench-env/lib/python3.12/site-packages/openai/_base_client.py", line 1259, in post
    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))
                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/OmniPerf-Bench/bench-env/lib/python3.12/site-packages/openai/_base_client.py", line 1047, in request
    raise self._make_status_error_from_response(err.response) from None
openai.RateLimitError: Error code: 429 - {'error': {'message': 'You exceeded your current quota, please check your plan and billing details. For more information on this error, read the docs: https://platform.openai.com/docs/guides/error-codes/api-errors.', 'type': 'insufficient_quota', 'param': None, 'code': 'insufficient_quota'}}

OpenAI API call failed: Error code: 429 - {'error': {'message': 'You exceeded your current quota, please check your plan and billing details. For more information on this error, read the docs: https://platform.openai.com/docs/guides/error-codes/api-errors.', 'type': 'insufficient_quota', 'param': None, 'code': 'insufficient_quota'}}. Will sleep for 6 seconds and will retry.
Traceback (most recent call last):
  File "/home/ubuntu/OmniPerf-Bench/third-party/trae-agent/trae_agent/utils/llm_clients/retry_utils.py", line 36, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/OmniPerf-Bench/third-party/trae-agent/trae_agent/utils/llm_clients/openai_client.py", line 86, in _create_openai_response
    response = self.client.responses.create(**api_params)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/OmniPerf-Bench/bench-env/lib/python3.12/site-packages/openai/resources/responses/responses.py", line 795, in create
    return self._post(
           ^^^^^^^^^^^
  File "/home/ubuntu/OmniPerf-Bench/bench-env/lib/python3.12/site-packages/openai/_base_client.py", line 1259, in post
    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))
                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/OmniPerf-Bench/bench-env/lib/python3.12/site-packages/openai/_base_client.py", line 1047, in request
    raise self._make_status_error_from_response(err.response) from None
openai.RateLimitError: Error code: 429 - {'error': {'message': 'You exceeded your current quota, please check your plan and billing details. For more information on this error, read the docs: https://platform.openai.com/docs/guides/error-codes/api-errors.', 'type': 'insufficient_quota', 'param': None, 'code': 'insufficient_quota'}}

OpenAI API call failed: Error code: 429 - {'error': {'message': 'You exceeded your current quota, please check your plan and billing details. For more information on this error, read the docs: https://platform.openai.com/docs/guides/error-codes/api-errors.', 'type': 'insufficient_quota', 'param': None, 'code': 'insufficient_quota'}}. Will sleep for 3 seconds and will retry.
Traceback (most recent call last):
  File "/home/ubuntu/OmniPerf-Bench/third-party/trae-agent/trae_agent/utils/llm_clients/retry_utils.py", line 36, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/OmniPerf-Bench/third-party/trae-agent/trae_agent/utils/llm_clients/openai_client.py", line 86, in _create_openai_response
    response = self.client.responses.create(**api_params)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/OmniPerf-Bench/bench-env/lib/python3.12/site-packages/openai/resources/responses/responses.py", line 795, in create
    return self._post(
           ^^^^^^^^^^^
  File "/home/ubuntu/OmniPerf-Bench/bench-env/lib/python3.12/site-packages/openai/_base_client.py", line 1259, in post
    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))
                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/OmniPerf-Bench/bench-env/lib/python3.12/site-packages/openai/_base_client.py", line 1047, in request
    raise self._make_status_error_from_response(err.response) from None
openai.RateLimitError: Error code: 429 - {'error': {'message': 'You exceeded your current quota, please check your plan and billing details. For more information on this error, read the docs: https://platform.openai.com/docs/guides/error-codes/api-errors.', 'type': 'insufficient_quota', 'param': None, 'code': 'insufficient_quota'}}

OpenAI API call failed: Error code: 429 - {'error': {'message': 'You exceeded your current quota, please check your plan and billing details. For more information on this error, read the docs: https://platform.openai.com/docs/guides/error-codes/api-errors.', 'type': 'insufficient_quota', 'param': None, 'code': 'insufficient_quota'}}. Will sleep for 14 seconds and will retry.
Traceback (most recent call last):
  File "/home/ubuntu/OmniPerf-Bench/third-party/trae-agent/trae_agent/utils/llm_clients/retry_utils.py", line 36, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/OmniPerf-Bench/third-party/trae-agent/trae_agent/utils/llm_clients/openai_client.py", line 86, in _create_openai_response
    response = self.client.responses.create(**api_params)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/OmniPerf-Bench/bench-env/lib/python3.12/site-packages/openai/resources/responses/responses.py", line 795, in create
    return self._post(
           ^^^^^^^^^^^
  File "/home/ubuntu/OmniPerf-Bench/bench-env/lib/python3.12/site-packages/openai/_base_client.py", line 1259, in post
    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))
                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/OmniPerf-Bench/bench-env/lib/python3.12/site-packages/openai/_base_client.py", line 1047, in request
    raise self._make_status_error_from_response(err.response) from None
openai.RateLimitError: Error code: 429 - {'error': {'message': 'You exceeded your current quota, please check your plan and billing details. For more information on this error, read the docs: https://platform.openai.com/docs/guides/error-codes/api-errors.', 'type': 'insufficient_quota', 'param': None, 'code': 'insufficient_quota'}}

OpenAI API call failed: Error code: 429 - {'error': {'message': 'You exceeded your current quota, please check your plan and billing details. For more information on this error, read the docs: https://platform.openai.com/docs/guides/error-codes/api-errors.', 'type': 'insufficient_quota', 'param': None, 'code': 'insufficient_quota'}}. Will sleep for 27 seconds and will retry.
Traceback (most recent call last):
  File "/home/ubuntu/OmniPerf-Bench/third-party/trae-agent/trae_agent/utils/llm_clients/retry_utils.py", line 36, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/OmniPerf-Bench/third-party/trae-agent/trae_agent/utils/llm_clients/openai_client.py", line 86, in _create_openai_response
    response = self.client.responses.create(**api_params)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/OmniPerf-Bench/bench-env/lib/python3.12/site-packages/openai/resources/responses/responses.py", line 795, in create
    return self._post(
           ^^^^^^^^^^^
  File "/home/ubuntu/OmniPerf-Bench/bench-env/lib/python3.12/site-packages/openai/_base_client.py", line 1259, in post
    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))
                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/OmniPerf-Bench/bench-env/lib/python3.12/site-packages/openai/_base_client.py", line 1047, in request
    raise self._make_status_error_from_response(err.response) from None
openai.RateLimitError: Error code: 429 - {'error': {'message': 'You exceeded your current quota, please check your plan and billing details. For more information on this error, read the docs: https://platform.openai.com/docs/guides/error-codes/api-errors.', 'type': 'insufficient_quota', 'param': None, 'code': 'insufficient_quota'}}

OpenAI API call failed: Error code: 429 - {'error': {'message': 'You exceeded your current quota, please check your plan and billing details. For more information on this error, read the docs: https://platform.openai.com/docs/guides/error-codes/api-errors.', 'type': 'insufficient_quota', 'param': None, 'code': 'insufficient_quota'}}. Will sleep for 15 seconds and will retry.
Traceback (most recent call last):
  File "/home/ubuntu/OmniPerf-Bench/third-party/trae-agent/trae_agent/utils/llm_clients/retry_utils.py", line 36, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/OmniPerf-Bench/third-party/trae-agent/trae_agent/utils/llm_clients/openai_client.py", line 86, in _create_openai_response
    response = self.client.responses.create(**api_params)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/OmniPerf-Bench/bench-env/lib/python3.12/site-packages/openai/resources/responses/responses.py", line 795, in create
    return self._post(
           ^^^^^^^^^^^
  File "/home/ubuntu/OmniPerf-Bench/bench-env/lib/python3.12/site-packages/openai/_base_client.py", line 1259, in post
    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))
                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/OmniPerf-Bench/bench-env/lib/python3.12/site-packages/openai/_base_client.py", line 1047, in request
    raise self._make_status_error_from_response(err.response) from None
openai.RateLimitError: Error code: 429 - {'error': {'message': 'You exceeded your current quota, please check your plan and billing details. For more information on this error, read the docs: https://platform.openai.com/docs/guides/error-codes/api-errors.', 'type': 'insufficient_quota', 'param': None, 'code': 'insufficient_quota'}}

OpenAI API call failed: Error code: 429 - {'error': {'message': 'You exceeded your current quota, please check your plan and billing details. For more information on this error, read the docs: https://platform.openai.com/docs/guides/error-codes/api-errors.', 'type': 'insufficient_quota', 'param': None, 'code': 'insufficient_quota'}}. Will sleep for 9 seconds and will retry.
Traceback (most recent call last):
  File "/home/ubuntu/OmniPerf-Bench/third-party/trae-agent/trae_agent/utils/llm_clients/retry_utils.py", line 36, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/OmniPerf-Bench/third-party/trae-agent/trae_agent/utils/llm_clients/openai_client.py", line 86, in _create_openai_response
    response = self.client.responses.create(**api_params)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/OmniPerf-Bench/bench-env/lib/python3.12/site-packages/openai/resources/responses/responses.py", line 795, in create
    return self._post(
           ^^^^^^^^^^^
  File "/home/ubuntu/OmniPerf-Bench/bench-env/lib/python3.12/site-packages/openai/_base_client.py", line 1259, in post
    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))
                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/OmniPerf-Bench/bench-env/lib/python3.12/site-packages/openai/_base_client.py", line 1047, in request
    raise self._make_status_error_from_response(err.response) from None
openai.RateLimitError: Error code: 429 - {'error': {'message': 'You exceeded your current quota, please check your plan and billing details. For more information on this error, read the docs: https://platform.openai.com/docs/guides/error-codes/api-errors.', 'type': 'insufficient_quota', 'param': None, 'code': 'insufficient_quota'}}

OpenAI API call failed: Error code: 429 - {'error': {'message': 'You exceeded your current quota, please check your plan and billing details. For more information on this error, read the docs: https://platform.openai.com/docs/guides/error-codes/api-errors.', 'type': 'insufficient_quota', 'param': None, 'code': 'insufficient_quota'}}. Will sleep for 28 seconds and will retry.
Traceback (most recent call last):
  File "/home/ubuntu/OmniPerf-Bench/third-party/trae-agent/trae_agent/utils/llm_clients/retry_utils.py", line 36, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/OmniPerf-Bench/third-party/trae-agent/trae_agent/utils/llm_clients/openai_client.py", line 86, in _create_openai_response
    response = self.client.responses.create(**api_params)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/OmniPerf-Bench/bench-env/lib/python3.12/site-packages/openai/resources/responses/responses.py", line 795, in create
    return self._post(
           ^^^^^^^^^^^
  File "/home/ubuntu/OmniPerf-Bench/bench-env/lib/python3.12/site-packages/openai/_base_client.py", line 1259, in post
    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))
                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/OmniPerf-Bench/bench-env/lib/python3.12/site-packages/openai/_base_client.py", line 1047, in request
    raise self._make_status_error_from_response(err.response) from None
openai.RateLimitError: Error code: 429 - {'error': {'message': 'You exceeded your current quota, please check your plan and billing details. For more information on this error, read the docs: https://platform.openai.com/docs/guides/error-codes/api-errors.', 'type': 'insufficient_quota', 'param': None, 'code': 'insufficient_quota'}}

OpenAI API call failed: Error code: 429 - {'error': {'message': 'You exceeded your current quota, please check your plan and billing details. For more information on this error, read the docs: https://platform.openai.com/docs/guides/error-codes/api-errors.', 'type': 'insufficient_quota', 'param': None, 'code': 'insufficient_quota'}}. Will sleep for 3 seconds and will retry.
Traceback (most recent call last):
  File "/home/ubuntu/OmniPerf-Bench/third-party/trae-agent/trae_agent/utils/llm_clients/retry_utils.py", line 36, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/OmniPerf-Bench/third-party/trae-agent/trae_agent/utils/llm_clients/openai_client.py", line 86, in _create_openai_response
    response = self.client.responses.create(**api_params)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/OmniPerf-Bench/bench-env/lib/python3.12/site-packages/openai/resources/responses/responses.py", line 795, in create
    return self._post(
           ^^^^^^^^^^^
  File "/home/ubuntu/OmniPerf-Bench/bench-env/lib/python3.12/site-packages/openai/_base_client.py", line 1259, in post
    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))
                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/OmniPerf-Bench/bench-env/lib/python3.12/site-packages/openai/_base_client.py", line 1047, in request
    raise self._make_status_error_from_response(err.response) from None
openai.RateLimitError: Error code: 429 - {'error': {'message': 'You exceeded your current quota, please check your plan and billing details. For more information on this error, read the docs: https://platform.openai.com/docs/guides/error-codes/api-errors.', 'type': 'insufficient_quota', 'param': None, 'code': 'insufficient_quota'}}

OpenAI API call failed: Error code: 429 - {'error': {'message': 'You exceeded your current quota, please check your plan and billing details. For more information on this error, read the docs: https://platform.openai.com/docs/guides/error-codes/api-errors.', 'type': 'insufficient_quota', 'param': None, 'code': 'insufficient_quota'}}. Will sleep for 7 seconds and will retry.
Traceback (most recent call last):
  File "/home/ubuntu/OmniPerf-Bench/third-party/trae-agent/trae_agent/utils/llm_clients/retry_utils.py", line 36, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/OmniPerf-Bench/third-party/trae-agent/trae_agent/utils/llm_clients/openai_client.py", line 86, in _create_openai_response
    response = self.client.responses.create(**api_params)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/OmniPerf-Bench/bench-env/lib/python3.12/site-packages/openai/resources/responses/responses.py", line 795, in create
    return self._post(
           ^^^^^^^^^^^
  File "/home/ubuntu/OmniPerf-Bench/bench-env/lib/python3.12/site-packages/openai/_base_client.py", line 1259, in post
    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))
                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/OmniPerf-Bench/bench-env/lib/python3.12/site-packages/openai/_base_client.py", line 1047, in request
    raise self._make_status_error_from_response(err.response) from None
openai.RateLimitError: Error code: 429 - {'error': {'message': 'You exceeded your current quota, please check your plan and billing details. For more information on this error, read the docs: https://platform.openai.com/docs/guides/error-codes/api-errors.', 'type': 'insufficient_quota', 'param': None, 'code': 'insufficient_quota'}}

OpenAI API call failed: Error code: 429 - {'error': {'message': 'You exceeded your current quota, please check your plan and billing details. For more information on this error, read the docs: https://platform.openai.com/docs/guides/error-codes/api-errors.', 'type': 'insufficient_quota', 'param': None, 'code': 'insufficient_quota'}}. Will sleep for 24 seconds and will retry.
Traceback (most recent call last):
  File "/home/ubuntu/OmniPerf-Bench/third-party/trae-agent/trae_agent/utils/llm_clients/retry_utils.py", line 36, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/OmniPerf-Bench/third-party/trae-agent/trae_agent/utils/llm_clients/openai_client.py", line 86, in _create_openai_response
    response = self.client.responses.create(**api_params)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/OmniPerf-Bench/bench-env/lib/python3.12/site-packages/openai/resources/responses/responses.py", line 795, in create
    return self._post(
           ^^^^^^^^^^^
  File "/home/ubuntu/OmniPerf-Bench/bench-env/lib/python3.12/site-packages/openai/_base_client.py", line 1259, in post
    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))
                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/OmniPerf-Bench/bench-env/lib/python3.12/site-packages/openai/_base_client.py", line 1047, in request
    raise self._make_status_error_from_response(err.response) from None
openai.RateLimitError: Error code: 429 - {'error': {'message': 'You exceeded your current quota, please check your plan and billing details. For more information on this error, read the docs: https://platform.openai.com/docs/guides/error-codes/api-errors.', 'type': 'insufficient_quota', 'param': None, 'code': 'insufficient_quota'}}

OpenAI API call failed: Error code: 429 - {'error': {'message': 'You exceeded your current quota, please check your plan and billing details. For more information on this error, read the docs: https://platform.openai.com/docs/guides/error-codes/api-errors.', 'type': 'insufficient_quota', 'param': None, 'code': 'insufficient_quota'}}. Will sleep for 11 seconds and will retry.
Traceback (most recent call last):
  File "/home/ubuntu/OmniPerf-Bench/third-party/trae-agent/trae_agent/utils/llm_clients/retry_utils.py", line 36, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/OmniPerf-Bench/third-party/trae-agent/trae_agent/utils/llm_clients/openai_client.py", line 86, in _create_openai_response
    response = self.client.responses.create(**api_params)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/OmniPerf-Bench/bench-env/lib/python3.12/site-packages/openai/resources/responses/responses.py", line 795, in create
    return self._post(
           ^^^^^^^^^^^
  File "/home/ubuntu/OmniPerf-Bench/bench-env/lib/python3.12/site-packages/openai/_base_client.py", line 1259, in post
    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))
                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/OmniPerf-Bench/bench-env/lib/python3.12/site-packages/openai/_base_client.py", line 1047, in request
    raise self._make_status_error_from_response(err.response) from None
openai.RateLimitError: Error code: 429 - {'error': {'message': 'You exceeded your current quota, please check your plan and billing details. For more information on this error, read the docs: https://platform.openai.com/docs/guides/error-codes/api-errors.', 'type': 'insufficient_quota', 'param': None, 'code': 'insufficient_quota'}}

OpenAI API call failed: Error code: 429 - {'error': {'message': 'You exceeded your current quota, please check your plan and billing details. For more information on this error, read the docs: https://platform.openai.com/docs/guides/error-codes/api-errors.', 'type': 'insufficient_quota', 'param': None, 'code': 'insufficient_quota'}}. Will sleep for 20 seconds and will retry.
Traceback (most recent call last):
  File "/home/ubuntu/OmniPerf-Bench/third-party/trae-agent/trae_agent/utils/llm_clients/retry_utils.py", line 36, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/OmniPerf-Bench/third-party/trae-agent/trae_agent/utils/llm_clients/openai_client.py", line 86, in _create_openai_response
    response = self.client.responses.create(**api_params)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/OmniPerf-Bench/bench-env/lib/python3.12/site-packages/openai/resources/responses/responses.py", line 795, in create
    return self._post(
           ^^^^^^^^^^^
  File "/home/ubuntu/OmniPerf-Bench/bench-env/lib/python3.12/site-packages/openai/_base_client.py", line 1259, in post
    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))
                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/OmniPerf-Bench/bench-env/lib/python3.12/site-packages/openai/_base_client.py", line 1047, in request
    raise self._make_status_error_from_response(err.response) from None
openai.RateLimitError: Error code: 429 - {'error': {'message': 'You exceeded your current quota, please check your plan and billing details. For more information on this error, read the docs: https://platform.openai.com/docs/guides/error-codes/api-errors.', 'type': 'insufficient_quota', 'param': None, 'code': 'insufficient_quota'}}

OpenAI API call failed: Error code: 429 - {'error': {'message': 'You exceeded your current quota, please check your plan and billing details. For more information on this error, read the docs: https://platform.openai.com/docs/guides/error-codes/api-errors.', 'type': 'insufficient_quota', 'param': None, 'code': 'insufficient_quota'}}. Will sleep for 13 seconds and will retry.
Traceback (most recent call last):
  File "/home/ubuntu/OmniPerf-Bench/third-party/trae-agent/trae_agent/utils/llm_clients/retry_utils.py", line 36, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/OmniPerf-Bench/third-party/trae-agent/trae_agent/utils/llm_clients/openai_client.py", line 86, in _create_openai_response
    response = self.client.responses.create(**api_params)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/OmniPerf-Bench/bench-env/lib/python3.12/site-packages/openai/resources/responses/responses.py", line 795, in create
    return self._post(
           ^^^^^^^^^^^
  File "/home/ubuntu/OmniPerf-Bench/bench-env/lib/python3.12/site-packages/openai/_base_client.py", line 1259, in post
    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))
                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/OmniPerf-Bench/bench-env/lib/python3.12/site-packages/openai/_base_client.py", line 1047, in request
    raise self._make_status_error_from_response(err.response) from None
openai.RateLimitError: Error code: 429 - {'error': {'message': 'You exceeded your current quota, please check your plan and billing details. For more information on this error, read the docs: https://platform.openai.com/docs/guides/error-codes/api-errors.', 'type': 'insufficient_quota', 'param': None, 'code': 'insufficient_quota'}}

OpenAI API call failed: Error code: 429 - {'error': {'message': 'You exceeded your current quota, please check your plan and billing details. For more information on this error, read the docs: https://platform.openai.com/docs/guides/error-codes/api-errors.', 'type': 'insufficient_quota', 'param': None, 'code': 'insufficient_quota'}}. Will sleep for 16 seconds and will retry.
Traceback (most recent call last):
  File "/home/ubuntu/OmniPerf-Bench/third-party/trae-agent/trae_agent/utils/llm_clients/retry_utils.py", line 36, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/OmniPerf-Bench/third-party/trae-agent/trae_agent/utils/llm_clients/openai_client.py", line 86, in _create_openai_response
    response = self.client.responses.create(**api_params)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/OmniPerf-Bench/bench-env/lib/python3.12/site-packages/openai/resources/responses/responses.py", line 795, in create
    return self._post(
           ^^^^^^^^^^^
  File "/home/ubuntu/OmniPerf-Bench/bench-env/lib/python3.12/site-packages/openai/_base_client.py", line 1259, in post
    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))
                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/OmniPerf-Bench/bench-env/lib/python3.12/site-packages/openai/_base_client.py", line 1047, in request
    raise self._make_status_error_from_response(err.response) from None
openai.RateLimitError: Error code: 429 - {'error': {'message': 'You exceeded your current quota, please check your plan and billing details. For more information on this error, read the docs: https://platform.openai.com/docs/guides/error-codes/api-errors.', 'type': 'insufficient_quota', 'param': None, 'code': 'insufficient_quota'}}

OpenAI API call failed: Error code: 429 - {'error': {'message': 'You exceeded your current quota, please check your plan and billing details. For more information on this error, read the docs: https://platform.openai.com/docs/guides/error-codes/api-errors.', 'type': 'insufficient_quota', 'param': None, 'code': 'insufficient_quota'}}. Will sleep for 6 seconds and will retry.
Traceback (most recent call last):
  File "/home/ubuntu/OmniPerf-Bench/third-party/trae-agent/trae_agent/utils/llm_clients/retry_utils.py", line 36, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/OmniPerf-Bench/third-party/trae-agent/trae_agent/utils/llm_clients/openai_client.py", line 86, in _create_openai_response
    response = self.client.responses.create(**api_params)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/OmniPerf-Bench/bench-env/lib/python3.12/site-packages/openai/resources/responses/responses.py", line 795, in create
    return self._post(
           ^^^^^^^^^^^
  File "/home/ubuntu/OmniPerf-Bench/bench-env/lib/python3.12/site-packages/openai/_base_client.py", line 1259, in post
    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))
                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/OmniPerf-Bench/bench-env/lib/python3.12/site-packages/openai/_base_client.py", line 1047, in request
    raise self._make_status_error_from_response(err.response) from None
openai.RateLimitError: Error code: 429 - {'error': {'message': 'You exceeded your current quota, please check your plan and billing details. For more information on this error, read the docs: https://platform.openai.com/docs/guides/error-codes/api-errors.', 'type': 'insufficient_quota', 'param': None, 'code': 'insufficient_quota'}}

â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Status      â”‚ âœ… Step 10: Completed                                                                                  â”‚
â”‚ Tools       â”‚ ðŸ”§ bash                                                                                                â”‚
â”‚ bash        â”‚ â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚
â”‚             â”‚ â”‚ {'command': 'grep -n                           â”‚                                                 â”‚   â”‚
â”‚             â”‚ â”‚ "prepare_mlp_sync_batch_raw\\(" -R             â”‚                                                 â”‚   â”‚
â”‚             â”‚ â”‚ /home/ubuntu/OmniPerf-Bench/perf-agents-benchâ€¦ â”‚                                                 â”‚   â”‚
â”‚             â”‚ â”‚ | sed -n \'1,200p\'', 'restart': False}        â”‚                                                 â”‚   â”‚
â”‚             â”‚ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚
â”‚ Total       â”‚ Input: 165840 Output: 5098                                                                             â”‚
â”‚ Tokens      â”‚                                                                                                        â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
OpenAI API call failed: Error code: 429 - {'error': {'message': 'You exceeded your current quota, please check your plan and billing details. For more information on this error, read the docs: https://platform.openai.com/docs/guides/error-codes/api-errors.', 'type': 'insufficient_quota', 'param': None, 'code': 'insufficient_quota'}}. Will sleep for 16 seconds and will retry.
Traceback (most recent call last):
  File "/home/ubuntu/OmniPerf-Bench/third-party/trae-agent/trae_agent/utils/llm_clients/retry_utils.py", line 36, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/OmniPerf-Bench/third-party/trae-agent/trae_agent/utils/llm_clients/openai_client.py", line 86, in _create_openai_response
    response = self.client.responses.create(**api_params)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/OmniPerf-Bench/bench-env/lib/python3.12/site-packages/openai/resources/responses/responses.py", line 795, in create
    return self._post(
           ^^^^^^^^^^^
  File "/home/ubuntu/OmniPerf-Bench/bench-env/lib/python3.12/site-packages/openai/_base_client.py", line 1259, in post
    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))
                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/OmniPerf-Bench/bench-env/lib/python3.12/site-packages/openai/_base_client.py", line 1047, in request
    raise self._make_status_error_from_response(err.response) from None
openai.RateLimitError: Error code: 429 - {'error': {'message': 'You exceeded your current quota, please check your plan and billing details. For more information on this error, read the docs: https://platform.openai.com/docs/guides/error-codes/api-errors.', 'type': 'insufficient_quota', 'param': None, 'code': 'insufficient_quota'}}

OpenAI API call failed: Error code: 429 - {'error': {'message': 'You exceeded your current quota, please check your plan and billing details. For more information on this error, read the docs: https://platform.openai.com/docs/guides/error-codes/api-errors.', 'type': 'insufficient_quota', 'param': None, 'code': 'insufficient_quota'}}. Will sleep for 21 seconds and will retry.
Traceback (most recent call last):
  File "/home/ubuntu/OmniPerf-Bench/third-party/trae-agent/trae_agent/utils/llm_clients/retry_utils.py", line 36, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/OmniPerf-Bench/third-party/trae-agent/trae_agent/utils/llm_clients/openai_client.py", line 86, in _create_openai_response
    response = self.client.responses.create(**api_params)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/OmniPerf-Bench/bench-env/lib/python3.12/site-packages/openai/resources/responses/responses.py", line 795, in create
    return self._post(
           ^^^^^^^^^^^
  File "/home/ubuntu/OmniPerf-Bench/bench-env/lib/python3.12/site-packages/openai/_base_client.py", line 1259, in post
    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))
                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/OmniPerf-Bench/bench-env/lib/python3.12/site-packages/openai/_base_client.py", line 1047, in request
    raise self._make_status_error_from_response(err.response) from None
openai.RateLimitError: Error code: 429 - {'error': {'message': 'You exceeded your current quota, please check your plan and billing details. For more information on this error, read the docs: https://platform.openai.com/docs/guides/error-codes/api-errors.', 'type': 'insufficient_quota', 'param': None, 'code': 'insufficient_quota'}}

OpenAI API call failed: Error code: 429 - {'error': {'message': 'You exceeded your current quota, please check your plan and billing details. For more information on this error, read the docs: https://platform.openai.com/docs/guides/error-codes/api-errors.', 'type': 'insufficient_quota', 'param': None, 'code': 'insufficient_quota'}}. Will sleep for 4 seconds and will retry.
Traceback (most recent call last):
  File "/home/ubuntu/OmniPerf-Bench/third-party/trae-agent/trae_agent/utils/llm_clients/retry_utils.py", line 36, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/OmniPerf-Bench/third-party/trae-agent/trae_agent/utils/llm_clients/openai_client.py", line 86, in _create_openai_response
    response = self.client.responses.create(**api_params)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/OmniPerf-Bench/bench-env/lib/python3.12/site-packages/openai/resources/responses/responses.py", line 795, in create
    return self._post(
           ^^^^^^^^^^^
  File "/home/ubuntu/OmniPerf-Bench/bench-env/lib/python3.12/site-packages/openai/_base_client.py", line 1259, in post
    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))
                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/OmniPerf-Bench/bench-env/lib/python3.12/site-packages/openai/_base_client.py", line 1047, in request
    raise self._make_status_error_from_response(err.response) from None
openai.RateLimitError: Error code: 429 - {'error': {'message': 'You exceeded your current quota, please check your plan and billing details. For more information on this error, read the docs: https://platform.openai.com/docs/guides/error-codes/api-errors.', 'type': 'insufficient_quota', 'param': None, 'code': 'insufficient_quota'}}

OpenAI API call failed: Error code: 429 - {'error': {'message': 'You exceeded your current quota, please check your plan and billing details. For more information on this error, read the docs: https://platform.openai.com/docs/guides/error-codes/api-errors.', 'type': 'insufficient_quota', 'param': None, 'code': 'insufficient_quota'}}. Will sleep for 10 seconds and will retry.
Traceback (most recent call last):
  File "/home/ubuntu/OmniPerf-Bench/third-party/trae-agent/trae_agent/utils/llm_clients/retry_utils.py", line 36, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/OmniPerf-Bench/third-party/trae-agent/trae_agent/utils/llm_clients/openai_client.py", line 86, in _create_openai_response
    response = self.client.responses.create(**api_params)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/OmniPerf-Bench/bench-env/lib/python3.12/site-packages/openai/resources/responses/responses.py", line 795, in create
    return self._post(
           ^^^^^^^^^^^
  File "/home/ubuntu/OmniPerf-Bench/bench-env/lib/python3.12/site-packages/openai/_base_client.py", line 1259, in post
    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))
                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/OmniPerf-Bench/bench-env/lib/python3.12/site-packages/openai/_base_client.py", line 1047, in request
    raise self._make_status_error_from_response(err.response) from None
openai.RateLimitError: Error code: 429 - {'error': {'message': 'You exceeded your current quota, please check your plan and billing details. For more information on this error, read the docs: https://platform.openai.com/docs/guides/error-codes/api-errors.', 'type': 'insufficient_quota', 'param': None, 'code': 'insufficient_quota'}}

OpenAI API call failed: Error code: 429 - {'error': {'message': 'You exceeded your current quota, please check your plan and billing details. For more information on this error, read the docs: https://platform.openai.com/docs/guides/error-codes/api-errors.', 'type': 'insufficient_quota', 'param': None, 'code': 'insufficient_quota'}}. Will sleep for 23 seconds and will retry.
Traceback (most recent call last):
  File "/home/ubuntu/OmniPerf-Bench/third-party/trae-agent/trae_agent/utils/llm_clients/retry_utils.py", line 36, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/OmniPerf-Bench/third-party/trae-agent/trae_agent/utils/llm_clients/openai_client.py", line 86, in _create_openai_response
    response = self.client.responses.create(**api_params)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/OmniPerf-Bench/bench-env/lib/python3.12/site-packages/openai/resources/responses/responses.py", line 795, in create
    return self._post(
           ^^^^^^^^^^^
  File "/home/ubuntu/OmniPerf-Bench/bench-env/lib/python3.12/site-packages/openai/_base_client.py", line 1259, in post
    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))
                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/OmniPerf-Bench/bench-env/lib/python3.12/site-packages/openai/_base_client.py", line 1047, in request
    raise self._make_status_error_from_response(err.response) from None
openai.RateLimitError: Error code: 429 - {'error': {'message': 'You exceeded your current quota, please check your plan and billing details. For more information on this error, read the docs: https://platform.openai.com/docs/guides/error-codes/api-errors.', 'type': 'insufficient_quota', 'param': None, 'code': 'insufficient_quota'}}

OpenAI API call failed: Error code: 429 - {'error': {'message': 'You exceeded your current quota, please check your plan and billing details. For more information on this error, read the docs: https://platform.openai.com/docs/guides/error-codes/api-errors.', 'type': 'insufficient_quota', 'param': None, 'code': 'insufficient_quota'}}. Will sleep for 10 seconds and will retry.
Traceback (most recent call last):
  File "/home/ubuntu/OmniPerf-Bench/third-party/trae-agent/trae_agent/utils/llm_clients/retry_utils.py", line 36, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/OmniPerf-Bench/third-party/trae-agent/trae_agent/utils/llm_clients/openai_client.py", line 86, in _create_openai_response
    response = self.client.responses.create(**api_params)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/OmniPerf-Bench/bench-env/lib/python3.12/site-packages/openai/resources/responses/responses.py", line 795, in create
    return self._post(
           ^^^^^^^^^^^
  File "/home/ubuntu/OmniPerf-Bench/bench-env/lib/python3.12/site-packages/openai/_base_client.py", line 1259, in post
    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))
                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/OmniPerf-Bench/bench-env/lib/python3.12/site-packages/openai/_base_client.py", line 1047, in request
    raise self._make_status_error_from_response(err.response) from None
openai.RateLimitError: Error code: 429 - {'error': {'message': 'You exceeded your current quota, please check your plan and billing details. For more information on this error, read the docs: https://platform.openai.com/docs/guides/error-codes/api-errors.', 'type': 'insufficient_quota', 'param': None, 'code': 'insufficient_quota'}}

OpenAI API call failed: Error code: 429 - {'error': {'message': 'You exceeded your current quota, please check your plan and billing details. For more information on this error, read the docs: https://platform.openai.com/docs/guides/error-codes/api-errors.', 'type': 'insufficient_quota', 'param': None, 'code': 'insufficient_quota'}}. Will sleep for 8 seconds and will retry.
Traceback (most recent call last):
  File "/home/ubuntu/OmniPerf-Bench/third-party/trae-agent/trae_agent/utils/llm_clients/retry_utils.py", line 36, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/OmniPerf-Bench/third-party/trae-agent/trae_agent/utils/llm_clients/openai_client.py", line 86, in _create_openai_response
    response = self.client.responses.create(**api_params)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/OmniPerf-Bench/bench-env/lib/python3.12/site-packages/openai/resources/responses/responses.py", line 795, in create
    return self._post(
           ^^^^^^^^^^^
  File "/home/ubuntu/OmniPerf-Bench/bench-env/lib/python3.12/site-packages/openai/_base_client.py", line 1259, in post
    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))
                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/OmniPerf-Bench/bench-env/lib/python3.12/site-packages/openai/_base_client.py", line 1047, in request
    raise self._make_status_error_from_response(err.response) from None
openai.RateLimitError: Error code: 429 - {'error': {'message': 'You exceeded your current quota, please check your plan and billing details. For more information on this error, read the docs: https://platform.openai.com/docs/guides/error-codes/api-errors.', 'type': 'insufficient_quota', 'param': None, 'code': 'insufficient_quota'}}

OpenAI API call failed: Error code: 429 - {'error': {'message': 'You exceeded your current quota, please check your plan and billing details. For more information on this error, read the docs: https://platform.openai.com/docs/guides/error-codes/api-errors.', 'type': 'insufficient_quota', 'param': None, 'code': 'insufficient_quota'}}. Will sleep for 6 seconds and will retry.
Traceback (most recent call last):
  File "/home/ubuntu/OmniPerf-Bench/third-party/trae-agent/trae_agent/utils/llm_clients/retry_utils.py", line 36, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/OmniPerf-Bench/third-party/trae-agent/trae_agent/utils/llm_clients/openai_client.py", line 86, in _create_openai_response
    response = self.client.responses.create(**api_params)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/OmniPerf-Bench/bench-env/lib/python3.12/site-packages/openai/resources/responses/responses.py", line 795, in create
    return self._post(
           ^^^^^^^^^^^
  File "/home/ubuntu/OmniPerf-Bench/bench-env/lib/python3.12/site-packages/openai/_base_client.py", line 1259, in post
    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))
                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/OmniPerf-Bench/bench-env/lib/python3.12/site-packages/openai/_base_client.py", line 1047, in request
    raise self._make_status_error_from_response(err.response) from None
openai.RateLimitError: Error code: 429 - {'error': {'message': 'You exceeded your current quota, please check your plan and billing details. For more information on this error, read the docs: https://platform.openai.com/docs/guides/error-codes/api-errors.', 'type': 'insufficient_quota', 'param': None, 'code': 'insufficient_quota'}}

OpenAI API call failed: Error code: 429 - {'error': {'message': 'You exceeded your current quota, please check your plan and billing details. For more information on this error, read the docs: https://platform.openai.com/docs/guides/error-codes/api-errors.', 'type': 'insufficient_quota', 'param': None, 'code': 'insufficient_quota'}}. Will sleep for 5 seconds and will retry.
Traceback (most recent call last):
  File "/home/ubuntu/OmniPerf-Bench/third-party/trae-agent/trae_agent/utils/llm_clients/retry_utils.py", line 36, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/OmniPerf-Bench/third-party/trae-agent/trae_agent/utils/llm_clients/openai_client.py", line 86, in _create_openai_response
    response = self.client.responses.create(**api_params)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/OmniPerf-Bench/bench-env/lib/python3.12/site-packages/openai/resources/responses/responses.py", line 795, in create
    return self._post(
           ^^^^^^^^^^^
  File "/home/ubuntu/OmniPerf-Bench/bench-env/lib/python3.12/site-packages/openai/_base_client.py", line 1259, in post
    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))
                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/OmniPerf-Bench/bench-env/lib/python3.12/site-packages/openai/_base_client.py", line 1047, in request
    raise self._make_status_error_from_response(err.response) from None
openai.RateLimitError: Error code: 429 - {'error': {'message': 'You exceeded your current quota, please check your plan and billing details. For more information on this error, read the docs: https://platform.openai.com/docs/guides/error-codes/api-errors.', 'type': 'insufficient_quota', 'param': None, 'code': 'insufficient_quota'}}

OpenAI API call failed: Error code: 429 - {'error': {'message': 'You exceeded your current quota, please check your plan and billing details. For more information on this error, read the docs: https://platform.openai.com/docs/guides/error-codes/api-errors.', 'type': 'insufficient_quota', 'param': None, 'code': 'insufficient_quota'}}. Will sleep for 23 seconds and will retry.
Traceback (most recent call last):
  File "/home/ubuntu/OmniPerf-Bench/third-party/trae-agent/trae_agent/utils/llm_clients/retry_utils.py", line 36, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/OmniPerf-Bench/third-party/trae-agent/trae_agent/utils/llm_clients/openai_client.py", line 86, in _create_openai_response
    response = self.client.responses.create(**api_params)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/OmniPerf-Bench/bench-env/lib/python3.12/site-packages/openai/resources/responses/responses.py", line 795, in create
    return self._post(
           ^^^^^^^^^^^
  File "/home/ubuntu/OmniPerf-Bench/bench-env/lib/python3.12/site-packages/openai/_base_client.py", line 1259, in post
    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))
                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/OmniPerf-Bench/bench-env/lib/python3.12/site-packages/openai/_base_client.py", line 1047, in request
    raise self._make_status_error_from_response(err.response) from None
openai.RateLimitError: Error code: 429 - {'error': {'message': 'You exceeded your current quota, please check your plan and billing details. For more information on this error, read the docs: https://platform.openai.com/docs/guides/error-codes/api-errors.', 'type': 'insufficient_quota', 'param': None, 'code': 'insufficient_quota'}}

â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Status      â”‚ âœ… Step 11: Completed                                                                                  â”‚
â”‚ Error       â”‚ âŒ Error code: 429 - {'error': {'message': 'You exceeded your current quota, please check your plan    â”‚
â”‚             â”‚ and billing details. For more information on this error, read the docs:                                â”‚
â”‚             â”‚ https://platform.openai.com/docs/guides/error-codes/api-errors.', 'type': 'insufficient_quota',        â”‚
â”‚             â”‚ 'param': None, 'code': 'insufficient_quota'}}                                                          â”‚
â”‚ Total       â”‚ Input: 165840 Output: 5098                                                                             â”‚
â”‚ Tokens      â”‚                                                                                                        â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
OpenAI API call failed: Error code: 429 - {'error': {'message': 'You exceeded your current quota, please check your plan and billing details. For more information on this error, read the docs: https://platform.openai.com/docs/guides/error-codes/api-errors.', 'type': 'insufficient_quota', 'param': None, 'code': 'insufficient_quota'}}. Will sleep for 16 seconds and will retry.
Traceback (most recent call last):
  File "/home/ubuntu/OmniPerf-Bench/third-party/trae-agent/trae_agent/utils/llm_clients/retry_utils.py", line 36, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/OmniPerf-Bench/third-party/trae-agent/trae_agent/utils/llm_clients/openai_client.py", line 86, in _create_openai_response
    response = self.client.responses.create(**api_params)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/OmniPerf-Bench/bench-env/lib/python3.12/site-packages/openai/resources/responses/responses.py", line 795, in create
    return self._post(
           ^^^^^^^^^^^
  File "/home/ubuntu/OmniPerf-Bench/bench-env/lib/python3.12/site-packages/openai/_base_client.py", line 1259, in post
    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))
                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/OmniPerf-Bench/bench-env/lib/python3.12/site-packages/openai/_base_client.py", line 1047, in request
    raise self._make_status_error_from_response(err.response) from None
openai.RateLimitError: Error code: 429 - {'error': {'message': 'You exceeded your current quota, please check your plan and billing details. For more information on this error, read the docs: https://platform.openai.com/docs/guides/error-codes/api-errors.', 'type': 'insufficient_quota', 'param': None, 'code': 'insufficient_quota'}}

OpenAI API call failed: Error code: 429 - {'error': {'message': 'You exceeded your current quota, please check your plan and billing details. For more information on this error, read the docs: https://platform.openai.com/docs/guides/error-codes/api-errors.', 'type': 'insufficient_quota', 'param': None, 'code': 'insufficient_quota'}}. Will sleep for 10 seconds and will retry.
Traceback (most recent call last):
  File "/home/ubuntu/OmniPerf-Bench/third-party/trae-agent/trae_agent/utils/llm_clients/retry_utils.py", line 36, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/OmniPerf-Bench/third-party/trae-agent/trae_agent/utils/llm_clients/openai_client.py", line 86, in _create_openai_response
    response = self.client.responses.create(**api_params)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/OmniPerf-Bench/bench-env/lib/python3.12/site-packages/openai/resources/responses/responses.py", line 795, in create
    return self._post(
           ^^^^^^^^^^^
  File "/home/ubuntu/OmniPerf-Bench/bench-env/lib/python3.12/site-packages/openai/_base_client.py", line 1259, in post
    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))
                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/OmniPerf-Bench/bench-env/lib/python3.12/site-packages/openai/_base_client.py", line 1047, in request
    raise self._make_status_error_from_response(err.response) from None
openai.RateLimitError: Error code: 429 - {'error': {'message': 'You exceeded your current quota, please check your plan and billing details. For more information on this error, read the docs: https://platform.openai.com/docs/guides/error-codes/api-errors.', 'type': 'insufficient_quota', 'param': None, 'code': 'insufficient_quota'}}

OpenAI API call failed: Error code: 429 - {'error': {'message': 'You exceeded your current quota, please check your plan and billing details. For more information on this error, read the docs: https://platform.openai.com/docs/guides/error-codes/api-errors.', 'type': 'insufficient_quota', 'param': None, 'code': 'insufficient_quota'}}. Will sleep for 15 seconds and will retry.
Traceback (most recent call last):
  File "/home/ubuntu/OmniPerf-Bench/third-party/trae-agent/trae_agent/utils/llm_clients/retry_utils.py", line 36, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/OmniPerf-Bench/third-party/trae-agent/trae_agent/utils/llm_clients/openai_client.py", line 86, in _create_openai_response
    response = self.client.responses.create(**api_params)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/OmniPerf-Bench/bench-env/lib/python3.12/site-packages/openai/resources/responses/responses.py", line 795, in create
    return self._post(
           ^^^^^^^^^^^
  File "/home/ubuntu/OmniPerf-Bench/bench-env/lib/python3.12/site-packages/openai/_base_client.py", line 1259, in post
    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))
                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/OmniPerf-Bench/bench-env/lib/python3.12/site-packages/openai/_base_client.py", line 1047, in request
    raise self._make_status_error_from_response(err.response) from None
openai.RateLimitError: Error code: 429 - {'error': {'message': 'You exceeded your current quota, please check your plan and billing details. For more information on this error, read the docs: https://platform.openai.com/docs/guides/error-codes/api-errors.', 'type': 'insufficient_quota', 'param': None, 'code': 'insufficient_quota'}}

OpenAI API call failed: Error code: 429 - {'error': {'message': 'You exceeded your current quota, please check your plan and billing details. For more information on this error, read the docs: https://platform.openai.com/docs/guides/error-codes/api-errors.', 'type': 'insufficient_quota', 'param': None, 'code': 'insufficient_quota'}}. Will sleep for 18 seconds and will retry.
Traceback (most recent call last):
  File "/home/ubuntu/OmniPerf-Bench/third-party/trae-agent/trae_agent/utils/llm_clients/retry_utils.py", line 36, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/OmniPerf-Bench/third-party/trae-agent/trae_agent/utils/llm_clients/openai_client.py", line 86, in _create_openai_response
    response = self.client.responses.create(**api_params)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/OmniPerf-Bench/bench-env/lib/python3.12/site-packages/openai/resources/responses/responses.py", line 795, in create
    return self._post(
           ^^^^^^^^^^^
  File "/home/ubuntu/OmniPerf-Bench/bench-env/lib/python3.12/site-packages/openai/_base_client.py", line 1259, in post
    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))
                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/OmniPerf-Bench/bench-env/lib/python3.12/site-packages/openai/_base_client.py", line 1047, in request
    raise self._make_status_error_from_response(err.response) from None
openai.RateLimitError: Error code: 429 - {'error': {'message': 'You exceeded your current quota, please check your plan and billing details. For more information on this error, read the docs: https://platform.openai.com/docs/guides/error-codes/api-errors.', 'type': 'insufficient_quota', 'param': None, 'code': 'insufficient_quota'}}

OpenAI API call failed: Error code: 429 - {'error': {'message': 'You exceeded your current quota, please check your plan and billing details. For more information on this error, read the docs: https://platform.openai.com/docs/guides/error-codes/api-errors.', 'type': 'insufficient_quota', 'param': None, 'code': 'insufficient_quota'}}. Will sleep for 23 seconds and will retry.
Traceback (most recent call last):
  File "/home/ubuntu/OmniPerf-Bench/third-party/trae-agent/trae_agent/utils/llm_clients/retry_utils.py", line 36, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/OmniPerf-Bench/third-party/trae-agent/trae_agent/utils/llm_clients/openai_client.py", line 86, in _create_openai_response
    response = self.client.responses.create(**api_params)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/OmniPerf-Bench/bench-env/lib/python3.12/site-packages/openai/resources/responses/responses.py", line 795, in create
    return self._post(
           ^^^^^^^^^^^
  File "/home/ubuntu/OmniPerf-Bench/bench-env/lib/python3.12/site-packages/openai/_base_client.py", line 1259, in post
    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))
                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/OmniPerf-Bench/bench-env/lib/python3.12/site-packages/openai/_base_client.py", line 1047, in request
    raise self._make_status_error_from_response(err.response) from None
openai.RateLimitError: Error code: 429 - {'error': {'message': 'You exceeded your current quota, please check your plan and billing details. For more information on this error, read the docs: https://platform.openai.com/docs/guides/error-codes/api-errors.', 'type': 'insufficient_quota', 'param': None, 'code': 'insufficient_quota'}}

OpenAI API call failed: Error code: 429 - {'error': {'message': 'You exceeded your current quota, please check your plan and billing details. For more information on this error, read the docs: https://platform.openai.com/docs/guides/error-codes/api-errors.', 'type': 'insufficient_quota', 'param': None, 'code': 'insufficient_quota'}}. Will sleep for 13 seconds and will retry.
Traceback (most recent call last):
  File "/home/ubuntu/OmniPerf-Bench/third-party/trae-agent/trae_agent/utils/llm_clients/retry_utils.py", line 36, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/OmniPerf-Bench/third-party/trae-agent/trae_agent/utils/llm_clients/openai_client.py", line 86, in _create_openai_response
    response = self.client.responses.create(**api_params)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/OmniPerf-Bench/bench-env/lib/python3.12/site-packages/openai/resources/responses/responses.py", line 795, in create
    return self._post(
           ^^^^^^^^^^^
  File "/home/ubuntu/OmniPerf-Bench/bench-env/lib/python3.12/site-packages/openai/_base_client.py", line 1259, in post
    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))
                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/OmniPerf-Bench/bench-env/lib/python3.12/site-packages/openai/_base_client.py", line 1047, in request
    raise self._make_status_error_from_response(err.response) from None
openai.RateLimitError: Error code: 429 - {'error': {'message': 'You exceeded your current quota, please check your plan and billing details. For more information on this error, read the docs: https://platform.openai.com/docs/guides/error-codes/api-errors.', 'type': 'insufficient_quota', 'param': None, 'code': 'insufficient_quota'}}

OpenAI API call failed: Error code: 429 - {'error': {'message': 'You exceeded your current quota, please check your plan and billing details. For more information on this error, read the docs: https://platform.openai.com/docs/guides/error-codes/api-errors.', 'type': 'insufficient_quota', 'param': None, 'code': 'insufficient_quota'}}. Will sleep for 10 seconds and will retry.
Traceback (most recent call last):
  File "/home/ubuntu/OmniPerf-Bench/third-party/trae-agent/trae_agent/utils/llm_clients/retry_utils.py", line 36, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/OmniPerf-Bench/third-party/trae-agent/trae_agent/utils/llm_clients/openai_client.py", line 86, in _create_openai_response
    response = self.client.responses.create(**api_params)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/OmniPerf-Bench/bench-env/lib/python3.12/site-packages/openai/resources/responses/responses.py", line 795, in create
    return self._post(
           ^^^^^^^^^^^
  File "/home/ubuntu/OmniPerf-Bench/bench-env/lib/python3.12/site-packages/openai/_base_client.py", line 1259, in post
    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))
                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/OmniPerf-Bench/bench-env/lib/python3.12/site-packages/openai/_base_client.py", line 1047, in request
    raise self._make_status_error_from_response(err.response) from None
openai.RateLimitError: Error code: 429 - {'error': {'message': 'You exceeded your current quota, please check your plan and billing details. For more information on this error, read the docs: https://platform.openai.com/docs/guides/error-codes/api-errors.', 'type': 'insufficient_quota', 'param': None, 'code': 'insufficient_quota'}}

OpenAI API call failed: Error code: 429 - {'error': {'message': 'You exceeded your current quota, please check your plan and billing details. For more information on this error, read the docs: https://platform.openai.com/docs/guides/error-codes/api-errors.', 'type': 'insufficient_quota', 'param': None, 'code': 'insufficient_quota'}}. Will sleep for 13 seconds and will retry.
Traceback (most recent call last):
  File "/home/ubuntu/OmniPerf-Bench/third-party/trae-agent/trae_agent/utils/llm_clients/retry_utils.py", line 36, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/OmniPerf-Bench/third-party/trae-agent/trae_agent/utils/llm_clients/openai_client.py", line 86, in _create_openai_response
    response = self.client.responses.create(**api_params)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/OmniPerf-Bench/bench-env/lib/python3.12/site-packages/openai/resources/responses/responses.py", line 795, in create
    return self._post(
           ^^^^^^^^^^^
  File "/home/ubuntu/OmniPerf-Bench/bench-env/lib/python3.12/site-packages/openai/_base_client.py", line 1259, in post
    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))
                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/OmniPerf-Bench/bench-env/lib/python3.12/site-packages/openai/_base_client.py", line 1047, in request
    raise self._make_status_error_from_response(err.response) from None
openai.RateLimitError: Error code: 429 - {'error': {'message': 'You exceeded your current quota, please check your plan and billing details. For more information on this error, read the docs: https://platform.openai.com/docs/guides/error-codes/api-errors.', 'type': 'insufficient_quota', 'param': None, 'code': 'insufficient_quota'}}

OpenAI API call failed: Error code: 429 - {'error': {'message': 'You exceeded your current quota, please check your plan and billing details. For more information on this error, read the docs: https://platform.openai.com/docs/guides/error-codes/api-errors.', 'type': 'insufficient_quota', 'param': None, 'code': 'insufficient_quota'}}. Will sleep for 4 seconds and will retry.
Traceback (most recent call last):
  File "/home/ubuntu/OmniPerf-Bench/third-party/trae-agent/trae_agent/utils/llm_clients/retry_utils.py", line 36, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/OmniPerf-Bench/third-party/trae-agent/trae_agent/utils/llm_clients/openai_client.py", line 86, in _create_openai_response
    response = self.client.responses.create(**api_params)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/OmniPerf-Bench/bench-env/lib/python3.12/site-packages/openai/resources/responses/responses.py", line 795, in create
    return self._post(
           ^^^^^^^^^^^
  File "/home/ubuntu/OmniPerf-Bench/bench-env/lib/python3.12/site-packages/openai/_base_client.py", line 1259, in post
    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))
                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/OmniPerf-Bench/bench-env/lib/python3.12/site-packages/openai/_base_client.py", line 1047, in request
    raise self._make_status_error_from_response(err.response) from None
openai.RateLimitError: Error code: 429 - {'error': {'message': 'You exceeded your current quota, please check your plan and billing details. For more information on this error, read the docs: https://platform.openai.com/docs/guides/error-codes/api-errors.', 'type': 'insufficient_quota', 'param': None, 'code': 'insufficient_quota'}}

OpenAI API call failed: Error code: 429 - {'error': {'message': 'You exceeded your current quota, please check your plan and billing details. For more information on this error, read the docs: https://platform.openai.com/docs/guides/error-codes/api-errors.', 'type': 'insufficient_quota', 'param': None, 'code': 'insufficient_quota'}}. Will sleep for 5 seconds and will retry.
Traceback (most recent call last):
  File "/home/ubuntu/OmniPerf-Bench/third-party/trae-agent/trae_agent/utils/llm_clients/retry_utils.py", line 36, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/OmniPerf-Bench/third-party/trae-agent/trae_agent/utils/llm_clients/openai_client.py", line 86, in _create_openai_response
    response = self.client.responses.create(**api_params)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/OmniPerf-Bench/bench-env/lib/python3.12/site-packages/openai/resources/responses/responses.py", line 795, in create
    return self._post(
           ^^^^^^^^^^^
  File "/home/ubuntu/OmniPerf-Bench/bench-env/lib/python3.12/site-packages/openai/_base_client.py", line 1259, in post
    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))
                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/OmniPerf-Bench/bench-env/lib/python3.12/site-packages/openai/_base_client.py", line 1047, in request
    raise self._make_status_error_from_response(err.response) from None
openai.RateLimitError: Error code: 429 - {'error': {'message': 'You exceeded your current quota, please check your plan and billing details. For more information on this error, read the docs: https://platform.openai.com/docs/guides/error-codes/api-errors.', 'type': 'insufficient_quota', 'param': None, 'code': 'insufficient_quota'}}


============================================================
[1mLakeview Summary[0m
============================================================
â•­â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ Step 1 (Lakeview) â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•®
â”‚ [ðŸ‘ï¸EXAMINE_CODE] The agent [1m<task>Exploring the repository to locate target [0m   â”‚
â”‚ [1mfiles.[0m                                                                       â”‚
â”‚ [3mSafely listing sglang_core to locate python/sglang/srt/managers/scheduler.py[0m â”‚
â”‚ [3mand plan a .bench_scratch benchmark; targeting torch.zeros/fill_ patterns [0m   â”‚
â”‚ [3mfor optimization.[0m                                                            â”‚
â•°â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•¯
â•­â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ Step 2 (Lakeview) â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•®
â”‚ [ðŸ‘ï¸EXAMINE_CODE] The agent [1m<task>The agent is examining source code.[0m          â”‚
â”‚ [3mViewing python/sglang/srt/managers/scheduler.py with a view tool to inspect [0m â”‚
â”‚ [3mits contents, focusing on torch.zeros and fill_ patterns before making [0m      â”‚
â”‚ [3moptimizations.[0m                                                               â”‚
â•°â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•¯
â•­â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ Step 3 (Lakeview) â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•®
â”‚ [ðŸ‘ï¸EXAMINE_CODE] The agent [1m<task>The agent is examining source code.[0m          â”‚
â”‚ [3mSearching scheduler.py for torch.zeros, fill_, torch.empty, and tensor() [0m    â”‚
â”‚ [3moccurrences using grep to locate potential optimization sites.[0m               â”‚
â•°â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•¯
â•­â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ Step 4 (Lakeview) â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•®
â”‚ [ðŸ‘ï¸EXAMINE_CODE] The agent [1m<task>The agent is examining source code.[0m          â”‚
â”‚ [3mRunning grep on scheduler.py to find torch.zeros, fill_, torch.empty, and [0m   â”‚
â”‚ [3mtensor() occurrences, identifying potential optimization spots related to [0m   â”‚
â”‚ [3mthe reported performance issue.[0m                                              â”‚
â•°â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•¯
â•­â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ Step 5 (Lakeview) â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•®
â”‚ [ðŸ‘ï¸EXAMINE_CODE] The agent [1m<task>The agent is examining source code.[0m          â”‚
â”‚ [3mViewing python/sglang/srt/managers/scheduler.py lines 1375â€“1430 to inspect [0m  â”‚
â”‚ [3mtensor initialization patterns (e.g., torch.zeros, fill_) as potential [0m      â”‚
â”‚ [3moptimization targets.[0m                                                        â”‚
â•°â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•¯
â•­â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ Step 6 (Lakeview) â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•®
â”‚ [ðŸ‘ï¸EXAMINE_CODE] The agent [1m<task>The agent is examining source code.[0m          â”‚
â”‚ [3mViewing scheduler.py lines 1825â€“1875 to inspect tensor initialization usage [0m â”‚
â”‚ [3m(e.g., torch.zeros/fill_) as potential optimization targets before editing.[0m  â”‚
â•°â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•¯

Unexpected error: Error code: [1m429[0m - [1m{[0m'error': [1m{[0m'message': 'You exceeded your current quota, please check your plan and billing details. For more information on this
error, read the docs: https://platform.openai.com/docs/guides/error-codes/api-errors.', 'type': 'insufficient_quota', 'param': [3mNone[0m, 'code': 'insufficient_quota'[1m}[0m[1m}[0m
Traceback [1m([0mmost recent call last[1m)[0m:
  File "/home/ubuntu/OmniPerf-Bench/third-party/trae-agent/trae_agent/cli.py", line [1m212[0m, in run
    _ = [1masyncio.run[0m[1m([0m[1magent.run[0m[1m([0mtask, task_args[1m)[0m[1m)[0m
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/lib/python3.12/asyncio/runners.py", line [1m194[0m, in run
    return [1mrunner.run[0m[1m([0mmain[1m)[0m
           ^^^^^^^^^^^^^^^^
  File "/usr/lib/python3.12/asyncio/runners.py", line [1m118[0m, in run
    return [1mself._loop.run_until_complete[0m[1m([0mtask[1m)[0m
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/lib/python3.12/asyncio/base_events.py", line [1m687[0m, in run_until_complete
    return [1mfuture.result[0m[1m([0m[1m)[0m
           ^^^^^^^^^^^^^^^
  File "/home/ubuntu/OmniPerf-Bench/third-party/trae-agent/trae_agent/agent/agent.py", line [1m93[0m, in run
    await cli_console_task
  File "/home/ubuntu/OmniPerf-Bench/third-party/trae-agent/trae_agent/utils/cli/simple_console.py", line [1m119[0m, in start
    await [1mself._print_lakeview_summary[0m[1m([0m[1m)[0m
  File "/home/ubuntu/OmniPerf-Bench/third-party/trae-agent/trae_agent/utils/cli/simple_console.py", line [1m154[0m, in _print_lakeview_summary
    lake_view_panel = await step.lake_view_panel_generator
                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/OmniPerf-Bench/third-party/trae-agent/trae_agent/utils/cli/simple_console.py", line [1m306[0m, in _create_lakeview_step_display
    lake_view_step = await [1mself.lake_view.create_lakeview_step[0m[1m([0magent_step[1m)[0m
                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/OmniPerf-Bench/third-party/trae-agent/trae_agent/utils/lake_view.py", line [1m205[0m, in create_lakeview_step
    tags = await [1mself.extract_tag_in_step[0m[1m([0mthis_step_str[1m)[0m
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/OmniPerf-Bench/third-party/trae-agent/trae_agent/utils/lake_view.py", line [1m160[0m, in extract_tag_in_step
    llm_response = [1mself.lakeview_llm_client.chat[0m[1m([0m
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/OmniPerf-Bench/third-party/trae-agent/trae_agent/utils/llm_clients/llm_client.py", line [1m80[0m, in chat
    return [1mself.client.chat[0m[1m([0mmessages, model_config, tools, reuse_history[1m)[0m
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/OmniPerf-Bench/third-party/trae-agent/trae_agent/utils/llm_clients/openai_client.py", line [1m131[0m, in chat
    response = [1mretry_decorator[0m[1m([0mapi_call_input, model_config, tool_schemas[1m)[0m
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/OmniPerf-Bench/third-party/trae-agent/trae_agent/utils/llm_clients/retry_utils.py", line [1m36[0m, in wrapper
    return [1mfunc[0m[1m([0m*args, **kwargs[1m)[0m
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/OmniPerf-Bench/third-party/trae-agent/trae_agent/utils/llm_clients/openai_client.py", line [1m86[0m, in _create_openai_response
    response = [1mself.client.responses.create[0m[1m([0m**api_params[1m)[0m
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/OmniPerf-Bench/bench-env/lib/python3.12/site-packages/openai/resources/responses/responses.py", line [1m795[0m, in create
    return [1mself._post[0m[1m([0m
           ^^^^^^^^^^^
  File "/home/ubuntu/OmniPerf-Bench/bench-env/lib/python3.12/site-packages/openai/_base_client.py", line [1m1259[0m, in post
    return [1mcast[0m[1m([0mResponseT, [1mself.request[0m[1m([0mcast_to, opts, stream=stream, stream_cls=stream_cls[1m)[0m[1m)[0m
                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/OmniPerf-Bench/bench-env/lib/python3.12/site-packages/openai/_base_client.py", line [1m1047[0m, in request
    raise [1mself._make_status_error_from_response[0m[1m([0merr.response[1m)[0m from [3mNone[0m
openai.RateLimitError: Error code: [1m429[0m - [1m{[0m'error': [1m{[0m'message': 'You exceeded your current quota, please check your plan and billing details. For more information on
this error, read the docs: https://platform.openai.com/docs/guides/error-codes/api-errors.', 'type': 'insufficient_quota', 'param': [3mNone[0m, 'code':
'insufficient_quota'[1m}[0m[1m}[0m

Trajectory saved to: /home/ubuntu/OmniPerf-Bench/perf-agents-bench/state/runs/sglang_core-ae58875a/sglang_033_73b13e69/trajectory.json