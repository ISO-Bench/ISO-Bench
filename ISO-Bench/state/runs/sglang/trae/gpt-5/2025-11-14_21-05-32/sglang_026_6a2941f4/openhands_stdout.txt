Changed working directory to: /home/ubuntu/OmniPerf-Bench/perf-agents-bench/.work/worktrees/sglang_core/sglang_026_6a2941f4
Initialising MCP tools...
â•­â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ Task Details â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•®
â”‚ [1mTask:[0m I've uploaded a python code repository in the directory /home/ubuntu/OmniPerf-Bench/perf-agents-bench/.work/worktrees/sglang_core/sglang_026_6a2941f4.     â”‚
â”‚ Consider the following test script showing an example usage of the repository:                                                                                   â”‚
â”‚                                                                                                                                                                  â”‚
â”‚ <test_script>                                                                                                                                                    â”‚
â”‚ # This is a performance optimization task                                                                                                                        â”‚
â”‚ # The specific operations to optimize are in the files listed below                                                                                              â”‚
â”‚ # Focus on performance improvements in the target functions                                                                                                      â”‚
â”‚                                                                                                                                                                  â”‚
â”‚ </test_script>                                                                                                                                                   â”‚
â”‚                                                                                                                                                                  â”‚
â”‚ Can you help me implement the necessary changes to the repository so that the runtime of the <test_script> is optimized?                                         â”‚
â”‚                                                                                                                                                                  â”‚
â”‚ Basic guidelines:                                                                                                                                                â”‚
â”‚ 1. Your task is to make changes to non-test files in the /home/ubuntu/OmniPerf-Bench/perf-agents-bench/.work/worktrees/sglang_core/sglang_026_6a2941f4 directory â”‚
â”‚ to improve the performance of the <test_script>.                                                                                                                 â”‚
â”‚ 2. Make changes while ensuring the repository is functionally equivalent to the original.                                                                        â”‚
â”‚ 3. Do not overoptimize for just the specific inputs in <test_script>. Make general performance improvements for the usage scenario shown.                        â”‚
â”‚ 4. You may need to rebuild the repo for your changes to take effect before testing. Some rebuilds may take time to run, so be patient with running them.         â”‚
â”‚                                                                                                                                                                  â”‚
â”‚ Follow these steps to improve performance:                                                                                                                       â”‚
â”‚ 1. As a first step, explore the repository structure.                                                                                                            â”‚
â”‚ 2. Create a script ONLY inside /home/ubuntu/OmniPerf-Bench/perf-agents-bench/.work/worktrees/sglang_core/sglang_026_6a2941f4/.bench_scratch (e.g.,               â”‚
â”‚ /home/ubuntu/OmniPerf-Bench/perf-agents-bench/.work/worktrees/sglang_core/sglang_026_6a2941f4/.bench_scratch/test_opt.py) to reproduce and time the example,     â”‚
â”‚ then execute it with python <filename.py> from the repo root.                                                                                                    â”‚
â”‚ 3. Edit the source code of the repository to improve performance.                                                                                                â”‚
â”‚ 4. Rebuild and rerun your script to confirm that performance has improved.                                                                                       â”‚
â”‚                                                                                                                                                                  â”‚
â”‚ Here is an example of the kind of optimizations that have been shown to improve performance in this codebase:                                                    â”‚
â”‚                                                                                                                                                                  â”‚
â”‚ <example_optimization_diff>                                                                                                                                      â”‚
â”‚ diff --git a/README.md b/README.md                                                                                                                               â”‚
â”‚ index 2ac666c6b..90822b176 100644                                                                                                                                â”‚
â”‚ --- a/README.md                                                                                                                                                  â”‚
â”‚ +++ b/README.md                                                                                                                                                  â”‚
â”‚ @@ -377,6 +377,14 @@ python -m sglang.launch_server --model-path meta-llama/Llama-2-7b-chat-hf --port                                                            â”‚
â”‚  python -m sglang.launch_server --model-path meta-llama/Llama-2-7b-chat-hf --port 30000 --mem-fraction-static 0.7                                                â”‚
â”‚  ```                                                                                                                                                             â”‚
â”‚  - See (docs/hyperparameter_tuning.md) on tuning hyperparameters for better performance.                                                                         â”‚
â”‚ +- Add `--nnodes 2` to run tensor parallelism on multiple nodes. If you have two nodes with two GPUs on each node and want to run TP=4, let `sgl-dev-1` be the   â”‚
â”‚ hostname of the first node and `50000` be an available port.                                                                                                     â”‚
â”‚ +```                                                                                                                                                             â”‚
â”‚ +# Node 0                                                                                                                                                        â”‚
â”‚ +python -m sglang.launch_server --model-path meta-llama/Llama-2-7b-chat-hf --tp 4 --nccl-init sgl-dev-1:50000 --nnodes 2 --node-rank 0                           â”‚
â”‚ +                                                                                                                                                                â”‚
â”‚ +# Node 1                                                                                                                                                        â”‚
â”‚ +python -m sglang.launch_server --model-path meta-llama/Llama-2-7b-chat-hf --tp 4 --nccl-init sgl-dev-1:50000 --nnodes 2 --node-rank 1                           â”‚
â”‚ +```                                                                                                                                                             â”‚
â”‚                                                                                                                                                                  â”‚
â”‚  ### Supported Models                                                                                                                                            â”‚
â”‚  - Llama                                                                                                                                                         â”‚
â”‚ diff --git a/benchmark/latency_throughput/bench_one.py b/benchmark/latency_throughput/bench_one.py                                                               â”‚
â”‚ index cfd96b54c..0bb26ee15 100644                                                                                                                                â”‚
â”‚ --- a/benchmark/latency_throughput/bench_one.py                                                                                                                  â”‚
â”‚ +++ b/benchmark/latency_throughput/bench_one.py                                                                                                                  â”‚
â”‚ @@ -96,8 +96,11 @@ def run_one_batch_size(bs):                                                                                                                   â”‚
â”‚          ret = response.json()                                                                                                                                   â”‚
â”‚      print(ret)                                                                                                                                                  â”‚
â”‚                                                                                                                                                                  â”‚
â”‚ +    input_len = args.input_len if args.input_len else 1                                                                                                         â”‚
â”‚ +    output_len = max_new_tokens                                                                                                                                 â”‚
â”‚ +                                                                                                                                                                â”‚
â”‚ </example_optimization_diff>                                                                                                                                     â”‚
â”‚                                                                                                                                                                  â”‚
â”‚ IMPORTANT: The above diff is an EXAMPLE of optimizations that were successful in a different context.                                                            â”‚
â”‚ These changes have NOT been applied to your codebase yet.                                                                                                        â”‚
â”‚ Your task is to:                                                                                                                                                 â”‚
â”‚ 1. Understand the optimization pattern shown (e.g., torch.zeros â†’ torch.empty)                                                                                   â”‚
â”‚ 2. Look at the CURRENT code in the target files                                                                                                                  â”‚
â”‚ 3. Find places where you can apply SIMILAR optimizations                                                                                                         â”‚
â”‚ 4. MAKE THE CHANGES yourself using str_replace_editor                                                                                                            â”‚
â”‚                                                                                                                                                                  â”‚
â”‚ The codebase you're working with is at the BASE commit - it does NOT have these optimizations yet.                                                               â”‚
â”‚ You need to IMPLEMENT similar optimizations yourself.                                                                                                            â”‚
â”‚                                                                                                                                                                  â”‚
â”‚ HERE'S WHAT YOU NEED TO DO:                                                                                                                                      â”‚
â”‚ 1. The files CURRENTLY contain torch.zeros() calls that need optimization                                                                                        â”‚
â”‚ 2. You need to CHANGE torch.zeros to torch.empty where appropriate                                                                                               â”‚
â”‚ 3. You need to REMOVE .fill_() operations that are unnecessary                                                                                                   â”‚
â”‚ 4. These are NEW changes you're making - not already in the code                                                                                                 â”‚
â”‚                                                                                                                                                                  â”‚
â”‚ START WITH THIS COMMAND to see what needs changing:                                                                                                              â”‚
â”‚ ```bash                                                                                                                                                          â”‚
â”‚ grep -n 'torch.zeros\|fill_' vllm/model_executor/layers/fused_moe/moe_align_block_size.py benchmarks/kernels/benchmark_moe_align_block_size.py                   â”‚
â”‚ ```                                                                                                                                                              â”‚
â”‚                                                                                                                                                                  â”‚
â”‚ CRITICAL: You MUST make actual code changes. Look for patterns like:                                                                                             â”‚
â”‚ - Remove unnecessary tensor filling operations                                                                                                                   â”‚
â”‚                                                                                                                                                                  â”‚
â”‚ Target files to optimize:                                                                                                                                        â”‚
â”‚ - README.md                                                                                                                                                      â”‚
â”‚ - benchmark/latency_throughput/bench_one.py                                                                                                                      â”‚
â”‚ - benchmark/latency_throughput/bench_serving.py                                                                                                                  â”‚
â”‚                                                                                                                                                                  â”‚
â”‚ IMPORTANT: You MUST make actual code changes to at least one file.                                                                                               â”‚
â”‚ The task will fail if no files are modified.                                                                                                                     â”‚
â”‚                                                                                                                                                                  â”‚
â”‚ ## Constraints                                                                                                                                                   â”‚
â”‚ - No public API breakage                                                                                                                                         â”‚
â”‚ - All TestPack checks must pass                                                                                                                                  â”‚
â”‚                                                                                                                                                                  â”‚
â”‚ ## Target Files (ONLY modify these)                                                                                                                              â”‚
â”‚ - `README.md`                                                                                                                                                    â”‚
â”‚ - `benchmark/latency_throughput/bench_one.py`                                                                                                                    â”‚
â”‚ - `benchmark/latency_throughput/bench_serving.py`                                                                                                                â”‚
â”‚ - `python/sglang/README.md`                                                                                                                                      â”‚
â”‚ - `python/sglang/srt/managers/controller/manager_multi.py`                                                                                                       â”‚
â”‚ - `python/sglang/srt/managers/controller/manager_single.py`                                                                                                      â”‚
â”‚ - `python/sglang/srt/managers/controller/model_runner.py`                                                                                                        â”‚
â”‚ - `python/sglang/srt/managers/controller/tp_worker.py`                                                                                                           â”‚
â”‚ - `python/sglang/srt/server.py`                                                                                                                                  â”‚
â”‚ - `python/sglang/srt/server_args.py`                                                                                                                             â”‚
â”‚                                                                                                                                                                  â”‚
â”‚ ## SPECIFIC OPTIMIZATION TARGETS:                                                                                                                                â”‚
â”‚ Based on the human commit analysis, focus on these areas:                                                                                                        â”‚
â”‚ - Memory allocation patterns (torch.zeros vs torch.empty)                                                                                                        â”‚
â”‚ - Tensor initialization strategies                                                                                                                               â”‚
â”‚ - Kernel parameter optimization                                                                                                                                  â”‚
â”‚ - Buffer reuse and caching                                                                                                                                       â”‚
â”‚                                                                                                                                                                  â”‚
â”‚ ### Human Developer's Approach:                                                                                                                                  â”‚
â”‚ ```                                                                                                                                                              â”‚
â”‚ Improve tensor parallel performance (#625)                                                                                                                       â”‚
â”‚                                                                                                                                                                  â”‚
â”‚ Co-authored-by: Mingyi <wisclmy0611@gmail.com>                                                                                                                   â”‚
â”‚ ```                                                                                                                                                              â”‚
â”‚                                                                                                                                                                  â”‚
â”‚ ### Files Modified (statistics):                                                                                                                                 â”‚
â”‚ ```                                                                                                                                                              â”‚
â”‚ README.md                                          |   8 +                                                                                                       â”‚
â”‚  benchmark/latency_throughput/bench_one.py          |   5 +-                                                                                                     â”‚
â”‚  benchmark/latency_throughput/bench_serving.py      |   3 +                                                                                                      â”‚
â”‚  python/sglang/README.md                            |   3 +-                                                                                                     â”‚
â”‚  .../srt/managers/controller/manager_multi.py       |   8 +-                                                                                                     â”‚
â”‚  .../srt/managers/controller/manager_single.py      | 175 +++++++++++++++------                                                                                  â”‚
â”‚  .../sglang/srt/managers/controller/model_runner.py |   3 +-                                                                                                     â”‚
â”‚  python/sglang/srt/managers/controller/tp_worker.py |  18 +--                                                                                                    â”‚
â”‚  python/sglang/srt/server.py                        |  25 ++-                                                                                                    â”‚
â”‚  python/sglang/srt/server_args.py                   |   6 +-                                                                                                     â”‚
â”‚  10 files changed, 172 insertions(+), 82 deletions(-)                                                                                                            â”‚
â”‚ ```                                                                                                                                                              â”‚
â”‚                                                                                                                                                                  â”‚
â”‚ ## IMMEDIATE ACTION REQUIREMENTS:                                                                                                                                â”‚
â”‚ 1. Start editing files by iteration 3                                                                                                                            â”‚
â”‚ 2. Create and run /home/ubuntu/OmniPerf-Bench/perf-agents-bench/.work/worktrees/sglang_core/sglang_026_6a2941f4/.bench_scratch/test_opt.py before and after      â”‚
â”‚ edits (do not create timing scripts outside /home/ubuntu/OmniPerf-Bench/perf-agents-bench/.work/worktrees/sglang_core/sglang_026_6a2941f4/.bench_scratch)        â”‚
â”‚ 3. Make at least 3 concrete optimizations                                                                                                                        â”‚
â”‚ 4. Commit changes by iteration 8                                                                                                                                 â”‚
â”‚ 5. Use finish command by iteration 10                                                                                                                            â”‚
â”‚                                                                                                                                                                  â”‚
â”‚ ## TASK COMPLETION COMMAND:                                                                                                                                      â”‚
â”‚ When you have made optimizations:                                                                                                                                â”‚
â”‚ ```bash                                                                                                                                                          â”‚
â”‚ git add -A                                                                                                                                                       â”‚
â”‚ git reset -q .bench_scratch || true                                                                                                                              â”‚
â”‚ git commit -m 'Optimize MoE align sum kernels performance'                                                                                                       â”‚
â”‚ git diff $(git merge-base HEAD origin/HEAD || git rev-parse HEAD~1) -- . ':(exclude).bench_scratch' >                                                            â”‚
â”‚ /home/ubuntu/OmniPerf-Bench/perf-agents-bench/.work/worktrees/sglang_core/sglang_026_6a2941f4/model_patch.diff || true                                           â”‚
â”‚ finish                                                                                                                                                           â”‚
â”‚ ```                                                                                                                                                              â”‚
â”‚                                                                                                                                                                  â”‚
â”‚ START IMPLEMENTING IMMEDIATELY. NO MORE ANALYSIS.                                                                                                                â”‚
â”‚                                                                                                                                                                  â”‚
â”‚ [1mModel Provider:[0m openai                                                                                                                                           â”‚
â”‚ [1mModel:[0m gpt-5-2025-08-07                                                                                                                                          â”‚
â”‚ [1mMax Steps:[0m 120                                                                                                                                                   â”‚
â”‚ [1mTrajectory File:[0m /home/ubuntu/OmniPerf-Bench/perf-agents-bench/state/runs/sglang_core-ae58875a/sglang_026_6a2941f4/trajectory.json                               â”‚
â”‚ [1mTools:[0m bash, str_replace_based_edit_tool, sequentialthinking, task_done                                                                                          â”‚
â”‚ [1mProject_path:[0m /home/ubuntu/OmniPerf-Bench/perf-agents-bench/.work/worktrees/sglang_core/sglang_026_6a2941f4                                                      â”‚
â”‚ [1mIssue:[0m I've uploaded a python code repository in the directory /home/ubuntu/OmniPerf-Bench/perf-agents-bench/.work/worktrees/sglang_core/sglang_026_6a2941f4.    â”‚
â”‚ Consider the following test script showing an example usage of the repository:                                                                                   â”‚
â”‚                                                                                                                                                                  â”‚
â”‚ <test_script>                                                                                                                                                    â”‚
â”‚ # This is a performance optimization task                                                                                                                        â”‚
â”‚ # The specific operations to optimize are in the files listed below                                                                                              â”‚
â”‚ # Focus on performance improvements in the target functions                                                                                                      â”‚
â”‚                                                                                                                                                                  â”‚
â”‚ </test_script>                                                                                                                                                   â”‚
â”‚                                                                                                                                                                  â”‚
â”‚ Can you help me implement the necessary changes to the repository so that the runtime of the <test_script> is optimized?                                         â”‚
â”‚                                                                                                                                                                  â”‚
â”‚ Basic guidelines:                                                                                                                                                â”‚
â”‚ 1. Your task is to make changes to non-test files in the /home/ubuntu/OmniPerf-Bench/perf-agents-bench/.work/worktrees/sglang_core/sglang_026_6a2941f4 directory â”‚
â”‚ to improve the performance of the <test_script>.                                                                                                                 â”‚
â”‚ 2. Make changes while ensuring the repository is functionally equivalent to the original.                                                                        â”‚
â”‚ 3. Do not overoptimize for just the specific inputs in <test_script>. Make general performance improvements for the usage scenario shown.                        â”‚
â”‚ 4. You may need to rebuild the repo for your changes to take effect before testing. Some rebuilds may take time to run, so be patient with running them.         â”‚
â”‚                                                                                                                                                                  â”‚
â”‚ Follow these steps to improve performance:                                                                                                                       â”‚
â”‚ 1. As a first step, explore the repository structure.                                                                                                            â”‚
â”‚ 2. Create a script ONLY inside /home/ubuntu/OmniPerf-Bench/perf-agents-bench/.work/worktrees/sglang_core/sglang_026_6a2941f4/.bench_scratch (e.g.,               â”‚
â”‚ /home/ubuntu/OmniPerf-Bench/perf-agents-bench/.work/worktrees/sglang_core/sglang_026_6a2941f4/.bench_scratch/test_opt.py) to reproduce and time the example,     â”‚
â”‚ then execute it with python <filename.py> from the repo root.                                                                                                    â”‚
â”‚ 3. Edit the source code of the repository to improve performance.                                                                                                â”‚
â”‚ 4. Rebuild and rerun your script to confirm that performance has improved.                                                                                       â”‚
â”‚                                                                                                                                                                  â”‚
â”‚ Here is an example of the kind of optimizations that have been shown to improve performance in this codebase:                                                    â”‚
â”‚                                                                                                                                                                  â”‚
â”‚ <example_optimization_diff>                                                                                                                                      â”‚
â”‚ diff --git a/README.md b/README.md                                                                                                                               â”‚
â”‚ index 2ac666c6b..90822b176 100644                                                                                                                                â”‚
â”‚ --- a/README.md                                                                                                                                                  â”‚
â”‚ +++ b/README.md                                                                                                                                                  â”‚
â”‚ @@ -377,6 +377,14 @@ python -m sglang.launch_server --model-path meta-llama/Llama-2-7b-chat-hf --port                                                            â”‚
â”‚  python -m sglang.launch_server --model-path meta-llama/Llama-2-7b-chat-hf --port 30000 --mem-fraction-static 0.7                                                â”‚
â”‚  ```                                                                                                                                                             â”‚
â”‚  - See (docs/hyperparameter_tuning.md) on tuning hyperparameters for better performance.                                                                         â”‚
â”‚ +- Add `--nnodes 2` to run tensor parallelism on multiple nodes. If you have two nodes with two GPUs on each node and want to run TP=4, let `sgl-dev-1` be the   â”‚
â”‚ hostname of the first node and `50000` be an available port.                                                                                                     â”‚
â”‚ +```                                                                                                                                                             â”‚
â”‚ +# Node 0                                                                                                                                                        â”‚
â”‚ +python -m sglang.launch_server --model-path meta-llama/Llama-2-7b-chat-hf --tp 4 --nccl-init sgl-dev-1:50000 --nnodes 2 --node-rank 0                           â”‚
â”‚ +                                                                                                                                                                â”‚
â”‚ +# Node 1                                                                                                                                                        â”‚
â”‚ +python -m sglang.launch_server --model-path meta-llama/Llama-2-7b-chat-hf --tp 4 --nccl-init sgl-dev-1:50000 --nnodes 2 --node-rank 1                           â”‚
â”‚ +```                                                                                                                                                             â”‚
â”‚                                                                                                                                                                  â”‚
â”‚  ### Supported Models                                                                                                                                            â”‚
â”‚  - Llama                                                                                                                                                         â”‚
â”‚ diff --git a/benchmark/latency_throughput/bench_one.py b/benchmark/latency_throughput/bench_one.py                                                               â”‚
â”‚ index cfd96b54c..0bb26ee15 100644                                                                                                                                â”‚
â”‚ --- a/benchmark/latency_throughput/bench_one.py                                                                                                                  â”‚
â”‚ +++ b/benchmark/latency_throughput/bench_one.py                                                                                                                  â”‚
â”‚ @@ -96,8 +96,11 @@ def run_one_batch_size(bs):                                                                                                                   â”‚
â”‚          ret = response.json()                                                                                                                                   â”‚
â”‚      print(ret)                                                                                                                                                  â”‚
â”‚                                                                                                                                                                  â”‚
â”‚ +    input_len = args.input_len if args.input_len else 1                                                                                                         â”‚
â”‚ +    output_len = max_new_tokens                                                                                                                                 â”‚
â”‚ +                                                                                                                                                                â”‚
â”‚ </example_optimization_diff>                                                                                                                                     â”‚
â”‚                                                                                                                                                                  â”‚
â”‚ IMPORTANT: The above diff is an EXAMPLE of optimizations that were successful in a different context.                                                            â”‚
â”‚ These changes have NOT been applied to your codebase yet.                                                                                                        â”‚
â”‚ Your task is to:                                                                                                                                                 â”‚
â”‚ 1. Understand the optimization pattern shown (e.g., torch.zeros â†’ torch.empty)                                                                                   â”‚
â”‚ 2. Look at the CURRENT code in the target files                                                                                                                  â”‚
â”‚ 3. Find places where you can apply SIMILAR optimizations                                                                                                         â”‚
â”‚ 4. MAKE THE CHANGES yourself using str_replace_editor                                                                                                            â”‚
â”‚                                                                                                                                                                  â”‚
â”‚ The codebase you're working with is at the BASE commit - it does NOT have these optimizations yet.                                                               â”‚
â”‚ You need to IMPLEMENT similar optimizations yourself.                                                                                                            â”‚
â”‚                                                                                                                                                                  â”‚
â”‚ HERE'S WHAT YOU NEED TO DO:                                                                                                                                      â”‚
â”‚ 1. The files CURRENTLY contain torch.zeros() calls that need optimization                                                                                        â”‚
â”‚ 2. You need to CHANGE torch.zeros to torch.empty where appropriate                                                                                               â”‚
â”‚ 3. You need to REMOVE .fill_() operations that are unnecessary                                                                                                   â”‚
â”‚ 4. These are NEW changes you're making - not already in the code                                                                                                 â”‚
â”‚                                                                                                                                                                  â”‚
â”‚ START WITH THIS COMMAND to see what needs changing:                                                                                                              â”‚
â”‚ ```bash                                                                                                                                                          â”‚
â”‚ grep -n 'torch.zeros\|fill_' vllm/model_executor/layers/fused_moe/moe_align_block_size.py benchmarks/kernels/benchmark_moe_align_block_size.py                   â”‚
â”‚ ```                                                                                                                                                              â”‚
â”‚                                                                                                                                                                  â”‚
â”‚ CRITICAL: You MUST make actual code changes. Look for patterns like:                                                                                             â”‚
â”‚ - Remove unnecessary tensor filling operations                                                                                                                   â”‚
â”‚                                                                                                                                                                  â”‚
â”‚ Target files to optimize:                                                                                                                                        â”‚
â”‚ - README.md                                                                                                                                                      â”‚
â”‚ - benchmark/latency_throughput/bench_one.py                                                                                                                      â”‚
â”‚ - benchmark/latency_throughput/bench_serving.py                                                                                                                  â”‚
â”‚                                                                                                                                                                  â”‚
â”‚ IMPORTANT: You MUST make actual code changes to at least one file.                                                                                               â”‚
â”‚ The task will fail if no files are modified.                                                                                                                     â”‚
â”‚                                                                                                                                                                  â”‚
â”‚ ## Constraints                                                                                                                                                   â”‚
â”‚ - No public API breakage                                                                                                                                         â”‚
â”‚ - All TestPack checks must pass                                                                                                                                  â”‚
â”‚                                                                                                                                                                  â”‚
â”‚ ## Target Files (ONLY modify these)                                                                                                                              â”‚
â”‚ - `README.md`                                                                                                                                                    â”‚
â”‚ - `benchmark/latency_throughput/bench_one.py`                                                                                                                    â”‚
â”‚ - `benchmark/latency_throughput/bench_serving.py`                                                                                                                â”‚
â”‚ - `python/sglang/README.md`                                                                                                                                      â”‚
â”‚ - `python/sglang/srt/managers/controller/manager_multi.py`                                                                                                       â”‚
â”‚ - `python/sglang/srt/managers/controller/manager_single.py`                                                                                                      â”‚
â”‚ - `python/sglang/srt/managers/controller/model_runner.py`                                                                                                        â”‚
â”‚ - `python/sglang/srt/managers/controller/tp_worker.py`                                                                                                           â”‚
â”‚ - `python/sglang/srt/server.py`                                                                                                                                  â”‚
â”‚ - `python/sglang/srt/server_args.py`                                                                                                                             â”‚
â”‚                                                                                                                                                                  â”‚
â”‚ ## SPECIFIC OPTIMIZATION TARGETS:                                                                                                                                â”‚
â”‚ Based on the human commit analysis, focus on these areas:                                                                                                        â”‚
â”‚ - Memory allocation patterns (torch.zeros vs torch.empty)                                                                                                        â”‚
â”‚ - Tensor initialization strategies                                                                                                                               â”‚
â”‚ - Kernel parameter optimization                                                                                                                                  â”‚
â”‚ - Buffer reuse and caching                                                                                                                                       â”‚
â”‚                                                                                                                                                                  â”‚
â”‚ ### Human Developer's Approach:                                                                                                                                  â”‚
â”‚ ```                                                                                                                                                              â”‚
â”‚ Improve tensor parallel performance (#625)                                                                                                                       â”‚
â”‚                                                                                                                                                                  â”‚
â”‚ Co-authored-by: Mingyi <wisclmy0611@gmail.com>                                                                                                                   â”‚
â”‚ ```                                                                                                                                                              â”‚
â”‚                                                                                                                                                                  â”‚
â”‚ ### Files Modified (statistics):                                                                                                                                 â”‚
â”‚ ```                                                                                                                                                              â”‚
â”‚ README.md                                          |   8 +                                                                                                       â”‚
â”‚  benchmark/latency_throughput/bench_one.py          |   5 +-                                                                                                     â”‚
â”‚  benchmark/latency_throughput/bench_serving.py      |   3 +                                                                                                      â”‚
â”‚  python/sglang/README.md                            |   3 +-                                                                                                     â”‚
â”‚  .../srt/managers/controller/manager_multi.py       |   8 +-                                                                                                     â”‚
â”‚  .../srt/managers/controller/manager_single.py      | 175 +++++++++++++++------                                                                                  â”‚
â”‚  .../sglang/srt/managers/controller/model_runner.py |   3 +-                                                                                                     â”‚
â”‚  python/sglang/srt/managers/controller/tp_worker.py |  18 +--                                                                                                    â”‚
â”‚  python/sglang/srt/server.py                        |  25 ++-                                                                                                    â”‚
â”‚  python/sglang/srt/server_args.py                   |   6 +-                                                                                                     â”‚
â”‚  10 files changed, 172 insertions(+), 82 deletions(-)                                                                                                            â”‚
â”‚ ```                                                                                                                                                              â”‚
â”‚                                                                                                                                                                  â”‚
â”‚ ## IMMEDIATE ACTION REQUIREMENTS:                                                                                                                                â”‚
â”‚ 1. Start editing files by iteration 3                                                                                                                            â”‚
â”‚ 2. Create and run /home/ubuntu/OmniPerf-Bench/perf-agents-bench/.work/worktrees/sglang_core/sglang_026_6a2941f4/.bench_scratch/test_opt.py before and after      â”‚
â”‚ edits (do not create timing scripts outside /home/ubuntu/OmniPerf-Bench/perf-agents-bench/.work/worktrees/sglang_core/sglang_026_6a2941f4/.bench_scratch)        â”‚
â”‚ 3. Make at least 3 concrete optimizations                                                                                                                        â”‚
â”‚ 4. Commit changes by iteration 8                                                                                                                                 â”‚
â”‚ 5. Use finish command by iteration 10                                                                                                                            â”‚
â”‚                                                                                                                                                                  â”‚
â”‚ ## TASK COMPLETION COMMAND:                                                                                                                                      â”‚
â”‚ When you have made optimizations:                                                                                                                                â”‚
â”‚ ```bash                                                                                                                                                          â”‚
â”‚ git add -A                                                                                                                                                       â”‚
â”‚ git reset -q .bench_scratch || true                                                                                                                              â”‚
â”‚ git commit -m 'Optimize MoE align sum kernels performance'                                                                                                       â”‚
â”‚ git diff $(git merge-base HEAD origin/HEAD || git rev-parse HEAD~1) -- . ':(exclude).bench_scratch' >                                                            â”‚
â”‚ /home/ubuntu/OmniPerf-Bench/perf-agents-bench/.work/worktrees/sglang_core/sglang_026_6a2941f4/model_patch.diff || true                                           â”‚
â”‚ finish                                                                                                                                                           â”‚
â”‚ ```                                                                                                                                                              â”‚
â”‚                                                                                                                                                                  â”‚
â”‚ START IMPLEMENTING IMMEDIATELY. NO MORE ANALYSIS.                                                                                                                â”‚
â”‚                                                                                                                                                                  â”‚
â”‚ [1mMust_patch:[0m true                                                                                                                                                 â”‚
â”‚ [1mPatch_path:[0m /home/ubuntu/OmniPerf-Bench/perf-agents-bench/state/runs/sglang_core-ae58875a/sglang_026_6a2941f4/model_patch.diff                                   â”‚
â•°â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•¯
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Status      â”‚ âœ… Step 1: Completed                                                                                   â”‚
â”‚ LLM         â”‚ ðŸ’¬ Plan:                                                                                               â”‚
â”‚ Response    â”‚ - Inspect repository and search for torch.zeros/fill_ usage as suggested.                              â”‚
â”‚             â”‚ - Create a .bench_scratch/test_opt.py to time a representative function or script.                     â”‚
â”‚             â”‚ - Implement small, safe performance optimizations in the specified target files.                       â”‚
â”‚             â”‚ - Re-run the timing script and commit changes.                                                         â”‚
â”‚             â”‚                                                                                                        â”‚
â”‚             â”‚ I will first search the repository for the specified patterns and list target files to understand      â”‚
â”‚             â”‚ context.                                                                                               â”‚
â”‚ Tools       â”‚ ðŸ”§ bash                                                                                                â”‚
â”‚ bash        â”‚ â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚
â”‚             â”‚ â”‚ {'command': "ls -la                            â”‚ total 84                                        â”‚   â”‚
â”‚             â”‚ â”‚ /home/ubuntu/OmniPerf-Bench/perf-agents-benchâ€¦ â”‚ drwxrwxr-x 11 ubuntu ubuntu  4096 Nov 15 01:30  â”‚   â”‚
â”‚             â”‚ â”‚ | sed -n '1,200p'", 'restart': False}          â”‚ .                                               â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ drwxrwxr-x 29 ubuntu ubuntu  4096 Nov 15 01:30  â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ ..                                              â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ -rw-rw-r--  1 ubuntu ubuntu   113 Nov 15 01:30  â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ .git                                            â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ -rw-rw-r--  1 ubuntu ubuntu  3295 Nov 15 01:30  â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ .gitignore                                      â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ -rw-rw-r--  1 ubuntu ubuntu     0 Nov 15 01:30  â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ .gitmodules                                     â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ -rw-rw-r--  1 ubuntu ubuntu    49 Nov 15 01:30  â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ .isort.cfg                                      â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ -rw-rw-r--  1 ubuntu ubuntu 11357 Nov 15 01:30  â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ LICENSE                                         â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ -rw-rw-r--  1 ubuntu ubuntu 15658 Nov 15 01:30  â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ README.md                                       â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ drwxrwxr-x  2 ubuntu ubuntu  4096 Nov 15 01:30  â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ assets                                          â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ drwxrwxr-x 22 ubuntu ubuntu  4096 Nov 15 01:30  â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ benchmark                                       â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ drwxrwxr-x  2 ubuntu ubuntu  4096 Nov 15 01:30  â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ docker                                          â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ drwxrwxr-x  2 ubuntu ubuntu  4096 Nov 15 01:30  â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ docs                                            â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ drwxrwxr-x  4 ubuntu ubuntu  4096 Nov 15 01:30  â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ examples                                        â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ drwxrwxr-x  2 ubuntu ubuntu  4096 Nov 15 01:30  â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ playground                                      â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ drwxrwxr-x  3 ubuntu ubuntu  4096 Nov 15 01:30  â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ python                                          â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ drwxrwxr-x  2 ubuntu ubuntu  4096 Nov 15 01:30  â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ scripts                                         â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ drwxrwxr-x  4 ubuntu ubuntu  4096 Nov 15 01:30  â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ test                                            â”‚   â”‚
â”‚             â”‚ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚
â”‚ Total       â”‚ Input: 8477 Output: 1308                                                                               â”‚
â”‚ Tokens      â”‚                                                                                                        â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Status      â”‚ âœ… Step 2: Completed                                                                                   â”‚
â”‚ Tools       â”‚ ðŸ”§ bash                                                                                                â”‚
â”‚ bash        â”‚ â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚
â”‚             â”‚ â”‚ {'command': "ls -la                            â”‚ total 36                                        â”‚   â”‚
â”‚             â”‚ â”‚ /home/ubuntu/OmniPerf-Bench/perf-agents-benchâ€¦ â”‚ drwxrwxr-x  2 ubuntu ubuntu  4096 Nov 15 01:30  â”‚   â”‚
â”‚             â”‚ â”‚ | sed -n '1,200p'", 'restart': False}          â”‚ .                                               â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ drwxrwxr-x 22 ubuntu ubuntu  4096 Nov 15 01:30  â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ ..                                              â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ -rw-rw-r--  1 ubuntu ubuntu  2146 Nov 15 01:30  â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ README.md                                       â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ -rw-rw-r--  1 ubuntu ubuntu  4387 Nov 15 01:30  â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ bench_one.py                                    â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ -rw-rw-r--  1 ubuntu ubuntu 12327 Nov 15 01:30  â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ bench_serving.py                                â”‚   â”‚
â”‚             â”‚ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚
â”‚ Total       â”‚ Input: 18689 Output: 1388                                                                              â”‚
â”‚ Tokens      â”‚                                                                                                        â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Status      â”‚ âœ… Step 3: Completed                                                                                   â”‚
â”‚ Tools       â”‚ ðŸ”§ str_replace_based_edit_tool                                                                         â”‚
â”‚ str_replacâ€¦ â”‚ â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚
â”‚             â”‚ â”‚ {'command': 'view', 'file_text': '',           â”‚ Here's the result of running `cat -n` on        â”‚   â”‚
â”‚             â”‚ â”‚ 'insert_line': 0, 'new_str': '', 'old_str':    â”‚ /home/ubuntu/OmniPerf-Bench/perf-agents-bench/â€¦ â”‚   â”‚
â”‚             â”‚ â”‚ '', 'path':                                    â”‚      1  """                                     â”‚   â”‚
â”‚             â”‚ â”‚ '/home/ubuntu/OmniPerf-Bench/perf-agents-bencâ€¦ â”‚      2  Usage:                                  â”‚   â”‚
â”‚             â”‚ â”‚ 'view_range': [1, -1]}                         â”‚      3  python3 bench_one.py --input-len 2048   â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ --batch-size 1 2 4 8 16 32 64 128 256 512       â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚      4  """                                     â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚      5                                          â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚      6  import argparse                         â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚      7  import json                             â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚      8  import time                             â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚      9                                          â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚     10  import numpy as np                      â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚     11  import requests                         â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚     12                                          â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚     13                                          â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚     14  def run_one_batch_size(bs):             â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚     15      url = f"{args.host}:{args.port}"    â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚     16      max_new_tokens = args.max_tokens    â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚     17                                          â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚     18      if args.input_len:                  â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚     19          input_ids = [                   â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚     20                                          â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚     21              for _ in range(bs)          â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚     22          ]                               â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚     23      else:                               â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚     24          text =                          â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚     25                                          â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚     26      tic = time.time()                   â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚     27      if args.backend == "srt":           â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚     28          if args.input_len:              â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚     29              inputs = {"input_ids":      â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ input_ids}                                      â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚     30          else:                           â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚     31              inputs = {"text": text}     â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚     32                                          â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚     33          response = requests.post(       â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚     34              url + "/generate",          â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚     35              json={                      â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚     36                  "sampling_params": {    â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚     37                      "temperature": 0,   â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚     38                      "max_new_tokens":   â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ max_new_tokens,                                 â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚     39                      "ignore_eos": True, â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚     40                  },                      â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚     41                  **inputs,               â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚     42              },                          â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚     43          )                               â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚     44      elif args.backend == "lightllm":    â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚     45          response = requests.post(       â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚     46              url + "/generate",          â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚     47              json={                      â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚     48                  "inputs": text[0],      â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚     49                  "parameters": {         â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚     50                      "temperature": 0,   â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚     51                      "max_new_tokens":   â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ max_new_tokens,                                 â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚     52                      "ignore_eos": True, â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚     53                  },                      â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚     54              },                          â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚     55          )                               â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚     56      elif args.backend == "vllm":        â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚     57          if args.input_len:              â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚     58              inputs = {"prompt":         â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ input_ids}                                      â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚     59          else:                           â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚     60              inputs = {"prompt": text}   â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚     61                                          â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚     62          response = requests.post(       â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚     63              url + "/v1/completions",    â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚     64              json={                      â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚     65                  "model":                â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ args.vllm_model_name,                           â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚     66                  "temperature": 0,       â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚     67                  "max_tokens":           â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ max_new_tokens,                                 â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚     68                  "ignore_eos": True,     â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚     69                  **inputs,               â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚     70              },                          â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚     71          )                               â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚     72      elif args.backend == "ginfer":      â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚     73          import grpc                     â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚     74          from ginfer import sampler_pb2, â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ sampler_pb2_grpc                                â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚     75                                          â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚     76          sampler_channel =               â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ grpc.insecure_channel(url.replace("http://",    â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ ""))                                            â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚     77          sampler =                       â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ sampler_pb2_grpc.SamplerStub(sampler_channel)   â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚     78                                          â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚     79          tic = time.time()               â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚     80          sample_request =                â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ sampler_pb2.SampleTextRequest(                  â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚     81              prompt=text[0],             â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚     82                                          â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ settings=sampler_pb2.SampleSettings(            â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚     83                  max_len=max_new_tokens, â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚     84                  rng_seed=0,             â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚     85                  temperature=0,          â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚     86                  nucleus_p=1,            â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚     87              ),                          â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚     88          )                               â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚     89          stream =                        â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ sampler.SampleText(sample_request)              â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚     90          response = "".join()            â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚     91      latency = time.time() - tic         â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚     92                                          â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚     93      if isinstance(response, str):       â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚     94          ret = response                  â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚     95      else:                               â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚     96          ret = response.json()           â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚     97      print(ret)                          â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚     98                                          â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚     99      output_throughput = bs *            â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ max_new_tokens / latency                        â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    100      overall_throughput = bs *           â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ (args.input_len + max_new_tokens) / latency     â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    101      print(f"latency: {latency:.2f} s")  â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    102      print(f"decode throughput:          â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ {output_throughput:.2f} token/s")               â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    103      print(f"overall throughput:         â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ {overall_throughput:.2f} token/s")              â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    104                                          â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    105      with open("results.jsonl", "a") as  â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ fout:                                           â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    106          res = {                         â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    107              "backend": args.backend,    â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    108              "input_len":                â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ args.input_len,                                 â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    109              "output_len":               â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ args.max_tokens,                                â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    110              "batch_size": bs,           â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    111              "latency": latency,         â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    112              "output_throughput":        â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ output_throughput,                              â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    113              "overall_throughput":       â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ overall_throughput,                             â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    114          }                               â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    115          fout.write(json.dumps(res) +    â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ "\n")                                           â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    116                                          â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    117                                          â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    118  if __name__ == "__main__":              â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    119      parser = argparse.ArgumentParser()  â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    120      parser.add_argument("--host",       â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ type=str, default="http://127.0.0.1")           â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    121      parser.add_argument("--port",       â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ type=int, default=None)                         â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    122      parser.add_argument("--backend",    â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ type=str, default="srt")                        â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    123      parser.add_argument("--input-len",  â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ type=int, default=None)                         â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    124      parser.add_argument("--batch-size", â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ type=int, nargs="*", default=[1])               â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    125      parser.add_argument("--max-tokens", â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ type=int, default=256)                          â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    126      parser.add_argument(                â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    127          "--vllm-model-name", type=str,  â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ default="meta-llama/Meta-Llama-3-70B"           â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    128      )                                   â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    129      args = parser.parse_args()          â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    130                                          â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    131      if args.port is None:               â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    132          if args.backend == "srt":       â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    133              args.port = 30000           â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    134          elif args.backend == "vllm":    â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    135              args.port = 21000           â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    136          elif args.backend ==            â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ "lightllm":                                     â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    137              args.port = 22000           â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    138          elif args.backend == "ginfer":  â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    139              args.port = 9988            â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    140          else:                           â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    141              raise ValueError(f"Invalid  â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ backend: {args.backend}")                       â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    142                                          â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    143      for bs in args.batch_size:          â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    144          run_one_batch_size(bs)          â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    145                                          â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                                                 â”‚   â”‚
â”‚             â”‚ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚
â”‚ Total       â”‚ Input: 29142 Output: 1483                                                                              â”‚
â”‚ Tokens      â”‚                                                                                                        â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Status      â”‚ âœ… Step 4: Completed                                                                                   â”‚
â”‚ Tools       â”‚ ðŸ”§ str_replace_based_edit_tool                                                                         â”‚
â”‚ str_replacâ€¦ â”‚ â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚
â”‚             â”‚ â”‚ {'command': 'view', 'file_text': '',           â”‚ Here's the result of running `cat -n` on        â”‚   â”‚
â”‚             â”‚ â”‚ 'insert_line': 0, 'new_str': '', 'old_str':    â”‚ /home/ubuntu/OmniPerf-Bench/perf-agents-bench/â€¦ â”‚   â”‚
â”‚             â”‚ â”‚ '', 'path':                                    â”‚      1  """Benchmark online serving throughput. â”‚   â”‚
â”‚             â”‚ â”‚ '/home/ubuntu/OmniPerf-Bench/perf-agents-bencâ€¦ â”‚      2                                          â”‚   â”‚
â”‚             â”‚ â”‚ 'view_range': [1, -1]}                         â”‚      3  On the server side, run one of the      â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ following commands:                             â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚      4      (vLLM backend)                      â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚      5      python -m                           â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ vllm.entrypoints.api_server \                   â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚      6          --model <your_model>            â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ --swap-space 16 \                               â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚      7          --disable-log-requests          â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚      8                                          â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚      9      (TGI backend)                       â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚     10      ./launch_hf_server.sh <your_model>  â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚     11                                          â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚     12  On the client side, run:                â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚     13      python                              â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ benchmarks/benchmark_serving.py \               â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚     14          --backend <backend> \           â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚     15          --tokenizer <your_model>        â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ --dataset <target_dataset> \                    â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚     16          --request-rate <request_rate>   â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚     17  """                                     â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚     18                                          â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚     19  import argparse                         â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚     20  import asyncio                          â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚     21  import json                             â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚     22  import os                               â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚     23  import random                           â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚     24  import time                             â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚     25  from typing import AsyncGenerator,      â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ List, Tuple                                     â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚     26                                          â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚     27  import aiohttp                          â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚     28  import numpy as np                      â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚     29  from tqdm.asyncio import tqdm_asyncio   â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚     30  from transformers import AutoTokenizer  â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚     31                                          â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚     32  # (prompt len, output len, latency)     â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚     33  REQUEST_LATENCY: List[Tuple] = []       â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚     34                                          â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚     35                                          â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚     36  def sample_requests(                    â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚     37      dataset_path: str,                  â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚     38      num_requests: int,                  â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚     39      tokenizer: AutoTokenizer,           â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚     40  ) -> List[Tuple]:                       â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚     41      def load_dataset():                 â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚     42          with open(dataset_path,         â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ encoding="utf-8") as f:                         â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚     43              dataset = json.load(f)      â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚     44          # Filter out the conversations  â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ with less than 2 turns.                         â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚     45          dataset = [data for data in     â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ dataset if len(data["conversations"]) >= 2]     â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚     46          # Only keep the first two turns â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ of each conversation.                           â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚     47          dataset = [                     â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚     48                                          â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ (data["conversations"][0]["value"],             â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ data["conversations"][1]["value"])              â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚     49              for data in dataset         â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚     50          ]                               â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚     51                                          â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚     52          # Tokenize the prompts and      â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ completions.                                    â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚     53          prompts =                       â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚     54          prompt_token_ids =              â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ tokenizer(prompts).input_ids                    â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚     55          completions =                   â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚     56          completion_token_ids =          â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ tokenizer(completions).input_ids                â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚     57          tokenized_dataset = []          â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚     58          for i in range(len(dataset)):   â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚     59              output_len =                â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ len(completion_token_ids[3m)[0m                       â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ [3m    60  [0m[3m            [0m                            â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ [3mtokenized_dataset.append((prompts[0m[3m, [0m             â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ [3mprompt_token_ids[0m[3m, output_len))[0m                  â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ [3m    61  [0m                                        â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ [3m    62  [0m[3m        # Filter out too long [0m          â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ [3msequences.[0m                                      â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ [3m    63  [0m[3m        filtered_dataset: List[Tuple[0m[3m] =[0m â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ [3m[][0m                                              â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ [3m    64  [0m[3m        for prompt, prompt_token_ids, [0m  â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ [3moutput_len in tokenized_dataset:[0m                â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ [3m    65  [0m[3m            prompt_len = [0m               â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ [3mlen(prompt_token_ids)[0m                           â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ [3m    66  [0m[3m            if prompt_len < 4 or [0m       â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ [3moutput_len < 4:[0m                                 â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ [3m    67  [0m[3m                # Prune too short [0m      â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ [3msequences.[0m                                      â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ [3m    68  [0m[3m                # This is because TGI [0m  â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ [3mcauses errors when the input or output length[0m   â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ [3m    69  [0m[3m                # is too short.[0m         â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ [3m    70  [0m[3m                continue[0m                â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ [3m    71  [0m[3m            if prompt_len > 1024 or [0m    â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ [3mprompt_len + output_len > 2048:[0m                 â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ [3m    72  [0m[3m                # Prune too long [0m       â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ [3msequences.[0m                                      â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ [3m    73  [0m[3m                continue[0m                â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ [3m    74  [0m[3m            [0m                            â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ [3mfiltered_dataset.append((prompt, prompt_len, [0m   â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ [3moutput_len))[0m                                    â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ [3m    75  [0m                                        â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ [3m    76  [0m[3m        return filtered_dataset[0m         â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ [3m    77  [0m                                        â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ [3m    78  [0m[3m    try:[0m                                â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ [3m    79  [0m[3m        from diskcache import Cache[0m     â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ [3m    80  [0m                                        â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ [3m    81  [0m[3m        home_dir = [0m                     â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ [3mos.path.expanduser("~")[0m                         â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ [3m    82  [0m[3m        cache = [0m                        â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ [3mCache(f"{home_dir}/.cache/sglang")[0m              â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ [3m    83  [0m[3m        with Cache(cache.directory) as [0m â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ [3mreference:[0m                                      â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ [3m    84  [0m[3m            reference_key = [0m            â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ [3mf"{dataset_path}_{tokenizer.name_or_path}"[0m      â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ [3m    85  [0m[3m            if reference_key in [0m        â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ [3mreference:[0m                                      â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ [3m    86  [0m[3m                print("Reading dataset [0m â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ [3mfrom cache...")[0m                                 â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ [3m    87  [0m[3m                dataset = reference[0m     â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ [3m    88  [0m[3m            else:[0m                       â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ [3m    89  [0m[3m                dataset = [0m              â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ [3mload_dataset()[0m                                  â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ [3m    90  [0m[3m                reference[0m[3m = dataset[0m     â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ [3m    91  [0m[3m    except ImportError:[0m                 â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ [3m    92  [0m[3m        dataset = load_dataset()[0m        â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ [3m    93  [0m                                        â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ [3m    94  [0m[3m    # Sample the requests.[0m              â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ [3m    95  [0m[3m    sampled_requests = [0m                 â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ [3mrandom.sample(dataset, num_requests)[0m            â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ [3m    96  [0m[3m    return sampled_requests[0m             â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ [3m    97  [0m                                        â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ [3m    98  [0m                                        â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ [3m    99  [0m[3masync def get_request([0m                  â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ [3m   100  [0m[3m    input_requests: List[Tuple[0m[3m],[0m        â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ [3m   101  [0m[3m    request_rate: float,[0m                â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ [3m   102  [0m[3m) -> AsyncGenerator[Tuple[0m[3m, None]:[0m       â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ [3m   103  [0m[3m    input_requests = [0m                   â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ [3miter(input_requests)[0m                            â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ [3m   104  [0m[3m    for request in input_requests:[0m      â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ [3m   105  [0m[3m        yield request[0m                   â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ [3m   106  [0m                                        â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ [3m   107  [0m[3m        if request_rate == [0m             â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ [3mfloat("inf"):[0m                                   â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ [3m   108  [0m[3m            # If the request rate is [0m   â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ [3minfinity, then we don't need to wait.[0m           â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ [3m   109  [0m[3m            continue[0m                    â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ [3m   110  [0m[3m        # Sample the request interval [0m  â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ [3mfrom the exponential distribution.[0m              â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ [3m   111  [0m[3m        interval = [0m                     â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ [3mnp.random.exponential(1.0 / request_rate)[0m       â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ [3m   112  [0m[3m        # The next request will be sent[0m â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ [3mafter the interval.[0m                             â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ [3m   113  [0m[3m        await asyncio.sleep(interval)[0m   â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ [3m   114  [0m                                        â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ [3m   115  [0m                                        â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ [3m   116  [0m[3masync def send_request([0m                 â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ [3m   117  [0m[3m    backend: str,[0m                       â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ [3m   118  [0m[3m    api_url: str,[0m                       â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ [3m   119  [0m[3m    prompt: str,[0m                        â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ [3m   120  [0m[3m    prompt_len: int,[0m                    â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ [3m   121  [0m[3m    output_len: int,[0m                    â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ [3m   122  [0m[3m    best_of: int,[0m                       â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ [3m   123  [0m[3m    use_beam_search: bool,[0m              â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ [3m   124  [0m[3m) -> None:[0m                              â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ [3m   125  [0m[3m    request_start_time = [0m               â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ [3mtime.perf_counter()[0m                             â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ [3m   126  [0m                                        â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ [3m   127  [0m[3m    headers = {"User-Agent": "Benchmark[0m â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ [3mClient"}[0m                                        â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ [3m   128  [0m[3m    if backend == "vllm":[0m               â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ [3m   129  [0m[3m        pload = {[0m                       â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ [3m   130  [0m[3m            "prompt": prompt,[0m           â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ [3m   131  [0m[3m            "n": 1,[0m                     â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ [3m   132  [0m[3m            "best_of": best_of,[0m         â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ [3m   133  [0m[3m            "use_beam_search": [0m         â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ [3muse_beam_search,[0m                                â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ [3m   134  [0m[3m            "temperature": 0.0 if [0m      â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ [3muse_beam_search else 1.0,[0m                       â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ [3m   135  [0m[3m            "top_p": 1.0,[0m               â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ [3m   136  [0m[3m            "max_tokens": output_len,[0m   â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ [3m   137  [0m[3m            "ignore_eos": True,[0m         â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ [3m   138  [0m[3m            "stream": False,[0m            â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ [3m   139  [0m[3m        }[0m                               â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ [3m   140  [0m[3m    elif backend == "tgi":[0m              â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ [3m   141  [0m[3m        assert not use_beam_search[0m      â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ [3m   142  [0m[3m        params = {[0m                      â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ [3m   143  [0m[3m            "best_of": best_of,[0m         â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ [3m   144  [0m[3m            "max_new_tokens": [0m          â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ [3moutput_len,[0m                                     â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ [3m   145  [0m[3m            "do_sample": True,[0m          â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ [3m   146  [0m[3m        }[0m                               â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ [3m   147  [0m[3m        pload = {[0m                       â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ [3m   148  [0m[3m            "inputs": prompt,[0m           â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ [3m   149  [0m[3m            "parameters": params,[0m       â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ [3m   150  [0m[3m        }[0m                               â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ [3m   151  [0m[3m    elif backend == "srt":[0m              â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ [3m   152  [0m[3m        assert not use_beam_search[0m      â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ [3m   153  [0m[3m        params = {[0m                      â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ [3m   154  [0m[3m            "ignore_eos": True,[0m         â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ [3m   155  [0m[3m            "max_new_tokens": [0m          â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ [3moutput_len,[0m                                     â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ [3m   156  [0m[3m        }[0m                               â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ [3m   157  [0m[3m        pload = {[0m                       â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ [3m   158  [0m[3m            "text": prompt,[0m             â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ [3m   159  [0m[3m            "sampling_params": params,[0m  â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ [3m   160  [0m[3m        }[0m                               â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ [3m   161  [0m[3m    elif backend == "lightllm":[0m         â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ [3m   162  [0m[3m        assert not use_beam_search[0m      â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ [3m   163  [0m[3m        params = {[0m                      â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ [3m   164  [0m[3m            "ignore_eos": True,[0m         â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ [3m   165  [0m[3m            "max_new_tokens": [0m          â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ [3moutput_len,[0m                                     â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ [3m   166  [0m[3m        }[0m                               â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ [3m   167  [0m[3m        pload = {[0m                       â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ [3m   168  [0m[3m            "inputs": prompt,[0m           â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ [3m   169  [0m[3m            "parameters": params,[0m       â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ [3m   170  [0m[3m        }[0m                               â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ [3m   171  [0m[3m    elif backend == "ginfer":[0m           â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ [3m   172  [0m[3m        pass[0m                            â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ [3m   173  [0m[3m    else:[0m                               â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ [3m   174  [0m[3m        raise ValueError(f"Unknown [0m     â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ [3mbackend: {backend}")[0m                            â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ [3m   175  [0m                                        â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ [3m   176  [0m[3m    if backend != "ginfer":[0m             â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ [3m   177  [0m[3m        timeout = [0m                      â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ [3maiohttp.ClientTimeout(total=3 * 3600)[0m           â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ [3m   178  [0m[3m        async with [0m                     â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ [3maiohttp.ClientSession(timeout=timeout) as [0m      â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ [3msession:[0m                                        â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ [3m   179  [0m[3m            while True:[0m                 â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ [3m   180  [0m[3m                async with [0m             â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ [3msession.post([0m                                   â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ [3m   181  [0m[3m                    api_url, [0m           â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ [3mheaders=headers, json=pload[0m                     â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ [3m   182  [0m[3m                ) as response:[0m          â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ [3m   183  [0m[3m                    chunks = [][0m         â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ [3m   184  [0m[3m                    async for chunk, _ [0m â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ [3min response.content.iter_chunks():[0m              â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ [3m   185  [0m[3m                        [0m                â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ [3mchunks.append(chunk)[0m                            â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ [3m   186  [0m[3m                output = [0m               â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ [3mb"".join(chunks).decode("utf-8")[0m                â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ [3m   187  [0m[3m                output = [0m               â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ [3mjson.loads(output)[0m                              â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ [3m   188  [0m                                        â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ [3m   189  [0m[3m                # Re-send the request [0m  â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ [3mif it failed.[0m                                   â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ [3m   190  [0m[3m                if "error" not in [0m      â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ [3moutput:[0m                                         â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ [3m   191  [0m[3m                    break[0m               â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ [3m   192  [0m[3m                else:[0m                   â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ [3m   193  [0m[3m                    print(output)[0m       â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ [3m   194  [0m[3m    else:[0m                               â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ [3m   195  [0m[3m        import grpc[0m                     â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ [3m   196  [0m[3m        from ginfer import sampler_pb2,[0m â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ [3msampler_pb2_grpc[0m                                â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ [3m   197  [0m                                        â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ [3m   198  [0m[3m        api_url = [0m                      â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ [3mapi_url.replace("http://", [0m                     â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ [3m"").replace("/generate", "")[0m                    â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ [3m   199  [0m[3m        sampler_channel = [0m              â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ [3mgrpc.aio.insecure_channel(api_url)[0m              â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ [3m   200  [0m[3m        sampler = [0m                      â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ [3msampler_pb2_grpc.SamplerStub(sampler_channel)[0m   â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ [3m   201  [0m                                        â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ [3m   202  [0m[3m        request_end_time = [0m             â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ [3mtime.perf_counter()[0m                             â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ [3m   203  [0m[3m        sample_request = [0m               â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ [3msampler_pb2.SampleTextRequest([0m                  â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ [3m   204  [0m[3m            prompt=prompt,[0m              â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ [3m   205  [0m[3m            [0m                            â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ [3msettings=sampler_pb2.SampleSettings([0m            â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ [3m   206  [0m[3m                max_len=output_len,[0m     â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ [3m   207  [0m[3m                rng_seed=0,[0m             â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ [3m   208  [0m[3m                temperature=0,[0m          â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ [3m   209  [0m[3m                nucleus_p=1,[0m            â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ [3m   210  [0m[3m            ),[0m                          â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ [3m   211  [0m[3m        )[0m                               â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ [3m   212  [0m[3m        stream = [0m                       â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ [3msampler.SampleText(sample_request)[0m              â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ [3m   213  [0m[3m        response = "".join([0m[3m)[0m            â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ [3m   214  [0m                                        â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ [3m   215  [0m[3m    request_end_time = [0m                 â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ [3mtime.perf_counter()[0m                             â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ [3m   216  [0m[3m    request_latency = request_end_time [0m â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ [3m- request_start_time[0m                            â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ [3m   217  [0m[3m    REQUEST_LATENCY.append((prompt_len,[0m â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ [3moutput_len, request_latency))[0m                   â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ [3m   218  [0m                                        â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ [3m   219  [0m                                        â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ [3m   220  [0m[3masync def benchmark([0m                    â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ [3m   221  [0m[3m    backend: str,[0m                       â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ [3m   222  [0m[3m    api_url: str,[0m                       â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ [3m   223  [0m[3m    input_requests: List[Tuple[0m[3m],[0m        â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ [3m   224  [0m[3m    best_of: int,[0m                       â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ [3m   225  [0m[3m    use_beam_search: bool,[0m              â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ [3m   226  [0m[3m    request_rate: float,[0m                â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ [3m   227  [0m[3m) -> None:[0m                              â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ [3m   228  [0m[3m    tasks: List[0m[3m = [][0m                    â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ [3m   229  [0m[3m    async for request in [0m               â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ [3mget_request(input_requests, request_rate):[0m      â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ [3m   230  [0m[3m        prompt, prompt_len, output_len [0m â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ [3m= request[0m                                       â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ [3m   231  [0m[3m        task = asyncio.create_task([0m     â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ [3m   232  [0m[3m            send_request([0m               â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ [3m   233  [0m[3m                backend,[0m                â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ [3m   234  [0m[3m                api_url,[0m                â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ [3m   235  [0m[3m                prompt,[0m                 â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ [3m   236  [0m[3m                prompt_len,[0m             â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ [3m   237  [0m[3m                output_len,[0m             â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ [3m   238  [0m[3m                best_of,[0m                â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ [3m   239  [0m[3m                use_beam_search,[0m        â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ [3m   240  [0m[3m            )[0m                           â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ [3m   241  [0m[3m        )[0m                               â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ [3m   242  [0m[3m        tasks.append(task)[0m              â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ [3m   243  [0m[3m    await tqdm_asyncio.gather(*tasks)[0m   â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ [3m   244  [0m                                        â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ [3m   245  [0m                                        â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ [3m   246  [0m[3mdef main(args: argparse.Namespace):[0m     â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ [3m   247  [0m[3m    print(args)[0m                         â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ [3m   248  [0m[3m    random.seed(args.seed)[0m              â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ [3m   249  [0m[3m    np.random.seed(args.seed)[0m           â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ [3m   250  [0m                                        â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ [3m   251  [0m[3m    api_url = [0m                          â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ [3mf"http://{args.host}:{args.port}/generate"[0m      â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ [3m   252  [0m[3m    if args.tokenizer.endswith(".json")[0m â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ [3mor args.tokenizer.endswith(".model"):[0m           â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ [3m   253  [0m[3m        from [0m                           â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ [3msglang.srt.hf_transformers_utils import [0m        â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ [3mget_tokenizer[0m                                   â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ [3m   254  [0m                                        â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ [3m   255  [0m[3m        tokenizer = [0m                    â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ [3mget_tokenizer(args.tokenizer)[0m                   â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ [3m   256  [0m[3m    else:[0m                               â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ [3m   257  [0m[3m        tokenizer = [0m                    â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ [3mAutoTokenizer.from_pretrained([0m                  â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ [3m   258  [0m[3m            args.tokenizer, [0m            â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ [3mtrust_remote_code=args.trust_remote_code[0m        â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ [3m   259  [0m[3m        )[0m                               â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ [3m   260  [0m                                        â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ [3m   261  [0m[3m    if args.dataset:[0m                    â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ [3m   262  [0m[3m        input_requests = [0m               â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ [3msample_requests(args.dataset, args.num_prompts,[0m â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ [3mtokenizer)[0m                                      â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ [3m   263  [0m[3m    else:[0m                               â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ [3m   264  [0m[3m        input_lens = np.random.randint([0m â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ [3m   265  [0m[3m            int(args.input_len * [0m       â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ [3margs.range_ratio),[0m                              â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ [3m   266  [0m[3m            args.input_len + 1,[0m         â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ [3m   267  [0m[3m            size=args.num_prompts,[0m      â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ [3m   268  [0m[3m        )[0m                               â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ [3m   269  [0m[3m        output_lens = [0m                  â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ [3mnp.random.randint([0m                              â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ [3m   270  [0m[3m            int(args.output_len * [0m      â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ [3margs.range_ratio),[0m                              â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ [3m   271  [0m[3m            args.output_len + 1,[0m        â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ [3m   272  [0m[3m            size=args.num_prompts,[0m      â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ [3m   273  [0m[3m        )[0m                               â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ [3m   274  [0m[3m        offsets = np.random.randint(0, [0m â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ [3mtokenizer.vocab_size, size=args.num_prompts)[0m    â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ [3m   275  [0m[3m        input_requests = [][0m             â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ [3m   276  [0m[3m        for i in [0m                       â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ [3mrange(args.num_prompts):[0m                        â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ [3m   277  [0m[3m            prompt = tokenizer.decode([0m  â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ [3m   278  [0m[3m                [[0m                       â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ [3m   279  [0m[3m                    (offsets[0m[3m + i + j) %[0m â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ [3m(tokenizer.vocab_size - 129) + 128[0m              â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ [3m   280  [0m[3m                    for j in [0m           â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ [3mrange(input_lens[0m[3m)[0m                               â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ [3m   281  [0m[3m                ][0m                       â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ [3m   282  [0m[3m            )[0m                           â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ [3m   283  [0m[3m            [0m                            â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ [3minput_requests.append((prompt, int(input_lens[0m[3m),[0m â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ [3mint(output_lens[0m[3m)))[0m                              â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ [3m   284  [0m                                        â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ [3m   285  [0m[3m    benchmark_start_time = [0m             â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ [3mtime.perf_counter()[0m                             â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ [3m   286  [0m[3m    asyncio.run([0m                        â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ [3m   287  [0m[3m        benchmark([0m                      â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ [3m   288  [0m[3m            args.backend,[0m               â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ [3m   289  [0m[3m            api_url,[0m                    â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ [3m   290  [0m[3m            input_requests,[0m             â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ [3m   291  [0m[3m            args.best_of,[0m               â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ [3m   292  [0m[3m            args.use_beam_search,[0m       â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ [3m   293  [0m[3m            args.request_rate,[0m          â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ [3m   294  [0m[3m        )[0m                               â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ [3m   295  [0m[3m    )[0m                                   â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ [3m   296  [0m[3m    benchmark_end_time = [0m               â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ [3mtime.perf_counter()[0m                             â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ [3m   297  [0m[3m    benchmark_time = benchmark_end_time[0m â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ [3m- benchmark_start_time[0m                          â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ [3m   298  [0m                                        â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ [3m   299  [0m[3m    # Compute the statistics.[0m           â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ [3m   300  [0m[3m    latencies = [0m                        â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ [3m   301  [0m[3m    avg_latency = np.mean(latencies)[0m    â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ [3m   302  [0m[3m    avg_per_token_latency = np.mean([0m    â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ [3m   303  [0m[3m        [[0m                               â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ [3m   304  [0m[3m            latency / (prompt_len + [0m    â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ [3moutput_len)[0m                                     â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ [3m   305  [0m[3m            for prompt_len, output_len,[0m â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ [3mlatency in REQUEST_LATENCY[0m                      â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ [3m   306  [0m[3m        ][0m                               â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ [3m   307  [0m[3m    )[0m                                   â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ [3m   308  [0m[3m    avg_per_output_token_latency = [0m     â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ [3mnp.mean([0m                                        â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ [3m   309  [0m[3m        [0m                                â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ [3m   310  [0m[3m    )[0m                                   â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ [3m   311  [0m[3m    decoding_throughput = ([0m             â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ [3m   312  [0m[3m        np.sum([0m[3m) / benchmark_time[0m       â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ [3m   313  [0m[3m    )[0m                                   â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ [3m   314  [0m                                        â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ [3m   315  [0m[3m    print(f"Total time: [0m                â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ [3m{benchmark_time:.2f} s")[0m                        â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ [3m   316  [0m[3m    print(f"Request throughput: [0m        â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ [3m{args.num_prompts / benchmark_time:.2f} [0m        â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ [3mrequests/s")[0m                                    â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ [3m   317  [0m[3m    print(f"Decoding throughput: [0m       â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ [3m{decoding_throughput:.2f} token/s")[0m             â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ [3m   318  [0m[3m    print(f"Average latency: [0m           â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ [3m{avg_latency:.2f} s")[0m                           â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ [3m   319  [0m[3m    print(f"Average latency per token: [0m â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ [3m{avg_per_token_latency:.2f} s")[0m                 â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ [3m   320  [0m[3m    print(f"Average latency per output [0m â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ [3mtoken: {avg_per_output_token_latency:.2f} s")[0m   â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ [3m   321  [0m                                        â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ [3m   322  [0m                                        â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ [3m   323  [0m[3mif __name__ == "__main__":[0m              â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ [3m   324  [0m[3m    parser = argparse.ArgumentParser([0m   â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ [3m   325  [0m[3m        description="Benchmark the [0m     â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ [3monline serving throughput."[0m                     â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ [3m   326  [0m[3m    )[0m                                   â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ [3m   327  [0m[3m    parser.add_argument([0m                â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ [3m   328  [0m[3m        "--backend",[0m                    â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ [3m   329  [0m[3m        type=str,[0m                       â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ [3m   330  [0m[3m        default="srt",[0m                  â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ [3m   331  [0m[3m        choices=["vllm", "tgi", "srt", [0m â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ [3m"lightllm", "ginfer"],[0m                          â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ [3m   332  [0m[3m    )[0m                                   â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ [3m   333  [0m[3m    parser.add_argument("--host", [0m      â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ [3mtype=str, default="localhost")[0m                  â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ [3m   334  [0m[3m    parser.add_argument("--port", [0m      â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ [3mtype=int, default=30000)[0m                        â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ [3m   335  [0m[3m    parser.add_argument("--dataset", [0m   â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ [3mtype=str, help="Path to the dataset.")[0m          â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ [3m   336  [0m[3m    parser.add_argument("--input-len", [0m â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ [3mtype=int, default=2048)[0m                         â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ [3m   337  [0m[3m    parser.add_argument("--output-len",[0m â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ [3mtype=int, default=256)[0m                          â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ [3m   338  [0m[3m    [0m                                    â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ [3mparser.add_argument("--range-ratio", [0m           â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ [3mtype=float, default=1.0)[0m                        â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ [3m   339  [0m[3m    parser.add_argument([0m                â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ [3m   340  [0m[3m        "--tokenizer",[0m                  â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ [3m   341  [0m[3m        type=str,[0m                       â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ [3m   342  [0m[3m        [0m                                â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ [3mdefault="NousResearch/Meta-Llama-3-8B",[0m         â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ [3m   343  [0m[3m        help="Name or path of the [0m      â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ [3mtokenizer.",[0m                                    â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ [3m   344  [0m[3m    )[0m                                   â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ [3m   345  [0m[3m    parser.add_argument([0m                â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ [3m   346  [0m[3m        "--best-of",[0m                    â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ [3m   347  [0m[3m        type=int,[0m                       â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ [3m   348  [0m[3m        default=1,[0m                      â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ [3m   349  [0m[3m        help="Generates `best_of` [0m      â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ [3msequences per prompt and " "returns the best [0m   â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ [3mone.",[0m                                          â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ [3m   350  [0m[3m    )[0m                                   â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ [3m   351  [0m[3m    [0m                                    â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ [3mparser.add_argument("--use-beam-search", [0m       â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ [3maction="store_true")[0m                            â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ [3m   352  [0m[3m    parser.add_argument([0m                â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ [3m   353  [0m[3m        "--num-prompts", type=int, [0m     â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ [3mdefault=1000, help="Number of prompts to [0m       â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ [3mprocess."[0m                                       â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ [3m   354  [0m[3m    )[0m                                   â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ [3m   355  [0m[3m    parser.add_argument([0m                â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ [3m   356  [0m[3m        "--request-rate",[0m               â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ [3m   357  [0m[3m        type=float,[0m                     â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ [3m   358  [0m[3m        default=float("inf"),[0m           â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ [3m   359  [0m[3m        help="Number of requests per [0m   â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ [3msecond. If this is inf, "[0m                       â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ [3m   360  [0m[3m        "then all the requests are sent[0m â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ [3mat time 0. "[0m                                    â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ [3m   361  [0m[3m        "Otherwise, we use Poisson [0m     â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ [3mprocess to synthesize "[0m                         â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ [3m   362  [0m[3m        "the request arrival times.",[0m   â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ [3m   363  [0m[3m    )[0m                                   â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ [3m   364  [0m[3m    parser.add_argument("--seed", [0m      â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ [3mtype=int, default=0)[0m                            â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ [3m   365  [0m[3m    parser.add_argument([0m                â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ [3m   366  [0m[3m        "--trust-remote-code",[0m          â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ [3m   367  [0m[3m        action="store_true",[0m            â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ [3m   368  [0m[3m        help="trust remote code from [0m   â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ [3mhuggingface",[0m                                   â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ [3m   369  [0m[3m    )[0m                                   â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ [3m   370  [0m[3m    args = parser.parse_args()[0m          â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ [3m   371  [0m[3m    main(args)[0m                          â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ [3m   372  [0m                                        â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                                                 â”‚   â”‚
â”‚             â”‚ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚
â”‚ Total       â”‚ Input: 41264 Output: 1579                                                                              â”‚
â”‚ Tokens      â”‚                                                                                                        â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Status      â”‚ âœ… Step 5: Completed                                                                                   â”‚
â”‚ Tools       â”‚ ðŸ”§ bash                                                                                                â”‚
â”‚ bash        â”‚ â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚
â”‚             â”‚ â”‚ {'command': "ls -la                            â”‚ total 72                                        â”‚   â”‚
â”‚             â”‚ â”‚ /home/ubuntu/OmniPerf-Bench/perf-agents-benchâ€¦ â”‚ drwxrwxr-x 6 ubuntu ubuntu  4096 Nov 15 01:30 . â”‚   â”‚
â”‚             â”‚ â”‚ | sed -n '1,120p'", 'restart': False}          â”‚ drwxrwxr-x 3 ubuntu ubuntu  4096 Nov 15 01:30   â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ ..                                              â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ -rw-rw-r-- 1 ubuntu ubuntu   405 Nov 15 01:30   â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ README.md                                       â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ -rw-rw-r-- 1 ubuntu ubuntu  1116 Nov 15 01:30   â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ __init__.py                                     â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ -rw-rw-r-- 1 ubuntu ubuntu  5560 Nov 15 01:30   â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ api.py                                          â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ drwxrwxr-x 2 ubuntu ubuntu  4096 Nov 15 01:30   â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ backend                                         â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ -rw-rw-r-- 1 ubuntu ubuntu 10409 Nov 15 01:30   â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ bench_latency.py                                â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ -rw-rw-r-- 1 ubuntu ubuntu  1711 Nov 15 01:30   â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ global_config.py                                â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ drwxrwxr-x 2 ubuntu ubuntu  4096 Nov 15 01:30   â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ lang                                            â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ -rw-rw-r-- 1 ubuntu ubuntu   364 Nov 15 01:30   â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ launch_server.py                                â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ -rw-rw-r-- 1 ubuntu ubuntu  1115 Nov 15 01:30   â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ launch_server_llavavid.py                       â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ drwxrwxr-x 6 ubuntu ubuntu  4096 Nov 15 01:30   â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ srt                                             â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ drwxrwxr-x 2 ubuntu ubuntu  4096 Nov 15 01:30   â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ test                                            â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ -rw-rw-r-- 1 ubuntu ubuntu  8168 Nov 15 01:30   â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ utils.py                                        â”‚   â”‚
â”‚             â”‚ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚
â”‚ Total       â”‚ Input: 57472 Output: 1655                                                                              â”‚
â”‚ Tokens      â”‚                                                                                                        â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Status      â”‚ âœ… Step 6: Completed                                                                                   â”‚
â”‚ Tools       â”‚ ðŸ”§ bash                                                                                                â”‚
â”‚ bash        â”‚ â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚
â”‚             â”‚ â”‚ {'command': "ls -la                            â”‚ total 132                                       â”‚   â”‚
â”‚             â”‚ â”‚ /home/ubuntu/OmniPerf-Bench/perf-agents-benchâ€¦ â”‚ drwxrwxr-x 2 ubuntu ubuntu  4096 Nov 15 01:30 . â”‚   â”‚
â”‚             â”‚ â”‚ | sed -n '1,200p'", 'restart': False}          â”‚ drwxrwxr-x 3 ubuntu ubuntu  4096 Nov 15 01:30   â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ ..                                              â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ -rw-rw-r-- 1 ubuntu ubuntu  7011 Nov 15 01:30   â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ cuda_graph_runner.py                            â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ -rw-rw-r-- 1 ubuntu ubuntu  3675 Nov 15 01:30   â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ dp_worker.py                                    â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ -rw-rw-r-- 1 ubuntu ubuntu 33066 Nov 15 01:30   â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ infer_batch.py                                  â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ -rw-rw-r-- 1 ubuntu ubuntu  6630 Nov 15 01:30   â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ manager_multi.py                                â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ -rw-rw-r-- 1 ubuntu ubuntu  3469 Nov 15 01:30   â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ manager_single.py                               â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ -rw-rw-r-- 1 ubuntu ubuntu 13334 Nov 15 01:30   â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ model_runner.py                                 â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ -rw-rw-r-- 1 ubuntu ubuntu  8210 Nov 15 01:30   â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ radix_cache.py                                  â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ -rw-rw-r-- 1 ubuntu ubuntu  2482 Nov 15 01:30   â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ schedule_heuristic.py                           â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ -rw-rw-r-- 1 ubuntu ubuntu 31841 Nov 15 01:30   â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ tp_worker.py                                    â”‚   â”‚
â”‚             â”‚ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚
â”‚ Total       â”‚ Input: 74116 Output: 1734                                                                              â”‚
â”‚ Tokens      â”‚                                                                                                        â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Status      â”‚ âœ… Step 7: Completed                                                                                   â”‚
â”‚ Tools       â”‚ ðŸ”§ bash                                                                                                â”‚
â”‚ bash        â”‚ â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚
â”‚             â”‚ â”‚ {'command': "sed -n '1,200p'                   â”‚ """                                             â”‚   â”‚
â”‚             â”‚ â”‚ /home/ubuntu/OmniPerf-Bench/perf-agents-benchâ€¦ â”‚ The entry point of inference server.            â”‚   â”‚
â”‚             â”‚ â”‚ 'restart': False}                              â”‚ SRT = SGLang Runtime.                           â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ """                                             â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                                                 â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ import asyncio                                  â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ import dataclasses                              â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ import json                                     â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ import logging                                  â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ import multiprocessing as mp                    â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ import os                                       â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ import sys                                      â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ import threading                                â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ import time                                     â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ from http import HTTPStatus                     â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ from typing import Dict, Optional               â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                                                 â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ # Fix a bug of Python threading                 â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ setattr(threading, "_register_atexit", lambda   â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ *args, **kwargs: None)                          â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                                                 â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ import aiohttp                                  â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ import psutil                                   â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ import requests                                 â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ import uvicorn                                  â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ import uvloop                                   â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ from fastapi import FastAPI, Request            â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ from fastapi.responses import JSONResponse,     â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ Response, StreamingResponse                     â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                                                 â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ from sglang.backend.runtime_endpoint import     â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ RuntimeEndpoint                                 â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ from sglang.srt.constrained import              â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ disable_cache                                   â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ from sglang.srt.hf_transformers_utils import    â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ get_tokenizer                                   â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ from                                            â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ sglang.srt.managers.controller.manager_multi    â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ import (                                        â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚     start_controller_process as                 â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ start_controller_process_multi,                 â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ )                                               â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ from                                            â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ sglang.srt.managers.controller.manager_single   â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ import (                                        â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚     start_controller_process as                 â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ start_controller_process_single,                â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ )                                               â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ from sglang.srt.managers.controller.tp_worker   â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ import ModelTpService                           â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ from sglang.srt.managers.detokenizer_manager    â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ import start_detokenizer_process                â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ from sglang.srt.managers.io_struct import       â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ GenerateReqInput                                â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ from sglang.srt.managers.tokenizer_manager      â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ import TokenizerManager                         â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ from sglang.srt.openai_api_adapter import (     â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚     load_chat_template_for_openai_api,          â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚     v1_chat_completions,                        â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚     v1_completions,                             â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ )                                               â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ from sglang.srt.server_args import              â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ ModelPortArgs, PortArgs, ServerArgs             â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ from sglang.srt.utils import (                  â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚     API_KEY_HEADER_NAME,                        â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚     APIKeyValidatorMiddleware,                  â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚     allocate_init_ports,                        â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚     assert_pkg_version,                         â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚     enable_show_time_cost,                      â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚     receive_addrs,                              â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚     send_addrs_to_rank_0,                       â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚     start_rpyc_service_process,                 â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ )                                               â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ from sglang.utils import                        â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ get_exception_traceback                         â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                                                 â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ logger = logging.getLogger(__name__)            â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                                                 â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ asyncio.set_event_loop_policy(uvloop.EventLoopâ€¦ â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                                                 â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                                                 â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ app = FastAPI()                                 â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ tokenizer_manager = None                        â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                                                 â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                                                 â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ @app.get("/health")                             â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ async def health() -> Response:                 â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚     """Health check."""                         â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚     return Response(status_code=200)            â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                                                 â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                                                 â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ @app.get("/get_model_info")                     â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ async def get_model_info():                     â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚     result = {                                  â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚         "model_path":                           â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ tokenizer_manager.model_path,                   â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚     }                                           â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚     return result                               â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                                                 â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                                                 â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ @app.get("/get_server_args")                    â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ async def get_server_args():                    â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚     return                                      â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ dataclasses.asdict(tokenizer_manager.server_arâ€¦ â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                                                 â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                                                 â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ @app.get("/flush_cache")                        â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ async def flush_cache():                        â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚     tokenizer_manager.flush_cache()             â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚     return Response(                            â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚         content="Cache flushed.\nPlease check   â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ backend logs for more details. "                â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚         "(When there are running or waiting     â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ requests, the operation will not be             â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ performed.)\n",                                 â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚         status_code=200,                        â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚     )                                           â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                                                 â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                                                 â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ async def generate_request(obj:                 â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ GenerateReqInput, request: Request):            â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚     if obj.stream:                              â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                                                 â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚         async def stream_results():             â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚             try:                                â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                 async for out in                â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ tokenizer_manager.generate_request(obj,         â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ request):                                       â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                     yield f"data:               â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ {json.dumps(out, ensure_ascii=False)}\n\n"      â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚             except ValueError as e:             â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                 out = {"error": {"message":     â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ str(e)}}                                        â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                 yield f"data: {json.dumps(out,  â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ ensure_ascii=False)}\n\n"                       â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚             yield "data: [DONE]\n\n"            â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                                                 â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚         return StreamingResponse(               â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚             stream_results(),                   â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚             media_type="text/event-stream",     â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚             background=tokenizer_manager.creatâ€¦ â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚         )                                       â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚     else:                                       â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚         try:                                    â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚             ret = await                         â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ tokenizer_manager.generate_request(obj,         â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ request).__anext__()                            â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚             return ret                          â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚         except ValueError as e:                 â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚             return JSONResponse(                â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                 {"error": {"message": str(e)}}, â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ status_code=HTTPStatus.BAD_REQUEST              â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚             )                                   â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                                                 â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                                                 â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ app.post("/generate")(generate_request)         â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ app.put("/generate")(generate_request)          â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                                                 â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                                                 â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ @app.post("/v1/completions")                    â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ async def openai_v1_completions(raw_request:    â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ Request):                                       â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚     return await                                â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ v1_completions(tokenizer_manager, raw_request)  â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                                                 â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                                                 â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ @app.post("/v1/chat/completions")               â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ async def                                       â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ openai_v1_chat_completions(raw_request:         â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ Request):                                       â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚     return await                                â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ v1_chat_completions(tokenizer_manager,          â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ raw_request)                                    â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                                                 â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                                                 â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ def launch_server(server_args: ServerArgs,      â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ pipe_finish_writer, model_overide_args=None):   â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚     global tokenizer_manager                    â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                                                 â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚     logging.basicConfig(                        â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚         level=getattr(logging,                  â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ server_args.log_level.upper()),                 â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚         format="%(message)s",                   â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚     )                                           â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                                                 â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚     # Set global environments                   â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚     os.environ["TF_CPP_MIN_LOG_LEVEL"] = "3"    â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚     os.environ["NCCL_CUMEM_ENABLE"] = "0"       â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚     if server_args.show_time_cost:              â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚         enable_show_time_cost()                 â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚     if server_args.disable_disk_cache:          â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚         disable_cache()                         â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚     if not server_args.disable_flashinfer:      â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚         assert_pkg_version(                     â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚             "flashinfer",                       â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚             "0.0.8",                            â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚             "Please uninstall the old version   â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ and "                                           â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚             "reinstall the latest version by    â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ following the instructions "                    â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚             "at                                 â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ https://docs.flashinfer.ai/installation.html.", â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚         )                                       â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚     if server_args.chat_template:               â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚         # TODO: replace this with huggingface   â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ transformers template                           â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚         load_chat_template_for_openai_api(servâ€¦ â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                                                 â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚     # Allocate ports                            â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚     assert server_args.tp_size %                â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ server_args.nnodes == 0                         â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚     tp_size_local = server_args.tp_size //      â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ server_args.nnodes                              â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚     server_args.port,                           â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ server_args.additional_ports =                  â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ allocate_init_ports(                            â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚         server_args.port,                       â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚         server_args.additional_ports,           â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚         tp_size_local,                          â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚         server_args.dp_size,                    â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚     )                                           â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                                                 â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚     ports = server_args.additional_ports        â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚     model_port_args = []                        â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚     for i in range(server_args.dp_size):        â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚         model_port_args.append(                 â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚             ModelPortArgs(                      â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                 nccl_port=ports[3 + i *         â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ (tp_size_local + 1)],                           â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                 model_tp_ips=[None] *           â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ tp_size_local,                                  â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                 model_tp_ports=ports[           â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                     3 + i * (tp_size_local + 1) â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ + 1 : 3 + (i + 1) * (tp_size_local + 1)         â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                 ],                              â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚             )                                   â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚         )                                       â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚     port_args = PortArgs(                       â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚         tokenizer_port=ports[0],                â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚         router_port=ports[1],                   â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚         detokenizer_port=ports[2],              â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚         model_port_args=model_port_args,        â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚     )                                           â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                                                 â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚     # TODO multi-node dp is not supported       â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚     assert not (server_args.dp_size > 1 and     â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ server_args.node_rank is not None)              â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚     if server_args.nnodes > 1:                  â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚         if server_args.node_rank != 0:          â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚             send_addrs_to_rank_0(model_port_arâ€¦ â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ server_args)                                    â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚         else:                                   â”‚   â”‚
â”‚             â”‚ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚
â”‚ Total       â”‚ Input: 91122 Output: 1808                                                                              â”‚
â”‚ Tokens      â”‚                                                                                                        â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Status      â”‚ âœ… Step 8: Completed                                                                                   â”‚
â”‚ Tools       â”‚ ðŸ”§ bash                                                                                                â”‚
â”‚ bash        â”‚ â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚
â”‚             â”‚ â”‚ {'command': "sed -n '200,520p'                 â”‚         else:                                   â”‚   â”‚
â”‚             â”‚ â”‚ /home/ubuntu/OmniPerf-Bench/perf-agents-benchâ€¦ â”‚             receive_addrs(model_port_args[0],   â”‚   â”‚
â”‚             â”‚ â”‚ 'restart': False}                              â”‚ server_args)                                    â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚         for i in range(tp_size_local):          â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚             start_rpyc_service_process(         â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                 ModelTpService,                 â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ model_port_args[0].model_tp_ports               â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ [3m            )[0m                                   â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ [3m        if server_args.node_rank != 0:[0m          â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ [3m            logger.info([0m                        â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ [3m                f"[0m[3m: Listen for connections..."[0m  â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ [3m            )[0m                                   â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ [3m            while True:[0m                         â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ [3m                pass[0m                            â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                                                 â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ [3m    # Launch processes[0m                          â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ [3m    tokenizer_manager = [0m                        â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ [3mTokenizerManager(server_args, port_args, [0m       â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ [3mmodel_overide_args)[0m                             â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ [3m    pipe_router_reader, pipe_router_writer = [0m   â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ [3mmp.Pipe(duplex=False)[0m                           â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ [3m    pipe_detoken_reader, pipe_detoken_writer = [0m â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ [3mmp.Pipe(duplex=False)[0m                           â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                                                 â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ [3m    if server_args.dp_size == 1:[0m                â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ [3m        start_process = [0m                        â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ [3mstart_controller_process_single[0m                 â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ [3m    else:[0m                                       â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ [3m        start_process = [0m                        â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ [3mstart_controller_process_multi[0m                  â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ [3m    proc_router = mp.Process([0m                   â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ [3m        target=start_process,[0m                   â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ [3m        args=(server_args, port_args, [0m          â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ [3mpipe_router_writer, model_overide_args),[0m        â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ [3m    )[0m                                           â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ [3m    proc_router.start()[0m                         â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ [3m    proc_detoken = mp.Process([0m                  â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ [3m        target=start_detokenizer_process,[0m       â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ [3m        args=([0m                                  â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ [3m            server_args,[0m                        â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ [3m            port_args,[0m                          â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ [3m            pipe_detoken_writer,[0m                â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ [3m        ),[0m                                      â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ [3m    )[0m                                           â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ [3m    proc_detoken.start()[0m                        â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                                                 â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ [3m    # Wait for the model to finish loading[0m      â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ [3m    router_init_state = [0m                        â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ [3mpipe_router_reader.recv()[0m                       â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ [3m    detoken_init_state = [0m                       â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ [3mpipe_detoken_reader.recv()[0m                      â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                                                 â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ [3m    if router_init_state != "init ok" or [0m       â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ [3mdetoken_init_state != "init ok":[0m                â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ [3m        proc_router.kill()[0m                      â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ [3m        proc_detoken.kill()[0m                     â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ [3m        print([0m                                  â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ [3m            f"Initialization failed. [0m           â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ [3mrouter_init_state: {router_init_state}", [0m       â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ [3mflush=True[0m                                      â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ [3m        )[0m                                       â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ [3m        print([0m                                  â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ [3m            f"Initialization failed. [0m           â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ [3mdetoken_init_state: {detoken_init_state}",[0m      â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ [3m            flush=True,[0m                         â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ [3m        )[0m                                       â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ [3m        sys.exit(1)[0m                             â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ [3m    assert proc_router.is_alive() and [0m          â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ [3mproc_detoken.is_alive()[0m                         â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                                                 â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ [3m    if server_args.api_key and [0m                 â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ [3mserver_args.api_key != "":[0m                      â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ [3m        app.add_middleware(APIKeyValidatorMiddâ€¦[0m â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ [3mapi_key=server_args.api_key)[0m                    â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                                                 â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ [3m    # Send a warmup request[0m                     â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ [3m    def _wait_and_warmup():[0m                     â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ [3m        headers = {}[0m                            â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ [3m        url = server_args.url()[0m                 â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ [3m        if server_args.api_key:[0m                 â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ [3m            headers[API_KEY_HEADER_NAME] = [0m     â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ [3mserver_args.api_key[0m                             â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                                                 â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ [3m        # Wait until the server is launched[0m     â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ [3m        for _ in range(120):[0m                    â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ [3m            time.sleep(0.5)[0m                     â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ [3m            try:[0m                                â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ [3m                requests.get(url + [0m             â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ [3m"/get_model_info", timeout=5, headers=headers)[0m  â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ [3m                break[0m                           â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ [3m            except [0m                             â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ [3mrequests.exceptions.RequestException:[0m           â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ [3m                pass[0m                            â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                                                 â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ [3m        # Send a warmup request[0m                 â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ [3m        try:[0m                                    â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ [3m            for _ in [0m                           â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ [3mrange(server_args.dp_size):[0m                     â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ [3m                res = requests.post([0m            â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ [3m                    url + "/generate",[0m          â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ [3m                    json={[0m                      â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ [3m                        "text": "The capital [0m   â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ [3mcity of France is",[0m                             â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ [3m                        "sampling_params": {[0m    â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ [3m                            "temperature": 0,[0m   â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ [3m                            "max_new_tokens": [0m  â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ [3m8,[0m                                              â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ [3m                        },[0m                      â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ [3m                    },[0m                          â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ [3m                    headers=headers,[0m            â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ [3m                    timeout=600,[0m                â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ [3m                )[0m                               â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ [3m                assert res.status_code == 200[0m   â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ [3m        except Exception as e:[0m                  â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ [3m            if pipe_finish_writer is not None:[0m  â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ [3m                pipe_finish_writer.send(get_exâ€¦[0m â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ [3m            print(f"Initialization failed. [0m     â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ [3mwarmup error: {e}", flush=True)[0m                 â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ [3m            raise e[0m                             â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                                                 â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ [3m        logger.info("The server is fired up and[0m â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ [3mready to roll!")[0m                                â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ [3m        if pipe_finish_writer is not None:[0m      â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ [3m            pipe_finish_writer.send("init ok")[0m  â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                                                 â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ [3m    t = [0m                                        â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ [3mthreading.Thread(target=_wait_and_warmup)[0m       â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ [3m    t.start()[0m                                   â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                                                 â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ [3m    # Listen for requests[0m                       â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ [3m    try:[0m                                        â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ [3m        uvicorn.run([0m                            â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ [3m            app,[0m                                â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ [3m            host=server_args.host,[0m              â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ [3m            port=server_args.port,[0m              â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ [3m            log_level=server_args.log_level_htâ€¦[0m â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ [3mor server_args.log_level,[0m                       â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ [3m            timeout_keep_alive=5,[0m               â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ [3m            loop="uvloop",[0m                      â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ [3m        )[0m                                       â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ [3m    finally:[0m                                    â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ [3m        t.join()[0m                                â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                                                 â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                                                 â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ [3mclass Runtime:[0m                                  â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ [3m    """[0m                                         â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ [3m    A wrapper for the server.[0m                   â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ [3m    This is used for launching the server in a [0m â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ [3mpython program without[0m                          â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ [3m    using the commond line interface.[0m           â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ [3m    """[0m                                         â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                                                 â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ [3m    def __init__([0m                               â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ [3m        self,[0m                                   â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ [3m        log_level: str = "error",[0m               â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ [3m        model_overide_args: Optional[0m[3m = None,[0m    â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ [3m        *args,[0m                                  â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ [3m        **kwargs,[0m                               â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ [3m    ):[0m                                          â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ [3m        """See the arguments in [0m                â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ [3mserver_args.py::ServerArgs"""[0m                   â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ [3m        self.server_args = ServerArgs(*args, [0m   â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ [3mlog_level=log_level, **kwargs)[0m                  â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                                                 â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ [3m        # Pre-allocate ports[0m                    â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ [3m        self.server_args.port, [0m                 â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ [3mself.server_args.additional_ports = [0m            â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ [3mallocate_init_ports([0m                            â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ [3m            self.server_args.port,[0m              â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ [3m            self.server_args.additional_ports,[0m  â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ [3m            self.server_args.tp_size,[0m           â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ [3m            self.server_args.dp_size,[0m           â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ [3m        )[0m                                       â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                                                 â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ [3m        self.url = self.server_args.url()[0m       â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ [3m        self.generate_url = ([0m                   â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ [3m            f"http://{self.server_args.host}:{â€¦[0m â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ [3m        )[0m                                       â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                                                 â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ [3m        self.pid = None[0m                         â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ [3m        pipe_reader, pipe_writer = [0m             â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ [3mmp.Pipe(duplex=False)[0m                           â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ [3m        proc = mp.Process([0m                      â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ [3m            target=launch_server,[0m               â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ [3m            args=(self.server_args, [0m            â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ [3mpipe_writer, model_overide_args),[0m               â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ [3m        )[0m                                       â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ [3m        proc.start()[0m                            â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ [3m        pipe_writer.close()[0m                     â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ [3m        self.pid = proc.pid[0m                     â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                                                 â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ [3m        try:[0m                                    â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ [3m            init_state = pipe_reader.recv()[0m     â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ [3m        except EOFError:[0m                        â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ [3m            init_state = ""[0m                     â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                                                 â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ [3m        if init_state != "init ok":[0m             â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ [3m            self.shutdown()[0m                     â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ [3m            raise RuntimeError([0m                 â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ [3m                "Initialization failed. Please [0m â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ [3msee the error messages above."[0m                  â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ [3m            )[0m                                   â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                                                 â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ [3m        self.endpoint = [0m                        â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ [3mRuntimeEndpoint(self.url)[0m                       â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                                                 â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ [3m    def shutdown(self):[0m                         â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ [3m        if self.pid is not None:[0m                â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ [3m            try:[0m                                â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ [3m                parent = [0m                       â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ [3mpsutil.Process(self.pid)[0m                        â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ [3m            except psutil.NoSuchProcess:[0m        â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ [3m                return[0m                          â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ [3m            children = [0m                         â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ [3mparent.children(recursive=True)[0m                 â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ [3m            for child in children:[0m              â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ [3m                child.kill()[0m                    â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ [3m            psutil.wait_procs(children, [0m        â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ [3mtimeout=5)[0m                                      â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ [3m            parent.kill()[0m                       â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ [3m            parent.wait(timeout=5)[0m              â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ [3m            self.pid = None[0m                     â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                                                 â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ [3m    def get_tokenizer(self):[0m                    â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ [3m        return get_tokenizer([0m                   â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ [3m            self.server_args.tokenizer_path,[0m    â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ [3m            tokenizer_mode=self.server_args.toâ€¦[0m â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ [3m            trust_remote_code=self.server_argsâ€¦[0m â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ [3m        )[0m                                       â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                                                 â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ [3m    async def add_request([0m                      â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ [3m        self,[0m                                   â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ [3m        prompt: str,[0m                            â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ [3m        sampling_params: Dict,[0m                  â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ [3m    ):[0m                                          â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ [3m        json_data = {[0m                           â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ [3m            "text": prompt,[0m                     â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ [3m            "sampling_params": sampling_params,[0m â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ [3m            "stream": True,[0m                     â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ [3m        }[0m                                       â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ [3m        pos = 0[0m                                 â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                                                 â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ [3m        timeout = aiohttp.ClientTimeout(total=3[0m â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ [3m* 3600)[0m                                         â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ [3m        async with [0m                             â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ [3maiohttp.ClientSession(timeout=timeout, [0m         â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ [3mtrust_env=True) as session:[0m                     â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ [3m            async with [0m                         â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ [3msession.post(self.generate_url, json=json_data)[0m â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ [3mas response:[0m                                    â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ [3m                async for chunk, _ in [0m          â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ [3mresponse.content.iter_chunks():[0m                 â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ [3m                    chunk = [0m                    â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ [3mchunk.decode("utf-8")[0m                           â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ [3m                    if chunk and [0m               â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ [3mchunk.startswith("data:"):[0m                      â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ [3m                        if chunk == "data: [0m     â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ [3m[DONE]\n\n":[0m                                    â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ [3m                            break[0m               â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ [3m                        data = [0m                 â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ [3mjson.loads(chunk[5:].strip("\n"))[0m               â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ [3m                        cur = data["text"][0m      â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ [3m                        if cur:[0m                 â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ [3m                            yield cur[0m           â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ [3m                        pos += len(cur)[0m         â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                                                 â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ [3m    def __del__(self):[0m                          â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ [3m        self.shutdown()[0m                         â”‚   â”‚
â”‚             â”‚ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚
â”‚ Total       â”‚ Input: 109603 Output: 1882                                                                             â”‚
â”‚ Tokens      â”‚                                                                                                        â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Status      â”‚ âœ… Step 9: Completed                                                                                   â”‚
â”‚ Tools       â”‚ ðŸ”§ bash                                                                                                â”‚
â”‚ bash        â”‚ â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚
â”‚             â”‚ â”‚ {'command': "sed -n '1,220p'                   â”‚ """The arguments of the server."""              â”‚   â”‚
â”‚             â”‚ â”‚ /home/ubuntu/OmniPerf-Bench/perf-agents-benchâ€¦ â”‚                                                 â”‚   â”‚
â”‚             â”‚ â”‚ 'restart': False}                              â”‚ import argparse                                 â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ import dataclasses                              â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ import random                                   â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ from typing import List, Optional, Union        â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                                                 â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                                                 â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ @dataclasses.dataclass                          â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ class ServerArgs:                               â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚     # Model and tokenizer                       â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚     model_path: str                             â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚     tokenizer_path: Optional = None             â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚     tokenizer_mode: str = "auto"                â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚     load_format: str = "auto"                   â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚     dtype: str = "auto"                         â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚     trust_remote_code: bool = True              â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚     context_length: Optional = None             â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚     quantization: Optional = None               â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚     chat_template: Optional = None              â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                                                 â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚     # Port                                      â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚     host: str = "127.0.0.1"                     â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚     port: int = 30000                           â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚     additional_ports: Optional[Union[List,      â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ int]] = None                                    â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                                                 â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚     # Memory and scheduling                     â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚     mem_fraction_static: Optional = None        â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚     max_prefill_tokens: Optional = None         â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚     max_running_requests: Optional = None       â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚     schedule_heuristic: str = "lpm"             â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚     schedule_conservativeness: float = 0.8      â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                                                 â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚     # Other runtime options                     â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚     tp_size: int = 1                            â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚     stream_interval: int = 8                    â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚     random_seed: Optional = None                â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                                                 â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚     # Logging                                   â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚     log_level: str = "info"                     â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚     log_level_http: Optional = None             â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚     log_requests: bool = False                  â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚     show_time_cost: bool = False                â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                                                 â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚     # Other                                     â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚     api_key: str = ""                           â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                                                 â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚     # Data parallelism                          â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚     dp_size: int = 1                            â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚     load_balance_method: str = "round_robin"    â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                                                 â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚     # Optimization/debug options                â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚     disable_flashinfer: bool = False            â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚     disable_radix_cache: bool = False           â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚     disable_regex_jump_forward: bool = False    â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚     disable_cuda_graph: bool = False            â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚     disable_disk_cache: bool = False            â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚     attention_reduce_in_fp32: bool = False      â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚     enable_p2p_check: bool = False              â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                                                 â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚     # Distributed args                          â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚     nccl_init_addr: Optional = None             â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚     nnodes: int = 1                             â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚     node_rank: Optional = None                  â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                                                 â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚     def __post_init__(self):                    â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚         if self.tokenizer_path is None:         â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚             self.tokenizer_path =               â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ self.model_path                                 â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚         if self.mem_fraction_static is None:    â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚             if self.tp_size >= 8:               â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                 self.mem_fraction_static = 0.78 â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚             elif self.tp_size >= 4:             â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                 self.mem_fraction_static = 0.80 â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚             elif self.tp_size >= 2:             â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                 self.mem_fraction_static = 0.85 â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚             else:                               â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                 self.mem_fraction_static = 0.88 â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚         if isinstance(self.additional_ports,    â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ int):                                           â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚             self.additional_ports =             â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚         elif self.additional_ports is None:     â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚             self.additional_ports = []          â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                                                 â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚         if self.random_seed is None:            â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚             self.random_seed =                  â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ random.randint(0, 1 << 30)                      â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                                                 â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚     @staticmethod                               â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚     def add_cli_args(parser:                    â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ argparse.ArgumentParser):                       â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚         parser.add_argument(                    â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚             "--model-path",                     â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚             type=str,                           â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚             help="The path of the model         â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ weights. This can be a local folder or a        â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ Hugging Face repo ID.",                         â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚             required=True,                      â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚         )                                       â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚         parser.add_argument(                    â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚             "--tokenizer-path",                 â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚             type=str,                           â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚             default=ServerArgs.tokenizer_path,  â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚             help="The path of the tokenizer.",  â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚         )                                       â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚         parser.add_argument(                    â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚             "--host", type=str,                 â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ default=ServerArgs.host, help="The host of the  â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ server."                                        â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚         )                                       â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚         parser.add_argument(                    â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚             "--port", type=int,                 â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ default=ServerArgs.port, help="The port of the  â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ server."                                        â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚         )                                       â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚         parser.add_argument(                    â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚             "--additional-ports",               â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚             type=int,                           â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚             nargs="*",                          â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚             default=[],                         â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚             help="The additional ports          â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ specified for the server.",                     â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚         )                                       â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚         parser.add_argument(                    â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚             "--tokenizer-mode",                 â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚             type=str,                           â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚             default=ServerArgs.tokenizer_mode,  â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚             choices=["auto", "slow"],           â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚             help="Tokenizer mode. 'auto' will   â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ use the fast "                                  â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚             "tokenizer if available, and 'slow' â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ will "                                          â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚             "always use the slow tokenizer.",   â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚         )                                       â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚         parser.add_argument(                    â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚             "--load-format",                    â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚             type=str,                           â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚             default=ServerArgs.load_format,     â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚             choices=["auto", "pt",              â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ "safetensors", "npcache", "dummy"],             â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚             help="The format of the model       â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ weights to load. "                              â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚             '"auto" will try to load the        â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ weights in the safetensors format '             â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚             "and fall back to the pytorch bin   â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ format if safetensors format "                  â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚             "is not available. "                â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚             '"pt" will load the weights in the  â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ pytorch bin format. '                           â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚             '"safetensors" will load the        â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ weights in the safetensors format. '            â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚             '"npcache" will load the weights in â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ pytorch format and store '                      â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚             "a numpy cache to speed up the      â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ loading. "                                      â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚             '"dummy" will initialize the        â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ weights with random values, '                   â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚             "which is mainly for profiling.",   â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚         )                                       â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚         parser.add_argument(                    â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚             "--dtype",                          â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚             type=str,                           â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚             default=ServerArgs.dtype,           â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚             choices=["auto", "half", "float16", â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ "bfloat16", "float", "float32"],                â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚             help="Data type for model weights   â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ and activations.\n\n"                           â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚             '* "auto" will use FP16 precision   â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ for FP32 and FP16 models, and '                 â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚             "BF16 precision for BF16 models.\n" â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚             '* "half" for FP16. Recommended for â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ AWQ quantization.\n'                            â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚             '* "float16" is the same as         â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ "half".\n'                                      â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚             '* "bfloat16" for a balance between â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ precision and range.\n'                         â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚             '* "float" is shorthand for FP32    â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ precision.\n'                                   â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚             '* "float32" for FP32 precision.',  â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚         )                                       â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚         parser.add_argument(                    â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚             "--trust-remote-code",              â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚             action="store_true",                â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚             help="Whether or not to allow for   â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ custom models defined on the Hub in their own   â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ modeling files.",                               â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚         )                                       â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚         parser.add_argument(                    â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚             "--context-length",                 â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚             type=int,                           â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚             default=ServerArgs.context_length,  â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚             help="The model's maximum context   â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ length. Defaults to None (will use the value    â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ from the model's config.json instead).",        â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚         )                                       â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚         parser.add_argument(                    â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚             "--quantization",                   â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚             type=str,                           â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚             default=ServerArgs.quantization,    â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚             help="The quantization method.",    â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚         )                                       â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚         parser.add_argument(                    â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚             "--chat-template",                  â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚             type=str,                           â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚             default=ServerArgs.chat_template,   â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚             help="The buliltin chat template    â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ name or the path of the chat template file.     â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ This is only used for OpenAI-compatible API     â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ server.",                                       â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚         )                                       â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚         parser.add_argument(                    â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚             "--mem-fraction-static",            â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚             type=float,                         â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚             default=ServerArgs.mem_fraction_stâ€¦ â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚             help="The fraction of the memory    â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ used for static allocation (model weights and   â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ KV cache memory pool). Use a smaller value if   â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ you see out-of-memory errors.",                 â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚         )                                       â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚         parser.add_argument(                    â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚             "--max-prefill-tokens",             â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚             type=int,                           â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚             default=ServerArgs.max_prefill_tokâ€¦ â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚             help="The maximum number of tokens  â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ in a prefill batch. The real bound will be the  â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ maximum of this value and the model's maximum   â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ context length.",                               â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚         )                                       â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚         parser.add_argument(                    â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚             "--max-running-requests",           â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚             type=int,                           â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚             default=ServerArgs.max_running_reqâ€¦ â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚             help="The maximum number of running â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ requests.",                                     â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚         )                                       â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚         parser.add_argument(                    â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚             "--schedule-heuristic",             â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚             type=str,                           â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚             default=ServerArgs.schedule_heurisâ€¦ â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚             choices=["lpm", "random", "fcfs",   â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ "dfs-weight"],                                  â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚             help="The scheduling heuristic.",   â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚         )                                       â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚         parser.add_argument(                    â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚             "--schedule-conservativeness",      â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚             type=float,                         â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚             default=ServerArgs.schedule_conserâ€¦ â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚             help="How conservative the schedule â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ policy is. A larger value means more            â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ conservative scheduling. Use a larger value if  â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ you see requests being retracted frequently.",  â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚         )                                       â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚         parser.add_argument(                    â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚             "--tp-size",                        â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚             type=int,                           â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚             default=ServerArgs.tp_size,         â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚             help="The tensor parallelism        â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ size.",                                         â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚         )                                       â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚         parser.add_argument(                    â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚             "--stream-interval",                â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚             type=int,                           â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚             default=ServerArgs.stream_interval, â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚             help="The interval (or buffer size) â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ for streaming in terms of the token length. A   â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ smaller value makes streaming smoother, while a â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ larger value makes the throughput higher",      â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚         )                                       â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚         parser.add_argument(                    â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚             "--random-seed",                    â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚             type=int,                           â”‚   â”‚
â”‚             â”‚ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚
â”‚ Total       â”‚ Input: 129606 Output: 1959                                                                             â”‚
â”‚ Tokens      â”‚                                                                                                        â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Status      â”‚ âœ… Step 10: Completed                                                                                  â”‚
â”‚ Tools       â”‚ ðŸ”§ bash                                                                                                â”‚
â”‚ bash        â”‚ â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚
â”‚             â”‚ â”‚ {'command': "sed -n '220,540p'                 â”‚             type=int,                           â”‚   â”‚
â”‚             â”‚ â”‚ /home/ubuntu/OmniPerf-Bench/perf-agents-benchâ€¦ â”‚             default=ServerArgs.random_seed,     â”‚   â”‚
â”‚             â”‚ â”‚ 'restart': False}                              â”‚             help="The random seed.",            â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚         )                                       â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚         parser.add_argument(                    â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚             "--log-level",                      â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚             type=str,                           â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚             default=ServerArgs.log_level,       â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚             help="The logging level of all      â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ loggers.",                                      â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚         )                                       â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚         parser.add_argument(                    â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚             "--log-level-http",                 â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚             type=str,                           â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚             default=ServerArgs.log_level_http,  â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚             help="The logging level of HTTP     â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ server. If not set, reuse --log-level by        â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ default.",                                      â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚         )                                       â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚         parser.add_argument(                    â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚             "--log-requests",                   â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚             action="store_true",                â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚             help="Log the inputs and outputs of â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ all requests.",                                 â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚         )                                       â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚         parser.add_argument(                    â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚             "--show-time-cost",                 â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚             action="store_true",                â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚             help="Show time cost of custom      â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ marks",                                         â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚         )                                       â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚         parser.add_argument(                    â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚             "--api-key",                        â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚             type=str,                           â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚             default=ServerArgs.api_key,         â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚             help="Set API key of the server",   â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚         )                                       â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                                                 â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚         # Data parallelism                      â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚         parser.add_argument(                    â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚             "--dp-size",                        â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚             type=int,                           â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚             default=ServerArgs.dp_size,         â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚             help="The data parallelism size.",  â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚         )                                       â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚         parser.add_argument(                    â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚             "--load-balance-method",            â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚             type=str,                           â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚             default=ServerArgs.load_balance_meâ€¦ â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚             help="The load balancing strategy   â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ for data parallelism.",                         â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚             choices=[                           â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                 "round_robin",                  â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                 "shortest_queue",               â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚             ],                                  â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚         )                                       â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                                                 â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚         # Multi-node distributed serving args   â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚         parser.add_argument(                    â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚             "--nccl-init-addr",                 â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚             type=str,                           â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚             help="The nccl init address of      â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ multi-node server.",                            â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚         )                                       â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚         parser.add_argument(                    â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚             "--nnodes", type=int, default=1,    â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ help="The number of nodes."                     â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚         )                                       â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚         parser.add_argument("--node-rank",      â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ type=int, help="The node rank.")                â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                                                 â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚         # Optimization/debug options            â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚         parser.add_argument(                    â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚             "--disable-flashinfer",             â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚             action="store_true",                â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚             help="Disable flashinfer inference  â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ kernels",                                       â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚         )                                       â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚         parser.add_argument(                    â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚             "--disable-radix-cache",            â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚             action="store_true",                â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚             help="Disable RadixAttention",      â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚         )                                       â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚         parser.add_argument(                    â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚             "--disable-regex-jump-forward",     â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚             action="store_true",                â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚             help="Disable regex jump-forward",  â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚         )                                       â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚         parser.add_argument(                    â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚             "--disable-cuda-graph",             â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚             action="store_true",                â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚             help="Disable cuda graph.",         â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚         )                                       â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚         parser.add_argument(                    â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚             "--disable-disk-cache",             â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚             action="store_true",                â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚             help="Disable disk cache to avoid   â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ possible crashes related to file system or high â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ concurrency.",                                  â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚         )                                       â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚         parser.add_argument(                    â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚             "--attention-reduce-in-fp32",       â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚             action="store_true",                â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚             help="Cast the intermidiate         â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ attention results to fp32 to avoid possible     â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ crashes related to fp16."                       â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚             "This only affects Triton attention â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ kernels",                                       â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚         )                                       â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚         parser.add_argument(                    â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚             "--enable-p2p-check",               â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚             action="store_true",                â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚             help="Enable P2P check for GPU      â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ access, otherwise the p2p access is allowed by  â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ default.",                                      â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚         )                                       â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                                                 â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚     @classmethod                                â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚     def from_cli_args(cls, args:                â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ argparse.Namespace):                            â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚         attrs =                                 â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚         return cls(**{attr: getattr(args, attr) â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ for attr in attrs})                             â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                                                 â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚     def url(self):                              â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚         return                                  â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ f"http://{self.host}:{self.port}"               â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                                                 â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚     def print_mode_args(self):                  â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚         return (                                â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚             f"disable_flashinfer={self.disableâ€¦ â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ "                                               â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚             f"attention_reduce_in_fp32={self.aâ€¦ â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ "                                               â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚             f"disable_radix_cache={self.disablâ€¦ â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ "                                               â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚             f"disable_regex_jump_forward={selfâ€¦ â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ "                                               â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚             f"disable_disk_cache={self.disableâ€¦ â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ "                                               â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚         )                                       â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                                                 â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                                                 â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ @dataclasses.dataclass                          â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ class ModelPortArgs:                            â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚     nccl_port: int                              â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚     model_tp_ips: List                          â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚     model_tp_ports: List                        â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                                                 â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                                                 â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ @dataclasses.dataclass                          â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ class PortArgs:                                 â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚     tokenizer_port: int                         â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚     router_port: int                            â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚     detokenizer_port: int                       â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚     model_port_args: List[ModelPortArgs]        â”‚   â”‚
â”‚             â”‚ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚
â”‚ Total       â”‚ Input: 151500 Output: 2034                                                                             â”‚
â”‚ Tokens      â”‚                                                                                                        â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Status      â”‚ âœ… Step 11: Completed                                                                                  â”‚
â”‚ Tools       â”‚ ðŸ”§ bash                                                                                                â”‚
â”‚ bash        â”‚ â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚
â”‚             â”‚ â”‚ {'command': "sed -n '1,240p'                   â”‚ """A controller that manages a group of tensor  â”‚   â”‚
â”‚             â”‚ â”‚ /home/ubuntu/OmniPerf-Bench/perf-agents-benchâ€¦ â”‚ parallel workers."""                            â”‚   â”‚
â”‚             â”‚ â”‚ 'restart': False}                              â”‚                                                 â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ import asyncio                                  â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ import logging                                  â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ from concurrent.futures import                  â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ ThreadPoolExecutor                              â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                                                 â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ import uvloop                                   â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ import zmq                                      â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ import zmq.asyncio                              â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                                                 â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ from sglang.global_config import global_config  â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ from sglang.srt.managers.controller.tp_worker   â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ import ModelTpClient                            â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ from sglang.srt.server_args import PortArgs,    â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ ServerArgs                                      â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ from sglang.srt.utils import                    â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ kill_parent_process                             â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ from sglang.utils import                        â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ get_exception_traceback                         â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                                                 â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ asyncio.set_event_loop_policy(uvloop.EventLoopâ€¦ â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                                                 â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ logger = logging.getLogger("srt.controller")    â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                                                 â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                                                 â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ class ControllerSingle:                         â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚     def __init__(self, model_client:            â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ ModelTpClient, port_args: PortArgs):            â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚         # Init communication                    â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚         context = zmq.asyncio.Context(2)        â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚         self.recv_from_tokenizer =              â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ context.socket(zmq.PULL)                        â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚         self.recv_from_tokenizer.bind(f"tcp://â€¦ â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                                                 â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚         self.send_to_detokenizer =              â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ context.socket(zmq.PUSH)                        â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚         self.send_to_detokenizer.connect(       â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚             f"tcp://127.0.0.1:{port_args.detokâ€¦ â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚         )                                       â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                                                 â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚         # Init status                           â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚         self.model_client = model_client        â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚         self.recv_reqs = []                     â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                                                 â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚         # Init some configs                     â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚         self.request_dependency_delay =         â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ global_config.request_dependency_delay          â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                                                 â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚     async def loop_for_forward(self):           â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚         while True:                             â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚             next_step_input =                   â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ list(self.recv_reqs)                            â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚             self.recv_reqs = []                 â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚             out_pyobjs = await                  â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ self.model_client.step(next_step_input)         â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                                                 â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚             for obj in out_pyobjs:              â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                 self.send_to_detokenizer.send_â€¦ â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                                                 â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚             # async sleep for receiving the     â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ subsequent request and avoiding cache miss      â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚             slept = False                       â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚             if len(out_pyobjs) != 0:            â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                 has_finished = any(             â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                                                 â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                 )                               â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                 if has_finished:                â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                     if                          â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ self.request_dependency_delay > 0:              â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                         slept = True            â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                         await                   â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ asyncio.sleep(self.request_dependency_delay)    â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                                                 â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚             if not slept:                       â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                 await                           â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ asyncio.sleep(global_config.wait_for_new_requeâ€¦ â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                                                 â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚     async def loop_for_recv_requests(self):     â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚         while True:                             â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚             recv_req = await                    â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ self.recv_from_tokenizer.recv_pyobj()           â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚             self.recv_reqs.append(recv_req)     â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                                                 â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                                                 â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ def start_controller_process(                   â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚     server_args: ServerArgs, port_args:         â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ PortArgs, pipe_writer, model_overide_args       â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ ):                                              â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚     logging.basicConfig(                        â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚         level=getattr(logging,                  â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ server_args.log_level.upper()),                 â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚         format="%(message)s",                   â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚     )                                           â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                                                 â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚     try:                                        â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚         tp_size_local = server_args.tp_size //  â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ server_args.nnodes                              â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚         model_client = ModelTpClient(           â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚             ,                                   â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚             server_args,                        â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚             port_args.model_port_args[0],       â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚             model_overide_args,                 â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚         )                                       â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚         controller =                            â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ ControllerSingle(model_client, port_args)       â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚     except Exception:                           â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚         pipe_writer.send(get_exception_tracebaâ€¦ â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚         raise                                   â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                                                 â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚     pipe_writer.send("init ok")                 â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                                                 â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚     loop = asyncio.new_event_loop()             â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚     loop.set_default_executor(ThreadPoolExecutâ€¦ â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚     asyncio.set_event_loop(loop)                â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚     loop.create_task(controller.loop_for_recv_â€¦ â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚     try:                                        â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚         loop.run_until_complete(controller.looâ€¦ â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚     except Exception:                           â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚         logger.error("Exception in              â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ ControllerSingle:\n" +                          â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ get_exception_traceback())                      â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚     finally:                                    â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚         kill_parent_process()                   â”‚   â”‚
â”‚             â”‚ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚
â”‚ Total       â”‚ Input: 174337 Output: 2115                                                                             â”‚
â”‚ Tokens      â”‚                                                                                                        â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Status      â”‚ âœ… Step 12: Completed                                                                                  â”‚
â”‚ Tools       â”‚ ðŸ”§ bash                                                                                                â”‚
â”‚ bash        â”‚ â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚
â”‚             â”‚ â”‚ {'command': "sed -n '1,220p'                   â”‚ """                                             â”‚   â”‚
â”‚             â”‚ â”‚ /home/ubuntu/OmniPerf-Bench/perf-agents-benchâ€¦ â”‚ A controller that manages multiple data         â”‚   â”‚
â”‚             â”‚ â”‚ 'restart': False}                              â”‚ parallel workers.                               â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ Each data parallel worker can manage multiple   â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ tensor parallel workers.                        â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ """                                             â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                                                 â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ import asyncio                                  â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ import logging                                  â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ from concurrent.futures import                  â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ ThreadPoolExecutor                              â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ from enum import Enum, auto                     â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ from typing import Dict                         â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                                                 â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ import zmq                                      â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ import zmq.asyncio                              â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                                                 â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ from sglang.global_config import global_config  â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ from sglang.srt.managers.controller.dp_worker   â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ import (                                        â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚     DataParallelWorkerThread,                   â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚     start_data_parallel_worker,                 â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ )                                               â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ from sglang.srt.managers.io_struct import (     â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚     AbortReq,                                   â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚     FlushCacheReq,                              â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚     TokenizedGenerateReqInput,                  â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ )                                               â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ from sglang.srt.server_args import PortArgs,    â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ ServerArgs                                      â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ from sglang.utils import                        â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ get_exception_traceback                         â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                                                 â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ logger = logging.getLogger("srt.controller")    â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                                                 â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                                                 â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ class LoadBalanceMethod(Enum):                  â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚     ROUND_ROBIN = auto()                        â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚     SHORTEST_QUEUE = auto()                     â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                                                 â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚     @classmethod                                â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚     def from_str(cls, method: str):             â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚         method = method.upper()                 â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚         try:                                    â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚             return cls                          â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚         except KeyError as exc:                 â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚             raise ValueError(f"Invalid load     â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ balance method: {method}") from exc             â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                                                 â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                                                 â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ class Controller:                               â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚     def __init__(                               â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚         self,                                   â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚         load_balance_method: str,               â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚         server_args: ServerArgs,                â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚         port_args: PortArgs,                    â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚         model_overide_args,                     â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚     ):                                          â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚         self.load_balance_method =              â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ LoadBalanceMethod.from_str(load_balance_method) â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚         self.server_args = server_args          â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚         self.port_args = port_args              â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                                                 â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚         if self.load_balance_method ==          â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ LoadBalanceMethod.ROUND_ROBIN:                  â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚             self.round_robin_counter = 0        â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                                                 â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚         self.dispatch_lookup = {                â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚             LoadBalanceMethod.ROUND_ROBIN:      â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ self.round_robin_scheduler,                     â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚             LoadBalanceMethod.SHORTEST_QUEUE:   â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ self.shortest_queue_scheduler,                  â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚         }                                       â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚         self.dispatching = self.dispatch_lookup â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                                                 â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚         # Init communication                    â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚         context = zmq.asyncio.Context()         â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚         self.recv_from_tokenizer =              â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ context.socket(zmq.PULL)                        â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚         self.recv_from_tokenizer.bind(f"tcp://â€¦ â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                                                 â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚         # Init status                           â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚         self.recv_reqs = []                     â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                                                 â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚         # Start data parallel workers           â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚         self.workers: Dict = {}                 â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚         tp_size = server_args.tp_size           â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                                                 â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚         def start_dp_worker(i):                 â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚             try:                                â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                 gpu_ids = list(range(i *        â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ tp_size, (i + 1) * tp_size))                    â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                 worker_thread =                 â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ start_data_parallel_worker(                     â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                     server_args, port_args,     â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ model_overide_args, gpu_ids, i                  â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                 )                               â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                 self.workers[3m = worker_thread[0m    â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ [3m            except Exception:[0m                   â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ [3m                logger.error([0m                   â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ [3m                    f"Failed to start local [0m    â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ [3mworker {i}\n{get_exception_traceback()}"[0m        â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ [3m                )[0m                               â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                                                 â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ [3m        for i in range(server_args.dp_size):[0m    â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ [3m            start_dp_worker(i)[0m                  â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                                                 â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ [3m        # Parallel launch is slower, probably [0m  â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ [3mdue to the disk bandwidth limitations.[0m          â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ [3m        # with [0m                                 â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ [3mThreadPoolExecutor(server_args.dp_size) as [0m     â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ [3mexecutor:[0m                                       â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ [3m        #     executor.map(start_dp_worker, [0m    â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ [3mrange(server_args.dp_size))[0m                     â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                                                 â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ [3m    def have_any_live_worker(self):[0m             â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ [3m        return any(worker_thread.liveness for [0m  â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ [3mworker_thread in self.workers.values())[0m         â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                                                 â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ [3m    def put_req_to_worker(self, worker_id, [0m     â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ [3mreq):[0m                                           â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ [3m        self.workers[0m[3m.request_queue.put(req)[0m     â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                                                 â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ [3m    async def round_robin_scheduler(self, [0m      â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ [3minput_requests):[0m                                â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ [3m        available_workers = [0m                    â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ [3mlist(self.workers.keys())[0m                       â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ [3m        for r in input_requests:[0m                â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ [3m            self.put_req_to_worker(available_wâ€¦[0m â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ [3mr)[0m                                              â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ [3m            self.round_robin_counter = [0m         â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ [3m(self.round_robin_counter + 1) % len([0m           â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ [3m                available_workers[0m               â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ [3m            )[0m                                   â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ [3m        return[0m                                  â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                                                 â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ [3m    async def shortest_queue_scheduler(self, [0m   â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ [3minput_requests):[0m                                â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ [3m        for r in input_requests:[0m                â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ [3m            worker = min([0m                       â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ [3m                self.workers, key=lambda w: [0m    â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ [3mself.workers[0m[3m.request_queue.qsize()[0m              â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ [3m            )[0m                                   â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ [3m            self.put_req_to_worker(worker, r)[0m   â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ [3m        return[0m                                  â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                                                 â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ [3m    async def remove_dead_workers(self):[0m        â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ [3m        for i in list(self.workers.keys()):[0m     â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ [3m            worker_thread = self.workers[0m        â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ [3m            if not worker_thread.liveness:[0m      â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ [3m                worker_thread.join()[0m            â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ [3m                # move unsuccessful requests [0m   â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ [3mback to the queue[0m                               â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ [3m                while not [0m                      â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ [3mworker_thread.request_queue.empty():[0m            â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ [3m                    self.recv_reqs.append(workâ€¦[0m â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ [3m                del self.workers[0m                â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ [3m                logger.info(f"Stale worker {i} [0m â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ [3mremoved")[0m                                       â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                                                 â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ [3m    async def loop_for_forward(self):[0m           â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ [3m        while True:[0m                             â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ [3m            await self.remove_dead_workers()[0m    â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                                                 â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ [3m            if self.have_any_live_worker():[0m     â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ [3m                next_step_input = [0m              â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ [3mlist(self.recv_reqs)[0m                            â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ [3m                self.recv_reqs = [][0m             â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ [3m                if next_step_input:[0m             â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ [3m                    await [0m                      â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ [3mself.dispatching(next_step_input)[0m               â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ [3m            # else:[0m                             â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ [3m            #    logger.error("There is no live[0m â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ [3mworker.")[0m                                       â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                                                 â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ [3m            await [0m                              â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ [3masyncio.sleep(global_config.wait_for_new_requeâ€¦[0m â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                                                 â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ [3m    async def loop_for_recv_requests(self):[0m     â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ [3m        while True:[0m                             â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ [3m            recv_req = await [0m                   â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ [3mself.recv_from_tokenizer.recv_pyobj()[0m           â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ [3m            if isinstance(recv_req, [0m            â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ [3mFlushCacheReq):[0m                                 â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ [3m                # TODO(lsyin): apply more [0m      â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ [3mspecific flushCacheReq[0m                          â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ [3m                for worker_thread in [0m           â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ [3mself.workers.values():[0m                          â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ [3m                    worker_thread.request_queuâ€¦[0m â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ [3m            elif isinstance(recv_req, [0m          â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ [3mTokenizedGenerateReqInput):[0m                     â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ [3m                self.recv_reqs.append(recv_req)[0m â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ [3m            elif isinstance(recv_req, [0m          â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ [3mAbortReq):[0m                                      â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ [3m                in_queue = False[0m                â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ [3m                for i, req in [0m                  â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ [3menumerate(self.recv_reqs):[0m                      â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ [3m                    if req.rid == recv_req.rid:[0m â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ [3m                        self.recv_reqs[0m[3m = [0m       â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ [3mrecv_req[0m                                        â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ [3m                        in_queue = True[0m         â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ [3m                        break[0m                   â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ [3m                if not in_queue:[0m                â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ [3m                    # Send abort req to all TP [0m â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ [3mgroups[0m                                          â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ [3m                    for worker in [0m              â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ [3mlist(self.workers.keys()):[0m                      â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ [3m                        self.put_req_to_workerâ€¦[0m â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ [3mrecv_req)[0m                                       â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ [3m            else:[0m                               â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ [3m                logger.error(f"Invalid object: [0m â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ [3m{recv_req}")[0m                                    â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                                                 â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                                                 â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ [3mdef start_controller_process([0m                   â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ [3m    server_args: ServerArgs,[0m                    â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ [3m    port_args: PortArgs,[0m                        â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ [3m    pipe_writer,[0m                                â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ [3m    model_overide_args=None,[0m                    â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ [3m):[0m                                              â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ [3m    logging.basicConfig([0m                        â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ [3m        level=getattr(logging, [0m                 â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ [3mserver_args.log_level.upper()),[0m                 â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ [3m        format="%(message)s",[0m                   â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ [3m    )[0m                                           â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                                                 â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ [3m    try:[0m                                        â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ [3m        controller = Controller([0m                â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ [3m            server_args.load_balance_method, [0m   â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ [3mserver_args, port_args, model_overide_args[0m      â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ [3m        )[0m                                       â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ [3m    except Exception:[0m                           â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ [3m        pipe_writer.send(get_exception_tracebaâ€¦[0m â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ [3m        raise[0m                                   â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                                                 â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ [3m    pipe_writer.send("init ok")[0m                 â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ [3m    loop = asyncio.get_event_loop()[0m             â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ [3m    asyncio.set_event_loop(loop)[0m                â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ [3m    loop.create_task(controller.loop_for_recv_â€¦[0m â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ [3m    loop.run_until_complete(controller.loop_foâ€¦[0m â”‚   â”‚
â”‚             â”‚ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚
â”‚ Total       â”‚ Input: 198010 Output: 2194                                                                             â”‚
â”‚ Tokens      â”‚                                                                                                        â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Status      â”‚ âœ… Step 13: Completed                                                                                  â”‚
â”‚ Tools       â”‚ ðŸ”§ bash                                                                                                â”‚
â”‚ bash        â”‚ â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚
â”‚             â”‚ â”‚ {'command': "sed -n '1,220p'                   â”‚ """ModelRunner runs the forward passes of the   â”‚   â”‚
â”‚             â”‚ â”‚ /home/ubuntu/OmniPerf-Bench/perf-agents-benchâ€¦ â”‚ models."""                                      â”‚   â”‚
â”‚             â”‚ â”‚ 'restart': False}                              â”‚                                                 â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ import importlib                                â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ import importlib.resources                      â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ import logging                                  â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ import pkgutil                                  â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ from functools import lru_cache                 â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ from typing import Optional, Type               â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                                                 â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ import torch                                    â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ import torch.nn as nn                           â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ from vllm.config import DeviceConfig,           â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ LoadConfig                                      â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ from vllm.config import ModelConfig as          â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ VllmModelConfig                                 â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ from vllm.distributed import                    â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ init_distributed_environment,                   â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ initialize_model_parallel                       â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ from vllm.model_executor.model_loader import    â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ get_model                                       â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ from vllm.model_executor.models import          â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ ModelRegistry                                   â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                                                 â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ from sglang.global_config import global_config  â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ from sglang.srt.managers.controller.infer_batch â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ import (                                        â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚     Batch,                                      â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚     ForwardMode,                                â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚     InputMetadata,                              â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚     global_server_args_dict,                    â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ )                                               â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ from sglang.srt.memory_pool import              â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ ReqToTokenPool, TokenToKVPool                   â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ from sglang.srt.server_args import ServerArgs   â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ from sglang.srt.utils import (                  â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚     get_available_gpu_memory,                   â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚     is_multimodal_model,                        â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚     monkey_patch_vllm_dummy_weight_loader,      â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚     monkey_patch_vllm_p2p_access_check,         â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ )                                               â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                                                 â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ logger = logging.getLogger("srt.model_runner")  â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                                                 â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                                                 â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ class ModelRunner:                              â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚     def __init__(                               â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚         self,                                   â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚         model_config,                           â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚         mem_fraction_static: float,             â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚         gpu_id: int,                            â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚         tp_rank: int,                           â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚         tp_size: int,                           â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚         nccl_port: int,                         â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚         server_args: ServerArgs,                â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚     ):                                          â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚         # Parse args                            â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚         self.model_config = model_config        â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚         self.mem_fraction_static =              â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ mem_fraction_static                             â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚         self.gpu_id = gpu_id                    â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚         self.tp_rank = tp_rank                  â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚         self.tp_size = tp_size                  â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚         self.nccl_port = nccl_port              â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚         self.server_args = server_args          â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚         self.is_multimodal_model =              â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ is_multimodal_model(self.model_config)          â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚         monkey_patch_vllm_dummy_weight_loader() â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                                                 â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚         # Init torch distributed                â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚         torch.cuda.set_device(self.gpu_id)      â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚         logger.info(f" Init nccl begin.")       â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                                                 â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚         if not server_args.enable_p2p_check:    â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚             monkey_patch_vllm_p2p_access_checkâ€¦ â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                                                 â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚         if server_args.nccl_init_addr:          â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚             nccl_init_method =                  â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ f"tcp://{server_args.nccl_init_addr}"           â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚         else:                                   â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚             nccl_init_method =                  â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ f"tcp://127.0.0.1:{self.nccl_port}"             â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚         init_distributed_environment(           â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚             backend="nccl",                     â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚             world_size=self.tp_size,            â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚             rank=self.tp_rank,                  â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚             local_rank=self.gpu_id,             â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚             distributed_init_method=nccl_init_â€¦ â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚         )                                       â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚         initialize_model_parallel(tensor_modelâ€¦ â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚         total_gpu_memory =                      â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ get_available_gpu_memory(                       â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚             self.gpu_id,                        â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ distributed=self.tp_size > 1                    â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚         )                                       â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                                                 â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚         if self.tp_size > 1:                    â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚             total_local_gpu_memory =            â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ get_available_gpu_memory(self.gpu_id)           â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚             if total_local_gpu_memory <         â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ total_gpu_memory * 0.9:                         â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                 raise ValueError(               â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                     "The memory capacity is     â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ unbalanced. Some GPUs may be occupied by other  â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ processes."                                     â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                 )                               â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                                                 â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚         # Set some global args                  â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚         global_server_args_dict["disable_flashâ€¦ â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ = server_args.disable_flashinfer                â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚         global_server_args_dict[                â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚             "attention_reduce_in_fp32"          â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚         ] =                                     â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ server_args.attention_reduce_in_fp32            â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                                                 â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚         # Load the model and create memory pool â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚         self.load_model()                       â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚         self.init_memory_pool(total_gpu_memory) â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚         self.init_cublas()                      â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚         self.init_flash_infer()                 â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                                                 â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚         # Capture cuda graphs                   â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚         self.init_cuda_graphs()                 â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                                                 â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚     def load_model(self):                       â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚         logger.info(                            â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚             f" Load weight begin. "             â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚             f"avail                             â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ mem={get_available_gpu_memory(self.gpu_id):.2f} â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ GB"                                             â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚         )                                       â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                                                 â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚         device_config = DeviceConfig()          â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚         load_config =                           â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ LoadConfig(load_format=self.server_args.load_fâ€¦ â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚         vllm_model_config = VllmModelConfig(    â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚             model=self.server_args.model_path,  â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚             quantization=self.server_args.quanâ€¦ â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚             tokenizer=None,                     â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚             tokenizer_mode=None,                â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚             trust_remote_code=self.server_argsâ€¦ â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚             dtype=self.server_args.dtype,       â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚             seed=42,                            â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚             skip_tokenizer_init=True,           â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚         )                                       â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚         self.dtype = vllm_model_config.dtype    â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚         if self.model_config.model_overide_args â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ is not None:                                    â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚             vllm_model_config.hf_config.updateâ€¦ â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                                                 â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚         self.model = get_model(                 â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚             model_config=vllm_model_config,     â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚             device_config=device_config,        â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚             load_config=load_config,            â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚             lora_config=None,                   â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚             multimodal_config=None,             â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚             parallel_config=None,               â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚             scheduler_config=None,              â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚             cache_config=None,                  â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚         )                                       â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚         logger.info(                            â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚             f" Load weight end. "               â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚             f"type={type(self.model).__name__}, â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ "                                               â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚             f"dtype={self.dtype}, "             â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚             f"avail                             â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ mem={get_available_gpu_memory(self.gpu_id):.2f} â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ GB"                                             â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚         )                                       â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                                                 â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚     def profile_max_num_token(self,             â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ total_gpu_memory):                              â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚         available_gpu_memory =                  â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ get_available_gpu_memory(                       â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚             self.gpu_id,                        â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ distributed=self.tp_size > 1                    â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚         )                                       â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚         head_dim = self.model_config.head_dim   â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚         head_num =                              â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ self.model_config.get_num_kv_heads(self.tp_sizâ€¦ â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚         cell_size = (                           â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚             head_num                            â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚             * head_dim                          â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚             *                                   â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ self.model_config.num_hidden_layers             â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚             * 2                                 â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚             *                                   â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ torch._utils._element_size(self.dtype)          â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚         )                                       â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚         rest_memory = available_gpu_memory -    â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ total_gpu_memory * (                            â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚             1 - self.mem_fraction_static        â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚         )                                       â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚         max_num_token = int(rest_memory * (1 << â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ 30) // cell_size)                               â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚         return max_num_token                    â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                                                 â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚     def init_memory_pool(self,                  â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ total_gpu_memory):                              â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚         self.max_total_num_tokens =             â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ self.profile_max_num_token(total_gpu_memory)    â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                                                 â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚         if self.max_total_num_tokens <= 0:      â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚             raise RuntimeError(                 â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                 "Not enough memory. Please try  â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ to increase --mem-fraction-static."             â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚             )                                   â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                                                 â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚         self.req_to_token_pool =                â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ ReqToTokenPool(                                 â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚             int(self.max_total_num_tokens /     â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ self.model_config.context_len * 256),           â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚             self.model_config.context_len + 8,  â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚         )                                       â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚         self.token_to_kv_pool = TokenToKVPool(  â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚             self.max_total_num_tokens,          â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚             dtype=self.dtype,                   â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚             head_num=self.model_config.get_numâ€¦ â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚             head_dim=self.model_config.head_diâ€¦ â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚             layer_num=self.model_config.num_hiâ€¦ â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚         )                                       â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚         logger.info(                            â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚             f" Memory pool end. "               â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚             f"avail                             â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ mem={get_available_gpu_memory(self.gpu_id):.2f} â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ GB"                                             â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚         )                                       â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                                                 â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚     def init_cublas(self):                      â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚         """We need to run a small matmul to     â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ init cublas. Otherwise, it will raise some      â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ errors later."""                                â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚         dtype = torch.float16                   â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚         device = "cuda"                         â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚         a = torch.ones((16, 16), dtype=dtype,   â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ device=device)                                  â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚         b = torch.ones((16, 16), dtype=dtype,   â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ device=device)                                  â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚         c = a @ b                               â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚         return c                                â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                                                 â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚     def init_flash_infer(self):                 â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚         if self.server_args.disable_flashinfer: â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚             self.flashinfer_prefill_wrapper_raâ€¦ â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ = None                                          â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚             self.flashinfer_prefill_wrapper_paâ€¦ â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ = None                                          â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚             self.flashinfer_decode_wrapper =    â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ None                                            â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚             return                              â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                                                 â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚         from flashinfer import (                â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚             BatchDecodeWithPagedKVCacheWrapper, â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚             BatchPrefillWithPagedKVCacheWrappeâ€¦ â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚             BatchPrefillWithRaggedKVCacheWrappâ€¦ â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚         )                                       â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚         from flashinfer.decode import           â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ _grouped_size_compiled_for_decode_kernels       â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                                                 â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚         if not                                  â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ _grouped_size_compiled_for_decode_kernels(      â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚             self.model_config.num_attention_heâ€¦ â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ // self.tp_size,                                â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚             self.model_config.get_num_kv_headsâ€¦ â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚         ):                                      â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚             use_tensor_cores = True             â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚         else:                                   â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚             use_tensor_cores = False            â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                                                 â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚         self.flashinfer_workspace_buffers =     â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ torch.empty(                                    â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚             2,                                  â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ global_config.flashinfer_workspace_size,        â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ dtype=torch.uint8, device="cuda"                â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚         )                                       â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚         self.flashinfer_prefill_wrapper_ragged  â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ = BatchPrefillWithRaggedKVCacheWrapper(         â”‚   â”‚
â”‚             â”‚ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚
â”‚ Total       â”‚ Input: 223118 Output: 2272                                                                             â”‚
â”‚ Tokens      â”‚                                                                                                        â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Status      â”‚ âœ… Step 14: Completed                                                                                  â”‚
â”‚ Tools       â”‚ ðŸ”§ bash                                                                                                â”‚
â”‚ bash        â”‚ â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚
â”‚             â”‚ â”‚ {'command': "sed -n '220,520p'                 â”‚         self.flashinfer_prefill_wrapper_ragged  â”‚   â”‚
â”‚             â”‚ â”‚ /home/ubuntu/OmniPerf-Bench/perf-agents-benchâ€¦ â”‚ = BatchPrefillWithRaggedKVCacheWrapper(         â”‚   â”‚
â”‚             â”‚ â”‚ 'restart': False}                              â”‚             self.flashinfer_workspace_buffers[â€¦ â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ "NHD"                                           â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚         )                                       â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚         self.flashinfer_prefill_wrapper_paged = â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ BatchPrefillWithPagedKVCacheWrapper(            â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚             self.flashinfer_workspace_buffers[â€¦ â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ "NHD"                                           â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚         )                                       â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚         self.flashinfer_decode_wrapper =        â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ BatchDecodeWithPagedKVCacheWrapper(             â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚             self.flashinfer_workspace_buffers[â€¦ â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚             "NHD",                              â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚             use_tensor_cores=use_tensor_cores,  â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚         )                                       â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                                                 â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚     def init_cuda_graphs(self):                 â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚         from                                    â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ sglang.srt.managers.controller.cuda_graph_runnâ€¦ â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ import CudaGraphRunner                          â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                                                 â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚         if self.server_args.disable_cuda_graph  â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ or self.server_args.disable_flashinfer:         â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚             self.cuda_graph_runner = None       â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚             return                              â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                                                 â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚         logger.info(f" Capture cuda graph       â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ begin.")                                        â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚         batch_size_list = [1, 2, 4] +           â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚         self.cuda_graph_runner =                â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ CudaGraphRunner(                                â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚             self,                               â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ max_batch_size_to_capture=max(batch_size_list)  â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚         )                                       â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚         self.cuda_graph_runner.capture(batch_sâ€¦ â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                                                 â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚     @torch.inference_mode()                     â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚     def forward_decode(self, batch: Batch):     â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚         if self.cuda_graph_runner and           â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ self.cuda_graph_runner.can_run(len(batch.reqs)â€¦ â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚             return                              â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ self.cuda_graph_runner.replay(batch)            â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                                                 â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚         input_metadata = InputMetadata.create(  â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚             self,                               â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚             forward_mode=ForwardMode.DECODE,    â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚             req_pool_indices=batch.req_pool_inâ€¦ â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚             seq_lens=batch.seq_lens,            â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚             prefix_lens=batch.prefix_lens,      â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚             position_ids_offsets=batch.positioâ€¦ â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚             out_cache_loc=batch.out_cache_loc,  â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚             top_logprobs_nums=batch.top_logproâ€¦ â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚             return_logprob=batch.return_logproâ€¦ â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚         )                                       â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚         return self.model.forward(              â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚             batch.input_ids,                    â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ input_metadata.positions, input_metadata        â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚         )                                       â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                                                 â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚     @torch.inference_mode()                     â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚     def forward_extend(self, batch: Batch):     â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚         input_metadata = InputMetadata.create(  â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚             self,                               â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚             forward_mode=ForwardMode.EXTEND,    â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚             req_pool_indices=batch.req_pool_inâ€¦ â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚             seq_lens=batch.seq_lens,            â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚             prefix_lens=batch.prefix_lens,      â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚             position_ids_offsets=batch.positioâ€¦ â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚             out_cache_loc=batch.out_cache_loc,  â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚             top_logprobs_nums=batch.top_logproâ€¦ â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚             return_logprob=batch.return_logproâ€¦ â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚         )                                       â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚         return self.model.forward(              â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚             batch.input_ids,                    â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ input_metadata.positions, input_metadata        â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚         )                                       â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                                                 â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚     @torch.inference_mode()                     â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚     def forward_extend_multi_modal(self, batch: â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ Batch):                                         â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚         input_metadata = InputMetadata.create(  â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚             self,                               â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚             forward_mode=ForwardMode.EXTEND,    â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚             req_pool_indices=batch.req_pool_inâ€¦ â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚             seq_lens=batch.seq_lens,            â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚             prefix_lens=batch.prefix_lens,      â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚             position_ids_offsets=batch.positioâ€¦ â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚             out_cache_loc=batch.out_cache_loc,  â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚             return_logprob=batch.return_logproâ€¦ â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚             top_logprobs_nums=batch.top_logproâ€¦ â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚         )                                       â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚         return self.model.forward(              â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚             batch.input_ids,                    â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚             input_metadata.positions,           â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚             input_metadata,                     â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚             batch.pixel_values,                 â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚             batch.image_sizes,                  â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚             batch.image_offsets,                â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚         )                                       â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                                                 â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚     def forward(self, batch: Batch,             â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ forward_mode: ForwardMode):                     â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚         if self.is_multimodal_model and         â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ forward_mode == ForwardMode.EXTEND:             â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚             return                              â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ self.forward_extend_multi_modal(batch)          â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚         elif forward_mode ==                    â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ ForwardMode.DECODE:                             â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚             return self.forward_decode(batch)   â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚         elif forward_mode ==                    â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ ForwardMode.EXTEND:                             â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚             return self.forward_extend(batch)   â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚         else:                                   â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚             raise ValueError(f"Invaid forward   â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ mode: {forward_mode}")                          â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                                                 â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                                                 â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ @lru_cache()                                    â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ def import_model_classes():                     â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚     model_arch_name_to_cls = {}                 â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚     package_name = "sglang.srt.models"          â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚     package =                                   â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ importlib.import_module(package_name)           â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚     for _, name, ispkg in                       â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ pkgutil.iter_modules(package.__path__,          â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ package_name + "."):                            â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚         if not ispkg:                           â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚             module =                            â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ importlib.import_module(name)                   â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚             if hasattr(module, "EntryClass"):   â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                 entry = module.EntryClass       â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                 if isinstance(                  â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                     entry, list                 â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                 ):  # To support multiple model â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ classes in one module                           â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                     for tmp in entry:           â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                         model_arch_name_to_cls  â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ = tmp                                           â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                 else:                           â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                     model_arch_name_to_cls =    â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ entry                                           â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                                                 â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚             # compat: some models such as       â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ chatglm has incorrect class set in config.json  â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚             # usage: [                          â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ tuple("From_Entry_Class_Name": EntryClass), ]   â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚             if hasattr(module,                  â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ "EntryClassRemapping") and isinstance(          â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                 module.EntryClassRemapping,     â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ list                                            â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚             ):                                  â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                 for remap in                    â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ module.EntryClassRemapping:                     â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                     if isinstance(remap, tuple) â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ and len(remap) == 2:                            â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                         model_arch_name_to_clsâ€¦ â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ = remap[1]                                      â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                                                 â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚     return model_arch_name_to_cls               â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                                                 â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                                                 â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ def load_model_cls_srt(model_arch: str) ->      â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ Optional[Type]:                                 â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚     model_arch_name_to_cls =                    â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ import_model_classes()                          â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                                                 â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚     if model_arch not in                        â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ model_arch_name_to_cls:                         â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚         raise ValueError(                       â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚             f"Unsupported architectures:        â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ {model_arch}. "                                 â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚             f"Supported list:                   â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ {list(model_arch_name_to_cls.keys())}"          â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚         )                                       â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚     return model_arch_name_to_cls               â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                                                 â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                                                 â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ # Monkey patch model loader                     â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ setattr(ModelRegistry, "load_model_cls",        â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ load_model_cls_srt)                             â”‚   â”‚
â”‚             â”‚ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚
â”‚ Total       â”‚ Input: 250057 Output: 2350                                                                             â”‚
â”‚ Tokens      â”‚                                                                                                        â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Status      â”‚ âœ… Step 15: Completed                                                                                  â”‚
â”‚ Tools       â”‚ ðŸ”§ bash                                                                                                â”‚
â”‚ bash        â”‚ â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚
â”‚             â”‚ â”‚ {'command': "sed -n '1,260p'                   â”‚ """A tensor parallel worker."""                 â”‚   â”‚
â”‚             â”‚ â”‚ /home/ubuntu/OmniPerf-Bench/perf-agents-benchâ€¦ â”‚                                                 â”‚   â”‚
â”‚             â”‚ â”‚ 'restart': False}                              â”‚ import asyncio                                  â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ import logging                                  â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ import time                                     â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ import warnings                                 â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ from concurrent.futures import                  â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ ThreadPoolExecutor                              â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ from typing import List, Optional               â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                                                 â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ import rpyc                                     â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ import torch                                    â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ from rpyc.utils.classic import obtain           â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                                                 â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ from sglang.global_config import global_config  â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ from sglang.srt.constrained.fsm_cache import    â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ FSMCache                                        â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ from sglang.srt.constrained.jump_forward import â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ JumpForwardCache                                â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ from sglang.srt.hf_transformers_utils import    â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ get_processor, get_tokenizer                    â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ from sglang.srt.managers.controller.infer_batch â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ import (                                        â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚     FINISH_ABORT,                               â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚     BaseFinishReason,                           â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚     Batch,                                      â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚     ForwardMode,                                â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚     Req,                                        â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ )                                               â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ from                                            â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ sglang.srt.managers.controller.model_runner     â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ import ModelRunner                              â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ from sglang.srt.managers.controller.radix_cache â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ import RadixCache                               â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ from                                            â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ sglang.srt.managers.controller.schedule_heurisâ€¦ â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ import ScheduleHeuristic                        â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ from sglang.srt.managers.io_struct import (     â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚     AbortReq,                                   â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚     BatchTokenIDOut,                            â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚     FlushCacheReq,                              â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚     TokenizedGenerateReqInput,                  â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ )                                               â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ from sglang.srt.model_config import ModelConfig â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ from sglang.srt.server_args import              â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ ModelPortArgs, ServerArgs                       â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ from sglang.srt.utils import (                  â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚     connect_rpyc_service,                       â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚     get_int_token_logit_bias,                   â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚     is_multimodal_model,                        â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚     set_random_seed,                            â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚     start_rpyc_service_process,                 â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚     suppress_other_loggers,                     â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ )                                               â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ from sglang.utils import                        â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ get_exception_traceback                         â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                                                 â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ logger = logging.getLogger("srt.tp_worker")     â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                                                 â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                                                 â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ class ModelTpServer:                            â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚     def __init__(                               â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚         self,                                   â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚         gpu_id: int,                            â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚         tp_rank: int,                           â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚         server_args: ServerArgs,                â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚         model_port_args: ModelPortArgs,         â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚         model_overide_args,                     â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚     ):                                          â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚         server_args, model_port_args =          â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ obtain(server_args), obtain(model_port_args)    â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚         suppress_other_loggers()                â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                                                 â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚         # Copy arguments                        â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚         self.gpu_id = gpu_id                    â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚         self.tp_rank = tp_rank                  â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚         self.tp_size = server_args.tp_size      â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚         self.dp_size = server_args.dp_size      â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚         self.schedule_heuristic =               â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ server_args.schedule_heuristic                  â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚         self.disable_regex_jump_forward =       â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ server_args.disable_regex_jump_forward          â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                                                 â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚         # Init model and tokenizer              â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚         self.model_config = ModelConfig(        â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚             server_args.model_path,             â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚             server_args.trust_remote_code,      â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚             context_length=server_args.contextâ€¦ â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚             model_overide_args=model_overide_aâ€¦ â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚         )                                       â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚         self.model_runner = ModelRunner(        â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚             model_config=self.model_config,     â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚             mem_fraction_static=server_args.meâ€¦ â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚             gpu_id=gpu_id,                      â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚             tp_rank=tp_rank,                    â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚             tp_size=server_args.tp_size,        â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚             nccl_port=model_port_args.nccl_porâ€¦ â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚             server_args=server_args,            â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚         )                                       â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                                                 â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚         if                                      â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ is_multimodal_model(server_args.model_path):    â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚             self.processor = get_processor(     â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                 server_args.tokenizer_path,     â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                 tokenizer_mode=server_args.tokâ€¦ â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                 trust_remote_code=server_args.â€¦ â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚             )                                   â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚             self.tokenizer =                    â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ self.processor.tokenizer                        â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚         else:                                   â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚             self.tokenizer = get_tokenizer(     â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                 server_args.tokenizer_path,     â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                 tokenizer_mode=server_args.tokâ€¦ â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                 trust_remote_code=server_args.â€¦ â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚             )                                   â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚         self.max_total_num_tokens =             â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ self.model_runner.max_total_num_tokens          â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚         self.max_prefill_tokens = (             â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚             16384                               â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚             if server_args.max_prefill_tokens   â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ is None                                         â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚             else server_args.max_prefill_tokens â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚         )                                       â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚         self.max_running_requests = (           â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚             self.max_total_num_tokens // 2      â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚             if server_args.max_running_requests â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ is None                                         â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚             else                                â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ server_args.max_running_requests                â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚         )                                       â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚         self.int_token_logit_bias =             â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ torch.tensor(                                   â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚             get_int_token_logit_bias(self.tokeâ€¦ â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ self.model_config.vocab_size)                   â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚         )                                       â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚         set_random_seed(server_args.random_seeâ€¦ â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                                                 â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚         # Print info                            â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚         logger.info(                            â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚             f" "                                â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚             f"max_total_num_tokens={self.max_tâ€¦ â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ "                                               â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚             f"max_prefill_tokens={self.max_preâ€¦ â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ "                                               â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚             f"context_len={self.model_config.câ€¦ â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚         )                                       â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚         if self.tp_rank == 0:                   â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚             logger.info(                        â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                 f" "                            â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                 f"server_args:                  â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ {server_args.print_mode_args()}"                â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚             )                                   â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                                                 â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚         # Init cache                            â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚         self.tree_cache = RadixCache(           â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚             req_to_token_pool=self.model_runneâ€¦ â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚             token_to_kv_pool=self.model_runnerâ€¦ â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚             disable=server_args.disable_radix_â€¦ â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚         )                                       â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚         self.tree_cache_metrics = {"total": 0,  â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ "hit": 0}                                       â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚         self.scheduler = ScheduleHeuristic(     â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚             self.schedule_heuristic,            â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚             self.max_running_requests,          â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚             self.max_prefill_tokens,            â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚             self.max_total_num_tokens,          â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚             self.tree_cache,                    â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚         )                                       â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚         self.req_to_token_pool =                â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ self.model_runner.req_to_token_pool             â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚         self.token_to_kv_pool =                 â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ self.model_runner.token_to_kv_pool              â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                                                 â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚         # Init running status                   â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚         self.forward_queue: List[Req] = []      â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚         self.running_batch: Batch = None        â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚         self.out_pyobjs = []                    â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚         self.decode_forward_ct = 0              â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚         self.stream_interval =                  â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ server_args.stream_interval                     â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚         self.num_generated_tokens = 0           â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚         self.last_stats_tic = time.time()       â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                                                 â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚         # Init the FSM cache for constrained    â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ generation                                      â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚         self.regex_fsm_cache = FSMCache(        â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚             server_args.tokenizer_path,         â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚             {                                   â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                 "tokenizer_mode":               â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ server_args.tokenizer_mode,                     â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                 "trust_remote_code":            â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ server_args.trust_remote_code,                  â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚             },                                  â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚         )                                       â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚         self.jump_forward_cache =               â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ JumpForwardCache()                              â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                                                 â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚         # Init new token estimation             â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚         assert (                                â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚             server_args.schedule_conservativenâ€¦ â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ >= 0                                            â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚         ), "Invalid schedule_conservativeness"  â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚         self.new_token_ratio = min(             â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚             global_config.base_new_token_ratio  â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ * server_args.schedule_conservativeness,        â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚             1.0,                                â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚         )                                       â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚         self.min_new_token_ratio = min(         â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚             global_config.base_min_new_token_râ€¦ â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚             *                                   â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ server_args.schedule_conservativeness,          â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚             1.0,                                â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚         )                                       â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚         self.new_token_ratio_decay =            â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ global_config.new_token_ratio_decay             â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚         self.new_token_ratio_recovery =         â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ global_config.new_token_ratio_recovery          â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                                                 â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚     def exposed_step(self, recv_reqs):          â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚         if self.tp_size * self.dp_size != 1:    â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚             recv_reqs = obtain(recv_reqs)       â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                                                 â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚         try:                                    â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚             # Recv requests                     â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚             for recv_req in recv_reqs:          â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                 if isinstance(recv_req,         â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ TokenizedGenerateReqInput):                     â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                     self.handle_generate_requeâ€¦ â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                 elif isinstance(recv_req,       â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ FlushCacheReq):                                 â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                     self.flush_cache()          â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                 elif isinstance(recv_req,       â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ AbortReq):                                      â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                     self.abort_request(recv_reâ€¦ â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                 else:                           â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                     raise ValueError(f"Invalid  â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ request: {recv_req}")                           â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                                                 â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚             # Forward                           â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚             self.forward_step()                 â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚         except Exception:                       â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚             logger.error("Exception in          â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ ModelTpServer:\n" + get_exception_traceback())  â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚             raise                               â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                                                 â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚         # Return results                        â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚         ret = self.out_pyobjs                   â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚         self.out_pyobjs = []                    â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚         return ret                              â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                                                 â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚     @torch.inference_mode()                     â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚     def forward_step(self):                     â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚         new_batch = self.get_new_fill_batch()   â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                                                 â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚         if new_batch is not None:               â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚             # Run a new fill batch              â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚             self.forward_fill_batch(new_batch)  â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚             self.cache_filled_batch(new_batch)  â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                                                 â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚             if not new_batch.is_empty():        â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                 if self.running_batch is None:  â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                     self.running_batch =        â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ new_batch                                       â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                 else:                           â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                     self.running_batch.merge(nâ€¦ â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚         else:                                   â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚             # Run decode batch                  â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚             if self.running_batch is not None:  â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                 # Run a few decode batches      â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ continuously for reducing overhead              â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                 for _ in                        â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ range(global_config.num_continue_decode_steps): â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                     self.num_generated_tokens   â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ += len(self.running_batch.reqs)                 â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                     self.forward_decode_batch(â€¦ â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                                                 â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                     # Print stats               â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                     if self.tp_rank == 0 and    â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ self.decode_forward_ct % 40 == 0:               â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                         num_used =              â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ self.max_total_num_tokens - (                   â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                             self.token_to_kv_pâ€¦ â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                             +                   â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ self.tree_cache.evictable_size()                â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                         )                       â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                         throughput =            â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ self.num_generated_tokens / (                   â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                             time.time() -       â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ self.last_stats_tic                             â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                         )                       â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                         self.num_generated_tokâ€¦ â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ = 0                                             â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                         self.last_stats_tic =   â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ time.time()                                     â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                         logger.info(            â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                             f" Decode batch. "  â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                             f"#running-req:     â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ {len(self.running_batch.reqs)}, "               â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                             f"#token:           â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ {num_used}, "                                   â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                             f"token usage:      â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ {num_used / self.max_total_num_tokens:.2f}, "   â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                             f"gen throughput    â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ (token/s): {throughput:.2f}, "                  â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                             f"#queue-req:       â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ {len(self.forward_queue)}"                      â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                         )                       â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                                                 â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                     if                          â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ self.running_batch.is_empty():                  â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                         self.running_batch =    â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ None                                            â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                         break                   â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                                                 â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                     if self.out_pyobjs and      â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ self.running_batch.has_stream():                â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                         break                   â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚             else:                               â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                 # Check the available size      â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                 available_size = (              â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                     self.token_to_kv_pool.avaiâ€¦ â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                     +                           â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ self.tree_cache.evictable_size()                â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                 )                               â”‚   â”‚
â”‚             â”‚ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚
â”‚ Total       â”‚ Input: 278233 Output: 2429                                                                             â”‚
â”‚ Tokens      â”‚                                                                                                        â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Status      â”‚ âœ… Step 16: Completed                                                                                  â”‚
â”‚ Tools       â”‚ ðŸ”§ bash                                                                                                â”‚
â”‚ bash        â”‚ â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚
â”‚             â”‚ â”‚ {'command': "sed -n '260,720p'                 â”‚                 )                               â”‚   â”‚
â”‚             â”‚ â”‚ /home/ubuntu/OmniPerf-Bench/perf-agents-benchâ€¦ â”‚                 if available_size !=            â”‚   â”‚
â”‚             â”‚ â”‚ 'restart': False}                              â”‚ self.max_total_num_tokens:                      â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                     warnings.warn(              â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                         "Warning: "             â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                         f"available_size={avaiâ€¦ â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ max_total_num_tokens={self.max_total_num_tokenâ€¦ â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                         "KV cache pool leak     â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ detected!"                                      â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                     )                           â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                                                 â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚     def handle_generate_request(                â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚         self,                                   â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚         recv_req: TokenizedGenerateReqInput,    â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚     ):                                          â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚         req = Req(recv_req.rid,                 â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ recv_req.input_text, recv_req.input_ids)        â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚         req.pixel_values =                      â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ recv_req.pixel_values                           â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚         if req.pixel_values is not None:        â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚             req.pad_value = [                   â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                 (recv_req.image_hash) %         â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ self.model_config.vocab_size,                   â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                 (recv_req.image_hash >> 16) %   â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ self.model_config.vocab_size,                   â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                 (recv_req.image_hash >> 32) %   â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ self.model_config.vocab_size,                   â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                 (recv_req.image_hash >> 64) %   â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ self.model_config.vocab_size,                   â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚             ]                                   â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚             req.image_size =                    â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ recv_req.image_size                             â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚             (                                   â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                 req.origin_input_ids,           â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                 req.image_offset,               â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚             ) =                                 â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ self.model_runner.model.pad_input_ids(          â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                 req.origin_input_ids_unpadded,  â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                 req.pad_value,                  â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                 req.pixel_values.shape,         â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                 req.image_size,                 â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚             )                                   â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚         req.sampling_params =                   â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ recv_req.sampling_params                        â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚         req.return_logprob =                    â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ recv_req.return_logprob                         â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚         req.logprob_start_len =                 â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ recv_req.logprob_start_len                      â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚         req.top_logprobs_num =                  â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ recv_req.top_logprobs_num                       â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚         req.stream = recv_req.stream            â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚         req.tokenizer = self.tokenizer          â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                                                 â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚         # Init regex fsm                        â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚         if req.sampling_params.regex is not     â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ None:                                           â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚             req.regex_fsm =                     â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ self.regex_fsm_cache.query(req.sampling_paramsâ€¦ â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚             if not                              â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ self.disable_regex_jump_forward:                â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                 req.jump_forward_map =          â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ self.jump_forward_cache.query(                  â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                     req.sampling_params.regex   â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                 )                               â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                                                 â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚         # Truncate prompts that are too long    â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚         req.origin_input_ids =                  â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ req.origin_input_ids[:                          â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ self.model_config.context_len - 1]              â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚         req.sampling_params.max_new_tokens =    â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ min(                                            â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚             req.sampling_params.max_new_tokens, â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚             self.model_config.context_len - 1 - â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ len(req.origin_input_ids),                      â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚             self.max_total_num_tokens - 128 -   â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ len(req.origin_input_ids),                      â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚         )                                       â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚         self.forward_queue.append(req)          â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                                                 â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚     def get_new_fill_batch(self) ->             â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ Optional[Batch]:                                â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚         running_bs = (                          â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚             len(self.running_batch.reqs) if     â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ self.running_batch is not None else 0           â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚         )                                       â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚         if running_bs >=                        â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ self.max_running_requests:                      â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚             return                              â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                                                 â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚         # Compute matched prefix length         â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚         for req in self.forward_queue:          â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚             req.input_ids =                     â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ req.origin_input_ids + req.output_ids           â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚             prefix_indices, last_node =         â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ self.tree_cache.match_prefix(req.input_ids)     â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚             if req.return_logprob:              â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                 prefix_indices =                â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ prefix_indices[: req.logprob_start_len]         â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚             req.extend_input_len =              â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ len(req.input_ids) - len(prefix_indices)        â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚             req.prefix_indices = prefix_indices â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚             req.last_node = last_node           â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                                                 â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚         # Get priority queue                    â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚         self.forward_queue =                    â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ self.scheduler.get_priority_queue(self.forwardâ€¦ â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                                                 â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚         # Add requests if there is available    â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ space                                           â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚         can_run_list = []                       â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚         new_batch_total_tokens = 0              â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚         new_batch_input_tokens = 0              â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                                                 â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚         available_size = (                      â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚             self.token_to_kv_pool.available_siâ€¦ â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ + self.tree_cache.evictable_size()              â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚         )                                       â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚         if self.running_batch:                  â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚             available_size -= sum(              â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                 [                               â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                     (r.sampling_params.max_newâ€¦ â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ - len(r.output_ids)) * self.new_token_ratio     â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                     for r in                    â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ self.running_batch.reqs                         â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                 ]                               â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚             )                                   â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                                                 â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚         for req in self.forward_queue:          â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚             if req.return_logprob and           â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ req.normalized_prompt_logprob is None:          â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                 # Need at least two tokens to   â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ compute normalized logprob                      â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                 if req.extend_input_len < 2:    â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                     delta = 2 -                 â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ req.extend_input_len                            â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                     req.extend_input_len +=     â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ delta                                           â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                     req.prefix_indices =        â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ req.prefix_indices[:-delta]                     â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                     if req.image_offset is not  â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ None:                                           â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                         req.image_offset +=     â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ delta                                           â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚             if req.extend_input_len == 0 and    â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ req.sampling_params.max_new_tokens > 0:         â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                 # Need at least one token to    â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ compute logits                                  â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                 req.extend_input_len = 1        â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                 req.prefix_indices =            â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ req.prefix_indices[:-1]                         â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                 if req.image_offset is not      â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ None:                                           â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                     req.image_offset += 1       â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                                                 â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚             if (                                â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                 req.extend_input_len +          â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ req.sampling_params.max_new_tokens +            â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ new_batch_total_tokens                          â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                 < available_size                â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                 and (                           â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                     req.extend_input_len +      â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ new_batch_input_tokens                          â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                     <= self.max_prefill_tokens  â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                     or len(can_run_list) == 0   â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                 )                               â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚             ):                                  â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                 delta =                         â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ self.tree_cache.inc_lock_ref(req.last_node)     â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                 available_size += delta         â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                                                 â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                 if not (                        â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                     req.extend_input_len +      â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ req.sampling_params.max_new_tokens +            â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ new_batch_total_tokens                          â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                     < available_size            â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                 ):                              â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                     # Undo locking              â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                     delta =                     â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ self.tree_cache.dec_lock_ref(req.last_node)     â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                     available_size += delta     â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                     break                       â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                 else:                           â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                     # Add this request to the   â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ running batch                                   â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                     can_run_list.append(req)    â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                     new_batch_total_tokens += ( â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                         req.extend_input_len +  â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ req.sampling_params.max_new_tokens              â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                     )                           â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                     new_batch_input_tokens +=   â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ req.extend_input_len                            â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚             else:                               â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                 break                           â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                                                 â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚             if running_bs + len(can_run_list)   â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ >= self.max_running_requests:                   â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                 break                           â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                                                 â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚         if len(can_run_list) == 0:              â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚             return None                         â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                                                 â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚         # Print stats                           â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚         if self.tp_rank == 0:                   â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚             hit_tokens =                        â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ sum(len(x.prefix_indices) for x in              â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ can_run_list)                                   â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚             self.tree_cache_metrics["total"] += â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ (                                               â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                 hit_tokens +                    â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ new_batch_input_tokens                          â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚             ) / 10**9                           â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚             self.tree_cache_metrics["hit"] +=   â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ hit_tokens / 10**9                              â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚             tree_cache_hit_rate = (             â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                 self.tree_cache_metrics["hit"]  â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ / self.tree_cache_metrics["total"]              â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚             )                                   â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚             logger.info(                        â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                 f" Prefill batch. "             â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                 f"#new-seq:                     â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ {len(can_run_list)}, "                          â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                 f"#new-token:                   â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ {new_batch_input_tokens}, "                     â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                 f"#cached-token: {hit_tokens},  â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ "                                               â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                 f"cache hit rate: {100.0 *      â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ tree_cache_hit_rate:.2f}%, "                    â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                 f"#running-req: {running_bs}, " â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                 f"#queue-req:                   â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ {len(self.forward_queue) - len(can_run_list)}"  â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚             )                                   â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚             # logger.debug(                     â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚             #    f"fsm_cache_hit_rate: {100.0 * â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ self.regex_fsm_cache.get_cache_hit_rate():.2f}â€¦ â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ "                                               â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚             #    f"fsm_cache_avg_init_time:     â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ {self.regex_fsm_cache.get_avg_init_time():.2f}â€¦ â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ "                                               â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚             #    f"ff_cache_hit_rate: {100.0 *  â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ self.jump_forward_cache.get_cache_hit_rate():.â€¦ â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ "                                               â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚             #    f"ff_cache_avg_init_time:      â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ {self.jump_forward_cache.get_avg_init_time():.â€¦ â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ "                                               â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚             # )                                 â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                                                 â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚         # Return the new batch                  â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚         new_batch = Batch.init_new(             â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚             can_run_list,                       â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚             self.req_to_token_pool,             â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚             self.token_to_kv_pool,              â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚             self.tree_cache,                    â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚         )                                       â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚         self.forward_queue =                    â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚         return new_batch                        â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                                                 â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚     def forward_fill_batch(self, batch: Batch): â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚         # Build batch tensors                   â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚         batch.prepare_for_extend(               â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚             self.model_config.vocab_size,       â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ self.int_token_logit_bias                       â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚         )                                       â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                                                 â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚         # Forward and sample the next tokens    â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚         if batch.extend_num_tokens != 0:        â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚             output =                            â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ self.model_runner.forward(batch,                â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ ForwardMode.EXTEND)                             â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚             next_token_ids, _ =                 â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ batch.sample(output.next_token_logits)          â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                                                 â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚             # Move logprobs to cpu              â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚             if output.next_token_logprobs is    â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ not None:                                       â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                 output.next_token_logprobs =    â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ output.next_token_logprobs[                     â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                     torch.arange(len(next_tokeâ€¦ â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ device=next_token_ids.device),                  â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                     next_token_ids,             â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                 ].tolist()                      â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                 output.prefill_token_logprobs = â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ output.prefill_token_logprobs.tolist()          â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                 output.normalized_prompt_logprâ€¦ â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ = (                                             â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                     output.normalized_prompt_lâ€¦ â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                 )                               â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                                                 â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚             next_token_ids =                    â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ next_token_ids.tolist()                         â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚         else:                                   â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚             next_token_ids =  * len(batch.reqs) â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                                                 â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚         # Check finish conditions               â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚         pt = 0                                  â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚         for i, req in enumerate(batch.reqs):    â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚             req.completion_tokens_wo_jump_forwâ€¦ â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ += 1                                            â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚             req.output_ids.append(next_token_iâ€¦ â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ [3m            req.check_finished()[0m                â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                                                 â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ [3m            if req.return_logprob:[0m              â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ [3m                self.add_logprob_return_valuesâ€¦[0m â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ [3mreq, pt, next_token_ids, output)[0m                â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ [3m                pt += req.extend_input_len[0m      â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                                                 â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ [3m        self.handle_finished_requests(batch)[0m    â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                                                 â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ [3m    def add_logprob_return_values(self, i, req,[0m â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ [3mpt, next_token_ids, output):[0m                    â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ [3m        if req.normalized_prompt_logprob is [0m    â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ [3mNone:[0m                                           â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ [3m            req.normalized_prompt_logprob = [0m    â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ [3moutput.normalized_prompt_logprobs[0m               â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                                                 â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ [3m        if req.prefill_token_logprobs is None:[0m  â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ [3m            # If logprob_start_len > 0, then [0m   â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ [3mfirst logprob_start_len prompt tokens will be [0m  â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ [3mignored.[0m                                        â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ [3m            req.prefill_token_logprobs = list([0m  â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ [3m                zip([0m                            â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ [3m                    output.prefill_token_logprâ€¦[0m â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ [3m                    req.input_ids[-req.extend_â€¦[0m â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ [3m+ 1 :],[0m                                         â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ [3m                )[0m                               â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ [3m            )[0m                                   â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ [3m            if req.logprob_start_len == 0:[0m      â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ [3m                req.prefill_token_logprobs = [[0m  â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ [3m                    (None, req.input_ids[0])[0m    â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ [3m                ] + req.prefill_token_logprobs[0m  â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                                                 â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ [3m        if req.last_update_decode_tokens != 0:[0m  â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ [3m            req.decode_token_logprobs.extend([0m   â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ [3m                list([0m                           â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ [3m                    zip([0m                        â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ [3m                        output.prefill_token_lâ€¦[0m â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ [3m                            pt[0m                  â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ [3m                            + [0m                  â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ [3mreq.extend_input_len[0m                            â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ [3m                            - [0m                  â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ [3mreq.last_update_decode_tokens : pt[0m              â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ [3m                            + [0m                  â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ [3mreq.extend_input_len[0m                            â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ [3m                            - 1[0m                 â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ [3m                        ],[0m                      â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ [3m                        req.input_ids[-req.lasâ€¦[0m â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ [3m+ 1 :],[0m                                         â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ [3m                    )[0m                           â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ [3m                )[0m                               â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ [3m            )[0m                                   â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                                                 â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ [3m        req.decode_token_logprobs.append([0m       â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ [3m            (output.next_token_logprobs[0m[3m, [0m       â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ [3mnext_token_ids[0m[3m)[0m                                 â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ [3m        )[0m                                       â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                                                 â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ [3m        if req.top_logprobs_num > 0:[0m            â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ [3m            if req.prefill_top_logprobs is [0m     â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ [3mNone:[0m                                           â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ [3m                req.prefill_top_logprobs = [0m     â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ [3moutput.prefill_top_logprobs[0m                     â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ [3m                if req.logprob_start_len == 0:[0m  â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ [3m                    req.prefill_top_logprobs = [0m â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ [3m[None] + req.prefill_top_logprobs[0m               â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                                                 â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ [3m            if req.last_update_decode_tokens !=[0m â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ [3m0:[0m                                              â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ [3m                req.decode_top_logprobs.extend([0m â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ [3m                    output.prefill_top_logprobâ€¦[0m â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ [3m+ 1 :][0m                                          â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ [3m                )[0m                               â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ [3m            req.decode_top_logprobs.append(outâ€¦[0m â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                                                 â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ [3m    def cache_filled_batch(self, batch: Batch):[0m â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ [3m        req_pool_indices_cpu = [0m                 â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ [3mbatch.req_pool_indices.cpu().numpy()[0m            â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ [3m        for i, req in enumerate(batch.reqs):[0m    â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ [3m            new_prefix_indices, new_last_node =[0m â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ [3mself.tree_cache.cache_req([0m                      â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ [3m                token_ids=tuple(req.origin_inpâ€¦[0m â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ [3m+ req.output_ids)[:-1],[0m                         â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ [3m                last_uncached_pos=len(req.prefâ€¦[0m â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ [3m                req_pool_idx=req_pool_indices_â€¦[0m â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ [3m                del_in_memory_pool=False,[0m       â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ [3m                old_last_node=req.last_node,[0m    â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ [3m            )[0m                                   â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ [3m            req.prefix_indices, req.last_node =[0m â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ [3mnew_prefix_indices, new_last_node[0m               â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                                                 â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ [3m    def forward_decode_batch(self, batch: [0m      â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ [3mBatch):[0m                                         â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ [3m        # Check if decode out of memory[0m         â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ [3m        if not batch.check_decode_mem():[0m        â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ [3m            old_ratio = self.new_token_ratio[0m    â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ [3m            self.new_token_ratio = [0m             â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ [3mmin(old_ratio + self.new_token_ratio_recovery, [0m â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ [3m1.0)[0m                                            â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                                                 â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ [3m            retracted_reqs = [0m                   â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ [3mbatch.retract_decode()[0m                          â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ [3m            logger.info([0m                        â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ [3m                "decode out of memory happened,[0m â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ [3m"[0m                                               â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ [3m                f"#retracted_reqs: [0m             â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ [3m{len(retracted_reqs)}, "[0m                        â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ [3m                f"#new_token_ratio: [0m            â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ [3m{old_ratio:.4f} -> {self.new_token_ratio:.4f}"[0m  â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ [3m            )[0m                                   â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ [3m            self.forward_queue.extend(retracteâ€¦[0m â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ [3m        else:[0m                                   â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ [3m            self.new_token_ratio = max([0m         â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ [3m                self.new_token_ratio - [0m         â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ [3mself.new_token_ratio_decay,[0m                     â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ [3m                self.min_new_token_ratio,[0m       â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ [3m            )[0m                                   â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                                                 â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ [3m        if not self.disable_regex_jump_forward:[0m â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ [3m            # Check for jump-forward[0m            â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ [3m            jump_forward_reqs = [0m                â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ [3mbatch.check_for_jump_forward(self.model_runner)[0m â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ [3m            self.forward_queue.extend(jump_forâ€¦[0m â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ [3m            if batch.is_empty():[0m                â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ [3m                return[0m                          â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                                                 â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ [3m        # Update batch tensors[0m                  â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ [3m        self.decode_forward_ct = [0m               â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ [3m(self.decode_forward_ct + 1) % (1 << 30)[0m        â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ [3m        batch.prepare_for_decode()[0m              â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                                                 â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ [3m        # Forward and sample the next tokens[0m    â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ [3m        output = [0m                               â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ [3mself.model_runner.forward(batch, [0m               â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ [3mForwardMode.DECODE)[0m                             â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ [3m        next_token_ids, _ = [0m                    â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ [3mbatch.sample(output.next_token_logits)[0m          â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                                                 â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ [3m        # Move logprobs to cpu[0m                  â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ [3m        if output.next_token_logprobs is not [0m   â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ [3mNone:[0m                                           â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ [3m            next_token_logprobs = [0m              â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ [3moutput.next_token_logprobs[[0m                     â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ [3m                torch.arange(len(next_token_idâ€¦[0m â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ [3mdevice=next_token_ids.device),[0m                  â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ [3m                next_token_ids,[0m                 â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ [3m            ].tolist()[0m                          â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                                                 â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ [3m        next_token_ids = [0m                       â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ [3mnext_token_ids.tolist()[0m                         â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                                                 â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ [3m        # Check finish condition[0m                â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ [3m        for i, (req, next_token_id) in [0m         â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ [3menumerate(zip(batch.reqs, next_token_ids)):[0m     â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ [3m            req.completion_tokens_wo_jump_forwâ€¦[0m â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ [3m+= 1[0m                                            â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ [3m            req.output_ids.append(next_token_iâ€¦[0m â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ [3m            req.check_finished()[0m                â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                                                 â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ [3m            if req.return_logprob:[0m              â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ [3m                req.decode_token_logprobs.appeâ€¦[0m â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ [3m                    (next_token_logprobs[0m[3m, [0m      â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ [3mnext_token_id)[0m                                  â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ [3m                )[0m                               â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ [3m                if req.top_logprobs_num > 0:[0m    â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ [3m                    req.decode_top_logprobs.apâ€¦[0m â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                                                 â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ [3m        self.handle_finished_requests(batch)[0m    â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                                                 â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ [3m    def handle_finished_requests(self, batch: [0m  â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ [3mBatch):[0m                                         â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ [3m        output_rids = [][0m                        â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ [3m        decoded_texts = [][0m                      â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ [3m        surr_output_ids = [][0m                    â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ [3m        read_output_ids = [][0m                    â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ [3m        output_skip_special_tokens = [][0m         â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ [3m        output_spaces_between_special_tokens = [0m â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ [3m[][0m                                              â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ [3m        output_meta_info = [][0m                   â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ [3m        output_finished_reason: [0m                â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ [3mList[BaseFinishReason] = [][0m                     â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ [3m        finished_indices = [][0m                   â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ [3m        unfinished_indices = [][0m                 â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ [3m        for i, req in enumerate(batch.reqs):[0m    â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ [3m            if req.finished():[0m                  â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ [3m                finished_indices.append(i)[0m      â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ [3m            else:[0m                               â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ [3m                unfinished_indices.append(i)[0m    â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                                                 â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ [3m            if req.finished() or ([0m              â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ [3m                ([0m                               â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ [3m                    req.stream[0m                  â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ [3m                    and ([0m                       â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ [3m                        self.decode_forward_ct [0m â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ [3m% self.stream_interval == 0[0m                     â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ [3m                        or len(req.output_ids) [0m â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ [3m== 1[0m                                            â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ [3m                    )[0m                           â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ [3m                )[0m                               â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ [3m            ):[0m                                  â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ [3m                output_rids.append(req.rid)[0m     â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ [3m                decoded_texts.append(req.decodâ€¦[0m â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ [3m                surr_ids, read_ids, _ = [0m        â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ [3mreq.init_detokenize_incrementally()[0m             â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ [3m                surr_output_ids.append(surr_idâ€¦[0m â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ [3m                read_output_ids.append(read_idâ€¦[0m â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ [3m                output_skip_special_tokens.appâ€¦[0m â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ [3m                    req.sampling_params.skip_sâ€¦[0m â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ [3m                )[0m                               â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ [3m                output_spaces_between_special_â€¦[0m â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ [3m                    req.sampling_params.spacesâ€¦[0m â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ [3m                )[0m                               â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                                                 â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ [3m                meta_info = {[0m                   â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ [3m                    "prompt_tokens": [0m           â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ [3mlen(req.origin_input_ids),[0m                      â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ [3m                    "completion_tokens": [0m       â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ [3mlen(req.output_ids),[0m                            â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ [3m                    "completion_tokens_wo_jumpâ€¦[0m â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ [3mreq.completion_tokens_wo_jump_forward,[0m          â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ [3m                    "finish_reason": [0m           â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ [3mstr(req.finished_reason),[0m                       â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ [3m                }[0m                               â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ [3m                if req.return_logprob:[0m          â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ [3m                    ([0m                           â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ [3m                        meta_info["prefill_tokâ€¦[0m â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ [3m                        meta_info["decode_tokeâ€¦[0m â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ [3m                        meta_info["prefill_topâ€¦[0m â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ [3m                        meta_info["decode_top_â€¦[0m â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ [3m                        meta_info["normalized_â€¦[0m â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ [3m                    ) = ([0m                       â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ [3m                        req.prefill_token_logpâ€¦[0m â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ [3m                        req.decode_token_logprâ€¦[0m â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ [3m                        req.prefill_top_logproâ€¦[0m â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ [3m                        req.decode_top_logprobâ€¦[0m â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ [3m                        req.normalized_prompt_â€¦[0m â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ [3m                    )[0m                           â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ [3m                output_meta_info.append(meta_iâ€¦[0m â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ [3m                output_finished_reason.append(â€¦[0m â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                                                 â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ [3m        # Send to detokenizer[0m                   â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ [3m        if output_rids:[0m                         â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ [3m            self.out_pyobjs.append([0m             â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ [3m                BatchTokenIDOut([0m                â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ [3m                    output_rids,[0m                â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ [3m                    decoded_texts,[0m              â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ [3m                    surr_output_ids,[0m            â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ [3m                    read_output_ids,[0m            â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ [3m                    output_skip_special_tokens,[0m â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ [3m                    output_spaces_between_specâ€¦[0m â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ [3m                    output_meta_info,[0m           â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ [3m                    output_finished_reason,[0m     â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ [3m                )[0m                               â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ [3m            )[0m                                   â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                                                 â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ [3m        # Remove finished reqs[0m                  â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ [3m        if finished_indices:[0m                    â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ [3m            # Update radix cache[0m                â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ [3m            req_pool_indices_cpu = [0m             â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ [3mbatch.req_pool_indices.tolist()[0m                 â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ [3m            for i in finished_indices:[0m          â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ [3m                req = batch.reqs[0m                â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ [3m                self.tree_cache.cache_req([0m      â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ [3m                    token_ids=tuple(req.originâ€¦[0m â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ [3m+ req.output_ids)[:-1],[0m                         â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ [3m                    last_uncached_pos=len(req.â€¦[0m â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ [3m                    req_pool_idx=req_pool_indiâ€¦[0m â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ [3m                )[0m                               â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                                                 â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ [3m                self.tree_cache.dec_lock_ref(râ€¦[0m â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                                                 â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ [3m            # Update batch tensors[0m              â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ [3m            if unfinished_indices:[0m              â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ [3m                batch.filter_batch(unfinished_â€¦[0m â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ [3m            else:[0m                               â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ [3m                batch.reqs = [][0m                 â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                                                 â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ [3m    def flush_cache(self):[0m                      â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ [3m        if len(self.forward_queue) == 0 and ([0m   â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ [3m            self.running_batch is None or [0m      â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ [3mlen(self.running_batch.reqs) == 0[0m               â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ [3m        ):[0m                                      â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ [3m            self.tree_cache.reset()[0m             â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ [3m            self.tree_cache_metrics = {"total":[0m â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ [3m0, "hit": 0}[0m                                    â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ [3m            self.regex_fsm_cache.reset()[0m        â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ [3m            self.req_to_token_pool.clear()[0m      â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ [3m            self.token_to_kv_pool.clear()[0m       â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ [3m            torch.cuda.empty_cache()[0m            â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ [3m            logger.info("Cache flushed [0m         â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ [3msuccessfully!")[0m                                 â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ [3m        else:[0m                                   â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ [3m            warnings.warn([0m                      â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ [3m                f"Cache not flushed because [0m    â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ [3mthere are pending requests. "[0m                   â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ [3m                f"#queue-req: [0m                  â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ [3m{len(self.forward_queue)}, "[0m                    â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ [3m                f"#running-req: {0 if [0m          â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ [3mself.running_batch is None else [0m                â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ [3mlen(self.running_batch.reqs)}"[0m                  â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ [3m            )[0m                                   â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                                                 â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ [3m    def abort_request(self, recv_req):[0m          â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ [3m        # Delete requests in the waiting queue[0m  â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ [3m        to_del = None[0m                           â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ [3m        for i, req in [0m                          â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ [3menumerate(self.forward_queue):[0m                  â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ [3m            if req.rid == recv_req.rid:[0m         â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ [3m                to_del = i[0m                      â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ [3m                break[0m                           â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                                                 â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ [3m        if to_del is not None:[0m                  â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ [3m            del self.forward_queue[0m              â”‚   â”‚
â”‚             â”‚ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚
â”‚ Total       â”‚ Input: 308447 Output: 2508                                                                             â”‚
â”‚ Tokens      â”‚                                                                                                        â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Status      â”‚ âœ… Step 17: Completed                                                                                  â”‚
â”‚ Tools       â”‚ ðŸ”§ bash                                                                                                â”‚
â”‚ bash        â”‚ â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚
â”‚             â”‚ â”‚ {'command': "sed -n '720,1200p'                â”‚             del self.forward_queue              â”‚   â”‚
â”‚             â”‚ â”‚ /home/ubuntu/OmniPerf-Bench/perf-agents-benchâ€¦ â”‚                                                 â”‚   â”‚
â”‚             â”‚ â”‚ 'restart': False}                              â”‚         # Delete requests in the running batch  â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚         if self.running_batch:                  â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚             for req in self.running_batch.reqs: â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                 if req.rid == recv_req.rid:     â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                     req.finished_reason =       â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ FINISH_ABORT()                                  â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                     break                       â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                                                 â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                                                 â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ class ModelTpService(rpyc.Service):             â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚     exposed_ModelTpServer = ModelTpServer       â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                                                 â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                                                 â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ class ModelTpClient:                            â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚     def __init__(                               â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚         self,                                   â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚         gpu_ids: List,                          â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚         server_args: ServerArgs,                â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚         model_port_args: ModelPortArgs,         â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚         model_overide_args,                     â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚     ):                                          â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚         server_args, model_port_args =          â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ obtain(server_args), obtain(model_port_args)    â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚         self.tp_size = server_args.tp_size      â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                                                 â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚         if self.tp_size * server_args.dp_size   â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ == 1:                                           â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚             # Init model                        â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚             assert len(gpu_ids) == 1            â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚             self.model_server =                 â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ ModelTpService().exposed_ModelTpServer(         â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                 0,                              â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                 gpu_ids[0],                     â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                 server_args,                    â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                 model_port_args,                â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                 model_overide_args,             â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚             )                                   â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                                                 â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚             # Wrap functions                    â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚             def async_wrap(f):                  â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                 async def _func(*args,          â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ **kwargs):                                      â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                     return f(*args, **kwargs)   â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                                                 â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                 return _func                    â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                                                 â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚             self.step =                         â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ async_wrap(self.model_server.exposed_step)      â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚         else:                                   â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚             with                                â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ ThreadPoolExecutor(self.tp_size) as executor:   â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                 # Launch model processes        â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                 if server_args.nnodes == 1:     â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                     self.procs = list(          â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                         executor.map(           â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                             lambda args:        â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ start_rpyc_service_process(*args),              â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                             [                   â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                                 (ModelTpServicâ€¦ â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ p)                                              â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                                 for p in        â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ model_port_args.model_tp_ports                  â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                             ],                  â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                         )                       â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                     )                           â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                     addrs = [("localhost", p)   â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ for p in model_port_args.model_tp_ports]        â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                 else:                           â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                     addrs = [                   â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                         (ip, port)              â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                         for ip, port in zip(    â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                             model_port_args.moâ€¦ â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ model_port_args.model_tp_ports                  â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                         )                       â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                     ]                           â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                                                 â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                 self.model_services = list(     â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                     executor.map(lambda args:   â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ connect_rpyc_service(*args), addrs)             â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                 )                               â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                                                 â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                 # Init model                    â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                 def init_model(i):              â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                     return                      â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ self.model_services[3m.ModelTpServer([0m              â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ [3m                        gpu_ids[0m[3m,[0m                â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ [3m                        i,[0m                      â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ [3m                        server_args,[0m            â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ [3m                        model_port_args,[0m        â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ [3m                        model_overide_args,[0m     â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ [3m                    )[0m                           â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                                                 â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ [3m                self.model_servers = [0m           â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ [3mlist(executor.map(init_model, [0m                  â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ [3mrange(self.tp_size)))[0m                           â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                                                 â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ [3m            # Wrap functions[0m                    â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ [3m            def async_wrap(func_name):[0m          â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ [3m                fs = [0m                           â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                                                 â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ [3m                async def _func(*args, [0m         â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ [3m**kwargs):[0m                                      â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ [3m                    tasks = [0m                    â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ [3m                    await asyncio.gather(*[0m[3m)[0m     â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ [3m                    return [0m                     â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ [3mobtain(tasks[0].value)[0m                          â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                                                 â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ [3m                return _func[0m                    â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                                                 â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ [3m            self.step = async_wrap("step")[0m      â”‚   â”‚
â”‚             â”‚ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚
â”‚ Total       â”‚ Input: 342521 Output: 2588                                                                             â”‚
â”‚ Tokens      â”‚                                                                                                        â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Status      â”‚ âœ… Step 18: Completed                                                                                  â”‚
â”‚ Tools       â”‚ ðŸ”§ bash                                                                                                â”‚
â”‚ bash        â”‚ â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚
â”‚             â”‚ â”‚ {'command': "sed -n '1,240p'                   â”‚ <div align="center">                            â”‚   â”‚
â”‚             â”‚ â”‚ /home/ubuntu/OmniPerf-Bench/perf-agents-benchâ€¦ â”‚ <img src="assets/logo.png" alt="logo"           â”‚   â”‚
â”‚             â”‚ â”‚ 'restart': False}                              â”‚ width="400"></img>                              â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ </div>                                          â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                                                 â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ ----------------------------------------------â€¦ â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                                                 â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ |                                               â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ [**Blog**](https://lmsys.org/blog/2024-01-17-sâ€¦ â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ | [**Paper**](https://arxiv.org/abs/2312.07104) â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ |                                               â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                                                 â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ SGLang is a structured generation language      â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ designed for large language models (LLMs).      â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ It makes your interaction with LLMs faster and  â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ more controllable by co-designing the frontend  â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ language and the runtime system.                â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                                                 â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ The core features include:                      â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ - **Flexible Frontend Language**: Enables easy  â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ programming of LLM applications with chained    â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ generation calls, advanced prompting, control   â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ flow, multiple modalities, parallelism, and     â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ external interactions.                          â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ - **High-Performance Backend Runtime**:         â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ Features RadixAttention for accelerating        â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ complex LLM programs by reusing the KV cache    â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ across multiple calls. It can also serve as a   â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ standalone inference engine with all common     â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ techniques implemented (e.g., continuous        â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ batching and tensor parallelism).               â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                                                 â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ ## News                                         â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ - [2024/02] ðŸ”¥ SGLang enables **3x faster JSON  â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ decoding** with compressed finite state machine â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ ((https://lmsys.org/blog/2024-02-05-compressedâ€¦ â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ - [2024/01] ðŸ”¥ SGLang powers the serving of thâ€¦ â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ official **LLaVA v1.6** release demo            â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ ((https://github.com/haotian-liu/LLaVA?tab=reaâ€¦ â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ - [2024/01] SGLang provides up to **5x faster   â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ inference** with RadixAttention                 â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ ((https://lmsys.org/blog/2024-01-17-sglang/)).  â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                                                 â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ ## Contents                                     â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ - [Install](#install)                           â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ - [Quick Start](#quick-start)                   â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ - [Frontend: Structured Generation Language     â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ (SGLang)](#frontend-structured-generation-langâ€¦ â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ - [Backend: SGLang Runtime                      â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ (SRT)](#backend-sglang-runtime-srt)             â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ - [Benchmark And                                â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ Performance](#benchmark-and-performance)        â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ - [Roadmap](#roadmap)                           â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ - [Citation And                                 â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ Acknowledgment](#citation-and-acknowledgment)   â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                                                 â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ ## Install                                      â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                                                 â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ ### Method 1: With pip                          â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ ```                                             â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ pip install "sglang"                            â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                                                 â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ # Install FlashInfer CUDA kernels               â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ pip install flashinfer -i                       â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ https://flashinfer.ai/whl/cu121/torch2.3/       â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ ```                                             â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                                                 â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ ### Method 2: From source                       â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ ```                                             â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ git clone                                       â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ https://github.com/sgl-project/sglang.git       â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ cd sglang                                       â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                                                 â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ pip install -e "python"                         â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                                                 â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ # Install FlashInfer CUDA kernels               â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ pip install flashinfer -i                       â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ https://flashinfer.ai/whl/cu121/torch2.3/       â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ ```                                             â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                                                 â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ ### Method 3: Using docker                      â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ The docker images are available on Docker Hub   â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ as                                              â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ (https://hub.docker.com/r/lmsysorg/sglang/tagsâ€¦ â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                                                 â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ ### Common Notes                                â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ - If you see errors from the Triton compiler,   â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ please install the [Triton                      â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ Nightly](https://triton-lang.org/main/getting-â€¦ â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ by                                              â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ ```                                             â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ pip uninstall -y triton triton-nightly          â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ pip install -U --index-url                      â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ https://aiinfra.pkgs.visualstudio.com/PublicPaâ€¦ â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ triton-nightly                                  â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ ```                                             â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ - If you cannot install FlashInfer, check out   â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ its                                             â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ (https://docs.flashinfer.ai/installation.html#) â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ page. If you still cannot install it, you can   â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ use the slower Triton kernels by adding         â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ `--disable-flashinfer` when launching the       â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ server.                                         â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ - If you only need to use the OpenAI backend,   â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ you can avoid installing other dependencies by  â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ using `pip install "sglang"`.                   â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                                                 â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ ## Quick Start                                  â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ The example below shows how to use sglang to    â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ answer a mulit-turn question.                   â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                                                 â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ ### Using Local Models                          â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ First, launch a server with                     â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ ```                                             â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ python -m sglang.launch_server --model-path     â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ meta-llama/Llama-2-7b-chat-hf --port 30000      â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ ```                                             â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                                                 â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ Then, connect to the server and answer a        â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ multi-turn question.                            â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                                                 â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ ```python                                       â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ from sglang import function, system, user,      â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ assistant, gen, set_default_backend,            â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ RuntimeEndpoint                                 â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                                                 â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ @function                                       â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ def multi_turn_question(s, question_1,          â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ question_2):                                    â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚     s += system("You are a helpful assistant.") â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚     s += user(question_1)                       â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚     s += assistant(gen("answer_1",              â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ max_tokens=256))                                â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚     s += user(question_2)                       â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚     s += assistant(gen("answer_2",              â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ max_tokens=256))                                â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                                                 â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ set_default_backend(RuntimeEndpoint("http://loâ€¦ â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                                                 â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ state = multi_turn_question.run(                â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚     question_1="What is the capital of the      â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ United States?",                                â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚     question_2="List two local attractions.",   â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ )                                               â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                                                 â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ for m in state.messages():                      â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚     print(m["role"], ":", m["content"])         â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                                                 â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ print(state["answer_1"])                        â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ ```                                             â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                                                 â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ ### Using OpenAI Models                         â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ Set the OpenAI API Key                          â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ ```                                             â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ export OPENAI_API_KEY=sk-******                 â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ ```                                             â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                                                 â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ Then, answer a multi-turn question.             â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ ```python                                       â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ from sglang import function, system, user,      â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ assistant, gen, set_default_backend, OpenAI     â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                                                 â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ @function                                       â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ def multi_turn_question(s, question_1,          â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ question_2):                                    â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚     s += system("You are a helpful assistant.") â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚     s += user(question_1)                       â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚     s += assistant(gen("answer_1",              â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ max_tokens=256))                                â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚     s += user(question_2)                       â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚     s += assistant(gen("answer_2",              â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ max_tokens=256))                                â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                                                 â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ set_default_backend(OpenAI("gpt-3.5-turbo"))    â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                                                 â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ state = multi_turn_question.run(                â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚     question_1="What is the capital of the      â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ United States?",                                â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚     question_2="List two local attractions.",   â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ )                                               â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                                                 â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ for m in state.messages():                      â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚     print(m["role"], ":", m["content"])         â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                                                 â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ print(state["answer_1"])                        â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ ```                                             â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                                                 â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ ### More Examples                               â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                                                 â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ Anthropic and VertexAI (Gemini) models are also â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ supported.                                      â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ You can find more examples at                   â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ (examples/quick_start).                         â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                                                 â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ ## Frontend: Structured Generation Language     â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ (SGLang)                                        â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                                                 â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ To begin with, import sglang.                   â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ ```python                                       â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ import sglang as sgl                            â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ ```                                             â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                                                 â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ `sglang` provides some simple primitives such   â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ as `gen`, `select`, `fork`, `image`.            â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ You can implement your prompt flow in a         â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ function decorated by `sgl.function`.           â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ You can then invoke the function with `run` or  â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ `run_batch`.                                    â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ The system will manage the state, chat          â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ template, parallelism and batching for you.     â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                                                 â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ The complete code for the examples below can be â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ found at (examples/usage/readme_examples.py)    â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                                                 â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ ### Control Flow                                â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ You can use any Python code within the function â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ body, including control flow, nested function   â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ calls, and external libraries.                  â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                                                 â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ ```python                                       â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ @sgl.function                                   â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ def tool_use(s, question):                      â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚     s += "To answer this question: " + question â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ + ". "                                          â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚     s += "I need to use a " + sgl.gen("tool",   â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ choices=["calculator", "search engine"]) + ". " â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                                                 â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚     if s["tool"] == "calculator":               â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚         s += "The math expression is" +         â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ sgl.gen("expression")                           â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚     elif s["tool"] == "search engine":          â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚         s += "The key word to search is" +      â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ sgl.gen("word")                                 â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ ```                                             â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                                                 â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ ### Parallelism                                 â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ Use `fork` to launch parallel prompts.          â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ Because `sgl.gen` is non-blocking, the for loop â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ below issues two generation calls in parallel.  â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                                                 â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ ```python                                       â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ @sgl.function                                   â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ def tip_suggestion(s):                          â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚     s += (                                      â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚         "Here are two tips for staying healthy: â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ "                                               â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚         "1. Balanced Diet. 2. Regular           â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ Exercise.\n\n"                                  â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚     )                                           â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                                                 â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚     forks = s.fork(2)                           â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚     for i, f in enumerate(forks):               â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚         f += f"Now, expand tip {i+1} into a     â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ paragraph:\n"                                   â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚         f += sgl.gen(f"detailed_tip",           â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ max_tokens=256, stop="\n\n")                    â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                                                 â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚     s += "Tip 1:" + forks[0]["detailed_tip"] +  â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ "\n"                                            â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚     s += "Tip 2:" + forks[1]["detailed_tip"] +  â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ "\n"                                            â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚     s += "In summary" + sgl.gen("summary")      â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ ```                                             â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                                                 â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ ### Multi Modality                              â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ Use `sgl.image` to pass an image as input.      â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                                                 â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ ```python                                       â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ @sgl.function                                   â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ def image_qa(s, image_file, question):          â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚     s += sgl.user(sgl.image(image_file) +       â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ question)                                       â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚     s += sgl.assistant(sgl.gen("answer",        â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ max_tokens=256)                                 â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ ```                                             â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                                                 â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ See also                                        â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ (examples/quick_start/srt_example_llava.py).    â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                                                 â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ ### Constrained Decoding                        â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ Use `regex` to specify a regular expression as  â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ a decoding constraint.                          â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ This is only supported for local models.        â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                                                 â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ ```python                                       â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ @sgl.function                                   â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ def regular_expression_gen(s):                  â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚     s += "Q: What is the IP address of the      â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ Google DNS servers?\n"                          â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚     s += "A: " + sgl.gen(                       â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚         "answer",                               â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚         temperature=0,                          â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚         regex=r"((25[0-5]|2[0-4]\d|[01]?\d\d?)â€¦ â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚     )                                           â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ ```                                             â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                                                 â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ ### JSON Decoding                               â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ Use `regex` to specify a JSON schema with a     â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ regular expression.                             â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                                                 â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ ```python                                       â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ character_regex = (                             â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚     r"""\{\n"""                                 â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚     + r"""    "name": "[\w\d\s]{1,16}",\n"""    â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚     + r"""    "house":                          â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ "(Gryffindor|Slytherin|Ravenclaw|Hufflepuff)",â€¦ â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚     + r"""    "blood status":                   â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ "(Pure-blood|Half-blood|Muggle-born)",\n"""     â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚     + r"""    "occupation":                     â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ "(student|teacher|auror|ministry of magic|death â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ eater|order of the phoenix)",\n"""              â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚     + r"""    "wand": \{\n"""                   â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚     + r"""        "wood":                       â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ "[\w\d\s]{1,16}",\n"""                          â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚     + r"""        "core":                       â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ "[\w\d\s]{1,16}",\n"""                          â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚     + r"""        "length":                     â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ [0-9]{1,2}\.[0-9]{0,2}\n"""                     â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚     + r"""    \},\n"""                          â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚     + r"""    "alive": "(Alive|Deceased)",\n""" â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚     + r"""    "patronus":                       â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ "[\w\d\s]{1,16}",\n"""                          â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚     + r"""    "bogart": "[\w\d\s]{1,16}"\n"""   â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚     + r"""\}"""                                 â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ )                                               â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                                                 â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ @sgl.function                                   â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ def character_gen(s, name):                     â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚     s += name + " is a character in Harry       â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ Potter. Please fill in the following            â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ information about this character.\n"            â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚     s += sgl.gen("json_output", max_tokens=256, â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ regex=character_regex)                          â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ ```                                             â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                                                 â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ See also (examples/usage/json_decode.py) for an â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ additional example on specifying formats with   â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ Pydantic models.                                â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                                                 â”‚   â”‚
â”‚             â”‚ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚
â”‚ Total       â”‚ Input: 377289 Output: 2659                                                                             â”‚
â”‚ Tokens      â”‚                                                                                                        â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Status      â”‚ âœ… Step 19: Completed                                                                                  â”‚
â”‚ Tools       â”‚ ðŸ”§ bash                                                                                                â”‚
â”‚ bash        â”‚ â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚
â”‚             â”‚ â”‚ {'command': "sed -n '240,540p'                 â”‚                                                 â”‚   â”‚
â”‚             â”‚ â”‚ /home/ubuntu/OmniPerf-Bench/perf-agents-benchâ€¦ â”‚                                                 â”‚   â”‚
â”‚             â”‚ â”‚ 'restart': False}                              â”‚ ### Batching                                    â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ Use `run_batch` to run a batch of requests with â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ continuous batching.                            â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                                                 â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ ```python                                       â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ @sgl.function                                   â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ def text_qa(s, question):                       â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚     s += "Q: " + question + "\n"                â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚     s += "A:" + sgl.gen("answer", stop="\n")    â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                                                 â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ states = text_qa.run_batch(                     â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚     [                                           â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚         {"question": "What is the capital of    â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ the United Kingdom?"},                          â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚         {"question": "What is the capital of    â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ France?"},                                      â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚         {"question": "What is the capital of    â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ Japan?"},                                       â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚     ],                                          â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚     progress_bar=True                           â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ )                                               â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ ```                                             â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                                                 â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ ### Streaming                                   â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ Add `stream=True` to enable streaming.          â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                                                 â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ ```python                                       â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ @sgl.function                                   â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ def text_qa(s, question):                       â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚     s += "Q: " + question + "\n"                â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚     s += "A:" + sgl.gen("answer", stop="\n")    â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                                                 â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ state = text_qa.run(                            â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚     question="What is the capital of France?",  â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚     temperature=0.1,                            â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚     stream=True                                 â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ )                                               â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                                                 â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ for out in state.text_iter():                   â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚     print(out, end="", flush=True)              â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ ```                                             â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                                                 â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ ### Tips and Implementation Details             â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ - The `choices` argument in `sgl.gen` is        â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ implemented by computing the                    â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ (https://blog.eleuther.ai/multiple-choice-normâ€¦ â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ of all choices and selecting the one with the   â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ highest probability.                            â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ - The `regex` argument in `sgl.gen` is          â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ implemented through autoregressive decoding     â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ with logit bias masking, according to the       â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ constraints set by the regex. It is compatible  â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ with `temperature=0` and `temperature != 0`.    â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                                                 â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ ## Backend: SGLang Runtime (SRT)                â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ The SGLang Runtime (SRT) is designed to work    â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ best with the SGLang frontend.                  â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ However, it can also be used as a standalone    â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ API server.                                     â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ In this case, the                               â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ [RadixAttention](https://arxiv.org/abs/2312.07â€¦ â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ can still greatly accelerate many use cases     â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ with automatic KV cache reuse.                  â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                                                 â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ ### Usage                                       â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ Launch a server                                 â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ ```                                             â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ python -m sglang.launch_server --model-path     â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ meta-llama/Llama-2-7b-chat-hf --port 30000      â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ ```                                             â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                                                 â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ Send a request                                  â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ ```                                             â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ curl http://localhost:30000/generate \          â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚   -H "Content-Type: application/json" \         â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚   -d '{                                         â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚     "text": "Once upon a time,",                â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚     "sampling_params": {                        â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚       "max_new_tokens": 16,                     â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚       "temperature": 0                          â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚     }                                           â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚   }'                                            â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ ```                                             â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ Learn more about the argument format            â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ (docs/sampling_params.md).                      â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                                                 â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ ### OpenAI Compatible API                       â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ In addition, the server supports an             â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ experimental OpenAI-compatible API.             â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                                                 â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ ```python                                       â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ import openai                                   â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ client = openai.Client(                         â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚     base_url="http://127.0.0.1:30000/v1",       â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ api_key="EMPTY")                                â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                                                 â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ # Text completion                               â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ response = client.completions.create(           â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚         model="default",                        â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚         prompt="The capital of France is",      â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚         temperature=0,                          â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚         max_tokens=32,                          â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ )                                               â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ print(response)                                 â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                                                 â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ # Chat completion                               â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ response = client.chat.completions.create(      â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚     model="default",                            â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚     messages=[                                  â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚         {"role": "system", "content": "You are  â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ a helpful AI assistant"},                       â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚         {"role": "user", "content": "List 3     â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ countries and their capitals."},                â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚     ],                                          â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚     temperature=0,                              â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚     max_tokens=64,                              â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ )                                               â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ print(response)                                 â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ ```                                             â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                                                 â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ By default, the server uses the chat template   â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ specified in the model tokenizer from Hugging   â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ Face. It should just work for most official     â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ models such as Llama-2/Llama-3.                 â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                                                 â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ If needed, you can also override the chat       â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ template when launching the server:             â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                                                 â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ ```                                             â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ python -m sglang.launch_server --model-path     â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ meta-llama/Llama-2-7b-chat-hf --port 30000      â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ --chat-template llama-2                         â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ ```                                             â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                                                 â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ If the chat template you are looking for is     â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ missing, you are welcome to contribute it.      â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ Meanwhile, you can also temporarily register    â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ your chat template as follows:                  â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                                                 â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ ```json                                         â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ {                                               â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚   "name": "my_model",                           â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚   "system": "<|im_start|>system",               â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚   "user": "<|im_start|>user",                   â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚   "assistant": "<|im_start|>assistant",         â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚   "sep_style": "CHATML",                        â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚   "sep": "<|im_end|>",                          â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚   "stop_str": ["<|im_end|>", "<|im_start|>"]    â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ }                                               â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ ```                                             â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                                                 â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ ```                                             â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ python -m sglang.launch_server --model-path     â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ meta-llama/Llama-2-7b-chat-hf --port 30000      â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ --chat-template ./my_model_template.json        â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ ```                                             â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                                                 â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ ### Additional Arguments                        â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ - Add `--tp 2` to enable tensor parallelism. If â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ it indicates `peer access is not supported      â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ between these two devices`, add                 â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ `--enable-p2p-check` option.                    â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ ```                                             â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ python -m sglang.launch_server --model-path     â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ meta-llama/Llama-2-7b-chat-hf --port 30000 --tp â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ 2                                               â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ ```                                             â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ - Add `--dp 2` to enable data parallelism. It   â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ can also be used together with tp. Data         â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ parallelism is better for throughput if there   â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ is enough memory.                               â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ ```                                             â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ python -m sglang.launch_server --model-path     â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ meta-llama/Llama-2-7b-chat-hf --port 30000 --dp â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ 2 --tp 2                                        â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ ```                                             â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ - If you see out-of-memory errors during        â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ serving, please try to reduce the memory usage  â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ of the KV cache pool by setting a smaller value â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ of `--mem-fraction-static`. The default value   â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ is `0.9`                                        â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ ```                                             â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ python -m sglang.launch_server --model-path     â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ meta-llama/Llama-2-7b-chat-hf --port 30000      â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ --mem-fraction-static 0.7                       â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ ```                                             â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ - See (docs/hyperparameter_tuning.md) on tuning â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ hyperparameters for better performance.         â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                                                 â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ ### Supported Models                            â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ - Llama                                         â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ - Mistral                                       â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ - Mixtral                                       â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ - Qwen / Qwen 2 / Qwen 2 MoE                    â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ - Gemma / Gemma 2                               â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚   - `python -m sglang.launch_server             â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ --model-path google/gemma-7b-it --port 30000    â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ --attention-reduce-in-fp32`                     â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ - LLaVA                                         â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚   - `python3 -m sglang.launch_server            â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ --model-path liuhaotian/llava-v1.5-7b           â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ --tokenizer-path llava-hf/llava-1.5-7b-hf       â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ --chat-template vicuna_v1.1 --port 30000`       â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚   - `python3 -m sglang.launch_server            â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ --model-path liuhaotian/llava-v1.6-vicuna-7b    â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ --tokenizer-path llava-hf/llava-1.5-7b-hf       â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ --chat-template vicuna_v1.1 --port 30000`       â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚   - `python3 -m sglang.launch_server            â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ --model-path liuhaotian/llava-v1.6-34b          â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ --tokenizer-path                                â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ liuhaotian/llava-v1.6-34b-tokenizer --port      â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ 3000`                                           â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ - LLaVA-NeXT-Video                              â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚   - see                                         â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ (examples/usage/llava_video/srt_example_llava_â€¦ â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ - Yi-VL                                         â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚   - see                                         â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ (examples/quick_start/srt_example_yi_vl.py).    â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ - StableLM                                      â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ - Command-R                                     â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ - DBRX                                          â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ - Grok                                          â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ - ChatGLM                                       â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ - AWQ/GPTQ/Marlin quantization                  â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                                                 â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ Instructions for supporting a new model are     â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ (https://github.com/sgl-project/sglang/blob/maâ€¦ â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                                                 â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ ## Benchmark And Performance                    â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ - Llama-7B on NVIDIA A10G, FP16, Tensor         â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ Parallelism=1                                   â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ !(assets/llama_7b.jpg)                          â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                                                 â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ - Mixtral-8x7B on NVIDIA A10G, FP16, Tensor     â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ Parallelism=8                                   â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ !(assets/mixtral_8x7b.jpg)                      â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                                                 â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ - Learn more about the above                    â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ (docs/benchmark_results.md).                    â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ - Synthetic latency and throughput benchmark    â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ (https://github.com/sgl-project/sglang/tree/maâ€¦ â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                                                 â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ ## Roadmap                                      â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ https://github.com/sgl-project/sglang/issues/1â€¦ â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                                                 â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ ## Citation And Acknowledgment                  â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ ```                                             â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ @misc{zheng2024sglang,                          â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚       title={SGLang: Efficient Execution of     â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ Structured Language Model Programs},            â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚       author={Lianmin Zheng and Liangsheng Yin  â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ and Zhiqiang Xie and Chuyue Sun and Jeff Huang  â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ and Cody Hao Yu and Shiyi Cao and Christos      â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ Kozyrakis and Ion Stoica and Joseph E. Gonzalez â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ and Clark Barrett and Ying Sheng},              â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚       year={2024},                              â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚       eprint={2312.07104},                      â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚       archivePrefix={arXiv},                    â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚       primaryClass={cs.AI}                      â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ }                                               â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ ```                                             â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                                                 â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ We learned from the design and reused some code â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ of the following projects:                      â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ [Guidance](https://github.com/guidance-ai/guidâ€¦ â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ (https://github.com/vllm-project/vllm),         â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ [LightLLM](https://github.com/ModelTC/lightllmâ€¦ â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ [FlashInfer](https://github.com/flashinfer-ai/â€¦ â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ [Outlines](https://github.com/outlines-dev/outâ€¦ â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ [LMQL](https://github.com/eth-sri/lmql).        â”‚   â”‚
â”‚             â”‚ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚
â”‚ Total       â”‚ Input: 414551 Output: 2728                                                                             â”‚
â”‚ Tokens      â”‚                                                                                                        â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Status      â”‚ âœ… Step 20: Completed                                                                                  â”‚
â”‚ Tools       â”‚ ðŸ”§ bash                                                                                                â”‚
â”‚ bash        â”‚ â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚
â”‚             â”‚ â”‚ {'command': "sed -n '1,200p'                   â”‚ # Code Structure                                â”‚   â”‚
â”‚             â”‚ â”‚ /home/ubuntu/OmniPerf-Bench/perf-agents-benchâ€¦ â”‚                                                 â”‚   â”‚
â”‚             â”‚ â”‚ 'restart': False}                              â”‚ - `backend`: Various backends for the language  â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ interpreter.                                    â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ - `lang`: The frontend language.                â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ - `srt`: The runtime for running local models.  â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ - `test`: Test utilities.                       â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ - `api.py`: Public API.                         â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ - `bench_latency.py`: Benchmark utilities.      â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ - `global_config.py`: The global configs and    â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ constants.                                      â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ - `launch_server.py`: The entry point of        â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ launching local server.                         â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ - `utils.py`: Common utilities.                 â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                                                 â”‚   â”‚
â”‚             â”‚ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚
â”‚ Total       â”‚ Input: 453925 Output: 2801                                                                             â”‚
â”‚ Tokens      â”‚                                                                                                        â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Status      â”‚ âœ… Step 21: Completed                                                                                  â”‚
â”‚ Tools       â”‚ ðŸ”§ bash                                                                                                â”‚
â”‚ bash        â”‚ â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚
â”‚             â”‚ â”‚ {'command': "cd                                â”‚                                                 â”‚   â”‚
â”‚             â”‚ â”‚ /home/ubuntu/OmniPerf-Bench/perf-agents-benchâ€¦ â”‚                                                 â”‚   â”‚
â”‚             â”‚ â”‚ && grep -n 'torch.zeros\\|fill_'               â”‚                                                 â”‚   â”‚
â”‚             â”‚ â”‚ vllm/model_executor/layers/fused_moe/moe_aligâ€¦ â”‚                                                 â”‚   â”‚
â”‚             â”‚ â”‚ benchmarks/kernels/benchmark_moe_align_block_â€¦ â”‚                                                 â”‚   â”‚
â”‚             â”‚ â”‚ || true", 'restart': False}                    â”‚                                                 â”‚   â”‚
â”‚             â”‚ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚
â”‚ Total       â”‚ Input: 493477 Output: 2966                                                                             â”‚
â”‚ Tokens      â”‚                                                                                                        â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Status      â”‚ âœ… Step 22: Completed                                                                                  â”‚
â”‚ Tools       â”‚ ðŸ”§ bash                                                                                                â”‚
â”‚ bash        â”‚ â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚
â”‚             â”‚ â”‚ {'command': 'grep -RIn                         â”‚                                                 â”‚   â”‚
â”‚             â”‚ â”‚ "torch.zeros\\|\\.fill_\\("                    â”‚                                                 â”‚   â”‚
â”‚             â”‚ â”‚ /home/ubuntu/OmniPerf-Bench/perf-agents-benchâ€¦ â”‚                                                 â”‚   â”‚
â”‚             â”‚ â”‚ | sed -n \'1,120p\'', 'restart': False}        â”‚                                                 â”‚   â”‚
â”‚             â”‚ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚
â”‚ Total       â”‚ Input: 533292 Output: 3049                                                                             â”‚
â”‚ Tokens      â”‚                                                                                                        â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Status      â”‚ âœ… Step 23: Completed                                                                                  â”‚
â”‚ Tools       â”‚ ðŸ”§ bash                                                                                                â”‚
â”‚ bash        â”‚ â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚
â”‚             â”‚ â”‚ {'command': 'grep -RIn -E                      â”‚ /home/ubuntu/OmniPerf-Bench/perf-agents-bench/â€¦ â”‚   â”‚
â”‚             â”‚ â”‚ "torch\\.zeros|\\.fill_"                       â”‚ tensor = torch.zeros(4 + num_tp_ports,          â”‚   â”‚
â”‚             â”‚ â”‚ /home/ubuntu/OmniPerf-Bench/perf-agents-benchâ€¦ â”‚ dtype=torch.int)                                â”‚   â”‚
â”‚             â”‚ â”‚ | sed -n \'1,200p\'', 'restart': False}        â”‚ /home/ubuntu/OmniPerf-Bench/perf-agents-bench/â€¦ â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ self.input_ids = torch.zeros((self.max_bs,),    â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ dtype=torch.int32, device="cuda")               â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ /home/ubuntu/OmniPerf-Bench/perf-agents-bench/â€¦ â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ self.req_pool_indices = torch.zeros(            â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ /home/ubuntu/OmniPerf-Bench/perf-agents-bench/â€¦ â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ self.position_ids_offsets = torch.zeros(        â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ /home/ubuntu/OmniPerf-Bench/perf-agents-bench/â€¦ â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ self.out_cache_loc = torch.zeros(               â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ /home/ubuntu/OmniPerf-Bench/perf-agents-bench/â€¦ â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ self.flashinfer_kv_indptr = torch.zeros(        â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ /home/ubuntu/OmniPerf-Bench/perf-agents-bench/â€¦ â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ self.flashinfer_kv_indices = torch.zeros(       â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ /home/ubuntu/OmniPerf-Bench/perf-agents-bench/â€¦ â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ self.position_ids_offsets.fill_(1)              â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ /home/ubuntu/OmniPerf-Bench/perf-agents-bench/â€¦ â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ position_ids_offsets = torch.zeros((bs,),       â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ dtype=torch.int32, device=device)               â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ /home/ubuntu/OmniPerf-Bench/perf-agents-bench/â€¦ â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ logit_bias = torch.zeros(                       â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ /home/ubuntu/OmniPerf-Bench/perf-agents-bench/â€¦ â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ self.logit_bias = torch.zeros(                  â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ /home/ubuntu/OmniPerf-Bench/perf-agents-bench/â€¦ â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ other.logit_bias = torch.zeros(                 â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ /home/ubuntu/OmniPerf-Bench/perf-agents-bench/â€¦ â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ extend_start_loc = torch.zeros_like(seq_lens)   â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ /home/ubuntu/OmniPerf-Bench/perf-agents-bench/â€¦ â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ kv_indptr = torch.zeros((batch_size + 1,),      â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ dtype=torch.int32, device="cuda")               â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ /home/ubuntu/OmniPerf-Bench/perf-agents-bench/â€¦ â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ qo_indptr = torch.zeros((batch_size + 1,),      â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ dtype=torch.int32, device="cuda")               â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ /home/ubuntu/OmniPerf-Bench/perf-agents-bench/â€¦ â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ start_loc = torch.zeros((batch_size,),          â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ dtype=torch.int32, device="cuda")               â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ /home/ubuntu/OmniPerf-Bench/perf-agents-bench/â€¦ â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ self.mem_state.fill_(True)                      â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ /home/ubuntu/OmniPerf-Bench/perf-agents-bench/â€¦ â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ self.mem_state.fill_(True)                      â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ /home/ubuntu/OmniPerf-Bench/perf-agents-bench/â€¦ â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ b_start_loc = torch.zeros((B,),                 â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ dtype=torch.int32, device="cuda")               â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ /home/ubuntu/OmniPerf-Bench/perf-agents-bench/â€¦ â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ b_start_loc_extend = torch.zeros((B,),          â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ dtype=torch.int32, device="cuda")               â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ /home/ubuntu/OmniPerf-Bench/perf-agents-bench/â€¦ â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ b_start_loc_extend =                            â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ torch.zeros_like(b_seq_len)                     â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ /home/ubuntu/OmniPerf-Bench/perf-agents-bench/â€¦ â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ sorted_ids.fill_(topk_ids.numel())              â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ /home/ubuntu/OmniPerf-Bench/perf-agents-bench/â€¦ â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ torch.zeros(self.num_total_experts,             â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ dtype=torch.float32),                           â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ /home/ubuntu/OmniPerf-Bench/perf-agents-bench/â€¦ â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ torch.zeros(self.num_total_experts,             â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ dtype=torch.float32),                           â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ /home/ubuntu/OmniPerf-Bench/perf-agents-bench/â€¦ â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ self.weight =                                   â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ nn.Parameter(torch.zeros(hidden_size))          â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ /home/ubuntu/OmniPerf-Bench/perf-agents-bench/â€¦ â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ final_hidden_states = torch.zeros(              â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ /home/ubuntu/OmniPerf-Bench/perf-agents-bench/â€¦ â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ torch.zeros(self.num_total_experts,             â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ dtype=torch.float32),                           â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ /home/ubuntu/OmniPerf-Bench/perf-agents-bench/â€¦ â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ torch.zeros(self.num_total_experts,             â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ dtype=torch.float32),                           â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ /home/ubuntu/OmniPerf-Bench/perf-agents-bench/â€¦ â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ #     self.backend.fill_image(self)             â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ /home/ubuntu/OmniPerf-Bench/perf-agents-bench/â€¦ â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ b_start_loc = torch.zeros((batch_size,),        â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ dtype=torch.int32).to(0)                        â”‚   â”‚
â”‚             â”‚ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚
â”‚ Total       â”‚ Input: 573234 Output: 3133                                                                             â”‚
â”‚ Tokens      â”‚                                                                                                        â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Status      â”‚ âœ… Step 24: Completed                                                                                  â”‚
â”‚ LLM         â”‚ ðŸ’¬ I'll create a benchmark script in .bench_scratch to measure the overhead of creating many           â”‚
â”‚ Response    â”‚ aiohttp.ClientSession instances vs reusing a single session, then run it. This doesn't modify the repo â”‚
â”‚             â”‚ and is safe.                                                                                           â”‚
â”‚ Tools       â”‚ ðŸ”§ bash                                                                                                â”‚
â”‚ bash        â”‚ â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚
â”‚             â”‚ â”‚ {'command': 'mkdir -p                          â”‚                                                 â”‚   â”‚
â”‚             â”‚ â”‚ /home/ubuntu/OmniPerf-Bench/perf-agents-benchâ€¦ â”‚                                                 â”‚   â”‚
â”‚             â”‚ â”‚ && cat >                                       â”‚                                                 â”‚   â”‚
â”‚             â”‚ â”‚ /home/ubuntu/OmniPerf-Bench/perf-agents-benchâ€¦ â”‚                                                 â”‚   â”‚
â”‚             â”‚ â”‚ << \'PY\'\nimport asyncio\nimport time\nimport â”‚                                                 â”‚   â”‚
â”‚             â”‚ â”‚ aiohttp\n\nN = 1000\n\nasync def               â”‚                                                 â”‚   â”‚
â”‚             â”‚ â”‚ create_close_sessions(n=N):\n    # Simulate    â”‚                                                 â”‚   â”‚
â”‚             â”‚ â”‚ old behavior: one session per request\n    t0  â”‚                                                 â”‚   â”‚
â”‚             â”‚ â”‚ = time.perf_counter()\n    for _ in            â”‚                                                 â”‚   â”‚
â”‚             â”‚ â”‚ range(n):\n        session =                   â”‚                                                 â”‚   â”‚
â”‚             â”‚ â”‚ aiohttp.ClientSession()\n        await         â”‚                                                 â”‚   â”‚
â”‚             â”‚ â”‚ session.close()\n    return                    â”‚                                                 â”‚   â”‚
â”‚             â”‚ â”‚ time.perf_counter() - t0\n\nasync def          â”‚                                                 â”‚   â”‚
â”‚             â”‚ â”‚ reuse_single_session(n=N):\n    # Simulate     â”‚                                                 â”‚   â”‚
â”‚             â”‚ â”‚ optimized behavior: reuse one session\n    t0  â”‚                                                 â”‚   â”‚
â”‚             â”‚ â”‚ = time.perf_counter()\n    session =           â”‚                                                 â”‚   â”‚
â”‚             â”‚ â”‚ aiohttp.ClientSession()\n    # simulate n      â”‚                                                 â”‚   â”‚
â”‚             â”‚ â”‚ logical requests without new sessions\n    for â”‚                                                 â”‚   â”‚
â”‚             â”‚ â”‚ _ in range(n):\n        pass\n    await        â”‚                                                 â”‚   â”‚
â”‚             â”‚ â”‚ session.close()\n    return                    â”‚                                                 â”‚   â”‚
â”‚             â”‚ â”‚ time.perf_counter() - t0\n\nasync def          â”‚                                                 â”‚   â”‚
â”‚             â”‚ â”‚ main():\n    t1 = await                        â”‚                                                 â”‚   â”‚
â”‚             â”‚ â”‚ create_close_sessions()\n    t2 = await        â”‚                                                 â”‚   â”‚
â”‚             â”‚ â”‚ reuse_single_session()\n                       â”‚                                                 â”‚   â”‚
â”‚             â”‚ â”‚ print({"create_close_sessions_s": round(t1,    â”‚                                                 â”‚   â”‚
â”‚             â”‚ â”‚ 6), "reuse_single_session_s": round(t2, 6),    â”‚                                                 â”‚   â”‚
â”‚             â”‚ â”‚ "speedup": round(t1/max(t2,1e-9), 2)})\n\nif   â”‚                                                 â”‚   â”‚
â”‚             â”‚ â”‚ __name__ == \'__main__\':\n                    â”‚                                                 â”‚   â”‚
â”‚             â”‚ â”‚ asyncio.run(main())\nPY', 'restart': False}    â”‚                                                 â”‚   â”‚
â”‚             â”‚ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚
â”‚ Total       â”‚ Input: 615216 Output: 6698                                                                             â”‚
â”‚ Tokens      â”‚                                                                                                        â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Status      â”‚ âœ… Step 25: Completed                                                                                  â”‚
â”‚ Tools       â”‚ ðŸ”§ bash                                                                                                â”‚
â”‚ bash        â”‚ â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚
â”‚             â”‚ â”‚ {'command': 'python3                           â”‚ {'create_close_sessions_s': 0.013521,           â”‚   â”‚
â”‚             â”‚ â”‚ /home/ubuntu/OmniPerf-Bench/perf-agents-benchâ€¦ â”‚ 'reuse_single_session_s': 2.7e-05, 'speedup':   â”‚   â”‚
â”‚             â”‚ â”‚ 'restart': False}                              â”‚ 492.36}                                         â”‚   â”‚
â”‚             â”‚ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚
â”‚ Total       â”‚ Input: 660797 Output: 6764                                                                             â”‚
â”‚ Tokens      â”‚                                                                                                        â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Status      â”‚ âœ… Step 26: Completed                                                                                  â”‚
â”‚ Tools       â”‚ ðŸ”§ str_replace_based_edit_tool                                                                         â”‚
â”‚ str_replacâ€¦ â”‚ â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚
â”‚             â”‚ â”‚ {'command': 'str_replace', 'file_text': '',    â”‚ The file                                        â”‚   â”‚
â”‚             â”‚ â”‚ 'insert_line': 0, 'new_str': '    url =        â”‚ /home/ubuntu/OmniPerf-Bench/perf-agents-bench/â€¦ â”‚   â”‚
â”‚             â”‚ â”‚ f"{args.host}:{args.port}"\n    max_new_tokens â”‚ has been edited. Here's the result of running   â”‚   â”‚
â”‚             â”‚ â”‚ = args.max_tokens\n    input_len =             â”‚ `cat -n` on a snippet of                        â”‚   â”‚
â”‚             â”‚ â”‚ args.input_len if args.input_len else 1\n      â”‚ /home/ubuntu/OmniPerf-Bench/perf-agents-bench/â€¦ â”‚   â”‚
â”‚             â”‚ â”‚ output_len = max_new_tokens\n', 'old_str': '   â”‚     11  import requests                         â”‚   â”‚
â”‚             â”‚ â”‚ url = f"{args.host}:{args.port}"\n             â”‚     12                                          â”‚   â”‚
â”‚             â”‚ â”‚ max_new_tokens = args.max_tokens\n', 'path':   â”‚     13                                          â”‚   â”‚
â”‚             â”‚ â”‚ '/home/ubuntu/OmniPerf-Bench/perf-agents-bencâ€¦ â”‚     14  def run_one_batch_size(bs):             â”‚   â”‚
â”‚             â”‚ â”‚ 'view_range': []}                              â”‚     15      url = f"{args.host}:{args.port}"    â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚     16      max_new_tokens = args.max_tokens    â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚     17      input_len = args.input_len if       â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ args.input_len else 1                           â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚     18      output_len = max_new_tokens         â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚     19                                          â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚     20      if args.input_len:                  â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚     21          input_ids = [                   â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚     22                                          â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚     23              for _ in range(bs)          â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ Review the changes and make sure they are as    â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ expected. Edit the file again if necessary.     â”‚   â”‚
â”‚             â”‚ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚
â”‚ Total       â”‚ Input: 706490 Output: 7952                                                                             â”‚
â”‚ Tokens      â”‚                                                                                                        â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Status      â”‚ âœ… Step 27: Completed                                                                                  â”‚
â”‚ Tools       â”‚ ðŸ”§ str_replace_based_edit_tool                                                                         â”‚
â”‚ str_replacâ€¦ â”‚ â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚
â”‚             â”‚ â”‚ {'command': 'str_replace', 'file_text': '',    â”‚ The file                                        â”‚   â”‚
â”‚             â”‚ â”‚ 'insert_line': 0, 'new_str': '                 â”‚ /home/ubuntu/OmniPerf-Bench/perf-agents-bench/â€¦ â”‚   â”‚
â”‚             â”‚ â”‚ output_throughput = bs * output_len /          â”‚ has been edited. Here's the result of running   â”‚   â”‚
â”‚             â”‚ â”‚ latency\n    overall_throughput = bs *         â”‚ `cat -n` on a snippet of                        â”‚   â”‚
â”‚             â”‚ â”‚ (input_len + output_len) / latency\n           â”‚ /home/ubuntu/OmniPerf-Bench/perf-agents-bench/â€¦ â”‚   â”‚
â”‚             â”‚ â”‚ print(f"latency: {latency:.2f} s")\n           â”‚     97      else:                               â”‚   â”‚
â”‚             â”‚ â”‚ print(f"decode throughput:                     â”‚     98          ret = response.json()           â”‚   â”‚
â”‚             â”‚ â”‚ {output_throughput:.2f} token/s")\n            â”‚     99      print(ret)                          â”‚   â”‚
â”‚             â”‚ â”‚ print(f"overall throughput:                    â”‚    100                                          â”‚   â”‚
â”‚             â”‚ â”‚ {overall_throughput:.2f} token/s")\n',         â”‚    101      output_throughput = bs * output_len â”‚   â”‚
â”‚             â”‚ â”‚ 'old_str': '    output_throughput = bs *       â”‚ / latency                                       â”‚   â”‚
â”‚             â”‚ â”‚ max_new_tokens / latency\n                     â”‚    102      overall_throughput = bs *           â”‚   â”‚
â”‚             â”‚ â”‚ overall_throughput = bs * (args.input_len +    â”‚ (input_len + output_len) / latency              â”‚   â”‚
â”‚             â”‚ â”‚ max_new_tokens) / latency\n                    â”‚    103      print(f"latency: {latency:.2f} s")  â”‚   â”‚
â”‚             â”‚ â”‚ print(f"latency: {latency:.2f} s")\n           â”‚    104      print(f"decode throughput:          â”‚   â”‚
â”‚             â”‚ â”‚ print(f"decode throughput:                     â”‚ {output_throughput:.2f} token/s")               â”‚   â”‚
â”‚             â”‚ â”‚ {output_throughput:.2f} token/s")\n            â”‚    105      print(f"overall throughput:         â”‚   â”‚
â”‚             â”‚ â”‚ print(f"overall throughput:                    â”‚ {overall_throughput:.2f} token/s")              â”‚   â”‚
â”‚             â”‚ â”‚ {overall_throughput:.2f} token/s")\n', 'path': â”‚    106                                          â”‚   â”‚
â”‚             â”‚ â”‚ '/home/ubuntu/OmniPerf-Bench/perf-agents-bencâ€¦ â”‚    107      with open("results.jsonl", "a") as  â”‚   â”‚
â”‚             â”‚ â”‚ 'view_range': []}                              â”‚ fout:                                           â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    108          res = {                         â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    109              "backend": args.backend,    â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    110              "input_len":                â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ args.input_len,                                 â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ Review the changes and make sure they are as    â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ expected. Edit the file again if necessary.     â”‚   â”‚
â”‚             â”‚ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚
â”‚ Total       â”‚ Input: 753673 Output: 8230                                                                             â”‚
â”‚ Tokens      â”‚                                                                                                        â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Status      â”‚ âœ… Step 28: Completed                                                                                  â”‚
â”‚ Tools       â”‚ ðŸ”§ str_replace_based_edit_tool                                                                         â”‚
â”‚ str_replacâ€¦ â”‚ â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚
â”‚             â”‚ â”‚ {'command': 'str_replace', 'file_text': '',    â”‚ The file                                        â”‚   â”‚
â”‚             â”‚ â”‚ 'insert_line': 0, 'new_str': 'async def        â”‚ /home/ubuntu/OmniPerf-Bench/perf-agents-bench/â€¦ â”‚   â”‚
â”‚             â”‚ â”‚ send_request(\n    backend: str,\n    api_url: â”‚ has been edited. Here's the result of running   â”‚   â”‚
â”‚             â”‚ â”‚ str,\n    prompt: str,\n    prompt_len: int,\n â”‚ `cat -n` on a snippet of                        â”‚   â”‚
â”‚             â”‚ â”‚ output_len: int,\n    best_of: int,\n          â”‚ /home/ubuntu/OmniPerf-Bench/perf-agents-bench/â€¦ â”‚   â”‚
â”‚             â”‚ â”‚ use_beam_search: bool,\n    session:           â”‚    112          # The next request will be sent â”‚   â”‚
â”‚             â”‚ â”‚ aiohttp.ClientSession,\n) -> None:\n',         â”‚ after the interval.                             â”‚   â”‚
â”‚             â”‚ â”‚ 'old_str': 'async def send_request(\n          â”‚    113          await asyncio.sleep(interval)   â”‚   â”‚
â”‚             â”‚ â”‚ backend: str,\n    api_url: str,\n    prompt:  â”‚    114                                          â”‚   â”‚
â”‚             â”‚ â”‚ str,\n    prompt_len: int,\n    output_len:    â”‚    115                                          â”‚   â”‚
â”‚             â”‚ â”‚ int,\n    best_of: int,\n    use_beam_search:  â”‚    116  async def send_request(                 â”‚   â”‚
â”‚             â”‚ â”‚ bool,\n) -> None:\n', 'path':                  â”‚    117      backend: str,                       â”‚   â”‚
â”‚             â”‚ â”‚ '/home/ubuntu/OmniPerf-Bench/perf-agents-bencâ€¦ â”‚    118      api_url: str,                       â”‚   â”‚
â”‚             â”‚ â”‚ 'view_range': []}                              â”‚    119      prompt: str,                        â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    120      prompt_len: int,                    â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    121      output_len: int,                    â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    122      best_of: int,                       â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    123      use_beam_search: bool,              â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    124      session: aiohttp.ClientSession,     â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    125  ) -> None:                              â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    126      request_start_time =                â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ time.perf_counter()                             â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    127                                          â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    128      headers = {"User-Agent": "Benchmark â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ Client"}                                        â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    129      if backend == "vllm":               â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    130          pload = {                       â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ Review the changes and make sure they are as    â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ expected. Edit the file again if necessary.     â”‚   â”‚
â”‚             â”‚ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚
â”‚ Total       â”‚ Input: 801459 Output: 8711                                                                             â”‚
â”‚ Tokens      â”‚                                                                                                        â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Status      â”‚ âœ… Step 29: Completed                                                                                  â”‚
â”‚ Tools       â”‚ ðŸ”§ str_replace_based_edit_tool                                                                         â”‚
â”‚ str_replacâ€¦ â”‚ â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚
â”‚             â”‚ â”‚ {'command': 'str_replace', 'file_text': '',    â”‚                                                 â”‚   â”‚
â”‚             â”‚ â”‚ 'insert_line': 0, 'new_str': '    if backend   â”‚                                                 â”‚   â”‚
â”‚             â”‚ â”‚ != "ginfer":\n        while True:\n            â”‚                                                 â”‚   â”‚
â”‚             â”‚ â”‚ async with session.post(\n                     â”‚                                                 â”‚   â”‚
â”‚             â”‚ â”‚ api_url, headers=headers, json=pload\n         â”‚                                                 â”‚   â”‚
â”‚             â”‚ â”‚ ) as response:\n                chunks = []\n  â”‚                                                 â”‚   â”‚
â”‚             â”‚ â”‚ async for chunk, _ in                          â”‚                                                 â”‚   â”‚
â”‚             â”‚ â”‚ response.content.iter_chunks():\n              â”‚                                                 â”‚   â”‚
â”‚             â”‚ â”‚ chunks.append(chunk)\n            output =     â”‚                                                 â”‚   â”‚
â”‚             â”‚ â”‚ b"".join(chunks).decode("utf-8")\n             â”‚                                                 â”‚   â”‚
â”‚             â”‚ â”‚ output = json.loads(output)\n\n            #   â”‚                                                 â”‚   â”‚
â”‚             â”‚ â”‚ Re-send the request if it failed.\n            â”‚                                                 â”‚   â”‚
â”‚             â”‚ â”‚ if "error" not in output:\n                    â”‚                                                 â”‚   â”‚
â”‚             â”‚ â”‚ break\n            else:\n                     â”‚                                                 â”‚   â”‚
â”‚             â”‚ â”‚ print(output)\n    else:\n        import       â”‚                                                 â”‚   â”‚
â”‚             â”‚ â”‚ grpc\n        from ginfer import sampler_pb2,  â”‚                                                 â”‚   â”‚
â”‚             â”‚ â”‚ sampler_pb2_grpc\n\n        api_url =          â”‚                                                 â”‚   â”‚
â”‚             â”‚ â”‚ api_url.replace("http://",                     â”‚                                                 â”‚   â”‚
â”‚             â”‚ â”‚ "").replace("/generate", "")\n                 â”‚                                                 â”‚   â”‚
â”‚             â”‚ â”‚ sampler_channel =                              â”‚                                                 â”‚   â”‚
â”‚             â”‚ â”‚ grpc.aio.insecure_channel(api_url)\n           â”‚                                                 â”‚   â”‚
â”‚             â”‚ â”‚ sampler =                                      â”‚                                                 â”‚   â”‚
â”‚             â”‚ â”‚ sampler_pb2_grpc.SamplerStub(sampler_channel)â€¦ â”‚                                                 â”‚   â”‚
â”‚             â”‚ â”‚ request_end_time = time.perf_counter()\n       â”‚                                                 â”‚   â”‚
â”‚             â”‚ â”‚ sample_request =                               â”‚                                                 â”‚   â”‚
â”‚             â”‚ â”‚ sampler_pb2.SampleTextRequest(\n               â”‚                                                 â”‚   â”‚
â”‚             â”‚ â”‚ prompt=prompt,\n                               â”‚                                                 â”‚   â”‚
â”‚             â”‚ â”‚ settings=sampler_pb2.SampleSettings(\n         â”‚                                                 â”‚   â”‚
â”‚             â”‚ â”‚ max_len=output_len,\n                          â”‚                                                 â”‚   â”‚
â”‚             â”‚ â”‚ rng_seed=0,\n                temperature=0,\n  â”‚                                                 â”‚   â”‚
â”‚             â”‚ â”‚ nucleus_p=1,\n            ),\n        )\n      â”‚                                                 â”‚   â”‚
â”‚             â”‚ â”‚ stream = sampler.SampleText(sample_request)\n  â”‚                                                 â”‚   â”‚
â”‚             â”‚ â”‚ response = "".join()\n', 'old_str': '    if    â”‚                                                 â”‚   â”‚
â”‚             â”‚ â”‚ backend != "ginfer":\n        timeout =        â”‚                                                 â”‚   â”‚
â”‚             â”‚ â”‚ aiohttp.ClientTimeout(total=3 * 3600)\n        â”‚                                                 â”‚   â”‚
â”‚             â”‚ â”‚ async with                                     â”‚                                                 â”‚   â”‚
â”‚             â”‚ â”‚ aiohttp.ClientSession(timeout=timeout) as      â”‚                                                 â”‚   â”‚
â”‚             â”‚ â”‚ session:\n            while True:\n            â”‚                                                 â”‚   â”‚
â”‚             â”‚ â”‚ async with session.post(\n                     â”‚                                                 â”‚   â”‚
â”‚             â”‚ â”‚ api_url, headers=headers, json=pload\n         â”‚                                                 â”‚   â”‚
â”‚             â”‚ â”‚ ) as response:\n                    chunks =   â”‚                                                 â”‚   â”‚
â”‚             â”‚ â”‚ []\n                    async for chunk, _ in  â”‚                                                 â”‚   â”‚
â”‚             â”‚ â”‚ response.content.iter_chunks():\n              â”‚                                                 â”‚   â”‚
â”‚             â”‚ â”‚ chunks.append(chunk)\n            output =     â”‚                                                 â”‚   â”‚
â”‚             â”‚ â”‚ b"".join(chunks).decode("utf-8")\n             â”‚                                                 â”‚   â”‚
â”‚             â”‚ â”‚ output = json.loads(output)\n\n            #   â”‚                                                 â”‚   â”‚
â”‚             â”‚ â”‚ Re-send the request if it failed.\n            â”‚                                                 â”‚   â”‚
â”‚             â”‚ â”‚ if "error" not in output:\n                    â”‚                                                 â”‚   â”‚
â”‚             â”‚ â”‚ break\n            else:\n                     â”‚                                                 â”‚   â”‚
â”‚             â”‚ â”‚ print(output)\n    else:\n        import       â”‚                                                 â”‚   â”‚
â”‚             â”‚ â”‚ grpc\n        from ginfer import sampler_pb2,  â”‚                                                 â”‚   â”‚
â”‚             â”‚ â”‚ sampler_pb2_grpc\n\n        api_url =          â”‚                                                 â”‚   â”‚
â”‚             â”‚ â”‚ api_url.replace("http://",                     â”‚                                                 â”‚   â”‚
â”‚             â”‚ â”‚ "").replace("/generate", "")\n                 â”‚                                                 â”‚   â”‚
â”‚             â”‚ â”‚ sampler_channel =                              â”‚                                                 â”‚   â”‚
â”‚             â”‚ â”‚ grpc.aio.insecure_channel(api_url)\n           â”‚                                                 â”‚   â”‚
â”‚             â”‚ â”‚ sampler =                                      â”‚                                                 â”‚   â”‚
â”‚             â”‚ â”‚ sampler_pb2_grpc.SamplerStub(sampler_channel)â€¦ â”‚                                                 â”‚   â”‚
â”‚             â”‚ â”‚ request_end_time = time.perf_counter()\n       â”‚                                                 â”‚   â”‚
â”‚             â”‚ â”‚ sample_request =                               â”‚                                                 â”‚   â”‚
â”‚             â”‚ â”‚ sampler_pb2.SampleTextRequest(\n               â”‚                                                 â”‚   â”‚
â”‚             â”‚ â”‚ prompt=prompt,\n                               â”‚                                                 â”‚   â”‚
â”‚             â”‚ â”‚ settings=sampler_pb2.SampleSettings(\n         â”‚                                                 â”‚   â”‚
â”‚             â”‚ â”‚ max_len=output_len,\n                          â”‚                                                 â”‚   â”‚
â”‚             â”‚ â”‚ rng_seed=0,\n                temperature=0,\n  â”‚                                                 â”‚   â”‚
â”‚             â”‚ â”‚ nucleus_p=1,\n            ),\n        )\n      â”‚                                                 â”‚   â”‚
â”‚             â”‚ â”‚ stream = sampler.SampleText(sample_request)\n  â”‚                                                 â”‚   â”‚
â”‚             â”‚ â”‚ response = "".join()\n', 'path':               â”‚                                                 â”‚   â”‚
â”‚             â”‚ â”‚ '/home/ubuntu/OmniPerf-Bench/perf-agents-bencâ€¦ â”‚                                                 â”‚   â”‚
â”‚             â”‚ â”‚ 'view_range': []}                              â”‚                                                 â”‚   â”‚
â”‚             â”‚ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚
â”‚ Total       â”‚ Input: 850100 Output: 9450                                                                             â”‚
â”‚ Tokens      â”‚                                                                                                        â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Status      â”‚ âœ… Step 30: Completed                                                                                  â”‚
â”‚ Tools       â”‚ ðŸ”§ str_replace_based_edit_tool                                                                         â”‚
â”‚ str_replacâ€¦ â”‚ â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚
â”‚             â”‚ â”‚ {'command': 'view', 'file_text': '',           â”‚ Here's the result of running `cat -n` on        â”‚   â”‚
â”‚             â”‚ â”‚ 'insert_line': 0, 'new_str': '', 'old_str':    â”‚ /home/ubuntu/OmniPerf-Bench/perf-agents-bench/â€¦ â”‚   â”‚
â”‚             â”‚ â”‚ '', 'path':                                    â”‚    168          pload = {                       â”‚   â”‚
â”‚             â”‚ â”‚ '/home/ubuntu/OmniPerf-Bench/perf-agents-bencâ€¦ â”‚    169              "inputs": prompt,           â”‚   â”‚
â”‚             â”‚ â”‚ 'view_range': [168, 220]}                      â”‚    170              "parameters": params,       â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    171          }                               â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    172      elif backend == "ginfer":           â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    173          pass                            â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    174      else:                               â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    175          raise ValueError(f"Unknown      â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ backend: {backend}")                            â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    176                                          â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    177      if backend != "ginfer":             â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    178          timeout =                       â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ aiohttp.ClientTimeout(total=3 * 3600)           â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    179          async with                      â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ aiohttp.ClientSession(timeout=timeout) as       â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ session:                                        â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    180              while True:                 â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    181                  async with              â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ session.post(                                   â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    182                      api_url,            â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ headers=headers, json=pload                     â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    183                  ) as response:          â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    184                      chunks = []         â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    185                      async for chunk, _  â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ in response.content.iter_chunks():              â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    186                                          â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ chunks.append(chunk)                            â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    187                  output =                â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ b"".join(chunks).decode("utf-8")                â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    188                  output =                â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ json.loads(output)                              â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    189                                          â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    190                  # Re-send the request   â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ if it failed.                                   â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    191                  if "error" not in       â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ output:                                         â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    192                      break               â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    193                  else:                   â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    194                      print(output)       â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    195      else:                               â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    196          import grpc                     â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    197          from ginfer import sampler_pb2, â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ sampler_pb2_grpc                                â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    198                                          â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    199          api_url =                       â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ api_url.replace("http://",                      â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ "").replace("/generate", "")                    â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    200          sampler_channel =               â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ grpc.aio.insecure_channel(api_url)              â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    201          sampler =                       â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ sampler_pb2_grpc.SamplerStub(sampler_channel)   â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    202                                          â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    203          request_end_time =              â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ time.perf_counter()                             â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    204          sample_request =                â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ sampler_pb2.SampleTextRequest(                  â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    205              prompt=prompt,              â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    206                                          â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ settings=sampler_pb2.SampleSettings(            â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    207                  max_len=output_len,     â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    208                  rng_seed=0,             â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    209                  temperature=0,          â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    210                  nucleus_p=1,            â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    211              ),                          â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    212          )                               â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    213          stream =                        â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ sampler.SampleText(sample_request)              â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    214          response = "".join()            â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    215                                          â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    216      request_end_time =                  â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ time.perf_counter()                             â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    217      request_latency = request_end_time  â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ - request_start_time                            â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    218      REQUEST_LATENCY.append((prompt_len, â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ output_len, request_latency))                   â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    219                                          â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    220                                          â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                                                 â”‚   â”‚
â”‚             â”‚ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚
â”‚ Total       â”‚ Input: 899850 Output: 9546                                                                             â”‚
â”‚ Tokens      â”‚                                                                                                        â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Status      â”‚ âœ… Step 31: Completed                                                                                  â”‚
â”‚ Tools       â”‚ ðŸ”§ str_replace_based_edit_tool                                                                         â”‚
â”‚ str_replacâ€¦ â”‚ â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚
â”‚             â”‚ â”‚ {'command': 'str_replace', 'file_text': '',    â”‚ The file                                        â”‚   â”‚
â”‚             â”‚ â”‚ 'insert_line': 0, 'new_str': '    if backend   â”‚ /home/ubuntu/OmniPerf-Bench/perf-agents-bench/â€¦ â”‚   â”‚
â”‚             â”‚ â”‚ != "ginfer":\n        while True:\n            â”‚ has been edited. Here's the result of running   â”‚   â”‚
â”‚             â”‚ â”‚ async with session.post(\n                     â”‚ `cat -n` on a snippet of                        â”‚   â”‚
â”‚             â”‚ â”‚ api_url, headers=headers, json=pload\n         â”‚ /home/ubuntu/OmniPerf-Bench/perf-agents-bench/â€¦ â”‚   â”‚
â”‚             â”‚ â”‚ ) as response:\n                chunks = []\n  â”‚    173          pass                            â”‚   â”‚
â”‚             â”‚ â”‚ async for chunk, _ in                          â”‚    174      else:                               â”‚   â”‚
â”‚             â”‚ â”‚ response.content.iter_chunks():\n              â”‚    175          raise ValueError(f"Unknown      â”‚   â”‚
â”‚             â”‚ â”‚ chunks.append(chunk)\n            output =     â”‚ backend: {backend}")                            â”‚   â”‚
â”‚             â”‚ â”‚ b"".join(chunks).decode("utf-8")\n             â”‚    176                                          â”‚   â”‚
â”‚             â”‚ â”‚ output = json.loads(output)\n\n            #   â”‚    177      if backend != "ginfer":             â”‚   â”‚
â”‚             â”‚ â”‚ Re-send the request if it failed.\n            â”‚    178          while True:                     â”‚   â”‚
â”‚             â”‚ â”‚ if "error" not in output:\n                    â”‚    179              async with session.post(    â”‚   â”‚
â”‚             â”‚ â”‚ break\n            else:\n                     â”‚    180                  api_url,                â”‚   â”‚
â”‚             â”‚ â”‚ print(output)\n', 'old_str': '    if backend   â”‚ headers=headers, json=pload                     â”‚   â”‚
â”‚             â”‚ â”‚ != "ginfer":\n        timeout =                â”‚    181              ) as response:              â”‚   â”‚
â”‚             â”‚ â”‚ aiohttp.ClientTimeout(total=3 * 3600)\n        â”‚    182                  chunks = []             â”‚   â”‚
â”‚             â”‚ â”‚ async with                                     â”‚    183                  async for chunk, _ in   â”‚   â”‚
â”‚             â”‚ â”‚ aiohttp.ClientSession(timeout=timeout) as      â”‚ response.content.iter_chunks():                 â”‚   â”‚
â”‚             â”‚ â”‚ session:\n            while True:\n            â”‚    184                                          â”‚   â”‚
â”‚             â”‚ â”‚ async with session.post(\n                     â”‚ chunks.append(chunk)                            â”‚   â”‚
â”‚             â”‚ â”‚ api_url, headers=headers, json=pload\n         â”‚    185              output =                    â”‚   â”‚
â”‚             â”‚ â”‚ ) as response:\n                    chunks =   â”‚ b"".join(chunks).decode("utf-8")                â”‚   â”‚
â”‚             â”‚ â”‚ []\n                    async for chunk, _ in  â”‚    186              output = json.loads(output) â”‚   â”‚
â”‚             â”‚ â”‚ response.content.iter_chunks():\n              â”‚    187                                          â”‚   â”‚
â”‚             â”‚ â”‚ chunks.append(chunk)\n                output = â”‚    188              # Re-send the request if it â”‚   â”‚
â”‚             â”‚ â”‚ b"".join(chunks).decode("utf-8")\n             â”‚ failed.                                         â”‚   â”‚
â”‚             â”‚ â”‚ output = json.loads(output)\n\n                â”‚    189              if "error" not in output:   â”‚   â”‚
â”‚             â”‚ â”‚ # Re-send the request if it failed.\n          â”‚    190                  break                   â”‚   â”‚
â”‚             â”‚ â”‚ if "error" not in output:\n                    â”‚    191              else:                       â”‚   â”‚
â”‚             â”‚ â”‚ break\n                else:\n                 â”‚    192                  print(output)           â”‚   â”‚
â”‚             â”‚ â”‚ print(output)\n', 'path':                      â”‚    193      else:                               â”‚   â”‚
â”‚             â”‚ â”‚ '/home/ubuntu/OmniPerf-Bench/perf-agents-bencâ€¦ â”‚    194          import grpc                     â”‚   â”‚
â”‚             â”‚ â”‚ 'view_range': []}                              â”‚    195          from ginfer import sampler_pb2, â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ sampler_pb2_grpc                                â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    196                                          â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    197          api_url =                       â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ api_url.replace("http://",                      â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ "").replace("/generate", "")                    â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ Review the changes and make sure they are as    â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ expected. Edit the file again if necessary.     â”‚   â”‚
â”‚             â”‚ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚
â”‚ Total       â”‚ Input: 950334 Output: 9925                                                                             â”‚
â”‚ Tokens      â”‚                                                                                                        â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Status      â”‚ âœ… Step 32: Completed                                                                                  â”‚
â”‚ Tools       â”‚ ðŸ”§ str_replace_based_edit_tool                                                                         â”‚
â”‚ str_replacâ€¦ â”‚ â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚
â”‚             â”‚ â”‚ {'command': 'view', 'file_text': '',           â”‚ Here's the result of running `cat -n` on        â”‚   â”‚
â”‚             â”‚ â”‚ 'insert_line': 0, 'new_str': '', 'old_str':    â”‚ /home/ubuntu/OmniPerf-Bench/perf-agents-bench/â€¦ â”‚   â”‚
â”‚             â”‚ â”‚ '', 'path':                                    â”‚    220      backend: str,                       â”‚   â”‚
â”‚             â”‚ â”‚ '/home/ubuntu/OmniPerf-Bench/perf-agents-bencâ€¦ â”‚    221      api_url: str,                       â”‚   â”‚
â”‚             â”‚ â”‚ 'view_range': [220, 246]}                      â”‚    222      input_requests: List[Tuple],        â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    223      best_of: int,                       â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    224      use_beam_search: bool,              â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    225      request_rate: float,                â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    226  ) -> None:                              â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    227      tasks: List = []                    â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    228      async for request in                â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ get_request(input_requests, request_rate):      â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    229          prompt, prompt_len, output_len  â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ = request                                       â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    230          task = asyncio.create_task(     â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    231              send_request(               â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    232                  backend,                â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    233                  api_url,                â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    234                  prompt,                 â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    235                  prompt_len,             â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    236                  output_len,             â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    237                  best_of,                â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    238                  use_beam_search,        â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    239              )                           â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    240          )                               â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    241          tasks.append(task)              â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    242      await tqdm_asyncio.gather(*tasks)   â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    243                                          â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    244                                          â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    245  def main(args: argparse.Namespace):     â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    246      print(args)                         â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                                                 â”‚   â”‚
â”‚             â”‚ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚
â”‚ Total       â”‚ Input: 1001636 Output: 10277                                                                           â”‚
â”‚ Tokens      â”‚                                                                                                        â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Status      â”‚ âœ… Step 33: Completed                                                                                  â”‚
â”‚ Tools       â”‚ ðŸ”§ str_replace_based_edit_tool                                                                         â”‚
â”‚ str_replacâ€¦ â”‚ â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚
â”‚             â”‚ â”‚ {'command': 'str_replace', 'file_text': '',    â”‚ The file                                        â”‚   â”‚
â”‚             â”‚ â”‚ 'insert_line': 0, 'new_str': '    timeout =    â”‚ /home/ubuntu/OmniPerf-Bench/perf-agents-bench/â€¦ â”‚   â”‚
â”‚             â”‚ â”‚ aiohttp.ClientTimeout(total=3 * 3600)\n        â”‚ has been edited. Here's the result of running   â”‚   â”‚
â”‚             â”‚ â”‚ async with                                     â”‚ `cat -n` on a snippet of                        â”‚   â”‚
â”‚             â”‚ â”‚ aiohttp.ClientSession(timeout=timeout) as      â”‚ /home/ubuntu/OmniPerf-Bench/perf-agents-bench/â€¦ â”‚   â”‚
â”‚             â”‚ â”‚ session:\n        tasks: List = []\n           â”‚    223      best_of: int,                       â”‚   â”‚
â”‚             â”‚ â”‚ async for request in                           â”‚    224      use_beam_search: bool,              â”‚   â”‚
â”‚             â”‚ â”‚ get_request(input_requests, request_rate):\n   â”‚    225      request_rate: float,                â”‚   â”‚
â”‚             â”‚ â”‚ prompt, prompt_len, output_len = request\n     â”‚    226  ) -> None:                              â”‚   â”‚
â”‚             â”‚ â”‚ task = asyncio.create_task(\n                  â”‚    227      timeout =                           â”‚   â”‚
â”‚             â”‚ â”‚ send_request(\n                    backend,\n  â”‚ aiohttp.ClientTimeout(total=3 * 3600)           â”‚   â”‚
â”‚             â”‚ â”‚ api_url,\n                    prompt,\n        â”‚    228      async with                          â”‚   â”‚
â”‚             â”‚ â”‚ prompt_len,\n                    output_len,\n â”‚ aiohttp.ClientSession(timeout=timeout) as       â”‚   â”‚
â”‚             â”‚ â”‚ best_of,\n                                     â”‚ session:                                        â”‚   â”‚
â”‚             â”‚ â”‚ use_beam_search,\n                             â”‚    229          tasks: List = []                â”‚   â”‚
â”‚             â”‚ â”‚ session,\n                )\n            )\n   â”‚    230          async for request in            â”‚   â”‚
â”‚             â”‚ â”‚ tasks.append(task)\n        await              â”‚ get_request(input_requests, request_rate):      â”‚   â”‚
â”‚             â”‚ â”‚ tqdm_asyncio.gather(*tasks)\n', 'old_str': '   â”‚    231              prompt, prompt_len,         â”‚   â”‚
â”‚             â”‚ â”‚ tasks: List = []\n    async for request in     â”‚ output_len = request                            â”‚   â”‚
â”‚             â”‚ â”‚ get_request(input_requests, request_rate):\n   â”‚    232              task = asyncio.create_task( â”‚   â”‚
â”‚             â”‚ â”‚ prompt, prompt_len, output_len = request\n     â”‚    233                  send_request(           â”‚   â”‚
â”‚             â”‚ â”‚ task = asyncio.create_task(\n                  â”‚    234                      backend,            â”‚   â”‚
â”‚             â”‚ â”‚ send_request(\n                backend,\n      â”‚    235                      api_url,            â”‚   â”‚
â”‚             â”‚ â”‚ api_url,\n                prompt,\n            â”‚    236                      prompt,             â”‚   â”‚
â”‚             â”‚ â”‚ prompt_len,\n                output_len,\n     â”‚    237                      prompt_len,         â”‚   â”‚
â”‚             â”‚ â”‚ best_of,\n                use_beam_search,\n   â”‚    238                      output_len,         â”‚   â”‚
â”‚             â”‚ â”‚ )\n        )\n        tasks.append(task)\n     â”‚    239                      best_of,            â”‚   â”‚
â”‚             â”‚ â”‚ await tqdm_asyncio.gather(*tasks)\n', 'path':  â”‚    240                      use_beam_search,    â”‚   â”‚
â”‚             â”‚ â”‚ '/home/ubuntu/OmniPerf-Bench/perf-agents-bencâ€¦ â”‚    241                      session,            â”‚   â”‚
â”‚             â”‚ â”‚ 'view_range': []}                              â”‚    242                  )                       â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    243              )                           â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    244              tasks.append(task)          â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    245          await                           â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ tqdm_asyncio.gather(*tasks)                     â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    246                                          â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    247                                          â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    248  def main(args: argparse.Namespace):     â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    249      print(args)                         â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    250      random.seed(args.seed)              â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ Review the changes and make sure they are as    â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ expected. Edit the file again if necessary.     â”‚   â”‚
â”‚             â”‚ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚
â”‚ Total       â”‚ Input: 1053630 Output: 10760                                                                           â”‚
â”‚ Tokens      â”‚                                                                                                        â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Status      â”‚ âœ… Step 34: Completed                                                                                  â”‚
â”‚ Tools       â”‚ ðŸ”§ bash                                                                                                â”‚
â”‚ bash        â”‚ â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚
â”‚             â”‚ â”‚ {'command': "nl -ba                            â”‚                                                 â”‚   â”‚
â”‚             â”‚ â”‚ /home/ubuntu/OmniPerf-Bench/perf-agents-benchâ€¦ â”‚                                                 â”‚   â”‚
â”‚             â”‚ â”‚ | sed -n '140,240p'", 'restart': False}        â”‚                                                 â”‚   â”‚
â”‚             â”‚ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚
â”‚ Total       â”‚ Input: 1106512 Output: 10904                                                                           â”‚
â”‚ Tokens      â”‚                                                                                                        â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Status      â”‚ âœ… Step 35: Completed                                                                                  â”‚
â”‚ Tools       â”‚ ðŸ”§ bash                                                                                                â”‚
â”‚ bash        â”‚ â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚
â”‚             â”‚ â”‚ {'command': "sed -n '160,260p'                 â”‚                                                 â”‚   â”‚
â”‚             â”‚ â”‚ /home/ubuntu/OmniPerf-Bench/perf-agents-benchâ€¦ â”‚                                                 â”‚   â”‚
â”‚             â”‚ â”‚ 'restart': False}                              â”‚                                                 â”‚   â”‚
â”‚             â”‚ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚
â”‚ Total       â”‚ Input: 1159628 Output: 10978                                                                           â”‚
â”‚ Tokens      â”‚                                                                                                        â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Status      â”‚ âœ… Step 36: Completed                                                                                  â”‚
â”‚ Tools       â”‚ ðŸ”§ bash                                                                                                â”‚
â”‚ bash        â”‚ â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚
â”‚             â”‚ â”‚ {'command': "sleep 1; sed -n '160,260p'        â”‚                                                 â”‚   â”‚
â”‚             â”‚ â”‚ /home/ubuntu/OmniPerf-Bench/perf-agents-benchâ€¦ â”‚                                                 â”‚   â”‚
â”‚             â”‚ â”‚ 'restart': False}                              â”‚                                                 â”‚   â”‚
â”‚             â”‚ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚
â”‚ Total       â”‚ Input: 1212853 Output: 11056                                                                           â”‚
â”‚ Tokens      â”‚                                                                                                        â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Status      â”‚ âœ… Step 37: Completed                                                                                  â”‚
â”‚ Tools       â”‚ ðŸ”§ bash                                                                                                â”‚
â”‚ bash        â”‚ â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚
â”‚             â”‚ â”‚ {'command': 'true', 'restart': True}           â”‚ tool has been restarted.                        â”‚   â”‚
â”‚             â”‚ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚
â”‚ Total       â”‚ Input: 1266191 Output: 11078                                                                           â”‚
â”‚ Tokens      â”‚                                                                                                        â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Status      â”‚ âœ… Step 38: Completed                                                                                  â”‚
â”‚ Tools       â”‚ ðŸ”§ bash                                                                                                â”‚
â”‚ bash        â”‚ â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚
â”‚             â”‚ â”‚ {'command': "sed -n '160,260p'                 â”‚             "at                                 â”‚   â”‚
â”‚             â”‚ â”‚ /home/ubuntu/OmniPerf-Bench/perf-agents-benchâ€¦ â”‚ https://docs.flashinfer.ai/installation.html.", â”‚   â”‚
â”‚             â”‚ â”‚ 'restart': False}                              â”‚         )                                       â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚     if server_args.chat_template:               â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚         # TODO: replace this with huggingface   â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ transformers template                           â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚         load_chat_template_for_openai_api(servâ€¦ â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                                                 â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚     # Allocate ports                            â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚     assert server_args.tp_size %                â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ server_args.nnodes == 0                         â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚     tp_size_local = server_args.tp_size //      â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ server_args.nnodes                              â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚     server_args.port,                           â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ server_args.additional_ports =                  â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ allocate_init_ports(                            â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚         server_args.port,                       â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚         server_args.additional_ports,           â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚         tp_size_local,                          â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚         server_args.dp_size,                    â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚     )                                           â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                                                 â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚     ports = server_args.additional_ports        â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚     model_port_args = []                        â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚     for i in range(server_args.dp_size):        â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚         model_port_args.append(                 â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚             ModelPortArgs(                      â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                 nccl_port=ports[3 + i *         â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ (tp_size_local + 1)],                           â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                 model_tp_ips=[None] *           â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ tp_size_local,                                  â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                 model_tp_ports=ports[           â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                     3 + i * (tp_size_local + 1) â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ + 1 : 3 + (i + 1) * (tp_size_local + 1)         â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                 ],                              â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚             )                                   â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚         )                                       â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚     port_args = PortArgs(                       â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚         tokenizer_port=ports[0],                â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚         router_port=ports[1],                   â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚         detokenizer_port=ports[2],              â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚         model_port_args=model_port_args,        â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚     )                                           â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                                                 â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚     # TODO multi-node dp is not supported       â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚     assert not (server_args.dp_size > 1 and     â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ server_args.node_rank is not None)              â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚     if server_args.nnodes > 1:                  â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚         if server_args.node_rank != 0:          â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚             send_addrs_to_rank_0(model_port_arâ€¦ â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ server_args)                                    â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚         else:                                   â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚             receive_addrs(model_port_args[0],   â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ server_args)                                    â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚         for i in range(tp_size_local):          â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚             start_rpyc_service_process(         â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                 ModelTpService,                 â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ model_port_args[0].model_tp_ports               â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ [3m            )[0m                                   â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ [3m        if server_args.node_rank != 0:[0m          â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ [3m            logger.info([0m                        â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ [3m                f"[0m[3m: Listen for connections..."[0m  â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ [3m            )[0m                                   â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ [3m            while True:[0m                         â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ [3m                pass[0m                            â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                                                 â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ [3m    # Launch processes[0m                          â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ [3m    tokenizer_manager = [0m                        â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ [3mTokenizerManager(server_args, port_args, [0m       â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ [3mmodel_overide_args)[0m                             â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ [3m    pipe_router_reader, pipe_router_writer = [0m   â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ [3mmp.Pipe(duplex=False)[0m                           â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ [3m    pipe_detoken_reader, pipe_detoken_writer = [0m â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ [3mmp.Pipe(duplex=False)[0m                           â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                                                 â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ [3m    if server_args.dp_size == 1:[0m                â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ [3m        start_process = [0m                        â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ [3mstart_controller_process_single[0m                 â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ [3m    else:[0m                                       â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ [3m        start_process = [0m                        â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ [3mstart_controller_process_multi[0m                  â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ [3m    proc_router = mp.Process([0m                   â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ [3m        target=start_process,[0m                   â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ [3m        args=(server_args, port_args, [0m          â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ [3mpipe_router_writer, model_overide_args),[0m        â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ [3m    )[0m                                           â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ [3m    proc_router.start()[0m                         â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ [3m    proc_detoken = mp.Process([0m                  â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ [3m        target=start_detokenizer_process,[0m       â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ [3m        args=([0m                                  â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ [3m            server_args,[0m                        â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ [3m            port_args,[0m                          â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ [3m            pipe_detoken_writer,[0m                â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ [3m        ),[0m                                      â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ [3m    )[0m                                           â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ [3m    proc_detoken.start()[0m                        â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                                                 â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ [3m    # Wait for the model to finish loading[0m      â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ [3m    router_init_state = [0m                        â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ [3mpipe_router_reader.recv()[0m                       â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ [3m    detoken_init_state = [0m                       â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ [3mpipe_detoken_reader.recv()[0m                      â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                                                 â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ [3m    if router_init_state != "init ok" or [0m       â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ [3mdetoken_init_state != "init ok":[0m                â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ [3m        proc_router.kill()[0m                      â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ [3m        proc_detoken.kill()[0m                     â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ [3m        print([0m                                  â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ [3m            f"Initialization failed. [0m           â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ [3mrouter_init_state: {router_init_state}", [0m       â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ [3mflush=True[0m                                      â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ [3m        )[0m                                       â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ [3m        print([0m                                  â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ [3m            f"Initialization failed. [0m           â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ [3mdetoken_init_state: {detoken_init_state}",[0m      â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ [3m            flush=True,[0m                         â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ [3m        )[0m                                       â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ [3m        sys.exit(1)[0m                             â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ [3m    assert proc_router.is_alive() and [0m          â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ [3mproc_detoken.is_alive()[0m                         â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                                                 â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ [3m    if server_args.api_key and [0m                 â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ [3mserver_args.api_key != "":[0m                      â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ [3m        app.add_middleware(APIKeyValidatorMiddâ€¦[0m â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ [3mapi_key=server_args.api_key)[0m                    â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                                                 â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ [3m    # Send a warmup request[0m                     â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ [3m    def _wait_and_warmup():[0m                     â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ [3m        headers = {}[0m                            â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ [3m        url = server_args.url()[0m                 â”‚   â”‚
â”‚             â”‚ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚
â”‚ Total       â”‚ Input: 1319567 Output: 11152                                                                           â”‚
â”‚ Tokens      â”‚                                                                                                        â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Status      â”‚ âœ… Step 39: Completed                                                                                  â”‚
â”‚ Tools       â”‚ ðŸ”§ str_replace_based_edit_tool                                                                         â”‚
â”‚ str_replacâ€¦ â”‚ â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚
â”‚             â”‚ â”‚ {'command': 'str_replace', 'file_text': '',    â”‚ The file                                        â”‚   â”‚
â”‚             â”‚ â”‚ 'insert_line': 0, 'new_str': '                 â”‚ /home/ubuntu/OmniPerf-Bench/perf-agents-bench/â€¦ â”‚   â”‚
â”‚             â”‚ â”‚ while True:\n                time.sleep(1)\n', â”‚ has been edited. Here's the result of running   â”‚   â”‚
â”‚             â”‚ â”‚ 'old_str': '            while True:\n          â”‚ `cat -n` on a snippet of                        â”‚   â”‚
â”‚             â”‚ â”‚ pass\n', 'path':                               â”‚ /home/ubuntu/OmniPerf-Bench/perf-agents-bench/â€¦ â”‚   â”‚
â”‚             â”‚ â”‚ '/home/ubuntu/OmniPerf-Bench/perf-agents-bencâ€¦ â”‚    206          if server_args.node_rank != 0:  â”‚   â”‚
â”‚             â”‚ â”‚ 'view_range': []}                              â”‚    207              logger.info(                â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    208                  f": Listen for          â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ connections..."                                 â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    209              )                           â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    210              while True:                 â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    211                  time.sleep(1)           â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    212                                          â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    213      # Launch processes                  â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    214      tokenizer_manager =                 â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ TokenizerManager(server_args, port_args,        â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ model_overide_args)                             â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    215      pipe_router_reader,                 â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ pipe_router_writer = mp.Pipe(duplex=False)      â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    216      pipe_detoken_reader,                â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ pipe_detoken_writer = mp.Pipe(duplex=False)     â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ Review the changes and make sure they are as    â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ expected. Edit the file again if necessary.     â”‚   â”‚
â”‚             â”‚ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚
â”‚ Total       â”‚ Input: 1373824 Output: 11263                                                                           â”‚
â”‚ Tokens      â”‚                                                                                                        â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Status      â”‚ âœ… Step 40: Completed                                                                                  â”‚
â”‚ Tools       â”‚ ðŸ”§ str_replace_based_edit_tool                                                                         â”‚
â”‚ str_replacâ€¦ â”‚ â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚
â”‚             â”‚ â”‚ {'command': 'str_replace', 'file_text': '',    â”‚ The file                                        â”‚   â”‚
â”‚             â”‚ â”‚ 'insert_line': 0, 'new_str': '        a =      â”‚ /home/ubuntu/OmniPerf-Bench/perf-agents-bench/â€¦ â”‚   â”‚
â”‚             â”‚ â”‚ torch.empty((16, 16), dtype=dtype,             â”‚ has been edited. Here's the result of running   â”‚   â”‚
â”‚             â”‚ â”‚ device=device)\n        b = torch.empty((16,   â”‚ `cat -n` on a snippet of                        â”‚   â”‚
â”‚             â”‚ â”‚ 16), dtype=dtype, device=device)\n',           â”‚ /home/ubuntu/OmniPerf-Bench/perf-agents-bench/â€¦ â”‚   â”‚
â”‚             â”‚ â”‚ 'old_str': '        a = torch.ones((16, 16),   â”‚    186      def init_cublas(self):              â”‚   â”‚
â”‚             â”‚ â”‚ dtype=dtype, device=device)\n        b =       â”‚    187          """We need to run a small       â”‚   â”‚
â”‚             â”‚ â”‚ torch.ones((16, 16), dtype=dtype,              â”‚ matmul to init cublas. Otherwise, it will raise â”‚   â”‚
â”‚             â”‚ â”‚ device=device)\n', 'path':                     â”‚ some errors later."""                           â”‚   â”‚
â”‚             â”‚ â”‚ '/home/ubuntu/OmniPerf-Bench/perf-agents-bencâ€¦ â”‚    188          dtype = torch.float16           â”‚   â”‚
â”‚             â”‚ â”‚ 'view_range': []}                              â”‚    189          device = "cuda"                 â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    190          a = torch.empty((16, 16),       â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ dtype=dtype, device=device)                     â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    191          b = torch.empty((16, 16),       â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ dtype=dtype, device=device)                     â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    192          c = a @ b                       â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    193          return c                        â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    194                                          â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    195      def init_flash_infer(self):         â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    196          if                              â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ self.server_args.disable_flashinfer:            â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ Review the changes and make sure they are as    â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ expected. Edit the file again if necessary.     â”‚   â”‚
â”‚             â”‚ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚
â”‚ Total       â”‚ Input: 1428522 Output: 11498                                                                           â”‚
â”‚ Tokens      â”‚                                                                                                        â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Status      â”‚ âœ… Step 41: Completed                                                                                  â”‚
â”‚ Tools       â”‚ ðŸ”§ str_replace_based_edit_tool                                                                         â”‚
â”‚ str_replacâ€¦ â”‚ â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚
â”‚             â”‚ â”‚ {'command': 'str_replace', 'file_text': '',    â”‚ The file                                        â”‚   â”‚
â”‚             â”‚ â”‚ 'insert_line': 0, 'new_str': '- See            â”‚ /home/ubuntu/OmniPerf-Bench/perf-agents-bench/â€¦ â”‚   â”‚
â”‚             â”‚ â”‚ (docs/hyperparameter_tuning.md) on tuning      â”‚ has been edited. Here's the result of running   â”‚   â”‚
â”‚             â”‚ â”‚ hyperparameters for better performance.\n- Add â”‚ `cat -n` on a snippet of                        â”‚   â”‚
â”‚             â”‚ â”‚ `--nnodes 2` to run tensor parallelism on      â”‚ /home/ubuntu/OmniPerf-Bench/perf-agents-bench/â€¦ â”‚   â”‚
â”‚             â”‚ â”‚ multiple nodes. If you have two nodes with two â”‚    375  - If you see out-of-memory errors       â”‚   â”‚
â”‚             â”‚ â”‚ GPUs on each node and want to run TP=4, let    â”‚ during serving, please try to reduce the memory â”‚   â”‚
â”‚             â”‚ â”‚ `sgl-dev-1` be the hostname of the first node  â”‚ usage of the KV cache pool by setting a smaller â”‚   â”‚
â”‚             â”‚ â”‚ and `50000` be an available port.\n```\n# Node â”‚ value of `--mem-fraction-static`. The default   â”‚   â”‚
â”‚             â”‚ â”‚ 0\npython -m sglang.launch_server --model-path â”‚ value is `0.9`                                  â”‚   â”‚
â”‚             â”‚ â”‚ meta-llama/Llama-2-7b-chat-hf --tp 4           â”‚    376  ```                                     â”‚   â”‚
â”‚             â”‚ â”‚ --nccl-init-addr sgl-dev-1:50000 --nnodes 2    â”‚    377  python -m sglang.launch_server          â”‚   â”‚
â”‚             â”‚ â”‚ --node-rank 0\n\n# Node 1\npython -m           â”‚ --model-path meta-llama/Llama-2-7b-chat-hf      â”‚   â”‚
â”‚             â”‚ â”‚ sglang.launch_server --model-path              â”‚ --port 30000 --mem-fraction-static 0.7          â”‚   â”‚
â”‚             â”‚ â”‚ meta-llama/Llama-2-7b-chat-hf --tp 4           â”‚    378  ```                                     â”‚   â”‚
â”‚             â”‚ â”‚ --nccl-init-addr sgl-dev-1:50000 --nnodes 2    â”‚    379  - See (docs/hyperparameter_tuning.md)   â”‚   â”‚
â”‚             â”‚ â”‚ --node-rank 1\n```\n', 'old_str': '- See       â”‚ on tuning hyperparameters for better            â”‚   â”‚
â”‚             â”‚ â”‚ (docs/hyperparameter_tuning.md) on tuning      â”‚ performance.                                    â”‚   â”‚
â”‚             â”‚ â”‚ hyperparameters for better performance.\n',    â”‚    380  - Add `--nnodes 2` to run tensor        â”‚   â”‚
â”‚             â”‚ â”‚ 'path':                                        â”‚ parallelism on multiple nodes. If you have two  â”‚   â”‚
â”‚             â”‚ â”‚ '/home/ubuntu/OmniPerf-Bench/perf-agents-bencâ€¦ â”‚ nodes with two GPUs on each node and want to    â”‚   â”‚
â”‚             â”‚ â”‚ 'view_range': []}                              â”‚ run TP=4, let `sgl-dev-1` be the hostname of    â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ the first node and `50000` be an available      â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ port.                                           â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    381  ```                                     â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    382  # Node 0                                â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    383  python -m sglang.launch_server          â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ --model-path meta-llama/Llama-2-7b-chat-hf --tp â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ 4 --nccl-init-addr sgl-dev-1:50000 --nnodes 2   â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ --node-rank 0                                   â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    384                                          â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    385  # Node 1                                â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    386  python -m sglang.launch_server          â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ --model-path meta-llama/Llama-2-7b-chat-hf --tp â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ 4 --nccl-init-addr sgl-dev-1:50000 --nnodes 2   â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ --node-rank 1                                   â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    387  ```                                     â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    388                                          â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    389  ### Supported Models                    â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    390  - Llama                                 â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    391  - Mistral                               â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    392  - Mixtral                               â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ Review the changes and make sure they are as    â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ expected. Edit the file again if necessary.     â”‚   â”‚
â”‚             â”‚ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚
â”‚ Total       â”‚ Input: 1483787 Output: 12018                                                                           â”‚
â”‚ Tokens      â”‚                                                                                                        â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Status      â”‚ âœ… Step 42: Completed                                                                                  â”‚
â”‚ Tools       â”‚ ðŸ”§ str_replace_based_edit_tool                                                                         â”‚
â”‚ str_replacâ€¦ â”‚ â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚
â”‚             â”‚ â”‚ {'command': 'str_replace', 'file_text': '',    â”‚ The file                                        â”‚   â”‚
â”‚             â”‚ â”‚ 'insert_line': 0, 'new_str': '- `srt`: The     â”‚ /home/ubuntu/OmniPerf-Bench/perf-agents-bench/â€¦ â”‚   â”‚
â”‚             â”‚ â”‚ runtime for running local models. Includes     â”‚ has been edited. Here's the result of running   â”‚   â”‚
â”‚             â”‚ â”‚ distributed serving options (e.g., `--nnodes`, â”‚ `cat -n` on a snippet of                        â”‚   â”‚
â”‚             â”‚ â”‚ `--node-rank`, `--nccl-init-addr`).\n',        â”‚ /home/ubuntu/OmniPerf-Bench/perf-agents-bench/â€¦ â”‚   â”‚
â”‚             â”‚ â”‚ 'old_str': '- `srt`: The runtime for running   â”‚      1  # Code Structure                        â”‚   â”‚
â”‚             â”‚ â”‚ local models.\n', 'path':                      â”‚      2                                          â”‚   â”‚
â”‚             â”‚ â”‚ '/home/ubuntu/OmniPerf-Bench/perf-agents-bencâ€¦ â”‚      3  - `backend`: Various backends for the   â”‚   â”‚
â”‚             â”‚ â”‚ 'view_range': []}                              â”‚ language interpreter.                           â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚      4  - `lang`: The frontend language.        â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚      5  - `srt`: The runtime for running local  â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ models. Includes distributed serving options    â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ (e.g., `--nnodes`, `--node-rank`,               â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ `--nccl-init-addr`).                            â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚      6  - `test`: Test utilities.               â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚      7  - `api.py`: Public API.                 â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚      8  - `bench_latency.py`: Benchmark         â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ utilities.                                      â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚      9  - `global_config.py`: The global        â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ configs and constants.                          â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚     10  - `launch_server.py`: The entry point   â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ of launching local server.                      â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ Review the changes and make sure they are as    â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ expected. Edit the file again if necessary.     â”‚   â”‚
â”‚             â”‚ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚
â”‚ Total       â”‚ Input: 1540119 Output: 12226                                                                           â”‚
â”‚ Tokens      â”‚                                                                                                        â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Status      â”‚ âœ… Step 43: Completed                                                                                  â”‚
â”‚ Tools       â”‚ ðŸ”§ bash                                                                                                â”‚
â”‚ bash        â”‚ â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚
â”‚             â”‚ â”‚ {'command': 'python3                           â”‚                                                 â”‚   â”‚
â”‚             â”‚ â”‚ /home/ubuntu/OmniPerf-Bench/perf-agents-benchâ€¦ â”‚                                                 â”‚   â”‚
â”‚             â”‚ â”‚ 'restart': False}                              â”‚                                                 â”‚   â”‚
â”‚             â”‚ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚
â”‚ Total       â”‚ Input: 1596975 Output: 12358                                                                           â”‚
â”‚ Tokens      â”‚                                                                                                        â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Status      â”‚ âœ… Step 44: Completed                                                                                  â”‚
â”‚ Tools       â”‚ ðŸ”§ bash                                                                                                â”‚
â”‚ bash        â”‚ â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚
â”‚             â”‚ â”‚ {'command': 'python3                           â”‚ tool has been restarted.                        â”‚   â”‚
â”‚             â”‚ â”‚ /home/ubuntu/OmniPerf-Bench/perf-agents-benchâ€¦ â”‚                                                 â”‚   â”‚
â”‚             â”‚ â”‚ 'restart': True}                               â”‚                                                 â”‚   â”‚
â”‚             â”‚ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚
â”‚ Total       â”‚ Input: 1654024 Output: 12424                                                                           â”‚
â”‚ Tokens      â”‚                                                                                                        â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Status      â”‚ âœ… Step 45: Completed                                                                                  â”‚
â”‚ Tools       â”‚ ðŸ”§ bash                                                                                                â”‚
â”‚ bash        â”‚ â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚
â”‚             â”‚ â”‚ {'command': 'python3                           â”‚ {'create_close_sessions_s': 0.012742,           â”‚   â”‚
â”‚             â”‚ â”‚ /home/ubuntu/OmniPerf-Bench/perf-agents-benchâ€¦ â”‚ 'reuse_single_session_s': 2.8e-05, 'speedup':   â”‚   â”‚
â”‚             â”‚ â”‚ 'restart': False}                              â”‚ 460.79}                                         â”‚   â”‚
â”‚             â”‚ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚
â”‚ Total       â”‚ Input: 1711155 Output: 12490                                                                           â”‚
â”‚ Tokens      â”‚                                                                                                        â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Status      â”‚ âœ… Step 46: Completed                                                                                  â”‚
â”‚ LLM         â”‚ ðŸ’¬ I will stage changes, unstage the .bench_scratch timing script, commit with the requested message,  â”‚
â”‚ Response    â”‚ and generate the patch file as instructed.                                                             â”‚
â”‚ Tools       â”‚ ðŸ”§ bash                                                                                                â”‚
â”‚ bash        â”‚ â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚
â”‚             â”‚ â”‚ {'command': "cd                                â”‚  Optimize MoE align sum kernels performance     â”‚   â”‚
â”‚             â”‚ â”‚ /home/ubuntu/OmniPerf-Bench/perf-agents-benchâ€¦ â”‚  Committer: Ubuntu                              â”‚   â”‚
â”‚             â”‚ â”‚ && git add -A && git reset -q .bench_scratch   â”‚ <ubuntu@ip-172-31-16-122.ec2.internal>          â”‚   â”‚
â”‚             â”‚ â”‚ || true && git commit -m 'Optimize MoE align   â”‚ Your name and email address were configured     â”‚   â”‚
â”‚             â”‚ â”‚ sum kernels performance' || true && git diff   â”‚ automatically based                             â”‚   â”‚
â”‚             â”‚ â”‚ $(git merge-base HEAD origin/HEAD || git       â”‚ on your username and hostname. Please check     â”‚   â”‚
â”‚             â”‚ â”‚ rev-parse HEAD~1) -- .                         â”‚ that they are accurate.                         â”‚   â”‚
â”‚             â”‚ â”‚ ':(exclude).bench_scratch' >                   â”‚ You can suppress this message by setting them   â”‚   â”‚
â”‚             â”‚ â”‚ /home/ubuntu/OmniPerf-Bench/perf-agents-benchâ€¦ â”‚ explicitly. Run the                             â”‚   â”‚
â”‚             â”‚ â”‚ || true", 'restart': False}                    â”‚ following command and follow the instructions   â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ in your editor to edit                          â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ your configuration file:                        â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                                                 â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚     git config --global --edit                  â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                                                 â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ After doing this, you may fix the identity used â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ for this commit with:                           â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                                                 â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚     git commit --amend --reset-author           â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                                                 â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚  6 files changed, 54 insertions(+), 42          â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ deletions(-)                                    â”‚   â”‚
â”‚             â”‚ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚
â”‚ Total       â”‚ Input: 1768398 Output: 12882                                                                           â”‚
â”‚ Tokens      â”‚                                                                                                        â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Status      â”‚ âœ… Step 47: Completed                                                                                  â”‚
â”‚ Total       â”‚ Input: 1826251 Output: 13687                                                                           â”‚
â”‚ Tokens      â”‚                                                                                                        â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
OpenAI API call failed: Error code: 400 - {'error': {'message': 'No tool output found for function call call_cu9qams7YpbiN3lWH5OmiEdv.', 'type': 'invalid_request_error', 'param': 'input', 'code': None}}. Will sleep for 16 seconds and will retry.
Traceback (most recent call last):
  File "/home/ubuntu/OmniPerf-Bench/third-party/trae-agent/trae_agent/utils/llm_clients/retry_utils.py", line 36, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/OmniPerf-Bench/third-party/trae-agent/trae_agent/utils/llm_clients/openai_client.py", line 86, in _create_openai_response
    response = self.client.responses.create(**api_params)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/OmniPerf-Bench/bench-env/lib/python3.12/site-packages/openai/resources/responses/responses.py", line 795, in create
    return self._post(
           ^^^^^^^^^^^
  File "/home/ubuntu/OmniPerf-Bench/bench-env/lib/python3.12/site-packages/openai/_base_client.py", line 1259, in post
    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))
                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/OmniPerf-Bench/bench-env/lib/python3.12/site-packages/openai/_base_client.py", line 1047, in request
    raise self._make_status_error_from_response(err.response) from None
openai.BadRequestError: Error code: 400 - {'error': {'message': 'No tool output found for function call call_cu9qams7YpbiN3lWH5OmiEdv.', 'type': 'invalid_request_error', 'param': 'input', 'code': None}}

OpenAI API call failed: Error code: 400 - {'error': {'message': 'No tool output found for function call call_cu9qams7YpbiN3lWH5OmiEdv.', 'type': 'invalid_request_error', 'param': 'input', 'code': None}}. Will sleep for 7 seconds and will retry.
Traceback (most recent call last):
  File "/home/ubuntu/OmniPerf-Bench/third-party/trae-agent/trae_agent/utils/llm_clients/retry_utils.py", line 36, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/OmniPerf-Bench/third-party/trae-agent/trae_agent/utils/llm_clients/openai_client.py", line 86, in _create_openai_response
    response = self.client.responses.create(**api_params)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/OmniPerf-Bench/bench-env/lib/python3.12/site-packages/openai/resources/responses/responses.py", line 795, in create
    return self._post(
           ^^^^^^^^^^^
  File "/home/ubuntu/OmniPerf-Bench/bench-env/lib/python3.12/site-packages/openai/_base_client.py", line 1259, in post
    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))
                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/OmniPerf-Bench/bench-env/lib/python3.12/site-packages/openai/_base_client.py", line 1047, in request
    raise self._make_status_error_from_response(err.response) from None
openai.BadRequestError: Error code: 400 - {'error': {'message': 'No tool output found for function call call_cu9qams7YpbiN3lWH5OmiEdv.', 'type': 'invalid_request_error', 'param': 'input', 'code': None}}

OpenAI API call failed: Error code: 400 - {'error': {'message': 'No tool output found for function call call_cu9qams7YpbiN3lWH5OmiEdv.', 'type': 'invalid_request_error', 'param': 'input', 'code': None}}. Will sleep for 20 seconds and will retry.
Traceback (most recent call last):
  File "/home/ubuntu/OmniPerf-Bench/third-party/trae-agent/trae_agent/utils/llm_clients/retry_utils.py", line 36, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/OmniPerf-Bench/third-party/trae-agent/trae_agent/utils/llm_clients/openai_client.py", line 86, in _create_openai_response
    response = self.client.responses.create(**api_params)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/OmniPerf-Bench/bench-env/lib/python3.12/site-packages/openai/resources/responses/responses.py", line 795, in create
    return self._post(
           ^^^^^^^^^^^
  File "/home/ubuntu/OmniPerf-Bench/bench-env/lib/python3.12/site-packages/openai/_base_client.py", line 1259, in post
    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))
                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/OmniPerf-Bench/bench-env/lib/python3.12/site-packages/openai/_base_client.py", line 1047, in request
    raise self._make_status_error_from_response(err.response) from None
openai.BadRequestError: Error code: 400 - {'error': {'message': 'No tool output found for function call call_cu9qams7YpbiN3lWH5OmiEdv.', 'type': 'invalid_request_error', 'param': 'input', 'code': None}}

OpenAI API call failed: Error code: 400 - {'error': {'message': 'No tool output found for function call call_cu9qams7YpbiN3lWH5OmiEdv.', 'type': 'invalid_request_error', 'param': 'input', 'code': None}}. Will sleep for 17 seconds and will retry.
Traceback (most recent call last):
  File "/home/ubuntu/OmniPerf-Bench/third-party/trae-agent/trae_agent/utils/llm_clients/retry_utils.py", line 36, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/OmniPerf-Bench/third-party/trae-agent/trae_agent/utils/llm_clients/openai_client.py", line 86, in _create_openai_response
    response = self.client.responses.create(**api_params)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/OmniPerf-Bench/bench-env/lib/python3.12/site-packages/openai/resources/responses/responses.py", line 795, in create
    return self._post(
           ^^^^^^^^^^^
  File "/home/ubuntu/OmniPerf-Bench/bench-env/lib/python3.12/site-packages/openai/_base_client.py", line 1259, in post
    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))
                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/OmniPerf-Bench/bench-env/lib/python3.12/site-packages/openai/_base_client.py", line 1047, in request
    raise self._make_status_error_from_response(err.response) from None
openai.BadRequestError: Error code: 400 - {'error': {'message': 'No tool output found for function call call_cu9qams7YpbiN3lWH5OmiEdv.', 'type': 'invalid_request_error', 'param': 'input', 'code': None}}

OpenAI API call failed: Error code: 400 - {'error': {'message': 'No tool output found for function call call_cu9qams7YpbiN3lWH5OmiEdv.', 'type': 'invalid_request_error', 'param': 'input', 'code': None}}. Will sleep for 21 seconds and will retry.
Traceback (most recent call last):
  File "/home/ubuntu/OmniPerf-Bench/third-party/trae-agent/trae_agent/utils/llm_clients/retry_utils.py", line 36, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/OmniPerf-Bench/third-party/trae-agent/trae_agent/utils/llm_clients/openai_client.py", line 86, in _create_openai_response
    response = self.client.responses.create(**api_params)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/OmniPerf-Bench/bench-env/lib/python3.12/site-packages/openai/resources/responses/responses.py", line 795, in create
    return self._post(
           ^^^^^^^^^^^
  File "/home/ubuntu/OmniPerf-Bench/bench-env/lib/python3.12/site-packages/openai/_base_client.py", line 1259, in post
    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))
                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/OmniPerf-Bench/bench-env/lib/python3.12/site-packages/openai/_base_client.py", line 1047, in request
    raise self._make_status_error_from_response(err.response) from None
openai.BadRequestError: Error code: 400 - {'error': {'message': 'No tool output found for function call call_cu9qams7YpbiN3lWH5OmiEdv.', 'type': 'invalid_request_error', 'param': 'input', 'code': None}}

OpenAI API call failed: Error code: 400 - {'error': {'message': 'No tool output found for function call call_cu9qams7YpbiN3lWH5OmiEdv.', 'type': 'invalid_request_error', 'param': 'input', 'code': None}}. Will sleep for 13 seconds and will retry.
Traceback (most recent call last):
  File "/home/ubuntu/OmniPerf-Bench/third-party/trae-agent/trae_agent/utils/llm_clients/retry_utils.py", line 36, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/OmniPerf-Bench/third-party/trae-agent/trae_agent/utils/llm_clients/openai_client.py", line 86, in _create_openai_response
    response = self.client.responses.create(**api_params)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/OmniPerf-Bench/bench-env/lib/python3.12/site-packages/openai/resources/responses/responses.py", line 795, in create
    return self._post(
           ^^^^^^^^^^^
  File "/home/ubuntu/OmniPerf-Bench/bench-env/lib/python3.12/site-packages/openai/_base_client.py", line 1259, in post
    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))
                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/OmniPerf-Bench/bench-env/lib/python3.12/site-packages/openai/_base_client.py", line 1047, in request
    raise self._make_status_error_from_response(err.response) from None
openai.BadRequestError: Error code: 400 - {'error': {'message': 'No tool output found for function call call_cu9qams7YpbiN3lWH5OmiEdv.', 'type': 'invalid_request_error', 'param': 'input', 'code': None}}

OpenAI API call failed: Error code: 400 - {'error': {'message': 'No tool output found for function call call_cu9qams7YpbiN3lWH5OmiEdv.', 'type': 'invalid_request_error', 'param': 'input', 'code': None}}. Will sleep for 11 seconds and will retry.
Traceback (most recent call last):
  File "/home/ubuntu/OmniPerf-Bench/third-party/trae-agent/trae_agent/utils/llm_clients/retry_utils.py", line 36, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/OmniPerf-Bench/third-party/trae-agent/trae_agent/utils/llm_clients/openai_client.py", line 86, in _create_openai_response
    response = self.client.responses.create(**api_params)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/OmniPerf-Bench/bench-env/lib/python3.12/site-packages/openai/resources/responses/responses.py", line 795, in create
    return self._post(
           ^^^^^^^^^^^
  File "/home/ubuntu/OmniPerf-Bench/bench-env/lib/python3.12/site-packages/openai/_base_client.py", line 1259, in post
    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))
                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/OmniPerf-Bench/bench-env/lib/python3.12/site-packages/openai/_base_client.py", line 1047, in request
    raise self._make_status_error_from_response(err.response) from None
openai.BadRequestError: Error code: 400 - {'error': {'message': 'No tool output found for function call call_cu9qams7YpbiN3lWH5OmiEdv.', 'type': 'invalid_request_error', 'param': 'input', 'code': None}}

OpenAI API call failed: Error code: 400 - {'error': {'message': 'No tool output found for function call call_cu9qams7YpbiN3lWH5OmiEdv.', 'type': 'invalid_request_error', 'param': 'input', 'code': None}}. Will sleep for 18 seconds and will retry.
Traceback (most recent call last):
  File "/home/ubuntu/OmniPerf-Bench/third-party/trae-agent/trae_agent/utils/llm_clients/retry_utils.py", line 36, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/OmniPerf-Bench/third-party/trae-agent/trae_agent/utils/llm_clients/openai_client.py", line 86, in _create_openai_response
    response = self.client.responses.create(**api_params)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/OmniPerf-Bench/bench-env/lib/python3.12/site-packages/openai/resources/responses/responses.py", line 795, in create
    return self._post(
           ^^^^^^^^^^^
  File "/home/ubuntu/OmniPerf-Bench/bench-env/lib/python3.12/site-packages/openai/_base_client.py", line 1259, in post
    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))
                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/OmniPerf-Bench/bench-env/lib/python3.12/site-packages/openai/_base_client.py", line 1047, in request
    raise self._make_status_error_from_response(err.response) from None
openai.BadRequestError: Error code: 400 - {'error': {'message': 'No tool output found for function call call_cu9qams7YpbiN3lWH5OmiEdv.', 'type': 'invalid_request_error', 'param': 'input', 'code': None}}

OpenAI API call failed: Error code: 400 - {'error': {'message': 'No tool output found for function call call_cu9qams7YpbiN3lWH5OmiEdv.', 'type': 'invalid_request_error', 'param': 'input', 'code': None}}. Will sleep for 7 seconds and will retry.
Traceback (most recent call last):
  File "/home/ubuntu/OmniPerf-Bench/third-party/trae-agent/trae_agent/utils/llm_clients/retry_utils.py", line 36, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/OmniPerf-Bench/third-party/trae-agent/trae_agent/utils/llm_clients/openai_client.py", line 86, in _create_openai_response
    response = self.client.responses.create(**api_params)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/OmniPerf-Bench/bench-env/lib/python3.12/site-packages/openai/resources/responses/responses.py", line 795, in create
    return self._post(
           ^^^^^^^^^^^
  File "/home/ubuntu/OmniPerf-Bench/bench-env/lib/python3.12/site-packages/openai/_base_client.py", line 1259, in post
    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))
                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/OmniPerf-Bench/bench-env/lib/python3.12/site-packages/openai/_base_client.py", line 1047, in request
    raise self._make_status_error_from_response(err.response) from None
openai.BadRequestError: Error code: 400 - {'error': {'message': 'No tool output found for function call call_cu9qams7YpbiN3lWH5OmiEdv.', 'type': 'invalid_request_error', 'param': 'input', 'code': None}}

OpenAI API call failed: Error code: 400 - {'error': {'message': 'No tool output found for function call call_cu9qams7YpbiN3lWH5OmiEdv.', 'type': 'invalid_request_error', 'param': 'input', 'code': None}}. Will sleep for 7 seconds and will retry.
Traceback (most recent call last):
  File "/home/ubuntu/OmniPerf-Bench/third-party/trae-agent/trae_agent/utils/llm_clients/retry_utils.py", line 36, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/OmniPerf-Bench/third-party/trae-agent/trae_agent/utils/llm_clients/openai_client.py", line 86, in _create_openai_response
    response = self.client.responses.create(**api_params)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/OmniPerf-Bench/bench-env/lib/python3.12/site-packages/openai/resources/responses/responses.py", line 795, in create
    return self._post(
           ^^^^^^^^^^^
  File "/home/ubuntu/OmniPerf-Bench/bench-env/lib/python3.12/site-packages/openai/_base_client.py", line 1259, in post
    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))
                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/OmniPerf-Bench/bench-env/lib/python3.12/site-packages/openai/_base_client.py", line 1047, in request
    raise self._make_status_error_from_response(err.response) from None
openai.BadRequestError: Error code: 400 - {'error': {'message': 'No tool output found for function call call_cu9qams7YpbiN3lWH5OmiEdv.', 'type': 'invalid_request_error', 'param': 'input', 'code': None}}

â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Status      â”‚ âœ… Step 48: Completed                                                                                  â”‚
â”‚ Error       â”‚ âŒ Error code: 400 - {'error': {'message': 'No tool output found for function call                     â”‚
â”‚             â”‚ call_cu9qams7YpbiN3lWH5OmiEdv.', 'type': 'invalid_request_error', 'param': 'input', 'code': None}}     â”‚
â”‚ Total       â”‚ Input: 1826251 Output: 13687                                                                           â”‚
â”‚ Tokens      â”‚                                                                                                        â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

============================================================
[1mLakeview Summary[0m
============================================================
â•­â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ Step 1 (Lakeview) â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•®
â”‚ [ðŸ‘ï¸EXAMINE_CODE] The agent [1m<task>The agent is exploring the repository [0m       â”‚
â”‚ [1mstructure.[0m                                                                   â”‚
â”‚ [3mBegins by listing the sglang_core worktree to find torch.zeros/fill_ usages [0m â”‚
â”‚ [3mand prepare a .bench_scratch/test_opt.py benchmark before applying [0m          â”‚
â”‚ [3mperformance optimizations.[0m                                                   â”‚
â•°â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•¯
â•­â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ Step 2 (Lakeview) â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•®
â”‚ [ðŸ‘ï¸EXAMINE_CODE] The agent [1m<task>The agent is examining source code.[0m          â”‚
â”‚ [3mListing benchmark/latency_throughput directory in the sglang_core worktree [0m  â”‚
â”‚ [3mto find relevant benchmarking scripts for performance investigation.[0m         â”‚
â•°â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•¯
â•­â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ Step 3 (Lakeview) â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•®
â”‚ [ðŸ‘ï¸EXAMINE_CODE] The agent [1m<task>The agent is examining source code.[0m          â”‚
â”‚ [3mViewing benchmark/latency_throughput/bench_one.py to inspect the [0m            â”‚
â”‚ [3mbenchmarking script for performance analysis.[0m                                â”‚
â•°â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•¯
â•­â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ Step 4 (Lakeview) â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•®
â”‚ [ðŸ‘ï¸EXAMINE_CODE] The agent [1m<task>The agent is examining source code.[0m          â”‚
â”‚ [3mViewing benchmark/latency_throughput/bench_serving.py to inspect the serving[0m â”‚
â”‚ [3mbenchmark script for performance-related code and context.[0m                   â”‚
â•°â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•¯
â•­â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ Step 5 (Lakeview) â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•®
â”‚ [ðŸ‘ï¸EXAMINE_CODE] The agent [1m<task>The agent is examining source code.[0m          â”‚
â”‚ [3mListing the python/sglang package directory to locate relevant modules and [0m  â”‚
â”‚ [3mtargets for performance investigation, including potential torch.zeros/fill_[0m â”‚
â”‚ [3musages to optimize.[0m                                                          â”‚
â•°â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•¯
â•­â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ Step 6 (Lakeview) â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•®
â”‚ [ðŸ‘ï¸EXAMINE_CODE] The agent [1m<task>The agent is examining source code.[0m          â”‚
â”‚ [3mListing python/sglang/srt/managers/controller directory to locate relevant [0m  â”‚
â”‚ [3mcontroller modules for performance investigation, identifying potential [0m     â”‚
â”‚ [3mtargets for torch.zeros/fill_ usage and future optimizations.[0m                â”‚
â•°â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•¯
â•­â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ Step 7 (Lakeview) â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•®
â”‚ [ðŸ‘ï¸EXAMINE_CODE] The agent [1m<task>The agent is examining source code.[0m          â”‚
â”‚ [3mViewing python/sglang/srt/server.pyâ€™s first 200 lines to inspect server [0m     â”‚
â”‚ [3mlogic and check for torch.zeros/fill_ patterns related to the performance [0m   â”‚
â”‚ [3missue.[0m                                                                       â”‚
â•°â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•¯

Unexpected error: list index out of range
Traceback [1m([0mmost recent call last[1m)[0m:
  File "/home/ubuntu/OmniPerf-Bench/third-party/trae-agent/trae_agent/cli.py", line [1m212[0m, in run
    _ = [1masyncio.run[0m[1m([0m[1magent.run[0m[1m([0mtask, task_args[1m)[0m[1m)[0m
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/lib/python3.12/asyncio/runners.py", line [1m194[0m, in run
    return [1mrunner.run[0m[1m([0mmain[1m)[0m
           ^^^^^^^^^^^^^^^^
  File "/usr/lib/python3.12/asyncio/runners.py", line [1m118[0m, in run
    return [1mself._loop.run_until_complete[0m[1m([0mtask[1m)[0m
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/lib/python3.12/asyncio/base_events.py", line [1m687[0m, in run_until_complete
    return [1mfuture.result[0m[1m([0m[1m)[0m
           ^^^^^^^^^^^^^^^
  File "/home/ubuntu/OmniPerf-Bench/third-party/trae-agent/trae_agent/agent/agent.py", line [1m93[0m, in run
    await cli_console_task
  File "/home/ubuntu/OmniPerf-Bench/third-party/trae-agent/trae_agent/utils/cli/simple_console.py", line [1m119[0m, in start
    await [1mself._print_lakeview_summary[0m[1m([0m[1m)[0m
  File "/home/ubuntu/OmniPerf-Bench/third-party/trae-agent/trae_agent/utils/cli/simple_console.py", line [1m154[0m, in _print_lakeview_summary
    lake_view_panel = await step.lake_view_panel_generator
                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/OmniPerf-Bench/third-party/trae-agent/trae_agent/utils/cli/simple_console.py", line [1m306[0m, in _create_lakeview_step_display
    lake_view_step = await [1mself.lake_view.create_lakeview_step[0m[1m([0magent_step[1m)[0m
                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/OmniPerf-Bench/third-party/trae-agent/trae_agent/utils/lake_view.py", line [1m205[0m, in create_lakeview_step
    tags = await [1mself.extract_tag_in_step[0m[1m([0mthis_step_str[1m)[0m
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/OmniPerf-Bench/third-party/trae-agent/trae_agent/utils/lake_view.py", line [1m169[0m, in extract_tag_in_step
    tags: list = [1m[[0m[1mtag.strip[0m[1m([0m[1m)[0m for tag in matched_tags[1m[[0m[1m0[0m[1m][0m[1m.split[0m[1m([0m","[1m)[0m[1m][0m
                                              ~~~~~~~~~~~~^^^
IndexError: list index out of range

Trajectory saved to: /home/ubuntu/OmniPerf-Bench/perf-agents-bench/state/runs/sglang_core-ae58875a/sglang_026_6a2941f4/trajectory.json