{"instance_id": "/home/ubuntu/OmniPerf-Bench/sglan__repo-5ac8b80", "model_patch": "diff --git a/README.md b/README.md\nindex 2ac666c6b..7b90a60d6 100644\n--- a/README.md\n+++ b/README.md\n@@ -316,10 +316,10 @@ client = openai.Client(\n \n # Text completion\n response = client.completions.create(\n-\tmodel=\"default\",\n-\tprompt=\"The capital of France is\",\n-\ttemperature=0,\n-\tmax_tokens=32,\n+        model=\"default\",\n+        prompt=\"The capital of France is\",\n+        temperature=0,\n+        max_tokens=32,\n )\n print(response)\n \n@@ -377,6 +377,14 @@ python -m sglang.launch_server --model-path meta-llama/Llama-2-7b-chat-hf --port\n python -m sglang.launch_server --model-path meta-llama/Llama-2-7b-chat-hf --port 30000 --mem-fraction-static 0.7\n ```\n - See [hyperparameter_tuning.md](docs/hyperparameter_tuning.md) on tuning hyperparameters for better performance.\n+- Add `--nnodes 2` to run tensor parallelism on multiple nodes. If you have two nodes with two GPUs on each node and want to run TP=4, let `sgl-dev-1` be the hostname of the first node and `50000` be an available port.\n+```\n+# Node 0\n+python -m sglang.launch_server --model-path meta-llama/Llama-2-7b-chat-hf --tp 4 --nccl-init-addr sgl-dev-1:50000 --nnodes 2 --node-rank 0\n+\n+# Node 1\n+python -m sglang.launch_server --model-path meta-llama/Llama-2-7b-chat-hf --tp 4 --nccl-init-addr sgl-dev-1:50000 --nnodes 2 --node-rank 1\n+```\n \n ### Supported Models\n - Llama\ndiff --git a/benchmark/latency_throughput/bench_one.py b/benchmark/latency_throughput/bench_one.py\nindex cfd96b54c..345b09de1 100644\n--- a/benchmark/latency_throughput/bench_one.py\n+++ b/benchmark/latency_throughput/bench_one.py\n@@ -14,6 +14,8 @@ import requests\n def run_one_batch_size(bs):\n     url = f\"{args.host}:{args.port}\"\n     max_new_tokens = args.max_tokens\n+    input_len = args.input_len if args.input_len else 1\n+    output_len = max_new_tokens\n \n     if args.input_len:\n         input_ids = [\n@@ -96,8 +98,8 @@ def run_one_batch_size(bs):\n         ret = response.json()\n     print(ret)\n \n-    output_throughput = bs * max_new_tokens / latency\n-    overall_throughput = bs * (args.input_len + max_new_tokens) / latency\n+    output_throughput = bs * output_len / latency\n+    overall_throughput = bs * (input_len + output_len) / latency\n     print(f\"latency: {latency:.2f} s\")\n     print(f\"decode throughput: {output_throughput:.2f} token/s\")\n     print(f\"overall throughput: {overall_throughput:.2f} token/s\")\ndiff --git a/benchmark/latency_throughput/bench_serving.py b/benchmark/latency_throughput/bench_serving.py\nindex 23e8245f2..b79dfb133 100644\n--- a/benchmark/latency_throughput/bench_serving.py\n+++ b/benchmark/latency_throughput/bench_serving.py\n@@ -121,6 +121,7 @@ async def send_request(\n     output_len: int,\n     best_of: int,\n     use_beam_search: bool,\n+    session: aiohttp.ClientSession,\n ) -> None:\n     request_start_time = time.perf_counter()\n \n@@ -174,23 +175,21 @@ async def send_request(\n         raise ValueError(f\"Unknown backend: {backend}\")\n \n     if backend != \"ginfer\":\n-        timeout = aiohttp.ClientTimeout(total=3 * 3600)\n-        async with aiohttp.ClientSession(timeout=timeout) as session:\n-            while True:\n-                async with session.post(\n-                    api_url, headers=headers, json=pload\n-                ) as response:\n-                    chunks = []\n-                    async for chunk, _ in response.content.iter_chunks():\n-                        chunks.append(chunk)\n-                output = b\"\".join(chunks).decode(\"utf-8\")\n-                output = json.loads(output)\n-\n-                # Re-send the request if it failed.\n-                if \"error\" not in output:\n-                    break\n-                else:\n-                    print(output)\n+        while True:\n+            async with session.post(\n+                api_url, headers=headers, json=pload\n+            ) as response:\n+                chunks = []\n+                async for chunk, _ in response.content.iter_chunks():\n+                    chunks.append(chunk)\n+            output = b\"\".join(chunks).decode(\"utf-8\")\n+            output = json.loads(output)\n+\n+            # Re-send the request if it failed.\n+            if \"error\" not in output:\n+                break\n+            else:\n+                print(output)\n     else:\n         import grpc\n         from ginfer import sampler_pb2, sampler_pb2_grpc\n@@ -225,22 +224,25 @@ async def benchmark(\n     use_beam_search: bool,\n     request_rate: float,\n ) -> None:\n-    tasks: List[asyncio.Task] = []\n-    async for request in get_request(input_requests, request_rate):\n-        prompt, prompt_len, output_len = request\n-        task = asyncio.create_task(\n-            send_request(\n-                backend,\n-                api_url,\n-                prompt,\n-                prompt_len,\n-                output_len,\n-                best_of,\n-                use_beam_search,\n+    timeout = aiohttp.ClientTimeout(total=3 * 3600)\n+    async with aiohttp.ClientSession(timeout=timeout) as session:\n+        tasks: List[asyncio.Task] = []\n+        async for request in get_request(input_requests, request_rate):\n+            prompt, prompt_len, output_len = request\n+            task = asyncio.create_task(\n+                send_request(\n+                    backend,\n+                    api_url,\n+                    prompt,\n+                    prompt_len,\n+                    output_len,\n+                    best_of,\n+                    use_beam_search,\n+                    session,\n+                )\n             )\n-        )\n-        tasks.append(task)\n-    await tqdm_asyncio.gather(*tasks)\n+            tasks.append(task)\n+        await tqdm_asyncio.gather(*tasks)\n \n \n def main(args: argparse.Namespace):\ndiff --git a/python/sglang/README.md b/python/sglang/README.md\nindex c8c093706..b931af656 100644\n--- a/python/sglang/README.md\n+++ b/python/sglang/README.md\n@@ -2,7 +2,7 @@\n \n - `backend`: Various backends for the language interpreter.\n - `lang`: The frontend language.\n-- `srt`: The runtime for running local models.\n+- `srt`: The runtime for running local models. Includes distributed serving options (e.g., `--nnodes`, `--node-rank`, `--nccl-init-addr`).\n - `test`: Test utilities.\n - `api.py`: Public API.\n - `bench_latency.py`: Benchmark utilities.\ndiff --git a/python/sglang/srt/managers/controller/model_runner.py b/python/sglang/srt/managers/controller/model_runner.py\nindex d68d9af32..989870857 100644\n--- a/python/sglang/srt/managers/controller/model_runner.py\n+++ b/python/sglang/srt/managers/controller/model_runner.py\n@@ -187,8 +187,8 @@ class ModelRunner:\n         \"\"\"We need to run a small matmul to init cublas. Otherwise, it will raise some errors later.\"\"\"\n         dtype = torch.float16\n         device = \"cuda\"\n-        a = torch.ones((16, 16), dtype=dtype, device=device)\n-        b = torch.ones((16, 16), dtype=dtype, device=device)\n+        a = torch.empty((16, 16), dtype=dtype, device=device)\n+        b = torch.empty((16, 16), dtype=dtype, device=device)\n         c = a @ b\n         return c\n \ndiff --git a/python/sglang/srt/server.py b/python/sglang/srt/server.py\nindex 6cda67dea..d4d28c5e4 100644\n--- a/python/sglang/srt/server.py\n+++ b/python/sglang/srt/server.py\n@@ -208,7 +208,7 @@ def launch_server(server_args: ServerArgs, pipe_finish_writer, model_overide_arg\n                 f\"[node_rank={server_args.node_rank}]: Listen for connections...\"\n             )\n             while True:\n-                pass\n+                time.sleep(1)\n \n     # Launch processes\n     tokenizer_manager = TokenizerManager(server_args, port_args, model_overide_args)\n", "model_name_or_path": "gpt-5-2025-08-07"}
