{"instance_id": "/home/ubuntu/OmniPerf-Bench/sglan__repo-22a6b9f", "model_patch": "diff --git a/model_patch.diff b/model_patch.diff\nnew file mode 100644\nindex 000000000..e69de29bb\ndiff --git a/python/sglang/srt/layers/moe/topk.py b/python/sglang/srt/layers/moe/topk.py\nindex f5dceac78..c44ecc15b 100644\n--- a/python/sglang/srt/layers/moe/topk.py\n+++ b/python/sglang/srt/layers/moe/topk.py\n@@ -50,11 +50,7 @@ def fused_topk_native(\n         hidden_states.shape[0] == gating_output.shape[0]\n     ), f\"Number of tokens mismatch, {hidden_states.shape=} vs {gating_output.shape=}\"\n     M, _ = hidden_states.shape\n-    topk_weights = torch.empty(\n-        M, topk, dtype=torch.float32, device=hidden_states.device\n-    )\n-    topk_ids = torch.empty(M, topk, dtype=torch.int32, device=hidden_states.device)\n-    topk_weights = F.softmax(gating_output.float(), dim=-1)\n+    topk_weights = gating_output.float().softmax(dim=-1)\n     topk_weights, topk_ids = torch.topk(topk_weights, topk, dim=-1)\n     if renormalize:\n         topk_weights = topk_weights / topk_weights.sum(dim=-1, keepdim=True)\n@@ -98,6 +94,15 @@ def fused_topk(\n     )\n \n \n+\n+@torch.compile(dynamic=True, backend=get_compiler_backend())\n+def _biased_grouped_topk_postprocess(\n+    topk_ids, expert_location_dispatch_info, num_token_non_padded\n+):\n+    topk_ids = topk_ids_logical_to_physical(topk_ids, expert_location_dispatch_info)\n+    _mask_topk_ids_padded_region(topk_ids, num_token_non_padded)\n+    return topk_ids\n+\n @torch.compile(dynamic=True, backend=get_compiler_backend())\n def _fused_topk_postprocess(\n     topk_weights,\n@@ -129,7 +134,7 @@ def grouped_topk(\n ):\n     assert hidden_states.shape[0] == gating_output.shape[0], \"Number of tokens mismatch\"\n \n-    scores = torch.softmax(gating_output, dim=-1)\n+    scores = gating_output.softmax(dim=-1)\n     num_token = scores.shape[0]\n     num_experts = scores.shape[1]\n     group_scores = (\n@@ -138,14 +143,14 @@ def grouped_topk(\n     group_idx = torch.topk(group_scores, k=topk_group, dim=-1, sorted=False)[\n         1\n     ]  # [n, top_k_group]\n-    group_mask = torch.zeros_like(group_scores)  # [n, n_group]\n-    group_mask.scatter_(1, group_idx, 1)  # [n, n_group]\n+    group_mask = torch.zeros_like(group_scores, dtype=torch.bool)  # [n, n_group]\n+    group_mask.scatter_(1, group_idx, True)  # [n, n_group]\n     score_mask = (\n         group_mask.unsqueeze(-1)\n         .expand(num_token, num_expert_group, scores.shape[-1] // num_expert_group)\n         .reshape(num_token, -1)\n     )  # [n, e]\n-    tmp_scores = scores.masked_fill(~score_mask.bool(), 0.0)  # [n, e]\n+    tmp_scores = scores.masked_fill(~score_mask, 0.0)  # [n, e]\n     topk_weights, topk_ids = torch.topk(tmp_scores, k=topk, dim=-1, sorted=False)\n     if num_fused_shared_experts:\n         topk_ids[:, -1] = torch.randint(\n@@ -198,15 +203,15 @@ def biased_grouped_topk_impl(\n     group_idx = torch.topk(group_scores, k=topk_group, dim=-1, sorted=False)[\n         1\n     ]  # [n, top_k_group]\n-    group_mask = torch.zeros_like(group_scores)  # [n, n_group]\n-    group_mask.scatter_(1, group_idx, 1)  # [n, n_group]\n+    group_mask = torch.zeros_like(group_scores, dtype=torch.bool)  # [n, n_group]\n+    group_mask.scatter_(1, group_idx, True)  # [n, n_group]\n     score_mask = (\n         group_mask.unsqueeze(-1)\n         .expand(num_token, num_expert_group, scores.shape[-1] // num_expert_group)\n         .reshape(num_token, -1)\n     )  # [n, e]\n     tmp_scores = scores_for_choice.masked_fill(\n-        ~score_mask.bool(), float(\"-inf\")\n+        ~score_mask, float(\"-inf\")\n     )  # [n, e]\n     _, topk_ids = torch.topk(tmp_scores, k=topk, dim=-1, sorted=False)\n     topk_weights = scores.gather(1, topk_ids)\n@@ -283,13 +288,12 @@ def biased_grouped_topk(\n             routed_scaling_factor,\n         )\n         # TODO merge into kernel for this branch\n-        topk_ids = topk_ids_logical_to_physical(topk_ids, expert_location_dispatch_info)\n-        # TODO will fuse this into kernel, thus use slow manual operation now\n         if num_token_non_padded is None:\n-            return topk_weights, topk_ids\n-        torch.compile(\n-            _mask_topk_ids_padded_region, dynamic=True, backend=get_compiler_backend()\n-        )(topk_ids, num_token_non_padded)\n+            topk_ids = topk_ids_logical_to_physical(topk_ids, expert_location_dispatch_info)\n+        else:\n+            topk_ids = _biased_grouped_topk_postprocess(\n+                topk_ids, expert_location_dispatch_info, num_token_non_padded\n+            )\n         return topk_weights, topk_ids\n     else:\n         biased_grouped_topk_fn = (\n", "model_name_or_path": "gpt-5-2025-08-07"}
