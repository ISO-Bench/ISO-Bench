{"instance_id": "/home/ubuntu/OmniPerf-Bench/sglan__repo-db45276", "model_patch": "diff --git a/python/sglang/srt/layers/attention/flashattention_backend.py b/python/sglang/srt/layers/attention/flashattention_backend.py\nindex 62604fe56..c1625b4d3 100644\n--- a/python/sglang/srt/layers/attention/flashattention_backend.py\n+++ b/python/sglang/srt/layers/attention/flashattention_backend.py\n@@ -1,9 +1,5 @@\n from __future__ import annotations\n \n-import numpy as np\n-\n-from sglang.srt.speculative.eagle_utils import EagleDraftInput, EagleVerifyInput\n-\n \"\"\"\n Support different attention backends.\n Now there are three backends: FlashInfer, Triton and FlashAttention.\n@@ -13,19 +9,20 @@ Each backend supports two operators: extend (i.e. prefill with cached prefix) an\n from dataclasses import dataclass\n from typing import TYPE_CHECKING, Optional, Union\n \n+import numpy as np\n import torch\n \n+from sgl_kernel.flash_attn import flash_attn_with_kvcache\n from sglang.srt.configs.model_config import AttentionArch\n from sglang.srt.layers.attention.base_attn_backend import AttentionBackend\n from sglang.srt.managers.schedule_batch import global_server_args_dict\n from sglang.srt.model_executor.forward_batch_info import ForwardBatch, ForwardMode\n+from sglang.srt.speculative.eagle_utils import EagleDraftInput, EagleVerifyInput\n \n if TYPE_CHECKING:\n     from sglang.srt.layers.radix_attention import RadixAttention\n     from sglang.srt.model_executor.model_runner import ModelRunner\n \n-from sgl_kernel.flash_attn import flash_attn_with_kvcache\n-\n \n @dataclass\n class FlashAttentionMetadata:\n@@ -248,6 +245,19 @@ def cdiv(a: int, b: int) -> int:\n     return -(a // -b)\n \n \n+def _compute_cu_seqlens(seq_lens: torch.Tensor, device: torch.device) -> torch.Tensor:\n+    \"\"\"Compute cumulative sequence lengths with optimized memory allocation.\n+    \n+    This avoids the overhead of torch.nn.functional.pad by pre-allocating\n+    the output tensor and using the out parameter of cumsum.\n+    \"\"\"\n+    batch_size = seq_lens.shape[0]\n+    cu_seqlens = torch.empty(batch_size + 1, dtype=torch.int32, device=device)\n+    cu_seqlens[0] = 0\n+    torch.cumsum(seq_lens, dim=0, dtype=torch.int32, out=cu_seqlens[1:])\n+    return cu_seqlens\n+\n+\n class FlashAttentionBackend(AttentionBackend):\n     \"\"\"FlashAttention backend implementation.\n \n@@ -324,11 +334,8 @@ class FlashAttentionBackend(AttentionBackend):\n                 )\n                 seq_lens_with_decode = seqlens_in_batch + (self.step_id + 1)\n                 metadata.cache_seqlens_int32 = seq_lens_with_decode.to(torch.int32)\n-                metadata.cu_seqlens_k = torch.nn.functional.pad(\n-                    torch.cumsum(\n-                        metadata.cache_seqlens_int32, dim=0, dtype=torch.int32\n-                    ),\n-                    (1, 0),\n+                metadata.cu_seqlens_k = _compute_cu_seqlens(\n+                    metadata.cache_seqlens_int32, device\n                 )\n                 metadata.max_seq_len_k = forward_batch.seq_lens_cpu.max().item() + (\n                     self.step_id + 1\n@@ -351,8 +358,8 @@ class FlashAttentionBackend(AttentionBackend):\n                     ]\n             else:  # Normal Decode without Spec Decoding\n                 metadata.cache_seqlens_int32 = seqlens_in_batch.to(torch.int32)\n-                metadata.cu_seqlens_k = torch.nn.functional.pad(\n-                    torch.cumsum(seqlens_in_batch, dim=0, dtype=torch.int32), (1, 0)\n+                metadata.cu_seqlens_k = _compute_cu_seqlens(\n+                    metadata.cache_seqlens_int32, device\n                 )\n                 metadata.max_seq_len_k = forward_batch.seq_lens_cpu.max().item()\n                 metadata.page_table = forward_batch.req_to_token_pool.req_to_token[\n@@ -378,9 +385,8 @@ class FlashAttentionBackend(AttentionBackend):\n                 dtype=torch.int32,\n                 device=device,\n             )\n-            metadata.cu_seqlens_k = torch.nn.functional.pad(\n-                torch.cumsum(metadata.cache_seqlens_int32, dim=0, dtype=torch.int32),\n-                (1, 0),\n+            metadata.cu_seqlens_k = _compute_cu_seqlens(\n+                metadata.cache_seqlens_int32, device\n             )\n             metadata.page_table = forward_batch.req_to_token_pool.req_to_token[\n                 forward_batch.req_pool_indices, : metadata.max_seq_len_k\n@@ -389,8 +395,8 @@ class FlashAttentionBackend(AttentionBackend):\n         elif forward_batch.forward_mode.is_extend_or_draft_extend():\n             # Normal or Draft Extend (Both of them will be ran on the Target Worker)\n             metadata.cache_seqlens_int32 = seqlens_in_batch.to(torch.int32)\n-            metadata.cu_seqlens_k = torch.nn.functional.pad(\n-                torch.cumsum(seqlens_in_batch, dim=0, dtype=torch.int32), (1, 0)\n+            metadata.cu_seqlens_k = _compute_cu_seqlens(\n+                metadata.cache_seqlens_int32, device\n             )\n             # Precompute maximum sequence length\n             metadata.max_seq_len_k = forward_batch.seq_lens_cpu.max().item()\n@@ -405,9 +411,7 @@ class FlashAttentionBackend(AttentionBackend):\n                 or forward_batch.forward_mode == ForwardMode.DRAFT_EXTEND\n             ):\n                 extend_seq_lens = forward_batch.extend_seq_lens\n-                metadata.cu_seqlens_q = torch.nn.functional.pad(\n-                    torch.cumsum(extend_seq_lens, dim=0, dtype=torch.int32), (1, 0)\n-                )\n+                metadata.cu_seqlens_q = _compute_cu_seqlens(extend_seq_lens, device)\n                 metadata.max_seq_len_q = max(forward_batch.extend_seq_lens_cpu)\n             else:\n                 metadata.cu_seqlens_q = metadata.cu_seqlens_k\n@@ -720,13 +724,13 @@ class FlashAttentionBackend(AttentionBackend):\n         \"\"\"\n         self.decode_cuda_graph_metadata = {\n             # Page table for token mapping (batch_size, max_context_len)\n-            \"page_table\": torch.zeros(\n+            \"page_table\": torch.empty(\n                 max_bs,\n                 (self.max_context_len + self.page_size - 1) // self.page_size,\n                 dtype=torch.int32,\n                 device=self.device,\n             ),\n-            \"page_table_draft_decode\": torch.zeros(\n+            \"page_table_draft_decode\": torch.empty(\n                 max_bs,\n                 (self.max_context_len + self.page_size - 1) // self.page_size,\n                 dtype=torch.int32,\n@@ -735,27 +739,27 @@ class FlashAttentionBackend(AttentionBackend):\n             \"strided_indices\": torch.arange(\n                 0, self.max_context_len, self.page_size, device=self.device\n             ),\n-            \"cache_seqlens\": torch.zeros(max_bs, dtype=torch.int32, device=self.device),\n+            \"cache_seqlens\": torch.empty(max_bs, dtype=torch.int32, device=self.device),\n             \"cu_seqlens_q\": torch.arange(\n                 0, max_bs + 128, dtype=torch.int32, device=self.device\n             ),\n-            \"cu_seqlens_k\": torch.zeros(\n+            \"cu_seqlens_k\": torch.empty(\n                 max_bs + 128, dtype=torch.int32, device=self.device\n             ),\n         }\n \n         self.target_verify_metadata = {\n-            \"page_table\": torch.zeros(\n+            \"page_table\": torch.empty(\n                 max_bs,\n                 (self.max_context_len + self.page_size - 1) // self.page_size,\n                 dtype=torch.int32,\n                 device=self.device,\n             ),\n-            \"cache_seqlens\": torch.zeros(max_bs, dtype=torch.int32, device=self.device),\n-            \"cu_seqlens_q\": torch.zeros(\n+            \"cache_seqlens\": torch.empty(max_bs, dtype=torch.int32, device=self.device),\n+            \"cu_seqlens_q\": torch.empty(\n                 max_bs + 128, dtype=torch.int32, device=self.device\n             ),\n-            \"cu_seqlens_k\": torch.zeros(\n+            \"cu_seqlens_k\": torch.empty(\n                 max_bs + 128, dtype=torch.int32, device=self.device\n             ),\n             \"max_seqlen_q\": 0,\n@@ -791,11 +795,8 @@ class FlashAttentionBackend(AttentionBackend):\n                     : bs + 1\n                 ]\n \n-                metadata.cu_seqlens_k = torch.nn.functional.pad(\n-                    torch.cumsum(\n-                        metadata.cache_seqlens_int32, dim=0, dtype=torch.int32\n-                    ),\n-                    (1, 0),\n+                metadata.cu_seqlens_k = _compute_cu_seqlens(\n+                    metadata.cache_seqlens_int32, device\n                 )\n                 metadata.max_seq_len_k = seq_lens.max().item() + (self.step_id + 1)\n                 metadata.page_table = self.decode_cuda_graph_metadata[\n@@ -807,8 +808,8 @@ class FlashAttentionBackend(AttentionBackend):\n                 metadata.cache_seqlens_int32 = seq_lens.to(torch.int32)\n                 batch_size = len(seq_lens)\n                 device = seq_lens.device\n-                metadata.cu_seqlens_k = torch.nn.functional.pad(\n-                    torch.cumsum(seq_lens, dim=0, dtype=torch.int32), (1, 0)\n+                metadata.cu_seqlens_k = _compute_cu_seqlens(\n+                    metadata.cache_seqlens_int32, device\n                 )\n                 # Precompute maximum sequence length\n                 metadata.max_seq_len_k = seq_lens.max().item()\n@@ -844,14 +845,7 @@ class FlashAttentionBackend(AttentionBackend):\n                 )\n             ]\n             cu_k = self.target_verify_metadata[\"cu_seqlens_k\"][: (bs + 1)]\n-            cu_k.copy_(\n-                torch.nn.functional.pad(\n-                    torch.cumsum(\n-                        metadata.cache_seqlens_int32, dim=0, dtype=torch.int32\n-                    ),\n-                    (1, 0),\n-                )\n-            )\n+            cu_k.copy_(_compute_cu_seqlens(metadata.cache_seqlens_int32, device))\n             metadata.cu_seqlens_k = cu_k\n             metadata.page_table = self.target_verify_metadata[\"page_table\"][\n                 req_pool_indices, :\n@@ -893,12 +887,7 @@ class FlashAttentionBackend(AttentionBackend):\n                 metadata.max_seq_len_k = seq_lens_cpu.max().item() + (self.step_id + 1)\n \n                 metadata.cu_seqlens_k.copy_(\n-                    torch.nn.functional.pad(\n-                        torch.cumsum(\n-                            metadata.cache_seqlens_int32, dim=0, dtype=torch.int32\n-                        ),\n-                        (1, 0),\n-                    )\n+                    _compute_cu_seqlens(metadata.cache_seqlens_int32, device)\n                 )\n \n                 page_table = self.req_to_token[\n@@ -912,8 +901,8 @@ class FlashAttentionBackend(AttentionBackend):\n                 metadata.max_seq_len_k = max_len\n \n                 metadata.cache_seqlens_int32 = seq_lens.to(torch.int32)\n-                metadata.cu_seqlens_k = torch.nn.functional.pad(\n-                    torch.cumsum(seq_lens, dim=0, dtype=torch.int32), (1, 0)\n+                metadata.cu_seqlens_k = _compute_cu_seqlens(\n+                    metadata.cache_seqlens_int32, device\n                 )\n \n                 max_seq_pages = (\n@@ -946,12 +935,7 @@ class FlashAttentionBackend(AttentionBackend):\n \n             metadata.max_seq_len_k = seq_lens_cpu.max().item() + draft_token_num\n             metadata.cu_seqlens_k.copy_(\n-                torch.nn.functional.pad(\n-                    torch.cumsum(\n-                        metadata.cache_seqlens_int32, dim=0, dtype=torch.int32\n-                    ),\n-                    (1, 0),\n-                )\n+                _compute_cu_seqlens(metadata.cache_seqlens_int32, device)\n             )\n             page_table = self.req_to_token[req_pool_indices, : metadata.max_seq_len_k]\n             metadata.page_table[:, : metadata.max_seq_len_k].copy_(page_table)\n", "model_name_or_path": "gpt-5-2025-08-07"}
