{"instance_id": "/home/ubuntu/OmniPerf-Bench/sglan__repo-8879944", "model_patch": "diff --git a/python/sglang/srt/models/llama4.py b/python/sglang/srt/models/llama4.py\nindex d933f27ae..26a99ec89 100644\n--- a/python/sglang/srt/models/llama4.py\n+++ b/python/sglang/srt/models/llama4.py\n@@ -243,6 +243,11 @@ class Llama4Attention(nn.Module):\n \n         return attn_scale.unsqueeze(-1)\n \n+    @torch.compile(dynamic=True, backend=get_compiler_backend())\n+    def _mul_attn_scale(self, positions: torch.Tensor, q: torch.Tensor) -> torch.Tensor:\n+        attn_scale = self._get_attn_scale(positions)\n+        return (q * attn_scale).to(q.dtype)\n+\n     def forward(\n         self,\n         positions: torch.Tensor,\n@@ -250,17 +255,19 @@ class Llama4Attention(nn.Module):\n         forward_batch: ForwardBatch,\n     ) -> torch.Tensor:\n         qkv, _ = self.qkv_proj(hidden_states)\n-        q, k, v = qkv.split([self.q_size, self.kv_size, self.kv_size], dim=-1)\n+        # Reduce the number of split ops: split qk and v first, then split q and k when needed\n+        qk, v = qkv.split([self.q_size + self.kv_size, self.kv_size], dim=-1)\n \n         if self.rotary_emb is not None:\n-            q, k = self.rotary_emb(positions, q, k)\n+            q_view, k_view = qk.split([self.q_size, self.kv_size], dim=-1)\n+            q, k = self.rotary_emb(positions, q_view, k_view)\n+        else:\n+            q, k = qk.split([self.q_size, self.kv_size], dim=-1)\n \n         if self.qk_norm is not None:\n             # TODO: support float\n-            q = q.reshape(-1, self.head_dim).contiguous().bfloat16()\n-            k = k.reshape(-1, self.head_dim).contiguous().bfloat16()\n-            q = self.qk_norm(q).to(q.dtype)\n-            k = self.qk_norm(k).to(k.dtype)\n+            q = self.qk_norm(q.reshape(-1, self.head_dim).to(torch.bfloat16)).to(q.dtype)\n+            k = self.qk_norm(k.reshape(-1, self.head_dim).to(torch.bfloat16)).to(k.dtype)\n             q = q.reshape(-1, self.q_size)\n             k = k.reshape(-1, self.kv_size)\n \n@@ -269,8 +276,7 @@ class Llama4Attention(nn.Module):\n         # while working at very long context\n         # https://arxiv.org/abs/2501.19399\n         if self.attn_temperature_tuning and not self.use_rope:\n-            attn_scale = self._get_attn_scale(positions)\n-            q = (q * attn_scale).to(q.dtype)\n+            q = self._mul_attn_scale(positions, q)\n \n         attn_output = self.attn(q, k, v, forward_batch)\n         output, _ = self.o_proj(attn_output)\n", "model_name_or_path": "gpt-5-2025-08-07"}
