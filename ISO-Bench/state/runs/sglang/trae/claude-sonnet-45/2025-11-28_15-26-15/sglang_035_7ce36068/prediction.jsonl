{"instance_id": "/home/ubuntu/OmniPerf-Bench/sglan__repo-efb099c", "model_patch": "diff --git a/python/sglang/srt/managers/tp_worker_overlap_thread.py b/python/sglang/srt/managers/tp_worker_overlap_thread.py\nindex 5d78b97ce..d40df8ea7 100644\n--- a/python/sglang/srt/managers/tp_worker_overlap_thread.py\n+++ b/python/sglang/srt/managers/tp_worker_overlap_thread.py\n@@ -54,8 +54,12 @@ class TpModelWorkerClient:\n         self.future_token_ids_map = torch.empty(\n             (self.max_running_requests * 5,), dtype=torch.int32, device=self.device\n         )\n+        \n+        # Cache for frequently used tensors to reduce allocations\n+        self.arange_cache = {}\n+        self.max_cache_size = 64\n \n-        # Launch a thread\n+        # Launch threads\n         self.input_queue = Queue()\n         self.output_queue = Queue()\n         self.forward_stream = torch.cuda.Stream()\n@@ -64,6 +68,12 @@ class TpModelWorkerClient:\n         )\n         self.forward_thread.start()\n \n+        self.copy_queue = Queue()\n+        self.copy_thread = threading.Thread(\n+            target=self.copy_thread_func,\n+        )\n+        self.copy_thread.start()\n+\n     def get_worker_info(self):\n         return self.worker.get_worker_info()\n \n@@ -79,10 +89,36 @@ class TpModelWorkerClient:\n             self.worker.model_runner.token_to_kv_pool,\n         )\n \n+    def copy_thread_func(self):\n+        while True:\n+            next_token_ids, future_next_token_ids = self.copy_queue.get()\n+            # Update the future token ids map\n+            self.future_token_ids_map[-future_next_token_ids] = next_token_ids.to(\n+                torch.int32\n+            )\n+\n     def forward_thread_func(self):\n         with torch.cuda.stream(self.forward_stream):\n             self.forward_thread_func_()\n \n+    def _get_arange_tensor(self, start, end):\n+        \"\"\"Get or create cached arange tensor for common batch sizes.\"\"\"\n+        bs = end - start\n+        if bs in self.arange_cache:\n+            cached = self.arange_cache[bs]\n+            # Adjust the cached tensor to the current range\n+            return cached + start\n+        \n+        # Create new tensor\n+        result = torch.arange(start, end, dtype=torch.int32, device=self.device)\n+        \n+        # Cache if size is reasonable and cache not full\n+        if bs <= self.max_cache_size and len(self.arange_cache) < 32:\n+            # Store normalized version (0 to bs)\n+            self.arange_cache[bs] = torch.arange(0, bs, dtype=torch.int32, device=self.device)\n+        \n+        return result\n+\n     @torch.inference_mode()\n     def forward_thread_func_(self):\n         while True:\n@@ -90,28 +126,25 @@ class TpModelWorkerClient:\n \n             # Resolve future tokens in the input\n             input_ids = model_worker_batch.input_ids\n-            input_ids[:] = torch.where(\n-                input_ids < 0,\n-                self.future_token_ids_map[torch.clamp(-input_ids, min=0)],\n-                input_ids,\n-            )\n+            # Optimize: Use in-place operations to avoid creating temporary tensors\n+            mask = input_ids < 0\n+            if mask.any():\n+                input_ids[mask] = self.future_token_ids_map[torch.clamp(-input_ids[mask], min=0)]\n \n             # Run forward\n             logits_output, next_token_ids = self.worker.forward_batch_generation(\n                 model_worker_batch\n             )\n \n-            # Update the future token ids map\n+            # Prepare future token ids update in parallel\n             bs = len(model_worker_batch.seq_lens)\n-            future_next_token_ids = torch.arange(\n+            future_next_token_ids = self._get_arange_tensor(\n                 -(future_token_ids_ct + bs),\n                 -(future_token_ids_ct),\n-                dtype=torch.int32,\n-                device=self.device,\n-            )\n-            self.future_token_ids_map[-future_next_token_ids] = next_token_ids.to(\n-                torch.int32\n             )\n+            \n+            # Offload the copy operation to the copy thread\n+            self.copy_queue.put((next_token_ids, future_next_token_ids))\n \n             # Set the result\n             next_token_ids = next_token_ids.tolist()\n@@ -128,11 +161,9 @@ class TpModelWorkerClient:\n \n         # Allocate output future objects\n         bs = len(model_worker_batch.seq_lens)\n-        future_next_token_ids = torch.arange(\n+        future_next_token_ids = self._get_arange_tensor(\n             -(self.future_token_ids_ct + bs),\n             -(self.future_token_ids_ct),\n-            dtype=torch.int32,\n-            device=self.device,\n         )\n         self.future_token_ids_ct = (\n             self.future_token_ids_ct + bs\n", "model_name_or_path": "gpt-5-2025-08-07"}
