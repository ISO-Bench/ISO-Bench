{"instance_id": "/home/ubuntu/OmniPerf-Bench/sglan__repo-5d264a9", "model_patch": "diff --git a/model_patch.diff b/model_patch.diff\nnew file mode 100644\nindex 000000000..e69de29bb\ndiff --git a/python/sglang/srt/managers/controller/infer_batch.py b/python/sglang/srt/managers/controller/infer_batch.py\nindex d89e9786e..84fe39353 100644\n--- a/python/sglang/srt/managers/controller/infer_batch.py\n+++ b/python/sglang/srt/managers/controller/infer_batch.py\n@@ -781,7 +781,8 @@ class InputMetadata:\n                 device=\"cuda\",\n             )\n             extend_seq_lens = seq_lens - prefix_lens\n-            extend_start_loc = torch.zeros_like(seq_lens)\n+            extend_start_loc = torch.empty_like(seq_lens)\n+            extend_start_loc[0] = 0\n             extend_start_loc[1:] = torch.cumsum(extend_seq_lens[:-1], dim=0)\n             extend_no_prefix = torch.all(prefix_lens == 0)\n             total_num_tokens = int(torch.sum(seq_lens))\n@@ -827,9 +828,10 @@ def init_flashinfer_args(forward_mode, model_runner, req_pool_indices, seq_lens,\n     else:\n         paged_kernel_lens = prefix_lens\n \n-    kv_indptr = torch.zeros(\n+    kv_indptr = torch.empty(\n         (batch_size + 1,), dtype=torch.int32, device=\"cuda\"\n     )\n+    kv_indptr[0] = 0\n     kv_indptr[1:] = torch.cumsum(paged_kernel_lens, dim=0)\n     req_pool_indices_cpu = req_pool_indices.cpu().numpy()\n     paged_kernel_lens_cpu = paged_kernel_lens.cpu().numpy()\n@@ -859,9 +861,10 @@ def init_flashinfer_args(forward_mode, model_runner, req_pool_indices, seq_lens,\n         )\n     else:\n         # extend part\n-        qo_indptr = torch.zeros(\n+        qo_indptr = torch.empty(\n             (batch_size + 1,), dtype=torch.int32, device=\"cuda\"\n         )\n+        qo_indptr[0] = 0\n         qo_indptr[1:] = torch.cumsum(seq_lens - prefix_lens, dim=0)\n \n         model_runner.flashinfer_prefill_wrapper_ragged.end_forward()\n@@ -890,7 +893,8 @@ def init_flashinfer_args(forward_mode, model_runner, req_pool_indices, seq_lens,\n def init_triton_args(forward_mode, seq_lens, prefix_lens):\n     batch_size = len(seq_lens)\n     max_seq_len = int(torch.max(seq_lens))\n-    start_loc = torch.zeros((batch_size,), dtype=torch.int32, device=\"cuda\")\n+    start_loc = torch.empty((batch_size,), dtype=torch.int32, device=\"cuda\")\n+    start_loc[0] = 0\n     start_loc[1:] = torch.cumsum(seq_lens[:-1], dim=0)\n \n     if forward_mode == ForwardMode.DECODE:\ndiff --git a/python/sglang/srt/memory_pool.py b/python/sglang/srt/memory_pool.py\nindex 245e6ef08..9b78636f2 100644\n--- a/python/sglang/srt/memory_pool.py\n+++ b/python/sglang/srt/memory_pool.py\n@@ -41,7 +41,7 @@ class TokenToKVPool:\n         self.size = size\n         # mem_state is the reference counter.\n         # We also add one slot. This slot is used for writing dummy output from padded tokens.\n-        self.mem_state = torch.zeros((self.size + 1,), dtype=torch.int16, device=\"cuda\")\n+        self.mem_state = torch.empty((self.size + 1,), dtype=torch.int16, device=\"cuda\")\n         self.total_ref_ct = 0\n \n         # [size, key/value, head_num, head_dim] for each layer\ndiff --git a/python/sglang/srt/utils.py b/python/sglang/srt/utils.py\nindex 03a2d60ab..d31650c47 100644\n--- a/python/sglang/srt/utils.py\n+++ b/python/sglang/srt/utils.py\n@@ -610,7 +610,7 @@ def receive_addrs(model_port_args, server_args):\n     )\n \n     for src_rank in range(1, server_args.nnodes):\n-        tensor = torch.zeros(4 + num_tp_ports, dtype=torch.int)\n+        tensor = torch.empty(4 + num_tp_ports, dtype=torch.int)\n         dist.recv(tensor, src=src_rank)\n         ip = \".\".join([str(x) for x in tensor[:4].tolist()])\n         ports = tensor[4:].tolist()\n", "model_name_or_path": "gpt-5-2025-08-07"}
