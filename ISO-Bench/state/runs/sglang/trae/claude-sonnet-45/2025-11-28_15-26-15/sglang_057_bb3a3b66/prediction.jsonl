{"instance_id": "/home/ubuntu/OmniPerf-Bench/sglan__repo-45d6592", "model_patch": "diff --git a/model_patch.diff b/model_patch.diff\nnew file mode 100644\nindex 000000000..e69de29bb\ndiff --git a/python/sglang/srt/managers/router/infer_batch.py b/python/sglang/srt/managers/router/infer_batch.py\nindex 88f6031f7..046eca01f 100644\n--- a/python/sglang/srt/managers/router/infer_batch.py\n+++ b/python/sglang/srt/managers/router/infer_batch.py\n@@ -2,11 +2,22 @@ from dataclasses import dataclass\n from enum import Enum, auto\n from typing import List\n \n-import numpy as np\n import torch\n from sglang.srt.managers.router.radix_cache import RadixCache\n from sglang.srt.memory_pool import ReqToTokenPool, TokenToKVPool\n \n+# Cache for frequently used arange tensors to reduce allocations in sampling\n+_ARANGE_CACHE = {}\n+\n+def _get_cached_arange(length: int, device):\n+    key = (str(device), int(length))\n+    t = _ARANGE_CACHE.get(key)\n+    if t is None or t.device != torch.device(device) or t.numel() != length:\n+        t = torch.arange(0, length, device=device)\n+        _ARANGE_CACHE[key] = t\n+    return t\n+\n+\n \n class ForwardMode(Enum):\n     PREFILL = auto()\n@@ -20,6 +31,7 @@ class FinishReason(Enum):\n     STOP_STR = auto()\n \n \n+\n class Req:\n     def __init__(self, rid, input_text, input_ids):\n         self.rid = rid\n@@ -31,6 +43,7 @@ class Req:\n         self.pixel_values = None\n         self.image_size = None\n         self.image_offset = 0\n+        self.pad_value = None\n \n         self.sampling_params = None\n         self.return_logprob = False\n@@ -58,7 +71,7 @@ class Req:\n     def max_new_tokens(self):\n         return self.sampling_params.max_new_tokens\n \n-    def tokenize_fast_forward(self, fast_forward_str, next_state):\n+    def fast_forward_and_retokenize(self, fast_forward_str, next_state):\n         old_output_str = self.tokenizer.decode(self.output_ids)\n         # FIXME: This logic does not really solve the problem of determining whether\n         # there should be a leading space.\n@@ -211,8 +224,7 @@ class Batch:\n         position_ids_offsets = torch.zeros((bs,), dtype=torch.int32, device=device)\n \n         # Alloc mem\n-        seq_lens, prefix_lens = np.array(seq_lens), np.array(prefix_lens)\n-        extend_num_tokens = seq_lens.sum() - prefix_lens.sum()\n+        extend_num_tokens = int(sum(seq_lens) - sum(prefix_lens))\n         out_cache_loc = self.token_to_kv_pool.alloc(extend_num_tokens)\n         if out_cache_loc is None:\n             if not self.tree_cache.disable:\n@@ -233,9 +245,10 @@ class Batch:\n \n         # Handle logit bias\n         logit_bias = torch.zeros((bs, vocab_size), dtype=torch.float32, device=device)\n-        for i in range(bs):\n-            if reqs[i].sampling_params.dtype == \"int\":\n-                logit_bias[i] = int_token_logit_bias\n+        int_rows = [i for i, r in enumerate(reqs) if r.sampling_params.dtype == \"int\"]\n+        if int_rows:\n+            idx_tensor = torch.tensor(int_rows, dtype=torch.long, device=device)\n+            logit_bias[idx_tensor] = int_token_logit_bias\n \n         # Set fields\n         self.input_ids = torch.tensor(\n@@ -351,7 +364,7 @@ class Batch:\n                     self.tree_cache.dec_ref_counter(req.last_node)\n \n                     # fast forward\n-                    req.tokenize_fast_forward(fast_forward_str, next_state)\n+                    req.fast_forward_and_retokenize(fast_forward_str, next_state)\n \n                     fast_forward_reqs.append(req)\n                     filter_indices.remove(i)\n@@ -482,8 +495,7 @@ def _top_p_top_k(probs: torch.Tensor, top_ps: torch.Tensor, top_ks: torch.Tensor\n     probs_sort, probs_idx = probs.sort(dim=-1, descending=True)\n     probs_sum = torch.cumsum(probs_sort, dim=-1)\n     probs_sort[(probs_sum - probs_sort) > top_ps] = 0.0\n-    probs_sort[\n-        torch.arange(0, probs.shape[-1], device=probs.device).view(1, -1) >= top_ks\n-    ] = 0.0\n+    arange_row = _get_cached_arange(probs.shape[-1], probs.device).view(1, -1)\n+    probs_sort[arange_row >= top_ks] = 0.0\n     probs_sort.div_(probs_sort.max(dim=-1, keepdim=True)[0])\n     return probs_sort, probs_idx\ndiff --git a/python/sglang/srt/managers/router/model_rpc.py b/python/sglang/srt/managers/router/model_rpc.py\nindex 8b7adf944..a752239cd 100644\n--- a/python/sglang/srt/managers/router/model_rpc.py\n+++ b/python/sglang/srt/managers/router/model_rpc.py\n@@ -239,6 +239,8 @@ class ModelRpcServer(rpyc.Service):\n                 (recv_req.image_hash >> 32) % self.model_config.vocab_size,\n                 (recv_req.image_hash >> 64) % self.model_config.vocab_size,\n             ]\n+            req.pad_value = pad_value\n+\n             req.image_size = recv_req.image_size\n             req.input_ids, req.image_offset = self.model_runner.model.pad_input_ids(\n                 req.input_ids, pad_value, req.pixel_values.shape, req.image_size\n", "model_name_or_path": "gpt-5-2025-08-07"}
