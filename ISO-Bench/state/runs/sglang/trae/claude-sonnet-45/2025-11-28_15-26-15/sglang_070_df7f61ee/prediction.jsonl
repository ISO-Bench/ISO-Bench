{"instance_id": "/home/ubuntu/OmniPerf-Bench/sglan__repo-ef21729", "model_patch": "diff --git a/python/sglang/srt/managers/expert_location.py b/python/sglang/srt/managers/expert_location.py\nindex 615e0a440..9897ba832 100644\n--- a/python/sglang/srt/managers/expert_location.py\n+++ b/python/sglang/srt/managers/expert_location.py\n@@ -26,6 +26,7 @@ from sglang.srt.configs.model_config import ModelConfig\n from sglang.srt.managers import eplb_algorithms\n from sglang.srt.model_loader import get_model_architecture\n from sglang.srt.server_args import ServerArgs\n+from sglang.srt.managers.schedule_batch import global_server_args_dict\n \n logger = logging.getLogger(__name__)\n \n@@ -35,7 +36,8 @@ class ExpertLocationMetadata:\n     physical_to_logical_map: torch.Tensor  # (layers, num_physical_experts)\n     logical_to_all_physical_map: torch.Tensor  # (layers, num_logical_experts, X)\n     logical_to_all_physical_map_num_valid: torch.Tensor  # (layers, num_logical_experts)\n-    logical_to_rank_dispatch_physical_map: torch.Tensor  # (layers, num_logical_experts)\n+    # (layers, num_logical_experts)\n+    logical_to_rank_dispatch_physical_map: Optional[torch.Tensor]\n \n     # -------------------------------- properties ------------------------------------\n \n@@ -70,11 +72,8 @@ class ExpertLocationMetadata:\n         num_layers_2, num_logical_experts_1 = (\n             self.logical_to_all_physical_map_num_valid.shape\n         )\n-        num_layers_3, num_logical_experts_2 = (\n-            self.logical_to_rank_dispatch_physical_map.shape\n-        )\n-        assert num_layers_0 == num_layers_1 == num_layers_2 == num_layers_3\n-        assert num_logical_experts_0 == num_logical_experts_1 == num_logical_experts_2\n+        assert num_layers_0 == num_layers_1 == num_layers_2\n+        assert num_logical_experts_0 == num_logical_experts_1\n         assert num_physical_experts_0 == num_physical_experts_1\n \n     # -------------------------------- construction ------------------------------------\n@@ -200,17 +199,22 @@ class ExpertLocationMetadata:\n             logical_to_all_physical_map != -1, dim=-1\n         )\n \n-        return ExpertLocationMetadata(\n-            physical_to_logical_map=physical_to_logical_map,\n-            logical_to_all_physical_map=logical_to_all_physical_map_padded,\n-            logical_to_all_physical_map_num_valid=logical_to_all_physical_map_num_valid,\n-            logical_to_rank_dispatch_physical_map=compute_logical_to_rank_dispatch_physical_map(\n+        ep_algo = global_server_args_dict.get(\"ep_dispatch_algorithm\")\n+        logical_to_rank_map = None\n+        if ep_algo == \"static\":\n+            logical_to_rank_map = compute_logical_to_rank_dispatch_physical_map(\n                 logical_to_all_physical_map=logical_to_all_physical_map,\n                 num_gpus=ep_size,\n                 num_physical_experts=num_physical_experts,\n                 # TODO improve when we have real EP rank\n                 ep_rank=torch.distributed.get_rank() % ep_size,\n-            ),\n+            )\n+\n+        return ExpertLocationMetadata(\n+            physical_to_logical_map=physical_to_logical_map,\n+            logical_to_all_physical_map=logical_to_all_physical_map_padded,\n+            logical_to_all_physical_map_num_valid=logical_to_all_physical_map_num_valid,\n+            logical_to_rank_dispatch_physical_map=logical_to_rank_map,\n         )\n \n     # -------------------------------- mutation ------------------------------------\n@@ -230,8 +234,14 @@ class ExpertLocationMetadata:\n             \"logical_to_all_physical_map_num_valid\",\n             \"logical_to_rank_dispatch_physical_map\",\n         ]:\n+            src = getattr(other, field)\n             dst = getattr(self, field)\n-            dst[...] = getattr(other, field)\n+            if src is None:\n+                setattr(self, field, None)\n+            elif dst is None:\n+                setattr(self, field, src)\n+            else:\n+                dst[...] = src\n \n     # -------------------------------- usage ------------------------------------\n \n@@ -310,44 +320,54 @@ def compute_logical_to_rank_dispatch_physical_map(\n     num_local_physical_experts = num_physical_experts // num_gpus\n     num_layers, num_logical_experts, _ = logical_to_all_physical_map.shape\n     dtype = logical_to_all_physical_map.dtype\n+    device = logical_to_all_physical_map.device\n \n-    logical_to_rank_dispatch_physical_map = torch.full(\n-        size=(num_gpus, num_layers, num_logical_experts),\n-        fill_value=-1,\n-        dtype=dtype,\n-    )\n+    # Only compute the mapping for the current ep_rank to reduce memory/time\n+    out = torch.empty((num_layers, num_logical_experts), dtype=dtype)\n+    out.fill_(-1)\n \n     for layer_id in range(num_layers):\n         for logical_expert_id in range(num_logical_experts):\n             candidate_physical_expert_ids = _logical_to_all_physical_raw(\n                 logical_to_all_physical_map, layer_id, logical_expert_id\n             )\n-            output_partial = logical_to_rank_dispatch_physical_map[\n-                :, layer_id, logical_expert_id\n-            ]\n-\n-            for gpu_id in range(num_gpus):\n-                same_gpu_physical_expert_ids = [\n-                    physical_expert_id\n-                    for physical_expert_id in candidate_physical_expert_ids\n-                    if _compute_gpu_id_of_physical_expert(\n-                        physical_expert_id, num_local_physical_experts\n-                    )\n-                    == gpu_id\n-                ]\n-                if len(same_gpu_physical_expert_ids) > 0:\n-                    output_partial[gpu_id] = same_gpu_physical_expert_ids[0]\n-\n-            num_remain = torch.sum(output_partial == -1).item()\n-            output_partial[output_partial == -1] = torch.tensor(\n-                _fair_choices(candidate_physical_expert_ids, k=num_remain, r=r),\n-                dtype=dtype,\n-            )\n \n-    assert torch.all(logical_to_rank_dispatch_physical_map != -1)\n+            # Track which GPUs have a local candidate and find the first local for ep_rank\n+            seen_gpus = set()\n+            local_choice = None\n+            for physical_expert_id in candidate_physical_expert_ids:\n+                gid = _compute_gpu_id_of_physical_expert(\n+                    physical_expert_id, num_local_physical_experts\n+                )\n+                if gid == ep_rank and local_choice is None:\n+                    local_choice = physical_expert_id\n+                seen_gpus.add(gid)\n+\n+            if local_choice is not None:\n+                out[layer_id, logical_expert_id] = local_choice\n+            else:\n+                # Number of GPUs that don't have a local candidate\n+                num_remain = max(0, num_gpus - len(seen_gpus))\n+                if num_remain == 0:\n+                    # Fallback: choose fairly from candidates\n+                    out[layer_id, logical_expert_id] = _fair_choices(\n+                        candidate_physical_expert_ids, k=1, r=r\n+                    )[0]\n+                else:\n+                    # Position of this ep_rank among GPUs without local candidates\n+                    pos = 0\n+                    for gid in range(ep_rank):\n+                        if gid not in seen_gpus:\n+                            pos += 1\n+                    choices = _fair_choices(\n+                        candidate_physical_expert_ids, k=num_remain, r=r\n+                    )\n+                    # Safety in case pos >= len(choices) due to any corner case\n+                    pos = min(pos, len(choices) - 1)\n+                    out[layer_id, logical_expert_id] = choices[pos]\n \n-    device = logical_to_all_physical_map.device\n-    return logical_to_rank_dispatch_physical_map[ep_rank, :, :].to(device)\n+    assert torch.all(out != -1)\n+    return out.to(device)\n \n \n def _logical_to_all_physical_raw(\ndiff --git a/python/sglang/srt/managers/expert_location_dispatch.py b/python/sglang/srt/managers/expert_location_dispatch.py\nindex 6880b01a2..2a31f5549 100644\n--- a/python/sglang/srt/managers/expert_location_dispatch.py\n+++ b/python/sglang/srt/managers/expert_location_dispatch.py\n@@ -17,15 +17,15 @@ from typing import Literal, Optional\n \n import torch\n \n-from sglang.srt.managers.expert_location import get_global_expert_location_metadata\n+from sglang.srt.managers.expert_location import get_global_expert_location_metadata, compute_logical_to_rank_dispatch_physical_map\n from sglang.srt.managers.schedule_batch import global_server_args_dict\n \n \n @dataclass\n class ExpertLocationDispatchInfo:\n-    ep_dispatch_algorithm: Literal[\"static\", \"random\"]\n+    ep_dispatch_algorithm: Optional[Literal[\"static\", \"dynamic\", \"fake\"]]\n     # (num_logical_experts,)\n-    partial_logical_to_rank_dispatch_physical_map: torch.Tensor\n+    partial_logical_to_rank_dispatch_physical_map: Optional[torch.Tensor]\n     # (num_logical_experts, X)\n     partial_logical_to_all_physical_map: torch.Tensor\n     # (num_logical_experts,)\n@@ -40,11 +40,22 @@ class ExpertLocationDispatchInfo:\n         if ep_dispatch_algorithm is None:\n             return None\n \n+        partial_rank_map = None\n+        if ep_dispatch_algorithm == \"static\":\n+            ltr = expert_location_metadata.logical_to_rank_dispatch_physical_map\n+            if ltr is None:\n+                ltr = compute_logical_to_rank_dispatch_physical_map(\n+                    logical_to_all_physical_map=expert_location_metadata.logical_to_all_physical_map,\n+                    num_gpus=expert_location_metadata.ep_size,\n+                    num_physical_experts=expert_location_metadata.num_physical_experts,\n+                    ep_rank=torch.distributed.get_rank() % expert_location_metadata.ep_size,\n+                )\n+                expert_location_metadata.logical_to_rank_dispatch_physical_map = ltr\n+            partial_rank_map = ltr[layer_id, :]\n+\n         return cls(\n             ep_dispatch_algorithm=ep_dispatch_algorithm,\n-            partial_logical_to_rank_dispatch_physical_map=expert_location_metadata.logical_to_rank_dispatch_physical_map[\n-                layer_id, :\n-            ],\n+            partial_logical_to_rank_dispatch_physical_map=partial_rank_map,\n             partial_logical_to_all_physical_map=expert_location_metadata.logical_to_all_physical_map[\n                 layer_id, :\n             ],\n", "model_name_or_path": "gpt-5-2025-08-07"}
