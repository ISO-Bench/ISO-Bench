{"instance_id": "/home/ubuntu/OmniPerf-Bench/sglan__repo-aac531c", "model_patch": "diff --git a/python/sglang/srt/layers/moe/ep_moe/token_dispatcher.py b/python/sglang/srt/layers/moe/ep_moe/token_dispatcher.py\nindex 100fa57fb..6ffa42f9f 100644\n--- a/python/sglang/srt/layers/moe/ep_moe/token_dispatcher.py\n+++ b/python/sglang/srt/layers/moe/ep_moe/token_dispatcher.py\n@@ -311,7 +311,7 @@ class _DeepEPDispatcherImplNormal(_DeepEPDispatcherImplBase):\n                 BLOCK_SIZE=512,\n             )\n         else:\n-            output = torch.zeros(\n+            output = torch.empty(\n                 (0, hidden_states.shape[1]),\n                 device=hidden_states.device,\n                 dtype=hidden_states.dtype,\ndiff --git a/python/sglang/srt/model_executor/forward_batch_info.py b/python/sglang/srt/model_executor/forward_batch_info.py\nindex 96a13a999..47c67d40a 100644\n--- a/python/sglang/srt/model_executor/forward_batch_info.py\n+++ b/python/sglang/srt/model_executor/forward_batch_info.py\n@@ -280,7 +280,7 @@ class ForwardBatch:\n             ).to(device, non_blocking=True)\n \n             sum_len = sum(batch.global_num_tokens)\n-            ret.gathered_buffer = torch.zeros(\n+            ret.gathered_buffer = torch.empty(\n                 (sum_len, model_runner.model_config.hidden_size),\n                 dtype=model_runner.dtype,\n                 device=device,\n@@ -515,7 +515,8 @@ def compute_position_torch(\n         ],\n         axis=0,\n     )\n-    extend_start_loc = torch.zeros_like(extend_seq_lens)\n+    extend_start_loc = torch.empty_like(extend_seq_lens)\n+    extend_start_loc[0] = 0\n     extend_start_loc[1:] = torch.cumsum(extend_seq_lens[:-1], dim=0)\n     return positions.to(torch.int64), extend_start_loc\n \n", "model_name_or_path": "gpt-5-2025-08-07"}
