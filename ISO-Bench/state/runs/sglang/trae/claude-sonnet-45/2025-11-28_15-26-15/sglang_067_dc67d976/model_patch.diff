diff --git a/python/sglang/srt/model_executor/model_runner.py b/python/sglang/srt/model_executor/model_runner.py
index 09b3c7127..c6a4c0132 100644
--- a/python/sglang/srt/model_executor/model_runner.py
+++ b/python/sglang/srt/model_executor/model_runner.py
@@ -161,7 +161,15 @@ class ModelRunner:
 
         return min_per_gpu_memory
 
+    @torch.inference_mode()
     def load_model(self):
+        # Limit CPU thread usage during weight loading to reduce overhead/contention.
+        torch.set_num_threads(1)
+        try:
+            torch.set_num_interop_threads(1)
+        except Exception:
+            # set_num_interop_threads may not be available in some torch builds
+            pass
         logger.info(
             f"Load weight begin. avail mem={get_available_gpu_memory(self.gpu_id):.2f} GB"
         )
@@ -415,8 +423,8 @@ class ModelRunner:
         """We need to run a small matmul to init cublas. Otherwise, it will raise some errors later."""
         dtype = torch.float16
         device = "cuda"
-        a = torch.ones((16, 16), dtype=dtype, device=device)
-        b = torch.ones((16, 16), dtype=dtype, device=device)
+        a = torch.empty((16, 16), dtype=dtype, device=device)
+        b = torch.empty((16, 16), dtype=dtype, device=device)
         c = a @ b
         return c
 
