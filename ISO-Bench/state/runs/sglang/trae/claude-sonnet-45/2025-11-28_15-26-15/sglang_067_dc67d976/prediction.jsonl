{"instance_id": "/home/ubuntu/OmniPerf-Bench/sglan__repo-1e495e0", "model_patch": "diff --git a/python/sglang/srt/model_executor/model_runner.py b/python/sglang/srt/model_executor/model_runner.py\nindex 09b3c7127..c6a4c0132 100644\n--- a/python/sglang/srt/model_executor/model_runner.py\n+++ b/python/sglang/srt/model_executor/model_runner.py\n@@ -161,7 +161,15 @@ class ModelRunner:\n \n         return min_per_gpu_memory\n \n+    @torch.inference_mode()\n     def load_model(self):\n+        # Limit CPU thread usage during weight loading to reduce overhead/contention.\n+        torch.set_num_threads(1)\n+        try:\n+            torch.set_num_interop_threads(1)\n+        except Exception:\n+            # set_num_interop_threads may not be available in some torch builds\n+            pass\n         logger.info(\n             f\"Load weight begin. avail mem={get_available_gpu_memory(self.gpu_id):.2f} GB\"\n         )\n@@ -415,8 +423,8 @@ class ModelRunner:\n         \"\"\"We need to run a small matmul to init cublas. Otherwise, it will raise some errors later.\"\"\"\n         dtype = torch.float16\n         device = \"cuda\"\n-        a = torch.ones((16, 16), dtype=dtype, device=device)\n-        b = torch.ones((16, 16), dtype=dtype, device=device)\n+        a = torch.empty((16, 16), dtype=dtype, device=device)\n+        b = torch.empty((16, 16), dtype=dtype, device=device)\n         c = a @ b\n         return c\n \n", "model_name_or_path": "gpt-5-2025-08-07"}
