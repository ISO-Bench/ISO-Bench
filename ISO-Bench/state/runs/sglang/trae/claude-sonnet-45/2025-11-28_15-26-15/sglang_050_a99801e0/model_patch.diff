diff --git a/python/sglang/srt/mem_cache/allocator.py b/python/sglang/srt/mem_cache/allocator.py
index 7dd488e9c..bb33a565d 100644
--- a/python/sglang/srt/mem_cache/allocator.py
+++ b/python/sglang/srt/mem_cache/allocator.py
@@ -51,6 +51,7 @@ class BaseTokenToKVPoolAllocator(abc.ABC):
         self._kvcache = kvcache
 
         self.free_pages = None
+        self.release_pages = None
         self.is_not_in_free_group = True
         self.free_group = []
 
@@ -58,16 +59,25 @@ class BaseTokenToKVPoolAllocator(abc.ABC):
         return ""
 
     def available_size(self):
-        return len(self.free_pages) * self.page_size
+        release_cnt = 0
+        if isinstance(self.release_pages, list) and self.release_pages:
+            release_cnt = sum(x.numel() for x in self.release_pages)
+        elif isinstance(self.release_pages, torch.Tensor):
+            release_cnt = len(self.release_pages)
+        return (len(self.free_pages) + release_cnt) * self.page_size
 
     def get_kvcache(self):
         return self._kvcache
 
-    def restore_state(self, free_pages):
-        self.free_pages = free_pages
+    def restore_state(self, state):
+        if isinstance(state, tuple) and len(state) == 2:
+            self.free_pages, self.release_pages = state
+        else:
+            self.free_pages = state
+            self.release_pages = []
 
     def backup_state(self):
-        return self.free_pages
+        return (self.free_pages, self.release_pages)
 
     def free_group_begin(self):
         self.is_not_in_free_group = False
@@ -110,6 +120,7 @@ class TokenToKVPoolAllocator(BaseTokenToKVPoolAllocator):
 
     def __init__(self, size: int, dtype: torch.dtype, device: str, kvcache: KVCache):
         super().__init__(size, 1, dtype, device, kvcache)
+        self._release_count = 0
         self.clear()
 
     def clear(self):
@@ -117,16 +128,21 @@ class TokenToKVPoolAllocator(BaseTokenToKVPoolAllocator):
         self.free_pages = torch.arange(
             1, self.size + 1, dtype=torch.int64, device=self.device
         )
+        self.release_pages = []
+        self._release_count = 0
         self.is_not_in_free_group = True
         self.free_group = []
 
     def available_size(self):
-        # To avoid minor "len(free_pages) * 1" overhead
-        return len(self.free_pages)
+        # Use cached release count for better performance
+        return len(self.free_pages) + self._release_count
 
     def alloc(self, need_size: int):
         if need_size > len(self.free_pages):
-            return None
+            if self.release_pages:
+                self._flush_release_pages()
+            if need_size > len(self.free_pages):
+                return None
 
         select_index = self.free_pages[:need_size]
         self.free_pages = self.free_pages[need_size:]
@@ -137,13 +153,25 @@ class TokenToKVPoolAllocator(BaseTokenToKVPoolAllocator):
             return
 
         if self.is_not_in_free_group:
-            self.free_pages = torch.cat((self.free_pages, free_index))
+            self.release_pages.append(free_index)
+            self._release_count += free_index.numel()
         else:
             self.free_group.append(free_index)
 
     def get_cpu_copy(self, indices):
         return self._kvcache.get_cpu_copy(indices)
 
+    def _flush_release_pages(self):
+        if not self.release_pages:
+            return
+        # Concatenate all tensors at once for better performance
+        all_pages = [self.free_pages] + self.release_pages
+        merged = torch.cat(all_pages)
+        self.free_pages = torch.sort(merged)[0]
+        self.release_pages = []
+        self._release_count = 0
+
+
     def load_cpu_copy(self, kv_cache_cpu, indices):
         return self._kvcache.load_cpu_copy(kv_cache_cpu, indices)
 
@@ -411,6 +439,7 @@ class PagedTokenToKVPoolAllocator(BaseTokenToKVPoolAllocator):
         self.num_pages = size // page_size
         self.debug_mode = get_bool_env_var("SGLANG_DEBUG_MEMORY_POOL")
         self.ret_values = torch.empty((), dtype=torch.int64, device=self.device)
+        self._page_offsets = torch.arange(self.page_size, device=self.device)
         self.clear()
 
     def alloc(self, need_size: int):
@@ -422,14 +451,17 @@ class PagedTokenToKVPoolAllocator(BaseTokenToKVPoolAllocator):
 
         num_pages = need_size // self.page_size
         if num_pages > len(self.free_pages):
-            return None
+            if self.release_pages:
+                self._flush_release_pages()
+            if num_pages > len(self.free_pages):
+                return None
 
         out_pages = self.free_pages[:num_pages]
         self.free_pages = self.free_pages[num_pages:]
 
         out_indices = (
             out_pages[:, None] * self.page_size
-            + torch.arange(self.page_size, device=self.device)
+            + self._page_offsets
         ).reshape(-1)
 
         return out_indices
@@ -446,6 +478,10 @@ class PagedTokenToKVPoolAllocator(BaseTokenToKVPoolAllocator):
                 (last_loc + 1) % self.page_size == prefix_lens % self.page_size
             )
 
+
+        if self.release_pages:
+            self._flush_release_pages()
+
         bs = len(prefix_lens)
         out_indices = torch.empty(
             (extend_num_tokens,), dtype=torch.int64, device=self.device
@@ -483,6 +519,10 @@ class PagedTokenToKVPoolAllocator(BaseTokenToKVPoolAllocator):
                 (last_loc + 2) % self.page_size == seq_lens % self.page_size
             )
 
+
+        if self.release_pages:
+            self._flush_release_pages()
+
         bs = len(seq_lens)
         out_indices = torch.empty((bs,), dtype=torch.int64, device=self.device)
         alloc_decode_kernel[(bs,)](
@@ -511,7 +551,7 @@ class PagedTokenToKVPoolAllocator(BaseTokenToKVPoolAllocator):
 
         if self.is_not_in_free_group:
             free_page_indices = torch.unique(free_index // self.page_size)
-            self.free_pages = torch.cat((free_page_indices, self.free_pages))
+            self.release_pages.append(free_page_indices)
         else:
             self.free_group.append(free_index)
 
@@ -523,9 +563,20 @@ class PagedTokenToKVPoolAllocator(BaseTokenToKVPoolAllocator):
         self.free_pages = torch.arange(
             1, self.num_pages + 1, dtype=torch.int64, device=self.device
         )
+        self.release_pages = []
         self.is_not_in_free_group = True
         self.free_group = []
 
+
+    def _flush_release_pages(self):
+        if not self.release_pages:
+            return
+        # Concatenate all tensors at once for better performance
+        all_pages = [self.free_pages] + self.release_pages
+        merged = torch.cat(all_pages)
+        self.free_pages = torch.sort(merged)[0]
+        self.release_pages = []
+
     def get_cpu_copy(self, indices):
         return self._kvcache.get_cpu_copy(indices)
 
@@ -633,6 +684,10 @@ class AscendPagedTokenToKVPoolAllocator(PagedTokenToKVPoolAllocator):
                 (last_loc + 1) % self.page_size == prefix_lens % self.page_size
             )
 
+
+        if self.release_pages:
+            self._flush_release_pages()
+
         bs = len(prefix_lens)
         out_indices = torch.empty(
             (extend_num_tokens,), dtype=torch.int32, device=self.device
@@ -668,6 +723,9 @@ class AscendPagedTokenToKVPoolAllocator(PagedTokenToKVPoolAllocator):
                 (last_loc + 2) % self.page_size == seq_lens % self.page_size
             )
 
+        if self.release_pages:
+            self._flush_release_pages()
+
         bs = len(seq_lens)
         out_indices = torch.empty((bs,), dtype=torch.int32, device=self.device)
 
