{"instance_id": "/home/ubuntu/OmniPerf-Bench/sglan__repo-ceba0ce", "model_patch": "diff --git a/python/sglang/srt/disaggregation/mooncake/memory_pool.py b/python/sglang/srt/disaggregation/mooncake/memory_pool.py\ndeleted file mode 100644\nindex 6e8edaf92..000000000\n--- a/python/sglang/srt/disaggregation/mooncake/memory_pool.py\n+++ /dev/null\n@@ -1,47 +0,0 @@\n-import os\n-import threading\n-from importlib import resources\n-from typing import Dict, Final, Optional\n-\n-import torch\n-from torch.cuda.memory import CUDAPluggableAllocator\n-\n-\n-# TODO(shangming): move this class into mooncake's package for more general use cases\n-class MooncakeNVLinkAllocator:\n-    _instances: Dict[torch.device, CUDAPluggableAllocator] = {}\n-    _lock: Final = threading.Lock()\n-\n-    @classmethod\n-    def _get_so_path(cls) -> str:\n-        \"\"\"Dynamically locate hook.so in the mooncake package installation\"\"\"\n-        try:\n-            # Attempt to locate package resource\n-            with resources.path(\"mooncake\", \"hook.so\") as so_path:\n-                if so_path.exists():\n-                    return str(so_path)\n-        except (ImportError, FileNotFoundError, TypeError):\n-            pass\n-\n-        # Fallback strategy: check in package location via import metadata\n-        try:\n-            import mooncake\n-\n-            base_path = os.path.dirname(os.path.abspath(mooncake.__file__))\n-            so_path = os.path.join(base_path, \"hook.so\")\n-            if os.path.exists(so_path):\n-                return so_path\n-        except (ImportError, FileNotFoundError, TypeError):\n-            raise ImportError(\n-                \"SGLANG_MOONCAKE_CUSTOM_MEM_POOL require mooncake-transfer-engine >= 0.3.3.post2.\"\n-            )\n-\n-    @classmethod\n-    def get_allocator(cls, device: torch.device) -> CUDAPluggableAllocator:\n-        with cls._lock:\n-            if device not in cls._instances:\n-                so_path = cls._get_so_path()\n-                cls._instances[device] = CUDAPluggableAllocator(\n-                    so_path, \"mc_nvlink_malloc\", \"mc_nvlink_free\"\n-                )\n-            return cls._instances[device]\ndiff --git a/python/sglang/srt/mem_cache/memory_pool.py b/python/sglang/srt/mem_cache/memory_pool.py\nindex c01807f1b..f2ae9efee 100644\n--- a/python/sglang/srt/mem_cache/memory_pool.py\n+++ b/python/sglang/srt/mem_cache/memory_pool.py\n@@ -64,7 +64,7 @@ class ReqToTokenPool:\n         self.max_context_len = max_context_len\n         self.device = device\n         with memory_saver_adapter.region(GPU_MEMORY_TYPE_KV_CACHE):\n-            self.req_to_token = torch.zeros(\n+            self.req_to_token = torch.empty(\n                 (size, max_context_len), dtype=torch.int32, device=device\n             )\n         self.free_slots = list(range(size))\n@@ -270,12 +270,13 @@ class MHATokenToKVPool(KVCache):\n             \"SGLANG_MOONCAKE_CUSTOM_MEM_POOL\", \"false\"\n         )\n         if self.enable_custom_mem_pool:\n-            from sglang.srt.disaggregation.mooncake.memory_pool import (\n-                MooncakeNVLinkAllocator,\n-            )\n+            from torch.cuda.memory import CUDAPluggableAllocator\n \n-            # TODO(shangming): abstract custom allocator class for more backends\n-            allocator = MooncakeNVLinkAllocator.get_allocator(self.device)\n+            allocator = CUDAPluggableAllocator(\n+                \"libmooncake_transfer_engine.so\",\n+                \"mc_malloc\",\n+                \"mc_free\",\n+            )\n             self.custom_mem_pool = torch.cuda.MemPool(allocator.allocator())\n         else:\n             self.custom_mem_pool = None\n@@ -303,7 +304,7 @@ class MHATokenToKVPool(KVCache):\n                 # [size, head_num, head_dim] for each layer\n                 # The padded slot 0 is used for writing dummy outputs from padded tokens.\n                 self.k_buffer = [\n-                    torch.zeros(\n+                    torch.empty(\n                         (self.size + self.page_size, self.head_num, self.head_dim),\n                         dtype=self.store_dtype,\n                         device=self.device,\n@@ -311,7 +312,7 @@ class MHATokenToKVPool(KVCache):\n                     for _ in range(self.layer_num)\n                 ]\n                 self.v_buffer = [\n-                    torch.zeros(\n+                    torch.empty(\n                         (self.size + self.page_size, self.head_num, self.head_dim),\n                         dtype=self.store_dtype,\n                         device=self.device,\n@@ -602,12 +603,13 @@ class MLATokenToKVPool(KVCache):\n             \"SGLANG_MOONCAKE_CUSTOM_MEM_POOL\", \"false\"\n         )\n         if self.enable_custom_mem_pool:\n-            from sglang.srt.disaggregation.mooncake.memory_pool import (\n-                MooncakeNVLinkAllocator,\n-            )\n+            from torch.cuda.memory import CUDAPluggableAllocator\n \n-            # TODO(shangming): abstract custom allocator class for more backends\n-            allocator = MooncakeNVLinkAllocator.get_allocator(self.device)\n+            allocator = CUDAPluggableAllocator(\n+                \"libmooncake_transfer_engine.so\",\n+                \"mc_malloc\",\n+                \"mc_free\",\n+            )\n             self.custom_mem_pool = torch.cuda.MemPool(allocator.allocator())\n         else:\n             self.custom_mem_pool = None\n@@ -620,7 +622,7 @@ class MLATokenToKVPool(KVCache):\n             ):\n                 # The padded slot 0 is used for writing dummy outputs from padded tokens.\n                 self.kv_buffer = [\n-                    torch.zeros(\n+                    torch.empty(\n                         (size + page_size, 1, kv_lora_rank + qk_rope_head_dim),\n                         dtype=self.store_dtype,\n                         device=device,\n@@ -758,13 +760,13 @@ class DoubleSparseTokenToKVPool(KVCache):\n         with self.memory_saver_adapter.region(GPU_MEMORY_TYPE_KV_CACHE):\n             # [size, head_num, head_dim] for each layer\n             self.k_buffer = [\n-                torch.zeros(\n+                torch.empty(\n                     (size + page_size, head_num, head_dim), dtype=dtype, device=device\n                 )\n                 for _ in range(layer_num)\n             ]\n             self.v_buffer = [\n-                torch.zeros(\n+                torch.empty(\n                     (size + page_size, head_num, head_dim), dtype=dtype, device=device\n                 )\n                 for _ in range(layer_num)\n@@ -772,7 +774,7 @@ class DoubleSparseTokenToKVPool(KVCache):\n \n             # [size, head_num, heavy_channel_num] for each layer\n             self.label_buffer = [\n-                torch.zeros(\n+                torch.empty(\n                     (size + 1, head_num, heavy_channel_num), dtype=dtype, device=device\n                 )\n                 for _ in range(layer_num)\ndiff --git a/scripts/ci_install_dependency.sh b/scripts/ci_install_dependency.sh\nindex 922c886c4..a1808019e 100755\n--- a/scripts/ci_install_dependency.sh\n+++ b/scripts/ci_install_dependency.sh\n@@ -23,7 +23,7 @@ pip install -e \"python[dev]\"\n pip list\n \n # Install additional dependencies\n-pip install mooncake-transfer-engine==0.3.2.post1 nvidia-cuda-nvrtc-cu12\n+pip install mooncake-transfer-engine==0.3.4 nvidia-cuda-nvrtc-cu12\n \n # For lmms_evals evaluating MMMU\n git clone --branch v0.3.3 --depth 1 https://github.com/EvolvingLMMs-Lab/lmms-eval.git\n", "model_name_or_path": "gpt-5-2025-08-07"}
