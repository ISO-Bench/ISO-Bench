{"instance_id": "/home/ubuntu/OmniPerf-Bench/sglan__repo-8609e63", "model_patch": "diff --git a/python/sglang/srt/managers/scheduler.py b/python/sglang/srt/managers/scheduler.py\nindex 8253a303b..d8d2307c8 100644\n--- a/python/sglang/srt/managers/scheduler.py\n+++ b/python/sglang/srt/managers/scheduler.py\n@@ -209,6 +209,11 @@ class Scheduler(\n     SchedulerDisaggregationPrefillMixin,\n ):\n     \"\"\"A scheduler that manages a tensor parallel GPU worker.\"\"\"\n+    # Small tensor caches to reduce per-step allocations in coordination/reduction paths.\n+    _mlp_sync_gather_bufs: Dict[Tuple[int, int], torch.Tensor] = {}\n+    _mlp_sync_local_buf: Optional[torch.Tensor] = None\n+    _tbo_preparer = None\n+\n \n     def __init__(\n         self,\n@@ -362,6 +367,10 @@ class Scheduler(\n         self.pp_group = get_pp_group()\n         self.world_group = get_world_group()\n \n+\n+        # Buffers reused across steps\n+        self._grammar_sync_tensor: Optional[torch.Tensor] = None\n+\n         self.pad_input_ids_func = self.tp_worker.get_pad_input_ids_func()\n         global_server_args_dict.update(worker_global_server_args_dict)\n         set_random_seed(self.random_seed)\n@@ -1399,28 +1408,7 @@ class Scheduler(\n             self.metrics_collector.log_stats(self.stats)\n         self._publish_kv_events()\n \n-    def coordinate_spec_dp_attn_batch(self, new_batch: Optional[ScheduleBatch]):\n-        \"\"\"Coordinate the DP attention batch.\"\"\"\n \n-        local_info = torch.tensor(\n-            [\n-                (new_batch is not None),\n-            ],\n-            dtype=torch.int64,\n-        )\n-        global_info = torch.empty(\n-            (self.server_args.dp_size, self.attn_tp_size, 1),\n-            dtype=torch.int64,\n-        )\n-        torch.distributed.all_gather_into_tensor(\n-            global_info.flatten(),\n-            local_info,\n-            group=self.tp_cpu_group,\n-        )\n-        any_new_batch = any(\n-            global_info[:, 0, 0].tolist()\n-        )  # Any DP worker has forward batch\n-        return any_new_batch\n \n     def get_next_batch_to_run(self) -> Optional[ScheduleBatch]:\n         # Merge the prefill batch into the running batch\n@@ -1456,13 +1444,7 @@ class Scheduler(\n \n         new_batch = self.get_new_batch_prefill()\n \n-        # TODO(ch-wan): minor refactor is needed here to improve readability\n-        any_new_batch = (\n-            self.server_args.enable_dp_attention\n-            and not self.spec_algorithm.is_none()\n-            and self.coordinate_spec_dp_attn_batch(new_batch)\n-        )\n-        if new_batch is not None or any_new_batch:\n+        if new_batch is not None:\n             # Run prefill first if possible\n             ret = new_batch\n         else:\n@@ -1817,13 +1799,10 @@ class Scheduler(\n         else:\n             num_tokens = local_batch.extend_num_tokens\n             num_tokens_for_logprob = sum(\n-                [\n-                    # We should have at least 1 token for sample in every case.\n-                    max(extend_len - logprob_start_len, 1)\n-                    for logprob_start_len, extend_len in zip(\n-                        local_batch.extend_logprob_start_lens, local_batch.extend_lens\n-                    )\n-                ]\n+                max(extend_len - logprob_start_len, 1)\n+                for logprob_start_len, extend_len in zip(\n+                    local_batch.extend_logprob_start_lens, local_batch.extend_lens\n+                )\n             )\n \n         if local_batch is None or local_batch.forward_mode.is_decode_or_idle():\n@@ -1840,42 +1819,50 @@ class Scheduler(\n             local_batch.forward_mode.is_extend() if local_batch else False\n         )\n \n-        tbo_preparer = TboDPAttentionPreparer()\n-\n-        local_info = torch.tensor(\n-            [\n-                num_tokens,\n-                can_cuda_graph,\n-                num_tokens_for_logprob,\n-                is_extend_in_batch,\n-                *tbo_preparer.prepare_all_gather(\n-                    local_batch,\n-                    deepep_mode,\n-                    enable_deepep_moe,\n-                    enable_two_batch_overlap,\n-                ),\n-            ],\n-            dtype=torch.int64,\n-        )\n-        global_info = torch.empty(\n-            (dp_size, attn_tp_size, 6),\n-            dtype=torch.int64,\n+        # Reuse a cached preparer and gather buffers to reduce overhead.\n+        if Scheduler._tbo_preparer is None:\n+            Scheduler._tbo_preparer = TboDPAttentionPreparer()\n+        tbo_preparer = Scheduler._tbo_preparer\n+\n+        # local info: [num_tokens, can_cuda_graph, num_tokens_for_logprob, is_extend_in_batch, ...]\n+        if Scheduler._mlp_sync_local_buf is None:\n+            Scheduler._mlp_sync_local_buf = torch.empty((6,), dtype=torch.int64)\n+        local_info = Scheduler._mlp_sync_local_buf\n+        local_info[0] = int(num_tokens)\n+        local_info[1] = int(can_cuda_graph)\n+        local_info[2] = int(num_tokens_for_logprob)\n+        local_info[3] = int(is_extend_in_batch)\n+        a5, a6 = tbo_preparer.prepare_all_gather(\n+            local_batch,\n+            deepep_mode,\n+            enable_deepep_moe,\n+            enable_two_batch_overlap,\n         )\n+        local_info[4] = int(a5)\n+        local_info[5] = int(a6)\n+\n+        shape = (dp_size, attn_tp_size, 6)\n+        key = (dp_size, attn_tp_size)\n+        global_info = Scheduler._mlp_sync_gather_bufs.get(key)\n+        if global_info is None or tuple(global_info.shape) != shape:\n+            global_info = torch.empty(shape, dtype=torch.int64)\n+            Scheduler._mlp_sync_gather_bufs[key] = global_info\n+\n         torch.distributed.all_gather_into_tensor(\n             global_info.flatten(),\n             local_info,\n             group=tp_cpu_group,\n         )\n-        global_num_tokens = global_info[:, 0, 0].tolist()\n-        can_cuda_graph = min(global_info[:, 0, 1].tolist())\n-        global_num_tokens_for_logprob = global_info[:, 0, 2].tolist()\n-        is_extend_in_batch = global_info[:, 0, 3].tolist()\n+        global_num_tokens_t = global_info[:, 0, 0]\n+        can_cuda_graph = int(global_info[:, 0, 1].min().item())\n+        global_num_tokens_for_logprob_t = global_info[:, 0, 2]\n+        is_extend_any = bool(global_info[:, 0, 3].any().item())\n \n         tbo_split_seq_index, global_forward_mode = tbo_preparer.compute_output(\n             global_info[:, :, 4:6]\n         )\n \n-        if local_batch is None and max(global_num_tokens) > 0:\n+        if local_batch is None and int(global_num_tokens_t.max().item()) > 0:\n             local_batch = get_idle_batch()\n \n         if local_batch is not None:\n@@ -1884,11 +1871,11 @@ class Scheduler(\n                 local_batch.global_num_tokens = [num_tokens]\n                 local_batch.global_num_tokens_for_logprob = [num_tokens_for_logprob]\n             else:\n-                local_batch.global_num_tokens = global_num_tokens\n+                local_batch.global_num_tokens = global_num_tokens_t.tolist()\n                 local_batch.global_num_tokens_for_logprob = (\n-                    global_num_tokens_for_logprob\n+                    global_num_tokens_for_logprob_t.tolist()\n                 )\n-            local_batch.is_extend_in_batch = any(is_extend_in_batch)\n+            local_batch.is_extend_in_batch = is_extend_any\n             local_batch.tbo_split_seq_index = tbo_split_seq_index\n             local_batch.global_forward_mode = global_forward_mode\n \n@@ -1896,8 +1883,7 @@ class Scheduler(\n             if not disable_cuda_graph:\n                 local_batch.can_run_dp_cuda_graph = can_cuda_graph\n \n-        # TODO(ch-wan): refactor: any(is_extend_in_batch) now is a part of local_batch. Remove it from here.\n-        return local_batch, any(is_extend_in_batch)\n+        return local_batch, is_extend_any\n \n     def get_idle_batch(self):\n         idle_batch = ScheduleBatch.init_new(\n@@ -1947,11 +1933,15 @@ class Scheduler(\n \n         if tp_size > 1:\n             # Sync across TP ranks to make sure they have the same number of ready requests\n-            tensor = torch.tensor([num_ready_reqs, num_timeout_reqs], dtype=torch.int32)\n+            if self._grammar_sync_tensor is None:\n+                self._grammar_sync_tensor = torch.empty(2, dtype=torch.int32)\n+            self._grammar_sync_tensor[0] = int(num_ready_reqs)\n+            self._grammar_sync_tensor[1] = int(num_timeout_reqs)\n             torch.distributed.all_reduce(\n-                tensor, op=torch.distributed.ReduceOp.MAX, group=tp_group\n+                self._grammar_sync_tensor, op=torch.distributed.ReduceOp.MAX, group=tp_group\n             )\n-            num_ready_reqs_max, num_timeout_reqs_max = tensor.tolist()\n+            num_ready_reqs_max = int(self._grammar_sync_tensor[0].item())\n+            num_timeout_reqs_max = int(self._grammar_sync_tensor[1].item())\n \n             for i in range(num_ready_reqs, num_ready_reqs_max):\n                 req = self.grammar_queue[i]\n", "model_name_or_path": "gpt-5-2025-08-07"}
