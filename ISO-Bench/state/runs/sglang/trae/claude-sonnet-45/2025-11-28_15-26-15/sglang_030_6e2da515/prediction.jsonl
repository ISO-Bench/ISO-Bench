{"instance_id": "/home/ubuntu/OmniPerf-Bench/sglan__repo-e9a47f4", "model_patch": "diff --git a/benchmark/bench_in_batch_prefix/bench_in_batch_prefix.py b/benchmark/bench_in_batch_prefix/bench_in_batch_prefix.py\nindex 86648e5ff..282097112 100644\n--- a/benchmark/bench_in_batch_prefix/bench_in_batch_prefix.py\n+++ b/benchmark/bench_in_batch_prefix/bench_in_batch_prefix.py\n@@ -64,11 +64,11 @@ def test_batch_by_batch(all_prompts, gen_len):\n \n     tot_time = 0\n     for i in range(len(all_prompts)):\n-        tic = time.time()\n+        tic = time.perf_counter()\n         text_qa.run_batch(\n             list(zip(all_prompts[i], [gen_len] * len(all_prompts[i]))),\n         )\n-        tot_time += time.time() - tic\n+        tot_time += time.perf_counter() - tic\n \n     return tot_time\n \n@@ -78,13 +78,13 @@ def test_batch_by_batch_with_hint(all_prompts, gen_len):\n \n     tot_time = 0\n     for i in range(len(all_prompts)):\n-        tic = time.time()\n+        tic = time.perf_counter()\n         # Send a hint to cache the prefix\n         text_qa.run_batch(list(zip(all_prompts[i][:1], [gen_len])))\n         # Send the batch\n         text_qa.run_batch(list(zip(all_prompts[i], [gen_len] * len(all_prompts[i]))))\n \n-        tot_time += time.time() - tic\n+        tot_time += time.perf_counter() - tic\n \n     return tot_time\n \n@@ -94,11 +94,11 @@ def test_send_all(all_prompts, gen_len):\n \n     all_prompts = [x for prompt_list in all_prompts for x in prompt_list]\n \n-    tic = time.time()\n+    tic = time.perf_counter()\n     text_qa.run_batch(\n         list(zip(all_prompts, [gen_len] * len(all_prompts))),\n     )\n-    tot_time = time.time() - tic\n+    tot_time = time.perf_counter() - tic\n \n     return tot_time\n \ndiff --git a/benchmark/benchmark_batch/benchmark_batch.py b/benchmark/benchmark_batch/benchmark_batch.py\nindex 15ef0ab6a..c23ea4654 100644\n--- a/benchmark/benchmark_batch/benchmark_batch.py\n+++ b/benchmark/benchmark_batch/benchmark_batch.py\n@@ -81,7 +81,7 @@ def send_batch_request(endpoint, prompts, gen_tokens, request_id):\n     }\n     data = {\"text\": prompts, \"sampling_params\": sampling_params}\n \n-    start_time = time.time()\n+    start_time = time.perf_counter()\n     try:\n         response = requests.post(\n             endpoint.base_url + \"/generate\", json=data, timeout=3600\n@@ -90,7 +90,7 @@ def send_batch_request(endpoint, prompts, gen_tokens, request_id):\n             error = response.json()\n             raise RuntimeError(f\"Request {request_id} failed: {error}\")\n         result = response.json()\n-        elapsed_time = (time.time() - start_time) * 1000  # Convert to ms\n+        elapsed_time = (time.perf_counter() - start_time) * 1000  # Convert to ms\n         avg_per_prompt = elapsed_time / len(prompts) if prompts else 0\n         return request_id, elapsed_time, avg_per_prompt, True, len(prompts)\n     except Exception as e:\n@@ -104,7 +104,7 @@ def run_benchmark(endpoint, batched_prompts, batch_size, gen_tokens):\n     num_requests = len(batched_prompts)\n \n     # Record start time for total latency\n-    benchmark_start_time = time.time()\n+    benchmark_start_time = time.perf_counter()\n \n     for i, batch_prompts in enumerate(batched_prompts):\n         request_id = i + 1\n@@ -113,13 +113,13 @@ def run_benchmark(endpoint, batched_prompts, batch_size, gen_tokens):\n         ), f\"Request {request_id} should have {batch_size} prompts, got {len(batch_prompts)}\"\n \n         print(\n-            f\"[Request] Sending request {request_id}/{num_requests} with {len(batch_prompts)} prompts at {int(time.time()*1000)}\"\n+            f\"[Request] Sending request {request_id}/{num_requests} with {len(batch_prompts)} prompts at {int(time.perf_counter()*1000)}\"\n         )\n         result = send_batch_request(endpoint, batch_prompts, gen_tokens, request_id)\n         results.append(result)\n \n     # Calculate total latency\n-    total_latency = (time.time() - benchmark_start_time) * 1000  # Convert to ms\n+    total_latency = (time.perf_counter() - benchmark_start_time) * 1000  # Convert to ms\n \n     return results, total_latency\n \ndiff --git a/benchmark/benchmark_batch/benchmark_tokenizer.py b/benchmark/benchmark_batch/benchmark_tokenizer.py\nindex c00bfb84b..88a5820b6 100644\n--- a/benchmark/benchmark_batch/benchmark_tokenizer.py\n+++ b/benchmark/benchmark_batch/benchmark_tokenizer.py\n@@ -44,10 +44,10 @@ def benchmark_sequential_vs_batch(prompts, batch_size, tokenizer):\n     for run in range(NUM_RUNS):\n         batch_prompts = prompts[:batch_size]  # Use same prompts for fair comparison\n \n-        start_time = time.time()\n+        start_time = time.perf_counter()\n         for prompt in batch_prompts:\n             tokens = tokenizer.encode(prompt)\n-        sequential_time = (time.time() - start_time) * 1000\n+        sequential_time = (time.perf_counter() - start_time) * 1000\n         sequential_times.append(sequential_time)\n \n     # Batch tokenization using tokenizer()\n@@ -55,9 +55,9 @@ def benchmark_sequential_vs_batch(prompts, batch_size, tokenizer):\n     for run in range(NUM_RUNS):\n         batch_prompts = prompts[:batch_size]  # Use same prompts for fair comparison\n \n-        start_time = time.time()\n+        start_time = time.perf_counter()\n         tokens = tokenizer(batch_prompts)\n-        batch_time = (time.time() - start_time) * 1000\n+        batch_time = (time.perf_counter() - start_time) * 1000\n         batch_times.append(batch_time)\n \n     return {\ndiff --git a/benchmark/kernels/fused_moe_triton/tuning_fused_moe_triton.py b/benchmark/kernels/fused_moe_triton/tuning_fused_moe_triton.py\nindex a3ead1eca..be349e456 100644\n--- a/benchmark/kernels/fused_moe_triton/tuning_fused_moe_triton.py\n+++ b/benchmark/kernels/fused_moe_triton/tuning_fused_moe_triton.py\n@@ -487,7 +487,7 @@ def main(args: argparse.Namespace):\n             ]\n         print(f\"Start tuning over {len(search_space)} configurations...\")\n \n-        start = time.time()\n+        start = time.perf_counter()\n         configs = _distribute(\n             \"tune\",\n             [\n@@ -522,7 +522,7 @@ def main(args: argparse.Namespace):\n             use_int8_w8a16,\n             block_shape,\n         )\n-        end = time.time()\n+        end = time.perf_counter()\n         print(f\"Tuning took {end - start:.2f} seconds\")\n     else:\n         outputs = _distribute(\ndiff --git a/benchmark/kernels/scheduler_batch/benchmark_write_req_to_token_pool_triton.py b/benchmark/kernels/scheduler_batch/benchmark_write_req_to_token_pool_triton.py\nindex 1ce43c8ba..111550821 100644\n--- a/benchmark/kernels/scheduler_batch/benchmark_write_req_to_token_pool_triton.py\n+++ b/benchmark/kernels/scheduler_batch/benchmark_write_req_to_token_pool_triton.py\n@@ -116,7 +116,7 @@ def test_write_req_to_token_pool():\n     extend_len = 14\n \n     # Initialize input tensors\n-    req_to_token = torch.zeros(\n+    req_to_token = torch.empty(\n         (max_batch, max_context_len), dtype=torch.int32, device=\"cuda\"\n     )\n     req_pool_indices = torch.tensor([42], dtype=torch.int32, device=\"cuda\")\n@@ -175,7 +175,7 @@ def test_write_req_to_token_pool():\n     extend_lens_list = [14, 20, 30]\n     total_extend_len = sum(extend_lens_list)\n \n-    req_to_token = torch.zeros(\n+    req_to_token = torch.empty(\n         (max_batch, max_context_len), dtype=torch.int32, device=\"cuda\"\n     )\n     req_pool_indices = torch.tensor([42, 100, 200], dtype=torch.int32, device=\"cuda\")\n@@ -251,7 +251,7 @@ def get_benchmark():\n         extend_lens_list = [extend_len] * batch_size\n         total_extend_len = sum(extend_lens_list)\n \n-        req_to_token = torch.zeros(\n+        req_to_token = torch.empty(\n             (max_batch, max_context_len), dtype=torch.int32, device=\"cuda\"\n         )\n         req_pool_indices = torch.arange(batch_size, dtype=torch.int32, device=\"cuda\")\n", "model_name_or_path": "gpt-5-2025-08-07"}
