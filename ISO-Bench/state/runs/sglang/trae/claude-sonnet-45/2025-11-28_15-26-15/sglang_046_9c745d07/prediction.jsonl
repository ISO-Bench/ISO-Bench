{"instance_id": "/home/ubuntu/OmniPerf-Bench/sglan__repo-ebaa2f3", "model_patch": "diff --git a/python/sglang/srt/constrained/outlines_backend.py b/python/sglang/srt/constrained/outlines_backend.py\nindex cc68b97f8..8b5eae898 100644\n--- a/python/sglang/srt/constrained/outlines_backend.py\n+++ b/python/sglang/srt/constrained/outlines_backend.py\n@@ -81,10 +81,21 @@ class OutlinesGrammar(BaseGrammarObject):\n     ):\n         self.state = next_state\n \n-    def fill_vocab_mask(self, vocab_mask: torch.Tensor):\n+    def fill_vocab_mask(self, vocab_mask: torch.Tensor, idx: int = None) -> None:\n+        if idx is not None:\n+            vocab_mask = vocab_mask[idx]\n         vocab_mask.fill_(1)\n         vocab_mask[self.guide.get_next_instruction(self.state).tokens] = 0\n \n+    def allocate_vocab_mask(\n+        self, vocab_size: int, batch_size: int, device\n+    ) -> torch.Tensor:\n+        return torch.empty(batch_size, vocab_size, dtype=torch.bool, device=device)\n+\n+    @staticmethod\n+    def apply_vocab_mask(logits: torch.Tensor, vocab_mask: torch.Tensor) -> None:\n+        logits.masked_fill_(vocab_mask, float(\"-inf\"))\n+\n     def copy(self):\n         return OutlinesGrammar(self.guide, self.jump_forward_map)\n \ndiff --git a/python/sglang/srt/constrained/xgrammar_backend.py b/python/sglang/srt/constrained/xgrammar_backend.py\nindex ab4df5c98..19f12bfca 100644\n--- a/python/sglang/srt/constrained/xgrammar_backend.py\n+++ b/python/sglang/srt/constrained/xgrammar_backend.py\n@@ -80,14 +80,26 @@ class XGrammarGrammar(BaseGrammarObject):\n         for i in range(k, len(new_output_ids)):\n             assert self.matcher.accept_token(new_output_ids[i])\n \n-    def fill_vocab_mask(self, vocab_mask: torch.Tensor):\n+    def fill_vocab_mask(self, vocab_mask: torch.Tensor, idx: int = None) -> None:\n         # Note that this bitmask is a bitset, not bool\n+        if idx is not None:\n+            vocab_mask = vocab_mask[idx]\n+        vocab_mask.fill_(0)\n         bitmask = self.matcher.get_next_token_bitmask()\n         # Mask the tokens that are not allowed\n         vocab_mask[\n             self.matcher.get_rejected_tokens_from_bitmask(bitmask, self.vocab_size)\n         ] = 1\n \n+    def allocate_vocab_mask(\n+        self, vocab_size: int, batch_size: int, device\n+    ) -> torch.Tensor:\n+        return torch.empty(batch_size, vocab_size, dtype=torch.bool, device=device)\n+\n+    @staticmethod\n+    def apply_vocab_mask(logits: torch.Tensor, vocab_mask: torch.Tensor) -> None:\n+        logits.masked_fill_(vocab_mask, float(\"-inf\"))\n+\n     def copy(self):\n         matcher = GrammarMatcher(\n             self.ctx,\ndiff --git a/python/sglang/srt/model_executor/model_runner.py b/python/sglang/srt/model_executor/model_runner.py\nindex 02750d5df..3eef1a0cd 100644\n--- a/python/sglang/srt/model_executor/model_runner.py\n+++ b/python/sglang/srt/model_executor/model_runner.py\n@@ -645,7 +645,15 @@ class ModelRunner:\n \n         # Apply regex vocab_mask\n         if sampling_info.vocab_mask is not None:\n-            logits = logits.masked_fill(sampling_info.vocab_mask, float(\"-inf\"))\n+            # Use grammar's apply_vocab_mask for backend-specific optimizations\n+            if sampling_info.grammars:\n+                first_grammar = next((g for g in sampling_info.grammars if g is not None), None)\n+                if first_grammar is not None and hasattr(first_grammar, \"apply_vocab_mask\"):\n+                    first_grammar.apply_vocab_mask(logits, sampling_info.vocab_mask)\n+                else:\n+                    logits.masked_fill_(sampling_info.vocab_mask, float(\"-inf\"))\n+            else:\n+                logits.masked_fill_(sampling_info.vocab_mask, float(\"-inf\"))\n \n         return logits\n \ndiff --git a/python/sglang/srt/sampling/sampling_batch_info.py b/python/sglang/srt/sampling/sampling_batch_info.py\nindex a341c2b17..e0248c541 100644\n--- a/python/sglang/srt/sampling/sampling_batch_info.py\n+++ b/python/sglang/srt/sampling/sampling_batch_info.py\n@@ -125,11 +125,12 @@ class SamplingBatchInfo:\n             else:\n                 if self.linear_penalties is None:\n                     bs = self.penalizer_orchestrator.batch.batch_size()\n-                    self.linear_penalties = torch.zeros(\n+                    self.linear_penalties = torch.empty(\n                         (bs, self.vocab_size),\n                         dtype=torch.float32,\n                         device=self.device,\n                     )\n+                    self.linear_penalties.fill_(0)\n                 self.linear_penalties = penalizer.apply(self.linear_penalties)\n \n     def update_regex_vocab_mask(self):\n@@ -137,15 +138,26 @@ class SamplingBatchInfo:\n             self.vocab_mask = None\n             return\n \n-        self.vocab_mask = torch.zeros(\n-            len(self.temperatures),\n-            self.vocab_size,\n-            dtype=torch.bool,\n-            device=self.device,\n-        )\n+        # Allocate vocab_mask via grammar to allow backend-specific optimizations\n+        bs = len(self.temperatures)\n+        # Find the first non-None grammar to allocate the shared mask buffer\n+        first_grammar = next((g for g in self.grammars if g is not None), None)\n+        if first_grammar is not None and hasattr(first_grammar, \"allocate_vocab_mask\"):\n+            self.vocab_mask = first_grammar.allocate_vocab_mask(\n+                self.vocab_size, bs, self.device\n+            )\n+        else:\n+            # Fallback: allocate uninitialized buffer; grammars must fully initialize rows\n+            self.vocab_mask = torch.empty(\n+                bs, self.vocab_size, dtype=torch.bool, device=self.device\n+            )\n         for i, grammar in enumerate(self.grammars):\n             if grammar is not None:\n-                grammar.fill_vocab_mask(self.vocab_mask[i])\n+                # Support both new (mask, idx) and legacy (mask_row) signatures\n+                try:\n+                    grammar.fill_vocab_mask(self.vocab_mask, i)\n+                except TypeError:\n+                    grammar.fill_vocab_mask(self.vocab_mask[i])\n \n     def filter_batch(self, unfinished_indices: List[int], new_indices: torch.Tensor):\n         if self.penalizer_orchestrator:\n@@ -180,9 +192,11 @@ class SamplingBatchInfo:\n                 shape, dtype = rhs.shape[1:], rhs.dtype\n             with torch.dtype(dtype):\n                 if lhs is None:\n-                    lhs = torch.empty((bs1, *shape), device=device).fill_(default)\n+                    lhs = torch.empty((bs1, *shape), device=device, dtype=dtype)\n+                    lhs.fill_(default)\n                 if rhs is None:\n-                    rhs = torch.empty((bs2, *shape), device=device).fill_(default)\n+                    rhs = torch.empty((bs2, *shape), device=device, dtype=dtype)\n+                    rhs.fill_(default)\n             return torch.cat([lhs, rhs])\n \n         return None\n", "model_name_or_path": "gpt-5-2025-08-07"}
