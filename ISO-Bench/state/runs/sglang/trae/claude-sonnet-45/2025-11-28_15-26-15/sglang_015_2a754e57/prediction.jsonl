{"instance_id": "/home/ubuntu/OmniPerf-Bench/sglan__repo-96c503e", "model_patch": "diff --git a/docs/test_process.md b/docs/test_process.md\nindex d22570b4e..18f91c6d4 100644\n--- a/docs/test_process.md\n+++ b/docs/test_process.md\n@@ -1,6 +1,7 @@\n ## SRT Unit Tests\n \n ### Latency Alignment\n+Make sure your changes do not slow down the following benchmarks\n ```\n # single gpu\n python -m sglang.bench_latency --model-path meta-llama/Llama-2-7b-chat-hf --mem-fraction-static 0.8 --batch 32 --input-len 512 --output-len 256\ndiff --git a/model_patch.diff b/model_patch.diff\nnew file mode 100644\nindex 000000000..e69de29bb\ndiff --git a/python/sglang/bench_latency.py b/python/sglang/bench_latency.py\nindex a163cbd30..b349c9971 100644\n--- a/python/sglang/bench_latency.py\n+++ b/python/sglang/bench_latency.py\n@@ -222,8 +222,8 @@ def latency_test(\n     @torch.inference_mode()\n     def run_once(output_len):\n         # Prefill\n-        torch.cuda.synchronize()\n         tot_latency = 0\n+        torch.cuda.synchronize()\n         tic = time.time()\n         next_token_ids, _, batch = extend(reqs, model_runner)\n         torch.cuda.synchronize()\ndiff --git a/python/sglang/global_config.py b/python/sglang/global_config.py\nindex 0cc0f747f..74655268d 100644\n--- a/python/sglang/global_config.py\n+++ b/python/sglang/global_config.py\n@@ -19,6 +19,7 @@ class GlobalConfig:\n         self.enable_precache_with_tracing = True\n         self.enable_parallel_encoding = True\n         self.enable_parallel_decoding = True\n+        self.enable_dp_attention = False\n \n         # Choices: [\"no_adjust\", \"adjust_cache\"]\n         # no_adjust: Do not adjust the position embedding of KV cache.\ndiff --git a/python/sglang/srt/layers/radix_attention.py b/python/sglang/srt/layers/radix_attention.py\nindex 66d206082..af5821d7b 100644\n--- a/python/sglang/srt/layers/radix_attention.py\n+++ b/python/sglang/srt/layers/radix_attention.py\n@@ -103,11 +103,20 @@ class RadixAttention(nn.Module):\n     def prefill_forward_flashinfer(self, q, k, v, input_metadata: InputMetadata):\n         self.store_kv_cache(k, v, input_metadata)\n \n-        o = input_metadata.flashinfer_prefill_wrapper.forward(\n-            q.contiguous().view(-1, self.tp_q_head_num, self.head_dim),\n-            input_metadata.token_to_kv_pool.kv_data[self.layer_id],\n-            logits_soft_cap=self.logit_cap,\n-        )\n+        from sglang.global_config import global_config\n+        if global_config.enable_dp_attention:\n+            o = input_metadata.flashinfer_prefill_wrapper.forward(\n+                q.contiguous().view(-1, self.tp_q_head_num, self.head_dim),\n+                input_metadata.token_to_kv_pool.kv_data[self.layer_id],\n+                logits_soft_cap=self.logit_cap,\n+                sm_scale=1.0 / (self.head_dim ** 0.5),\n+            )\n+        else:\n+            o = input_metadata.flashinfer_prefill_wrapper.forward(\n+                q.contiguous().view(-1, self.tp_q_head_num, self.head_dim),\n+                input_metadata.token_to_kv_pool.kv_data[self.layer_id],\n+                logits_soft_cap=self.logit_cap,\n+            )\n \n         return o.view(-1, self.tp_q_head_num * self.head_dim)\n \ndiff --git a/python/sglang/srt/managers/controller/model_runner.py b/python/sglang/srt/managers/controller/model_runner.py\nindex c2fe5c6e3..edcaaf7e2 100644\n--- a/python/sglang/srt/managers/controller/model_runner.py\n+++ b/python/sglang/srt/managers/controller/model_runner.py\n@@ -69,10 +69,9 @@ class InputMetadata:\n     flashinfer_decode_wrapper: \"BatchDecodeWithPagedKVCacheWrapper\" = None\n \n     def init_flashinfer_args(self, num_qo_heads, num_kv_heads, head_dim):\n-        self.kv_indptr = torch.empty(\n+        self.kv_indptr = torch.zeros(\n             (self.batch_size + 1,), dtype=torch.int32, device=\"cuda\"\n         )\n-        self.kv_indptr[0] = 0\n         self.kv_indptr[1:] = torch.cumsum(self.seq_lens, dim=0)\n         self.kv_last_page_len = torch.ones(\n             (self.batch_size,), dtype=torch.int32, device=\"cuda\"\n@@ -93,10 +92,9 @@ class InputMetadata:\n             self.forward_mode == ForwardMode.PREFILL\n             or self.forward_mode == ForwardMode.EXTEND\n         ):\n-            self.qo_indptr = torch.empty(\n+            self.qo_indptr = torch.zeros(\n                 (self.batch_size + 1,), dtype=torch.int32, device=\"cuda\"\n             )\n-            self.qo_indptr[0] = 0\n             self.qo_indptr[1:] = torch.cumsum(self.extend_seq_lens, dim=0)\n \n             self.flashinfer_prefill_wrapper.end_forward()\n@@ -126,8 +124,7 @@ class InputMetadata:\n \n     def init_extend_args(self):\n         self.extend_seq_lens = self.seq_lens - self.prefix_lens\n-        self.extend_start_loc = torch.empty_like(self.seq_lens)\n-        self.extend_start_loc[0] = 0\n+        self.extend_start_loc = torch.zeros_like(self.seq_lens)\n         self.extend_start_loc[1:] = torch.cumsum(self.extend_seq_lens[:-1], dim=0)\n         self.max_extend_len = int(torch.max(self.extend_seq_lens))\n \n@@ -150,8 +147,7 @@ class InputMetadata:\n         flashinfer_decode_wrapper=None,\n     ):\n         batch_size = len(req_pool_indices)\n-        start_loc = torch.empty((batch_size,), dtype=torch.int32, device=\"cuda\")\n-        start_loc[0] = 0\n+        start_loc = torch.zeros((batch_size,), dtype=torch.int32, device=\"cuda\")\n         start_loc[1:] = torch.cumsum(seq_lens[:-1], dim=0)\n         total_num_tokens = int(torch.sum(seq_lens))\n         max_seq_len = int(torch.max(seq_lens))\n@@ -377,14 +373,17 @@ class ModelRunner:\n             else:\n                 use_tensor_cores = False\n \n-            workspace_buffer = torch.empty(\n+            workspace_buffer_prefill = torch.empty(\n+                128 * 1024 * 1024, dtype=torch.int8, device=\"cuda\"\n+            )\n+            workspace_buffer_decode = torch.empty(\n                 128 * 1024 * 1024, dtype=torch.int8, device=\"cuda\"\n             )\n             self.flashinfer_prefill_wrapper = BatchPrefillWithPagedKVCacheWrapper(\n-                workspace_buffer, \"NHD\"\n+                workspace_buffer_prefill, \"NHD\"\n             )\n             self.flashinfer_decode_wrapper = BatchDecodeWithPagedKVCacheWrapper(\n-                workspace_buffer, \"NHD\", use_tensor_cores=use_tensor_cores\n+                workspace_buffer_decode, \"NHD\", use_tensor_cores=use_tensor_cores\n             )\n         else:\n             self.flashinfer_prefill_wrapper = self.flashinfer_decode_wrapper = None\ndiff --git a/python/sglang/srt/server.py b/python/sglang/srt/server.py\nindex e28530889..43d93aac3 100644\n--- a/python/sglang/srt/server.py\n+++ b/python/sglang/srt/server.py\n@@ -203,10 +203,10 @@ def launch_server(server_args: ServerArgs, pipe_finish_writer, model_overide_arg\n     pipe_router_reader, pipe_router_writer = mp.Pipe(duplex=False)\n     pipe_detoken_reader, pipe_detoken_writer = mp.Pipe(duplex=False)\n \n-    if server_args.dp_size == 1:\n-        start_process = start_controller_process_single\n-    else:\n+    if server_args.dp_size > 1:\n         start_process = start_controller_process_multi\n+    else:\n+        start_process = start_controller_process_single\n     proc_router = mp.Process(\n         target=start_process,\n         args=(server_args, port_args, pipe_router_writer, model_overide_args),\n", "model_name_or_path": "gpt-5-2025-08-07"}
