{"instance_id": "/home/ubuntu/OmniPerf-Bench/sglan__repo-48efec7", "model_patch": "diff --git a/python/sglang/srt/entrypoints/http_server.py b/python/sglang/srt/entrypoints/http_server.py\nindex 32a11c15c..c46ac075d 100644\n--- a/python/sglang/srt/entrypoints/http_server.py\n+++ b/python/sglang/srt/entrypoints/http_server.py\n@@ -259,6 +259,36 @@ async def generate_request(obj: GenerateReqInput, request: Request):\n             return _create_error_response(e)\n \n \n+\n+@app.api_route(\"/generate_from_file\", methods=[\"POST\"])\n+async def generate_from_file_request(file: UploadFile, request: Request):\n+    \"\"\"Handle a generate request via file upload for input_embeds only.\n+    This avoids large JSON body parsing overhead by directly loading bytes.\n+    \"\"\"\n+    content = await file.read()\n+    try:\n+        # orjson.loads accepts bytes directly\n+        input_embeds = orjson.loads(content)\n+    except Exception as e:\n+        return _create_error_response(e)\n+\n+    # Provide minimal sampling params to match common defaults in tests\n+    obj = GenerateReqInput(\n+        input_embeds=input_embeds,\n+        sampling_params={\n+            \"temperature\": 0,\n+            \"max_new_tokens\": 50,\n+        },\n+    )\n+\n+    try:\n+        ret = await _global_state.tokenizer_manager.generate_request(obj, request).__anext__()\n+        return ret\n+    except ValueError as e:\n+        logger.error(f\"Error: {e}\")\n+        return _create_error_response(e)\n+\n+\n @app.api_route(\"/encode\", methods=[\"POST\", \"PUT\"])\n async def encode_request(obj: EmbeddingReqInput, request: Request):\n     \"\"\"Handle an embedding request.\"\"\"\ndiff --git a/test/srt/test_input_embeddings.py b/test/srt/test_input_embeddings.py\nindex 015aabe78..72b56dd5d 100644\n--- a/test/srt/test_input_embeddings.py\n+++ b/test/srt/test_input_embeddings.py\n@@ -50,6 +50,25 @@ class TestInputEmbeds(unittest.TestCase):\n             \"error\": f\"Request failed with status {response.status_code}: {response.text}\"\n         }\n \n+    def send_request_via_file(self, embeddings):\n+        \"\"\"Send a POST request with input_embeds uploaded as a file to reduce JSON overhead.\"\"\"\n+        import io\n+        # Encode embeddings as JSON bytes\n+        embeds_bytes = json.dumps(embeddings).encode(\"utf-8\")\n+        files = {\n+            \"file\": (\"embeds.json\", io.BytesIO(embeds_bytes), \"application/json\"),\n+        }\n+        response = requests.post(\n+            self.base_url + \"/generate_from_file\",\n+            files=files,\n+            timeout=30,\n+        )\n+        if response.status_code == 200:\n+            return response.json()\n+        return {\n+            \"error\": f\"Request failed with status {response.status_code}: {response.text}\"\n+        }\n+\n     def test_text_based_response(self):\n         \"\"\"Print API response using text-based input.\"\"\"\n         for text in self.texts:\n@@ -77,6 +96,15 @@ class TestInputEmbeds(unittest.TestCase):\n                 f\"Embeddings Input (for text '{text}'):\\nResponse: {json.dumps(response, indent=2)}\\n{'-' * 80}\"\n             )\n \n+    def test_embedding_based_response_via_file(self):\n+        \"\"\"Print API response using input embeddings uploaded as a file.\"\"\"\n+        for text in self.texts:\n+            embeddings = self.generate_input_embeddings(text)\n+            response = self.send_request_via_file(embeddings)\n+            print(\n+                f\"Embeddings File Upload (for text '{text}'):\\nResponse: {json.dumps(response, indent=2)}\\n{'-' * 80}\"\n+            )\n+\n     def test_compare_text_vs_embedding(self):\n         \"\"\"Print responses for both text-based and embedding-based inputs.\"\"\"\n         for text in self.texts:\n", "model_name_or_path": "gpt-5-2025-08-07"}
