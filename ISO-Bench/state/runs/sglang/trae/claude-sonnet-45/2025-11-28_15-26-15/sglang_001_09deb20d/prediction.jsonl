{"instance_id": "/home/ubuntu/OmniPerf-Bench/sglan__repo-33b242d", "model_patch": "diff --git a/python/sglang/srt/layers/logits_processor.py b/python/sglang/srt/layers/logits_processor.py\nindex f95c30786..1ce18b8ee 100644\n--- a/python/sglang/srt/layers/logits_processor.py\n+++ b/python/sglang/srt/layers/logits_processor.py\n@@ -38,30 +38,50 @@ class LogitsProcessor(nn.Module):\n \n     def _get_top_logprobs(self, all_logprobs, input_metadata: InputMetadata):\n         if input_metadata.forward_mode == ForwardMode.DECODE:\n+            # Vectorize topk by computing once with the maximum k and slicing per row\n             decode_top_logprobs = []\n+            k_list = input_metadata.top_logprobs_nums\n+            k_max = max(k_list) if len(k_list) > 0 else 0\n+            if k_max > 0:\n+                t = all_logprobs.topk(k_max, dim=-1)\n+                values, indices = t.values, t.indices\n             for i in range(all_logprobs.shape[0]):\n-                k = input_metadata.top_logprobs_nums[i]\n-                t = all_logprobs[i].topk(k)\n-                v_cpu = t.values.cpu().tolist()\n-                p_cpu = t.indices.cpu().tolist()\n+                k = k_list[i]\n+                if k == 0:\n+                    decode_top_logprobs.append([])\n+                    continue\n+                v_cpu = values[i, :k].tolist()\n+                p_cpu = indices[i, :k].tolist()\n                 decode_top_logprobs.append(list(zip(v_cpu, p_cpu)))\n             return None, decode_top_logprobs\n         else:\n+            # Compute topk once for all rows with the maximum k to reduce kernel launches\n             prefill_top_logprobs, decode_top_logprobs = [], []\n+            k_list = input_metadata.top_logprobs_nums\n+            k_max = max(k_list) if len(k_list) > 0 else 0\n+            if k_max > 0:\n+                t_all = all_logprobs.topk(k_max, dim=-1)\n+                vs_all, ps_all = t_all.values, t_all.indices\n             pt = 0\n-            # NOTE: the GPU-CPU overhead can be reduced\n             extend_seq_lens_cpu = input_metadata.extend_seq_lens\n-            for i in range(len(input_metadata.extend_seq_lens)):\n-                if extend_seq_lens_cpu[i] == 0:\n+            for i in range(len(extend_seq_lens_cpu)):\n+                L = int(extend_seq_lens_cpu[i])\n+                if L == 0:\n                     continue\n-                k = input_metadata.top_logprobs_nums[i]\n-                t = all_logprobs[pt : pt + extend_seq_lens_cpu[i]].topk(k)\n-                vs_cpu = t.values.cpu().tolist()\n-                ps_cpu = t.indices.cpu().tolist()\n-                prefill_top_logprobs.append(\n-                    [list(zip(vs_cpu[j], ps_cpu[j])) for j in range(len(vs_cpu) - 1)]\n-                )\n-                decode_top_logprobs.append(list(zip(vs_cpu[-1], ps_cpu[-1])))\n+                k = k_list[i]\n+                if k == 0:\n+                    prefill_top_logprobs.append([[] for _ in range(L - 1)])\n+                    decode_top_logprobs.append([])\n+                else:\n+                    vs_seg = vs_all[pt : pt + L, :k]\n+                    ps_seg = ps_all[pt : pt + L, :k]\n+                    vs_cpu = vs_seg.tolist()\n+                    ps_cpu = ps_seg.tolist()\n+                    prefill_top_logprobs.append(\n+                        [list(zip(vs_cpu[j], ps_cpu[j])) for j in range(L - 1)]\n+                    )\n+                    decode_top_logprobs.append(list(zip(vs_cpu[-1], ps_cpu[-1])))\n+                pt += L\n             return prefill_top_logprobs, decode_top_logprobs\n \n     def forward(self, input_ids, hidden_states, weight, input_metadata: InputMetadata):\n@@ -98,7 +118,9 @@ class LogitsProcessor(nn.Module):\n                     all_logits = tensor_model_parallel_all_gather(all_logits)\n                 all_logits = all_logits[:, : self.config.vocab_size]\n \n-            all_logprobs = torch.log(torch.softmax(all_logits.float(), dim=-1) + 1e-6)\n+            all_logprobs = all_logits.float()\n+            all_logits = None\n+            all_logprobs[:] = torch.nn.functional.log_softmax(all_logprobs, dim=-1)\n \n             prefill_top_logprobs, decode_top_logprobs = self._get_top_logprobs(\n                 all_logprobs, input_metadata\n@@ -119,9 +141,10 @@ class LogitsProcessor(nn.Module):\n \n                 # Compute the logprobs and normalized logprobs for the prefill tokens.\n                 # Note that we pad a zero at the end of each sequence for easy computation.\n+                device = all_logprobs.device\n                 prefill_token_logprobs = all_logprobs[\n-                    torch.arange(all_logprobs.shape[0], device=\"cuda\"),\n-                    torch.cat([input_ids[1:], torch.tensor([0], device=\"cuda\")]),\n+                    torch.arange(all_logprobs.shape[0], device=device),\n+                    torch.cat([input_ids[1:], input_ids.new_zeros(1)]),\n                 ]\n \n                 normalized_prompt_logprobs = self._get_normalized_prompt_logprobs(\ndiff --git a/python/sglang/srt/managers/router/model_rpc.py b/python/sglang/srt/managers/router/model_rpc.py\nindex f283635c3..55bd9e80c 100644\n--- a/python/sglang/srt/managers/router/model_rpc.py\n+++ b/python/sglang/srt/managers/router/model_rpc.py\n@@ -589,7 +589,7 @@ class ModelRpcServer:\n                     + len(req.output_ids)\n                     - req.prompt_tokens,\n                     \"completion_tokens_wo_jump_forward\": req.completion_tokens_wo_jump_forward,\n-                    \"finish_reason\": req.finish_reason,\n+                    \"finish_reason\": str(req.finish_reason),\n                     \"hit_stop_str\": req.hit_stop_str,\n                 }\n                 if req.return_logprob:\n", "model_name_or_path": "gpt-5-2025-08-07"}
