{"instance_id": "/home/ubuntu/OmniPerf-Bench/sglan__repo-b146555", "model_patch": "diff --git a/docs/backend/native_api.ipynb b/docs/backend/native_api.ipynb\nindex 04b8ec0ed..d741e7466 100644\n--- a/docs/backend/native_api.ipynb\n+++ b/docs/backend/native_api.ipynb\n@@ -390,7 +390,7 @@\n    \"outputs\": [],\n    \"source\": [\n     \"expert_record_server_process, port = launch_server_cmd(\\n\",\n-    \"    \\\"python3 -m sglang.launch_server --model-path Qwen/Qwen1.5-MoE-A2.7B --host 0.0.0.0\\\"\\n\",\n+    \"    \\\"python3 -m sglang.launch_server --model-path Qwen/Qwen1.5-MoE-A2.7B --host 0.0.0.0 --expert-distribution-recorder-mode stat\\\"\\n\",\n     \")\\n\",\n     \"\\n\",\n     \"wait_for_server(f\\\"http://localhost:{port}\\\")\"\ndiff --git a/python/sglang/srt/layers/moe/ep_moe/token_dispatcher.py b/python/sglang/srt/layers/moe/ep_moe/token_dispatcher.py\nindex 4d165dbd2..e006254f4 100644\n--- a/python/sglang/srt/layers/moe/ep_moe/token_dispatcher.py\n+++ b/python/sglang/srt/layers/moe/ep_moe/token_dispatcher.py\n@@ -400,7 +400,7 @@ class _DeepEPDispatcherImplNormal(_DeepEPDispatcherImplBase):\n                     BLOCK_SIZE=512,\n                 )\n             else:\n-                output = torch.zeros(\n+                output = torch.empty(\n                     (0, hidden_states.shape[1]),\n                     device=hidden_states.device,\n                     dtype=hidden_states.dtype,\ndiff --git a/python/sglang/srt/layers/moe/fused_moe_triton/fused_moe.py b/python/sglang/srt/layers/moe/fused_moe_triton/fused_moe.py\nindex 4d949a843..7ca6e8143 100644\n--- a/python/sglang/srt/layers/moe/fused_moe_triton/fused_moe.py\n+++ b/python/sglang/srt/layers/moe/fused_moe_triton/fused_moe.py\n@@ -566,6 +566,7 @@ def moe_align_block_size_stage3(\n ):\n     last_cumsum = 0\n     off_cnt = num_experts * num_experts\n+    tl.store(cumsum_ptr + 0, 0)\n     for i in range(1, num_experts + 1):\n         token_cnt = tl.load(tokens_cnts_ptr + off_cnt + i - 1)\n         last_cumsum = last_cumsum + tl.cdiv(token_cnt, block_size) * block_size\n@@ -616,7 +617,7 @@ def moe_align_block_size_triton(\n     tokens_cnts = torch.zeros(\n         (num_experts + 1, num_experts), dtype=torch.int32, device=topk_ids.device\n     )\n-    cumsum = torch.zeros((num_experts + 1,), dtype=torch.int32, device=topk_ids.device)\n+    cumsum = torch.empty((num_experts + 1,), dtype=torch.int32, device=topk_ids.device)\n     tokens_per_thread = ceil_div(numel, num_experts)\n \n     moe_align_block_size_stage1[grid](\ndiff --git a/python/sglang/srt/utils.py b/python/sglang/srt/utils.py\nindex 0c16667ef..64a3f8a3e 100644\n--- a/python/sglang/srt/utils.py\n+++ b/python/sglang/srt/utils.py\n@@ -2100,7 +2100,7 @@ def is_fa3_default_architecture(hf_config):\n # Can be more general if it is used in multiple places (keep it simple and thus not general now)\n class BumpAllocator:\n     def __init__(self, buffer_size: int, dtype, device):\n-        self._buffer = torch.zeros((buffer_size,), dtype=dtype, device=device)\n+        self._buffer = torch.empty((buffer_size,), dtype=dtype, device=device)\n         self._pointer = 0\n \n     def allocate(self, size: int):\n", "model_name_or_path": "gpt-5-2025-08-07"}
