diff --git a/docs/backend/native_api.ipynb b/docs/backend/native_api.ipynb
index 04b8ec0ed..d741e7466 100644
--- a/docs/backend/native_api.ipynb
+++ b/docs/backend/native_api.ipynb
@@ -390,7 +390,7 @@
    "outputs": [],
    "source": [
     "expert_record_server_process, port = launch_server_cmd(\n",
-    "    \"python3 -m sglang.launch_server --model-path Qwen/Qwen1.5-MoE-A2.7B --host 0.0.0.0\"\n",
+    "    \"python3 -m sglang.launch_server --model-path Qwen/Qwen1.5-MoE-A2.7B --host 0.0.0.0 --expert-distribution-recorder-mode stat\"\n",
     ")\n",
     "\n",
     "wait_for_server(f\"http://localhost:{port}\")"
diff --git a/python/sglang/srt/layers/moe/ep_moe/token_dispatcher.py b/python/sglang/srt/layers/moe/ep_moe/token_dispatcher.py
index 4d165dbd2..e006254f4 100644
--- a/python/sglang/srt/layers/moe/ep_moe/token_dispatcher.py
+++ b/python/sglang/srt/layers/moe/ep_moe/token_dispatcher.py
@@ -400,7 +400,7 @@ class _DeepEPDispatcherImplNormal(_DeepEPDispatcherImplBase):
                     BLOCK_SIZE=512,
                 )
             else:
-                output = torch.zeros(
+                output = torch.empty(
                     (0, hidden_states.shape[1]),
                     device=hidden_states.device,
                     dtype=hidden_states.dtype,
diff --git a/python/sglang/srt/layers/moe/fused_moe_triton/fused_moe.py b/python/sglang/srt/layers/moe/fused_moe_triton/fused_moe.py
index 4d949a843..7ca6e8143 100644
--- a/python/sglang/srt/layers/moe/fused_moe_triton/fused_moe.py
+++ b/python/sglang/srt/layers/moe/fused_moe_triton/fused_moe.py
@@ -566,6 +566,7 @@ def moe_align_block_size_stage3(
 ):
     last_cumsum = 0
     off_cnt = num_experts * num_experts
+    tl.store(cumsum_ptr + 0, 0)
     for i in range(1, num_experts + 1):
         token_cnt = tl.load(tokens_cnts_ptr + off_cnt + i - 1)
         last_cumsum = last_cumsum + tl.cdiv(token_cnt, block_size) * block_size
@@ -616,7 +617,7 @@ def moe_align_block_size_triton(
     tokens_cnts = torch.zeros(
         (num_experts + 1, num_experts), dtype=torch.int32, device=topk_ids.device
     )
-    cumsum = torch.zeros((num_experts + 1,), dtype=torch.int32, device=topk_ids.device)
+    cumsum = torch.empty((num_experts + 1,), dtype=torch.int32, device=topk_ids.device)
     tokens_per_thread = ceil_div(numel, num_experts)
 
     moe_align_block_size_stage1[grid](
diff --git a/python/sglang/srt/utils.py b/python/sglang/srt/utils.py
index 0c16667ef..64a3f8a3e 100644
--- a/python/sglang/srt/utils.py
+++ b/python/sglang/srt/utils.py
@@ -2100,7 +2100,7 @@ def is_fa3_default_architecture(hf_config):
 # Can be more general if it is used in multiple places (keep it simple and thus not general now)
 class BumpAllocator:
     def __init__(self, buffer_size: int, dtype, device):
-        self._buffer = torch.zeros((buffer_size,), dtype=dtype, device=device)
+        self._buffer = torch.empty((buffer_size,), dtype=dtype, device=device)
         self._pointer = 0
 
     def allocate(self, size: int):
