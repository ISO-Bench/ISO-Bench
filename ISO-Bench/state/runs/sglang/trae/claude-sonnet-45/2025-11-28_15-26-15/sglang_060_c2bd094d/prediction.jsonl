{"instance_id": "/home/ubuntu/OmniPerf-Bench/sglan__repo-f8f9244", "model_patch": "diff --git a/python/sglang/srt/layers/moe/ep_moe/kernels.py b/python/sglang/srt/layers/moe/ep_moe/kernels.py\nindex 6d6c432f8..daa510018 100644\n--- a/python/sglang/srt/layers/moe/ep_moe/kernels.py\n+++ b/python/sglang/srt/layers/moe/ep_moe/kernels.py\n@@ -6,14 +6,10 @@ import triton\n import triton.language as tl\n \n from sglang.srt.distributed import get_tensor_model_parallel_rank\n-from sglang.srt.layers.quantization.fp8_kernel import per_token_group_quant_fp8\n from sglang.srt.utils import is_cuda\n \n _is_cuda = is_cuda()\n-if _is_cuda:\n-    from sglang.srt.layers.quantization.fp8_kernel import (\n-        sglang_per_token_group_quant_fp8,\n-    )\n+# Lazy import quantization kernels inside functions to avoid heavy package imports at module load time.\n logger = logging.getLogger(__name__)\n \n \n@@ -42,7 +38,7 @@ def deepep_compute_src2dst_triton_kernel(\n \n def deepep_run_moe_deep_preprocess(topk_ids: torch.Tensor, num_experts: int):\n     reorder_topk_ids, reorder_ids = torch.sort(topk_ids.view(-1), stable=True)\n-    seg_indptr = torch.zeros(num_experts + 1, device=topk_ids.device, dtype=torch.int64)\n+    seg_indptr = torch.empty(num_experts + 1, device=topk_ids.device, dtype=torch.int64)\n     src2dst = torch.empty(topk_ids.numel(), device=topk_ids.device, dtype=torch.int32)\n \n     # Find offet\n@@ -147,7 +143,7 @@ def compute_seg_indptr_triton_kernel(reorder_topk_ids, seg_indptr, num_toks):\n \n def run_moe_ep_preproess(topk_ids: torch.Tensor, num_experts: int):\n     reorder_topk_ids, reorder_ids = torch.sort(topk_ids.view(-1), stable=True)\n-    seg_indptr = torch.zeros(num_experts + 1, device=topk_ids.device, dtype=torch.int64)\n+    seg_indptr = torch.empty(num_experts + 1, device=topk_ids.device, dtype=torch.int64)\n     src2dst = torch.empty(topk_ids.numel(), device=topk_ids.device, dtype=torch.int32)\n \n     compute_seg_indptr_triton_kernel[(num_experts,)](\n@@ -515,9 +511,10 @@ def grouped_gemm_triton(\n         assert len(block_shape) == 2\n         block_n, block_k = block_shape[0], block_shape[1]\n         if _is_cuda:\n-            a, scale_a = sglang_per_token_group_quant_fp8(a, block_k)\n+            from sglang.srt.layers.quantization.fp8_kernel import sglang_per_token_group_quant_fp8 as _ptgq\n         else:\n-            a, scale_a = per_token_group_quant_fp8(a, block_k)\n+            from sglang.srt.layers.quantization.fp8_kernel import per_token_group_quant_fp8 as _ptgq\n+        a, scale_a = _ptgq(a, block_k)\n \n         assert triton.cdiv(a.shape[-1], block_k) == scale_a.shape[-1]\n         assert triton.cdiv(b.shape[-2], block_n) == scale_b.shape[-2]\n@@ -531,7 +528,8 @@ def grouped_gemm_triton(\n         \"BLOCK_SIZE_K\": 128,\n     }\n \n-    m_num_tiles_indptr = torch.zeros(batch_size + 1, device=a.device, dtype=torch.int64)\n+    m_num_tiles_indptr = torch.empty(batch_size + 1, device=a.device, dtype=torch.int64)\n+    m_num_tiles_indptr[0] = 0\n     compute_m_num_tiles_indptr[(1,)](\n         m_num_tiles_indptr, seg_indptr, batch_size, config[\"BLOCK_SIZE_M\"]\n     )\ndiff --git a/python/sglang/srt/layers/moe/ep_moe/layer.py b/python/sglang/srt/layers/moe/ep_moe/layer.py\nindex a9b443a75..404cb00c0 100644\n--- a/python/sglang/srt/layers/moe/ep_moe/layer.py\n+++ b/python/sglang/srt/layers/moe/ep_moe/layer.py\n@@ -851,14 +851,13 @@ class DeepEPMoE(EPMoE):\n             self.grouped_gemm_runner = GroupedGemmRunner(\n                 hidden_states.device, use_flashinfer=False  # TODO: use flashinfer\n             )\n-        seg_indptr_cur_rank = torch.cat(\n-            [\n-                torch.zeros(\n-                    1, device=tokens_per_expert.device, dtype=tokens_per_expert.dtype\n-                ),\n-                torch.cumsum(tokens_per_expert, dim=0),\n-            ]\n+        seg_indptr_cur_rank = torch.empty(\n+            tokens_per_expert.shape[0] + 1,\n+            device=tokens_per_expert.device,\n+            dtype=tokens_per_expert.dtype,\n         )\n+        seg_indptr_cur_rank[0] = 0\n+        seg_indptr_cur_rank[1:] = torch.cumsum(tokens_per_expert, dim=0)\n         reorder_topk_ids = torch.repeat_interleave(tokens_per_expert)\n         if self.activation_scheme == \"dynamic\" and not self.use_block_quant:\n             max_value = (\ndiff --git a/python/sglang/srt/layers/moe/ep_moe/token_dispatcher.py b/python/sglang/srt/layers/moe/ep_moe/token_dispatcher.py\nindex c91ccd633..c06bf431f 100644\n--- a/python/sglang/srt/layers/moe/ep_moe/token_dispatcher.py\n+++ b/python/sglang/srt/layers/moe/ep_moe/token_dispatcher.py\n@@ -472,7 +472,7 @@ class DeepEPDispatcher:\n         batch_size = indices.shape[0]\n         multihot_routing_map = torch.zeros(\n             (batch_size, self.num_local_experts),\n-            dtype=torch.long,\n+            dtype=torch.bool,\n             device=indices.device,\n         )\n \n", "model_name_or_path": "gpt-5-2025-08-07"}
