{"instance_id": "/home/ubuntu/OmniPerf-Bench/sglan__repo-f4a8987", "model_patch": "diff --git a/benchmark/kernels/fused_moe_triton/benchmark_ep_pre_reorder_triton.py b/benchmark/kernels/fused_moe_triton/benchmark_ep_pre_reorder_triton.py\nnew file mode 100644\nindex 000000000..a01e1413c\n--- /dev/null\n+++ b/benchmark/kernels/fused_moe_triton/benchmark_ep_pre_reorder_triton.py\n@@ -0,0 +1,81 @@\n+import argparse\n+import itertools\n+import time\n+\n+import torch\n+import triton\n+\n+from sglang.srt.layers.moe.ep_moe.kernels import pre_reorder_triton_kernel\n+\n+\n+def benchmark_pre_reorder(batch_size, topk, hidden_size, num_experts, block_size, iters=100):\n+    device = torch.device(\"cuda\")\n+\n+    input_ptr = torch.randn(batch_size, hidden_size, dtype=torch.float16, device=device)\n+    # Use empty to avoid redundant memory zeroing\n+    gateup_input_ptr = torch.empty(\n+        batch_size * topk, hidden_size, dtype=torch.float16, device=device\n+    )\n+    src2dst_ptr = torch.arange(batch_size * topk, dtype=torch.int32, device=device).view(\n+        batch_size, topk\n+    )\n+    topk_ids_ptr = torch.randint(0, num_experts, (batch_size, topk), dtype=torch.int32, device=device)\n+\n+    start_expert_id = 0\n+    end_expert_id = num_experts - 1\n+\n+    grid = (batch_size,)\n+\n+    # Warmup (JIT compile)\n+    pre_reorder_triton_kernel[grid](\n+        input_ptr,\n+        gateup_input_ptr,\n+        src2dst_ptr,\n+        topk_ids_ptr,\n+        None,\n+        start_expert_id,\n+        end_expert_id,\n+        topk,\n+        hidden_size,\n+        BLOCK_SIZE=block_size,\n+    )\n+    torch.cuda.synchronize()\n+\n+    start = time.time()\n+    for _ in range(iters):\n+        pre_reorder_triton_kernel[grid](\n+            input_ptr,\n+            gateup_input_ptr,\n+            src2dst_ptr,\n+            topk_ids_ptr,\n+            None,\n+            start_expert_id,\n+            end_expert_id,\n+            topk,\n+            hidden_size,\n+            BLOCK_SIZE=block_size,\n+        )\n+    torch.cuda.synchronize()\n+    return (time.time() - start) / iters\n+\n+\n+def main():\n+    parser = argparse.ArgumentParser()\n+    parser.add_argument(\"--batch\", type=int, default=4096)\n+    parser.add_argument(\"--topk\", type=int, default=2)\n+    parser.add_argument(\"--hidden\", type=int, default=4096)\n+    parser.add_argument(\"--experts\", type=int, default=64)\n+    parser.add_argument(\"--block\", type=int, default=128)\n+    parser.add_argument(\"--iters\", type=int, default=50)\n+    args = parser.parse_args()\n+\n+    ms = benchmark_pre_reorder(\n+        args.batch, args.topk, args.hidden, args.experts, args.block, args.iters\n+    )\n+    print(\n+        f\"avg per-iter latency: {ms*1000:.3f} ms | batch={args.batch} hidden={args.hidden} topk={args.topk} experts={args.experts} block={args.block}\"\n+    )\n+\n+\n+if __name__ == \"__main__\":\n+    main()\ndiff --git a/python/sglang/srt/layers/moe/ep_moe/kernels.py b/python/sglang/srt/layers/moe/ep_moe/kernels.py\nindex 8c005527a..1f70b3ada 100644\n--- a/python/sglang/srt/layers/moe/ep_moe/kernels.py\n+++ b/python/sglang/srt/layers/moe/ep_moe/kernels.py\n@@ -151,12 +151,14 @@ def compute_seg_indptr_triton_kernel(reorder_topk_ids, seg_indptr, num_toks):\n \n def run_moe_ep_preproess(topk_ids: torch.Tensor, num_experts: int):\n     reorder_topk_ids, reorder_ids = torch.sort(topk_ids.view(-1), stable=True)\n-    seg_indptr = torch.zeros(num_experts + 1, device=topk_ids.device, dtype=torch.int64)\n+    seg_indptr = torch.empty(num_experts + 1, device=topk_ids.device, dtype=torch.int64)\n     src2dst = torch.empty(topk_ids.numel(), device=topk_ids.device, dtype=torch.int32)\n \n-    compute_seg_indptr_triton_kernel[(num_experts,)](\n-        reorder_topk_ids, seg_indptr, topk_ids.numel()\n+    # Compute segment indptr via searchsorted to avoid a per-expert kernel\n+    expert_ids = torch.arange(\n+        num_experts + 1, device=topk_ids.device, dtype=reorder_topk_ids.dtype\n     )\n+    torch.searchsorted(reorder_topk_ids, expert_ids, right=True, out=seg_indptr)\n \n     BLOCK_SIZE = 512\n     grid = (triton.cdiv(topk_ids.numel(), BLOCK_SIZE),)\n@@ -679,7 +681,8 @@ def grouped_gemm_triton(\n         \"BLOCK_SIZE_K\": 128,\n     }\n \n-    m_num_tiles_indptr = torch.zeros(batch_size + 1, device=a.device, dtype=torch.int64)\n+    m_num_tiles_indptr = torch.empty(batch_size + 1, device=a.device, dtype=torch.int64)\n+    m_num_tiles_indptr[0] = 0  # initialize prefix sum base; kernel populates the rest\n     compute_m_num_tiles_indptr[(1,)](\n         m_num_tiles_indptr, seg_indptr, batch_size, config[\"BLOCK_SIZE_M\"]\n     )\n", "model_name_or_path": "gpt-5-2025-08-07"}
