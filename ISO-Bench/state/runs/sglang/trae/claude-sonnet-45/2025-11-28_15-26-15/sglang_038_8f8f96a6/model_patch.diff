diff --git a/python/sglang/srt/hf_transformers_utils.py b/python/sglang/srt/hf_transformers_utils.py
index 56d7c8a1f..e989c1466 100644
--- a/python/sglang/srt/hf_transformers_utils.py
+++ b/python/sglang/srt/hf_transformers_utils.py
@@ -164,7 +164,7 @@ def get_tokenizer(
             "slowdown. Consider using a fast tokenizer instead."
         )
 
-    handle_additional_stop_token_ids(tokenizer)
+    attach_additional_stop_token_ids(tokenizer)
     return tokenizer
 
 
@@ -184,11 +184,14 @@ def get_processor(
         **kwargs,
     )
 
-    handle_additional_stop_token_ids(processor.tokenizer)
+    attach_additional_stop_token_ids(processor.tokenizer)
     return processor
 
 
-def handle_additional_stop_token_ids(tokenizer):
+def attach_additional_stop_token_ids(tokenizer):
+    # Avoid recomputing if already attached
+    if hasattr(tokenizer, "additional_stop_token_ids"):
+        return
     # Special handling for stop token <|eom_id|> generated by llama 3 tool use.
     if "<|eom_id|>" in tokenizer.get_added_vocab():
         tokenizer.additional_stop_token_ids = set(
diff --git a/python/sglang/srt/layers/sampler.py b/python/sglang/srt/layers/sampler.py
index 9ae5801cc..a3f1ca66d 100644
--- a/python/sglang/srt/layers/sampler.py
+++ b/python/sglang/srt/layers/sampler.py
@@ -80,7 +80,7 @@ class Sampler(nn.Module):
 
                 if not torch.all(success):
                     logger.warning("Detected errors during sampling!")
-                    batch_next_token_ids = torch.zeros_like(batch_next_token_ids)
+                    batch_next_token_ids.zero_()
             elif global_server_args_dict["sampling_backend"] == "pytorch":
                 # A slower fallback implementation with torch native operations.
                 batch_next_token_ids = top_k_top_p_min_p_sampling_from_probs_torch(
diff --git a/python/sglang/srt/managers/schedule_batch.py b/python/sglang/srt/managers/schedule_batch.py
index fac008d3f..e11fa03df 100644
--- a/python/sglang/srt/managers/schedule_batch.py
+++ b/python/sglang/srt/managers/schedule_batch.py
@@ -339,6 +339,12 @@ class Req:
         if self.tokenizer is not None:
             matched_eos |= last_token_id == self.tokenizer.eos_token_id
 
+
+            # Also respect additional stop token ids if attached to the tokenizer
+            extra_ids = getattr(self.tokenizer, "additional_stop_token_ids", None)
+            if extra_ids:
+                matched_eos |= last_token_id in extra_ids
+
         if matched_eos and not self.sampling_params.ignore_eos:
             self.finished_reason = FINISH_MATCHED_TOKEN(matched=last_token_id)
             return
diff --git a/python/sglang/srt/sampling/penaltylib/penalizers/min_new_tokens.py b/python/sglang/srt/sampling/penaltylib/penalizers/min_new_tokens.py
index c9e0f078e..a11a4f0b0 100644
--- a/python/sglang/srt/sampling/penaltylib/penalizers/min_new_tokens.py
+++ b/python/sglang/srt/sampling/penaltylib/penalizers/min_new_tokens.py
@@ -34,6 +34,7 @@ class BatchedMinNewTokensPenalizer(_BatchedPenalizer):
                     data=list(
                         req.sampling_params.stop_token_ids
                         | {req.tokenizer.eos_token_id}
+                        | (getattr(req.tokenizer, "additional_stop_token_ids", None) or set())
                     ),
                     dtype=torch.int64,
                     device=self.orchestrator.device,
diff --git a/python/sglang/srt/sampling/sampling_params.py b/python/sglang/srt/sampling/sampling_params.py
index b0863b557..3edce468c 100644
--- a/python/sglang/srt/sampling/sampling_params.py
+++ b/python/sglang/srt/sampling/sampling_params.py
@@ -135,8 +135,11 @@ class SamplingParams:
             self.stop_str_max_len = stop_str_max_len
 
         # Process stop token ids
-        if tokenizer and tokenizer.additional_stop_token_ids:
-            self.stop_token_ids.update(tokenizer.additional_stop_token_ids)
+        extra_ids = getattr(tokenizer, "additional_stop_token_ids", None)
+        if extra_ids:
+            # Additional stop token ids are checked dynamically to avoid mutating state here.
+            # Keep self.stop_token_ids untouched for user-specified stops.
+            pass
 
     def to_srt_kwargs(self):
         return {
