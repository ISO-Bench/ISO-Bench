{"instance_id": "/home/ubuntu/OmniPerf-Bench/sglan__repo-f081541", "model_patch": "diff --git a/python/sglang/srt/models/deepseek_v2.py b/python/sglang/srt/models/deepseek_v2.py\nindex 26c5e617a..db15d71eb 100644\n--- a/python/sglang/srt/models/deepseek_v2.py\n+++ b/python/sglang/srt/models/deepseek_v2.py\n@@ -1607,31 +1607,31 @@ class DeepseekV2ForCausalLM(nn.Module):\n                     \"up_proj.weight\",\n                     \"up_proj.weight_scale_inv\",\n                 ]\n-            names_to_remove = []\n+            names_to_remove = set()\n             for moe_layer in tqdm(\n                 range(\n                     self.config.first_k_dense_replace,\n                     self.config.num_hidden_layers,\n                     self.config.moe_layer_freq,\n                 ),\n-                desc=f\"Cloning {self.n_share_experts_fusion} \"\n-                \"replicas of the shared expert into MoE\",\n+                desc=f\"Cloning {self.n_share_experts_fusion} replicas of the shared expert into MoE\",\n+                disable=True,\n             ):\n+                base_layer = f\"model.layers.{moe_layer}.mlp\"\n+                shared_prefix = f\"{base_layer}.shared_experts.\"\n+                experts_prefix = f\"{base_layer}.experts.\"\n                 for num_repeat in range(self.n_share_experts_fusion):\n+                    expert_id = self.config.n_routed_experts + num_repeat\n+                    expert_prefix = f\"{experts_prefix}{expert_id}.\"\n                     for suffix in suffix_list:\n-                        shared_expert_weight_name = (\n-                            f\"model.layers.{moe_layer}.mlp.shared_experts.{suffix}\"\n-                        )\n+                        shared_expert_weight_name = f\"{shared_prefix}{suffix}\"\n                         weights_list.append(\n                             (\n-                                f\"model.layers.{moe_layer}.\"\n-                                f\"mlp.experts.\"\n-                                f\"{self.config.n_routed_experts + num_repeat}\"\n-                                f\".{suffix}\",\n-                                weights_dict[shared_expert_weight_name].clone(),\n+                                f\"{expert_prefix}{suffix}\",\n+                                weights_dict[shared_expert_weight_name],\n                             )\n                         )\n-                        names_to_remove += [shared_expert_weight_name]\n+                        names_to_remove.add(shared_expert_weight_name)\n             weights = [w for w in weights_list if w[0] not in names_to_remove]\n \n         # Params for weights, fp8 weight scales, fp8 activation scales\n", "model_name_or_path": "gpt-5-2025-08-07"}
