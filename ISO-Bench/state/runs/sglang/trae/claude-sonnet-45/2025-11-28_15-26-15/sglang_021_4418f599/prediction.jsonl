{"instance_id": "/home/ubuntu/OmniPerf-Bench/sglan__repo-04f2abc", "model_patch": "diff --git a/python/sglang/srt/models/deepseek_v2.py b/python/sglang/srt/models/deepseek_v2.py\nindex b299c3037..99fc479c0 100644\n--- a/python/sglang/srt/models/deepseek_v2.py\n+++ b/python/sglang/srt/models/deepseek_v2.py\n@@ -569,14 +569,23 @@ class DeepseekV2AttentionMLA(nn.Module):\n     def dispatch_attn_forward_method(\n         self, forward_batch: ForwardBatch\n     ) -> AttnForwardMethod:\n+        # Precompute frequently used checks to reduce Python-side overhead in hot path.\n+        mode = forward_batch.forward_mode\n+        is_extend = mode.is_extend()\n+        is_target_verify = mode.is_target_verify()\n+        is_draft_extend = mode.is_draft_extend()\n+        if forward_batch.extend_prefix_lens_cpu is not None:\n+            sum_extend_prefix_lens = sum(forward_batch.extend_prefix_lens_cpu)\n+        else:\n+            sum_extend_prefix_lens = 0\n         if self.attention_backend == \"flashinfer\":\n             # Flashinfer MLA: Do not absorb when enabling ragged prefill\n             if (\n                 not self.flashinfer_mla_disable_ragged\n-                and forward_batch.forward_mode.is_extend()\n-                and not forward_batch.forward_mode.is_target_verify()\n-                and not forward_batch.forward_mode.is_draft_extend()\n-                and sum(forward_batch.extend_prefix_lens_cpu) == 0\n+                and is_extend\n+                and not is_target_verify\n+                and not is_draft_extend\n+                and sum_extend_prefix_lens == 0\n             ):\n                 return AttnForwardMethod.MHA\n             else:\n@@ -584,12 +593,14 @@ class DeepseekV2AttentionMLA(nn.Module):\n         elif self.attention_backend == \"fa3\":\n             # Flash Attention: Use MHA with chunked KV cache when prefilling on long sequences.\n             if (\n-                forward_batch.forward_mode.is_extend()\n+                is_extend\n                 and not self.disable_chunked_prefix_cache\n-                and not forward_batch.forward_mode.is_target_verify()\n-                and not forward_batch.forward_mode.is_draft_extend()\n-                and sum(forward_batch.extend_prefix_lens_cpu)\n-                >= self.chunked_prefix_cache_threshold\n+                and not is_target_verify\n+                and not is_draft_extend\n+                and (\n+                    sum_extend_prefix_lens >= self.chunked_prefix_cache_threshold\n+                    or sum_extend_prefix_lens == 0\n+                )\n             ):\n                 return AttnForwardMethod.MHA_CHUNKED_KV\n             else:\n@@ -597,10 +608,10 @@ class DeepseekV2AttentionMLA(nn.Module):\n         else:\n             # Triton: Use normal computation for prefill and use weight absorption for extend/decode\n             if (\n-                forward_batch.forward_mode.is_extend()\n-                and not forward_batch.forward_mode.is_target_verify()\n-                and not forward_batch.forward_mode.is_draft_extend()\n-                and sum(forward_batch.extend_prefix_lens_cpu) == 0\n+                is_extend\n+                and not is_target_verify\n+                and not is_draft_extend\n+                and sum_extend_prefix_lens == 0\n             ):\n                 return AttnForwardMethod.MHA\n             else:\n", "model_name_or_path": "gpt-5-2025-08-07"}
