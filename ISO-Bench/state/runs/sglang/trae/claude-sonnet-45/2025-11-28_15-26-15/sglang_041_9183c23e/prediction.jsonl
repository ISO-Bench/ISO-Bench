{"instance_id": "/home/ubuntu/OmniPerf-Bench/sglan__repo-148254d", "model_patch": "diff --git a/python/sglang/srt/managers/io_struct.py b/python/sglang/srt/managers/io_struct.py\nindex 13eb233bd..1aae28b00 100644\n--- a/python/sglang/srt/managers/io_struct.py\n+++ b/python/sglang/srt/managers/io_struct.py\n@@ -426,8 +426,7 @@ class UpdateWeightsFromDistributedReqOutput:\n \n @dataclass\n class UpdateWeightsFromTensorReqInput:\n-    name: str\n-    tensor: torch.Tensor\n+    serialized_named_tensors: bytes  # indeed Dict[str, torch.Tensor]\n \n \n @dataclass\ndiff --git a/python/sglang/srt/managers/tp_worker.py b/python/sglang/srt/managers/tp_worker.py\nindex c8e14a746..020abea9b 100644\n--- a/python/sglang/srt/managers/tp_worker.py\n+++ b/python/sglang/srt/managers/tp_worker.py\n@@ -30,7 +30,7 @@ from sglang.srt.managers.schedule_batch import ModelWorkerBatch, global_server_a\n from sglang.srt.model_executor.forward_batch_info import ForwardBatch\n from sglang.srt.model_executor.model_runner import ModelRunner\n from sglang.srt.server_args import ServerArgs\n-from sglang.srt.utils import broadcast_pyobj, set_random_seed\n+from sglang.srt.utils import MultiprocessingSerializer, broadcast_pyobj, set_random_seed\n \n logger = logging.getLogger(__name__)\n \n@@ -197,7 +197,7 @@ class TpModelWorker:\n \n     def update_weights_from_tensor(self, recv_req: UpdateWeightsFromTensorReqInput):\n         success, message = self.model_runner.update_weights_from_tensor(\n-            recv_req.name, recv_req.tensor\n+            recv_req.serialized_named_tensors\n         )\n         return success, message\n \ndiff --git a/python/sglang/srt/model_executor/model_runner.py b/python/sglang/srt/model_executor/model_runner.py\nindex 786f654de..a51d6d845 100644\n--- a/python/sglang/srt/model_executor/model_runner.py\n+++ b/python/sglang/srt/model_executor/model_runner.py\n@@ -428,8 +428,11 @@ class ModelRunner:\n             logger.error(error_msg)\n             return False, error_msg\n \n-    def update_weights_from_tensor(self, name, tensor: torch.Tensor):\n-        self.model.load_weights([(name, tensor)])\n+    def update_weights_from_tensor(self, serialized_named_tensors: bytes):\n+        from sglang.srt.utils import MultiprocessingSerializer\n+        named_tensors = MultiprocessingSerializer.loads(serialized_named_tensors)\n+        # named_tensors: Dict[str, torch.Tensor]\n+        self.model.load_weights(list(named_tensors.items()))\n         return True, \"Success\"  # TODO error handling\n \n     def get_weights_by_name(\ndiff --git a/python/sglang/srt/server.py b/python/sglang/srt/server.py\nindex d95ce5931..491f03d78 100644\n--- a/python/sglang/srt/server.py\n+++ b/python/sglang/srt/server.py\n@@ -89,6 +89,7 @@ from sglang.srt.utils import (\n     prepare_model_and_tokenizer,\n     set_prometheus_multiproc_dir,\n     set_ulimit,\n+    MultiprocessingSerializer,\n )\n from sglang.utils import get_exception_traceback\n from sglang.version import __version__\n@@ -875,8 +876,15 @@ class Engine:\n         )\n \n     def update_weights_from_tensor(self, name, tensor):\n-        \"\"\"Update weights from distributed source.\"\"\"\n-        obj = UpdateWeightsFromTensorReqInput(name=name, tensor=tensor)\n+        \"\"\"Update weights from tensor(s).\n+\n+        This method preserves the public API by accepting (name, tensor),\n+        but internally serializes a dict to bytes for efficient IPC.\n+        \"\"\"\n+        payload = {name: tensor}\n+        obj = UpdateWeightsFromTensorReqInput(\n+            serialized_named_tensors=MultiprocessingSerializer.dumps(payload)\n+        )\n         loop = asyncio.get_event_loop()\n         return loop.run_until_complete(\n             tokenizer_manager.update_weights_from_tensor(obj, None)\ndiff --git a/python/sglang/srt/utils.py b/python/sglang/srt/utils.py\nindex 7c3efa9a2..e4d0e8e85 100644\n--- a/python/sglang/srt/utils.py\n+++ b/python/sglang/srt/utils.py\n@@ -742,6 +742,27 @@ def broadcast_pyobj(\n         return data\n \n \n+\n+class MultiprocessingSerializer:\n+    \"\"\"Efficient serializer for transferring named tensors between processes.\n+\n+    Uses torch.save/torch.load to handle tensors efficiently and reduce Python\n+    pickle overhead compared to generic pickle.dumps for large tensors.\n+    \"\"\"\n+\n+    @staticmethod\n+    def dumps(named_tensors: Dict[str, torch.Tensor]) -> bytes:\n+        buf = BytesIO()\n+        # torch.save is generally faster and more tensor-aware than pickle.dumps\n+        torch.save(named_tensors, buf)\n+        return buf.getvalue()\n+\n+    @staticmethod\n+    def loads(buf: bytes) -> Dict[str, torch.Tensor]:\n+        bio = BytesIO(buf)\n+        # Load to CPU by default; the consumer can move to appropriate device\n+        return torch.load(bio, map_location=\"cpu\")\n+\n step_counter = 0\n \n \n", "model_name_or_path": "gpt-5-2025-08-07"}
