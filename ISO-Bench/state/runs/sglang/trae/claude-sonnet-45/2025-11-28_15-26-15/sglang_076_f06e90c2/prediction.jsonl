{"instance_id": "/home/ubuntu/OmniPerf-Bench/sglan__repo-2cea614", "model_patch": "diff --git a/examples/usage/json_logprobs.py b/examples/usage/json_logprobs.py\nnew file mode 100644\nindex 000000000..4400f3fe5\n--- /dev/null\n+++ b/examples/usage/json_logprobs.py\n@@ -0,0 +1,47 @@\n+# Example usage for requesting JSON logprobs via HTTP-compatible interface\n+# Note: This is an example; adjust base_url/model to your deployment.\n+import json\n+from concurrent.futures import ThreadPoolExecutor\n+\n+from examples.usage.json_decode import character_regex\n+from sglang.utils import http_request\n+\n+character_names = [\"Hermione Granger\", \"Ron Weasley\", \"Harry Potter\"]\n+base_url = \"http://localhost:30000\"\n+prompt = \"is a character in Harry Potter. Please fill in the following information about this character.\\n\"\n+\n+\n+def openai_api_request(name):\n+    data = {\n+        \"model\": \"\",\n+        \"prompt\": name + prompt,\n+        \"temperature\": 0,\n+        \"max_tokens\": 128,\n+        \"regex\": character_regex,\n+        \"logprobs\": 3,\n+    }\n+    r = http_request(\n+        base_url + \"/v1/completions\",\n+        json.dumps(data).encode(\"utf-8\"),\n+        headers={\"Content-Type\": \"application/json\"},\n+    )\n+    return json.loads(r.decode(\"utf-8\"))\n+\n+\n+def parse_result(result):\n+    # Extract the JSON object from the response\n+    text = result[\"choices\"][0][\"text\"].strip()\n+    try:\n+        obj = json.loads(text)\n+    except Exception:\n+        obj = {\"raw\": text}\n+    return obj\n+\n+\n+if __name__ == \"__main__\":\n+    with ThreadPoolExecutor(max_workers=3) as ex:\n+        futures = [ex.submit(openai_api_request, n) for n in character_names]\n+    results = [f.result() for f in futures]\n+    # Print parsed results\n+    for r in results:\n+        print(parse_result(r))\ndiff --git a/python/sglang/srt/layers/logits_processor.py b/python/sglang/srt/layers/logits_processor.py\nindex 53d6620e9..3106d7cec 100644\n--- a/python/sglang/srt/layers/logits_processor.py\n+++ b/python/sglang/srt/layers/logits_processor.py\n@@ -38,13 +38,23 @@ class LogitsProcessor(nn.Module):\n \n     def _get_top_logprobs(self, all_logprobs, input_metadata: InputMetadata):\n         if input_metadata.forward_mode == ForwardMode.DECODE:\n-            decode_top_logprobs = []\n-            for i in range(all_logprobs.shape[0]):\n-                k = input_metadata.top_logprobs_nums[i]\n-                t = all_logprobs[i].topk(k)\n-                v_cpu = t.values.tolist()\n-                p_cpu = t.indices.tolist()\n-                decode_top_logprobs.append(list(zip(v_cpu, p_cpu)))\n+            decode_top_logprobs = [[] for _ in range(all_logprobs.shape[0])]\n+            # Group by identical k to reduce number of topk kernel launches\n+            topk_nums = input_metadata.top_logprobs_nums\n+            ks_cpu = topk_nums.tolist() if hasattr(topk_nums, \"tolist\") else list(topk_nums)\n+            from collections import defaultdict\n+            groups = defaultdict(list)\n+            for i, k in enumerate(ks_cpu):\n+                if k > 0:\n+                    groups[int(k)].append(i)\n+                else:\n+                    decode_top_logprobs[i] = []\n+            for k, idxs in groups.items():\n+                t = all_logprobs[idxs].topk(k)\n+                vs_cpu = t.values.tolist()\n+                ps_cpu = t.indices.tolist()\n+                for j, i in enumerate(idxs):\n+                    decode_top_logprobs[i] = list(zip(vs_cpu[j], ps_cpu[j]))\n             return None, decode_top_logprobs\n         else:\n             prefill_top_logprobs, decode_top_logprobs = [], []\n@@ -82,7 +92,7 @@ class LogitsProcessor(nn.Module):\n         else:\n             last_hidden = hidden_states[last_index]\n \n-        last_logits = torch.matmul(last_hidden, weight.T)\n+        last_logits = torch.nn.functional.linear(last_hidden, weight)\n         if self.tp_size > 1:\n             last_logits = tensor_model_parallel_all_gather(last_logits)\n         last_logits = last_logits[:, : self.config.vocab_size]\n@@ -96,14 +106,13 @@ class LogitsProcessor(nn.Module):\n             if input_metadata.forward_mode == ForwardMode.DECODE:\n                 all_logits = last_logits\n             else:\n-                all_logits = torch.matmul(hidden_states, weight.T)\n+                all_logits = torch.nn.functional.linear(hidden_states, weight)\n                 if self.tp_size > 1:\n                     all_logits = tensor_model_parallel_all_gather(all_logits)\n                 all_logits = all_logits[:, : self.config.vocab_size]\n \n-            all_logprobs = all_logits.float()\n+            all_logprobs = torch.nn.functional.log_softmax(all_logits, dim=-1, dtype=torch.float32)\n             del all_logits\n-            all_logprobs[:] = torch.nn.functional.log_softmax(all_logprobs, dim=-1)\n \n             return_top_logprob = any(x > 0 for x in input_metadata.top_logprobs_nums)\n             if return_top_logprob:\ndiff --git a/python/sglang/srt/managers/router/infer_batch.py b/python/sglang/srt/managers/router/infer_batch.py\nindex dbe94371b..e7bf9ec81 100644\n--- a/python/sglang/srt/managers/router/infer_batch.py\n+++ b/python/sglang/srt/managers/router/infer_batch.py\n@@ -274,13 +274,14 @@ class Batch:\n \n         # Handle logit bias but only allocate when needed\n         logit_bias = None\n-        for i in range(bs):\n-            if reqs[i].sampling_params.dtype == \"int\":\n-                if logit_bias is None:\n-                    logit_bias = torch.zeros(\n-                        (bs, vocab_size), dtype=torch.float32, device=device\n-                    )\n-                logit_bias[i] = int_token_logit_bias\n+        int_indices = [i for i in range(bs) if reqs[i].sampling_params.dtype == \"int\"]\n+        if int_indices:\n+            logit_bias = torch.zeros((bs, vocab_size), dtype=torch.float32, device=device)\n+            ib = int_token_logit_bias\n+            if getattr(ib, \"device\", None) is None or str(ib.device) != device:\n+                ib = ib.to(device)\n+            # Vectorized row assignment for all integer dtype requests\n+            logit_bias[int_indices] = ib\n \n         # Set fields\n         self.input_ids = torch.tensor(\ndiff --git a/python/sglang/srt/managers/router/model_rpc.py b/python/sglang/srt/managers/router/model_rpc.py\nindex 6abb20b25..ee9394111 100644\n--- a/python/sglang/srt/managers/router/model_rpc.py\n+++ b/python/sglang/srt/managers/router/model_rpc.py\n@@ -101,7 +101,8 @@ class ModelRpcServer:\n             ),\n         )\n         self.int_token_logit_bias = torch.tensor(\n-            get_int_token_logit_bias(self.tokenizer, self.model_config.vocab_size)\n+            get_int_token_logit_bias(self.tokenizer, self.model_config.vocab_size),\n+            device=\"cuda\",\n         )\n         set_random_seed(server_args.random_seed)\n \n", "model_name_or_path": "gpt-5-2025-08-07"}
