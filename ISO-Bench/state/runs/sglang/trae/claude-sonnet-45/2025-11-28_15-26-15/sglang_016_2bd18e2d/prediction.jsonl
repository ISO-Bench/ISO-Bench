{"instance_id": "/home/ubuntu/OmniPerf-Bench/sglan__repo-83452db", "model_patch": "diff --git a/model_patch.diff b/model_patch.diff\nnew file mode 100644\nindex 000000000..e69de29bb\ndiff --git a/python/sglang/srt/managers/schedule_batch.py b/python/sglang/srt/managers/schedule_batch.py\nindex faf05a7ff..44cb873ab 100644\n--- a/python/sglang/srt/managers/schedule_batch.py\n+++ b/python/sglang/srt/managers/schedule_batch.py\n@@ -668,8 +668,8 @@ class ScheduleBatch:\n                     or len(req.prefix_indices) >= im.num_image_tokens\n                 )\n \n-        self.encoder_lens = torch.tensor(self.encoder_lens_cpu, dtype=torch.int32).to(\n-            self.device, non_blocking=True\n+        self.encoder_lens = torch.tensor(\n+            self.encoder_lens_cpu, dtype=torch.int64, device=self.device\n         )\n \n         # Strip encoder infos\n@@ -699,23 +699,19 @@ class ScheduleBatch:\n             pt += req.extend_input_len\n \n         # Reassign\n-        self.input_ids = torch.tensor(sum(input_ids, []), dtype=torch.int32).to(\n-            self.device, non_blocking=True\n-        )\n-        self.seq_lens = torch.tensor(seq_lens, dtype=torch.int32).to(\n-            self.device, non_blocking=True\n+        self.input_ids = torch.tensor(\n+            sum(input_ids, []), dtype=torch.int32, device=self.device\n         )\n+        self.seq_lens = torch.tensor(seq_lens, dtype=torch.int64, device=self.device)\n \n         if not decoder_out_cache_loc:\n-            self.out_cache_loc = torch.zeros(0, dtype=torch.int32).to(\n-                self.device, non_blocking=True\n-            )\n+            self.out_cache_loc = torch.empty(0, dtype=torch.int32, device=self.device)\n         else:\n             self.out_cache_loc = torch.cat(decoder_out_cache_loc)\n \n         if not encoder_out_cache_loc:\n-            self.encoder_out_cache_loc = torch.zeros(0, dtype=torch.int32).to(\n-                self.device, non_blocking=True\n+            self.encoder_out_cache_loc = torch.empty(\n+                0, dtype=torch.int32, device=self.device\n             )\n         else:\n             self.encoder_out_cache_loc = torch.cat(encoder_out_cache_loc)\n@@ -775,19 +771,15 @@ class ScheduleBatch:\n             pre_lens.append(pre_len)\n \n         # Set fields\n-        self.input_ids = torch.tensor(sum(input_ids, []), dtype=torch.int32).to(\n-            self.device, non_blocking=True\n-        )\n-        self.req_pool_indices = torch.tensor(req_pool_indices, dtype=torch.int32).to(\n-            self.device, non_blocking=True\n+        self.input_ids = torch.tensor(\n+            sum(input_ids, []), dtype=torch.int32, device=self.device\n         )\n-        self.seq_lens = torch.tensor(seq_lens, dtype=torch.int32).to(\n-            self.device, non_blocking=True\n+        self.req_pool_indices = torch.tensor(\n+            req_pool_indices, dtype=torch.int64, device=self.device\n         )\n+        self.seq_lens = torch.tensor(seq_lens, dtype=torch.int64, device=self.device)\n         self.input_embeds = (\n-            torch.tensor(input_embeds).to(self.device, non_blocking=True)\n-            if input_embeds\n-            else None\n+            torch.tensor(input_embeds, device=self.device) if input_embeds else None\n         )\n \n         self.out_cache_loc = out_cache_loc\n@@ -801,11 +793,9 @@ class ScheduleBatch:\n         self.extend_logprob_start_lens = [r.extend_logprob_start_len for r in reqs]\n \n         # Write to req_to_token_pool\n-        pre_lens = torch.tensor(pre_lens, dtype=torch.int32).to(\n-            self.device, non_blocking=True\n-        )\n-        extend_lens = torch.tensor(self.extend_lens, dtype=torch.int32).to(\n-            self.device, non_blocking=True\n+        pre_lens = torch.tensor(pre_lens, dtype=torch.int32, device=self.device)\n+        extend_lens = torch.tensor(\n+            self.extend_lens, dtype=torch.int32, device=self.device\n         )\n         if global_server_args_dict[\"attention_backend\"] != \"torch_native\":\n             write_req_to_token_pool_triton[(bs,)](\n@@ -1014,9 +1004,9 @@ class ScheduleBatch:\n     def prepare_for_idle(self):\n         self.forward_mode = ForwardMode.IDLE\n         self.input_ids = torch.empty(0, dtype=torch.int32, device=self.device)\n-        self.seq_lens = torch.empty(0, dtype=torch.int32, device=self.device)\n+        self.seq_lens = torch.empty(0, dtype=torch.int64, device=self.device)\n         self.out_cache_loc = torch.empty(0, dtype=torch.int32, device=self.device)\n-        self.req_pool_indices = torch.empty(0, dtype=torch.int32, device=self.device)\n+        self.req_pool_indices = torch.empty(0, dtype=torch.int64, device=self.device)\n         self.seq_lens_sum = 0\n         self.extend_num_tokens = 0\n         self.sampling_info = SamplingBatchInfo.from_schedule_batch(\n@@ -1084,9 +1074,7 @@ class ScheduleBatch:\n             self.encoder_lens_cpu = [self.encoder_lens_cpu[i] for i in keep_indices]\n \n         self.reqs = [self.reqs[i] for i in keep_indices]\n-        new_indices = torch.tensor(keep_indices, dtype=torch.int32).to(\n-            self.device, non_blocking=True\n-        )\n+        new_indices = torch.tensor(keep_indices, dtype=torch.int64, device=self.device)\n         self.req_pool_indices = self.req_pool_indices[new_indices]\n         self.seq_lens = self.seq_lens[new_indices]\n         self.out_cache_loc = None\n", "model_name_or_path": "gpt-5-2025-08-07"}
