{"instance_id": "/home/ubuntu/OmniPerf-Bench/sglan__repo-73fa2d4", "model_patch": "diff --git a/model_patch.diff b/model_patch.diff\nnew file mode 100644\nindex 000000000..e69de29bb\ndiff --git a/python/sglang/srt/managers/policy_scheduler.py b/python/sglang/srt/managers/policy_scheduler.py\nindex 30a009c2e..5b00db1ac 100644\n--- a/python/sglang/srt/managers/policy_scheduler.py\n+++ b/python/sglang/srt/managers/policy_scheduler.py\n@@ -18,27 +18,18 @@ limitations under the License.\n import random\n from collections import defaultdict\n from contextlib import contextmanager\n+from typing import List\n \n from sglang.srt.managers.schedule_batch import Req, ScheduleBatch\n \n \n class PolicyScheduler:\n-    def __init__(\n-        self,\n-        policy,\n-        max_running_seqs,\n-        max_prefill_num_tokens,\n-        max_total_num_tokens,\n-        tree_cache,\n-    ):\n-        if tree_cache.disable and policy == \"lpm\":\n-            # LMP is meaningless when the tree cache is disabled.\n+    def __init__(self, policy, tree_cache):\n+        if tree_cache.disable and policy in [\"lpm\", \"dfs-weight\"]:\n+            # LPM and DFS-weight are meaningless when the tree cache is disabled.\n             policy = \"fcfs\"\n \n         self.policy = policy\n-        self.max_running_seqs = max_running_seqs\n-        self.max_prefill_num_tokens = max_prefill_num_tokens\n-        self.max_total_num_tokens = max_total_num_tokens\n         self.tree_cache = tree_cache\n \n     def get_priority_queue(self, waiting_queue):\n@@ -157,6 +148,9 @@ class PrefillAdder:\n \n     @contextmanager\n     def _lock_node(self, last_node):\n+        if self.tree_cache.disable:\n+            yield None\n+            return\n         try:\n             delta = self.tree_cache.inc_lock_ref(last_node)\n             self.rem_total_tokens += delta\n@@ -187,7 +181,8 @@ class PrefillAdder:\n             ):\n                 # Non-chunked prefill\n                 self.can_run_list.append(req)\n-                self.tree_cache.inc_lock_ref(req.last_node)\n+                if not self.tree_cache.disable:\n+                    self.tree_cache.inc_lock_ref(req.last_node)\n                 self._prefill_one_req(\n                     prefix_len, input_tokens, req.sampling_params.max_new_tokens\n                 )\n@@ -201,7 +196,8 @@ class PrefillAdder:\n                 req.input_ids = req.input_ids[: len(req.prefix_indices) + trunc_len]\n                 self.can_run_list.append(req)\n                 self.new_inflight_req = req\n-                self.tree_cache.inc_lock_ref(req.last_node)\n+                if not self.tree_cache.disable:\n+                    self.tree_cache.inc_lock_ref(req.last_node)\n                 self._prefill_one_req(prefix_len, trunc_len, 0)\n \n         return True\ndiff --git a/python/sglang/srt/managers/schedule_batch.py b/python/sglang/srt/managers/schedule_batch.py\nindex 2489abd5d..3c6e9008d 100644\n--- a/python/sglang/srt/managers/schedule_batch.py\n+++ b/python/sglang/srt/managers/schedule_batch.py\n@@ -363,13 +363,14 @@ class ScheduleBatch:\n         out_cache_loc = self.token_to_kv_pool.alloc(num_tokens)\n \n         if out_cache_loc is None:\n-            if self.tree_cache is not None:\n+            cache_enabled = self.tree_cache is not None and not getattr(self.tree_cache, \"disable\", False)\n+            if cache_enabled:\n                 self.tree_cache.evict(num_tokens, self.token_to_kv_pool.free)\n                 out_cache_loc = self.token_to_kv_pool.alloc(num_tokens)\n \n             if out_cache_loc is None:\n                 logger.error(\"Prefill out of memory. Try to lower your batch size.\")\n-                if self.tree_cache is not None:\n+                if cache_enabled:\n                     self.tree_cache.pretty_print()\n                 exit(1)\n \n@@ -519,15 +520,16 @@ class ScheduleBatch:\n                 self.req_to_token_pool.free(req.req_pool_idx)\n \n                 # release the last node\n-                self.tree_cache.dec_lock_ref(req.last_node)\n+                if not getattr(self.tree_cache, \"disable\", False):\n+                    self.tree_cache.dec_lock_ref(req.last_node)\n \n-                # NOTE(lsyin): we should use the newly evictable memory instantly.\n-                residual_size = (\n-                    len(sorted_indices) * global_config.retract_decode_steps\n-                    - self.token_to_kv_pool.available_size()\n-                )\n-                residual_size = max(0, residual_size)\n-                self.tree_cache.evict(residual_size, self.token_to_kv_pool.free)\n+                    # NOTE(lsyin): we should use the newly evictable memory instantly.\n+                    residual_size = (\n+                        len(sorted_indices) * global_config.retract_decode_steps\n+                        - self.token_to_kv_pool.available_size()\n+                    )\n+                    residual_size = max(0, residual_size)\n+                    self.tree_cache.evict(residual_size, self.token_to_kv_pool.free)\n \n             req.prefix_indices = None\n             req.last_node = None\ndiff --git a/python/sglang/srt/managers/tp_worker.py b/python/sglang/srt/managers/tp_worker.py\nindex 0228073c7..fc16af585 100644\n--- a/python/sglang/srt/managers/tp_worker.py\n+++ b/python/sglang/srt/managers/tp_worker.py\n@@ -167,9 +167,6 @@ class ModelTpServer:\n         self.tree_cache_metrics = {\"total\": 0, \"hit\": 0}\n         self.scheduler = PolicyScheduler(\n             self.schedule_policy,\n-            self.max_running_requests,\n-            self.max_prefill_tokens,\n-            self.max_total_num_tokens,\n             self.tree_cache,\n         )\n         self.req_to_token_pool = self.model_runner.req_to_token_pool\n@@ -270,8 +267,10 @@ class ModelTpServer:\n                 self.new_token_ratio = global_config.init_new_token_ratio\n \n     def print_stats(self):\n+        cache_disabled = getattr(self.tree_cache, \"disable\", False)\n         num_used = self.max_total_num_tokens - (\n-            self.token_to_kv_pool.available_size() + self.tree_cache.evictable_size()\n+            self.token_to_kv_pool.available_size()\n+            + (0 if cache_disabled else self.tree_cache.evictable_size())\n         )\n         throughput = self.num_generated_tokens / (time.time() - self.last_stats_tic)\n         self.num_generated_tokens = 0\n@@ -287,8 +286,10 @@ class ModelTpServer:\n \n     def check_memory(self):\n         crash = os.getenv(\"CI\", \"false\") == \"true\"\n+        cache_disabled = getattr(self.tree_cache, \"disable\", False)\n         available_size = (\n-            self.token_to_kv_pool.available_size() + self.tree_cache.evictable_size()\n+            self.token_to_kv_pool.available_size()\n+            + (0 if cache_disabled else self.tree_cache.evictable_size())\n         )\n         if available_size != self.max_total_num_tokens:\n             warnings.warn(\n@@ -385,9 +386,11 @@ class ModelTpServer:\n         # Get priority queue\n         self.waiting_queue = self.scheduler.get_priority_queue(self.waiting_queue)\n \n+        cache_disabled = getattr(self.tree_cache, \"disable\", False)\n         adder = PrefillAdder(\n             self.tree_cache,\n-            self.token_to_kv_pool.available_size() + self.tree_cache.evictable_size(),\n+            self.token_to_kv_pool.available_size()\n+            + (0 if cache_disabled else self.tree_cache.evictable_size()),\n             self.max_prefill_tokens,\n             self.chunked_prefill_size,\n         )\ndiff --git a/python/sglang/srt/mem_cache/radix_cache.py b/python/sglang/srt/mem_cache/radix_cache.py\nindex c23812049..8b72aa84b 100644\n--- a/python/sglang/srt/mem_cache/radix_cache.py\n+++ b/python/sglang/srt/mem_cache/radix_cache.py\n@@ -91,17 +91,21 @@ class RadixCache(BasePrefixCache):\n \n     def cache_finished_req(self, req: \"Req\", token_ids=None):\n         \"\"\"Cache request when it finishes.\"\"\"\n+        if self.disable:\n+            # Early return to avoid unnecessary computation\n+            kv_indices = self.req_to_token_pool.req_to_token[\n+                req.req_pool_idx, : len(req.input_ids + req.output_ids) - 1\n+            ]\n+            self.token_to_kv_pool.free(kv_indices)\n+            self.req_to_token_pool.free(req.req_pool_idx)\n+            return\n+        \n         if token_ids is None:\n             token_ids = (req.input_ids + req.output_ids)[:-1]\n         kv_indices = self.req_to_token_pool.req_to_token[\n             req.req_pool_idx, : len(token_ids)\n         ]\n \n-        if self.disable:\n-            self.token_to_kv_pool.free(kv_indices)\n-            self.req_to_token_pool.free(req.req_pool_idx)\n-            return\n-\n         # Radix Cache takes one ref in memory pool\n         new_prefix_len = self.insert(token_ids, kv_indices.clone())\n         self.token_to_kv_pool.free(kv_indices[len(req.prefix_indices) : new_prefix_len])\n@@ -169,6 +173,8 @@ class RadixCache(BasePrefixCache):\n                 heapq.heappush(leaves, x.parent)\n \n     def inc_lock_ref(self, node: TreeNode):\n+        if self.disable:\n+            return 0\n         delta = 0\n         while node != self.root_node:\n             if node.lock_ref == 0:\n@@ -179,6 +185,8 @@ class RadixCache(BasePrefixCache):\n         return delta\n \n     def dec_lock_ref(self, node: TreeNode):\n+        if self.disable:\n+            return 0\n         delta = 0\n         while node != self.root_node:\n             if node.lock_ref == 1:\n", "model_name_or_path": "gpt-5-2025-08-07"}
