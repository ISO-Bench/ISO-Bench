{"instance_id": "/home/ubuntu/OmniPerf-Bench/sglan__repo-62f15ee", "model_patch": "diff --git a/python/sglang/srt/managers/policy_scheduler.py b/python/sglang/srt/managers/policy_scheduler.py\nindex 04169e808..810f0167a 100644\n--- a/python/sglang/srt/managers/policy_scheduler.py\n+++ b/python/sglang/srt/managers/policy_scheduler.py\n@@ -112,19 +112,30 @@ class PrefillAdder:\n         rem_input_tokens: int,\n         rem_chunk_tokens: Optional[int],\n         mixed_with_decode_tokens: int = 0,\n+        running_batch: Optional[\"ScheduleBatch\"] = None,\n+        new_token_ratio: float = 0.0,\n     ):\n         self.tree_cache = tree_cache\n+        # Track total before deductions for logging/metrics if needed\n+        self.total_tokens = rem_total_tokens\n+        # Remaining capacities after accounting for mixed decode tokens\n         self.rem_total_tokens = rem_total_tokens - mixed_with_decode_tokens\n         self.rem_input_tokens = rem_input_tokens - mixed_with_decode_tokens\n         self.rem_chunk_tokens = rem_chunk_tokens\n         if self.rem_chunk_tokens is not None:\n             self.rem_chunk_tokens -= mixed_with_decode_tokens\n \n+        # Caches / states for scheduling\n+        self.req_states = None\n         self.can_run_list = []\n         self.new_inflight_req = None\n         self.log_hit_tokens = 0\n         self.log_input_tokens = 0\n \n+        # Optionally deduct running tokens upfront\n+        if running_batch is not None and new_token_ratio > 0.0:\n+            self.remove_running_tokens(running_batch, new_token_ratio)\n+\n     def no_remaining_tokens(self):\n         return (\n             self.rem_total_tokens <= 0\n@@ -139,15 +150,14 @@ class PrefillAdder:\n     def remove_running_tokens(\n         self, running_batch: ScheduleBatch, new_token_ratio: float\n     ):\n+        # Deduct the estimated number of decode tokens from capacity using a generator\n         self.rem_total_tokens -= sum(\n-            [\n-                min(\n-                    (r.sampling_params.max_new_tokens - len(r.output_ids)),\n-                    CLIP_MAX_NEW_TOKENS,\n-                )\n-                * new_token_ratio\n-                for r in running_batch.reqs\n-            ]\n+            min(\n+                (r.sampling_params.max_new_tokens - len(r.output_ids)),\n+                CLIP_MAX_NEW_TOKENS,\n+            )\n+            * new_token_ratio\n+            for r in running_batch.reqs\n         )\n \n     def _prefill_one_req(\ndiff --git a/python/sglang/srt/managers/tp_worker.py b/python/sglang/srt/managers/tp_worker.py\nindex 8fc03b859..21cf3844c 100644\n--- a/python/sglang/srt/managers/tp_worker.py\n+++ b/python/sglang/srt/managers/tp_worker.py\n@@ -23,6 +23,7 @@ import time\n import warnings\n from typing import Any, List, Optional, Union\n \n+import numpy as np\n import torch\n import torch.distributed\n import torch.distributed as dist\n@@ -413,10 +414,11 @@ class ModelTpServer:\n             self.max_prefill_tokens,\n             self.chunked_prefill_size,\n             num_mixed_running,\n+            running_batch=self.running_batch,\n+            new_token_ratio=self.new_token_ratio,\n         )\n \n-        if self.running_batch is not None:\n-            adder.remove_running_tokens(self.running_batch, self.new_token_ratio)\n+        # Tokens from running decode requests are already deducted in PrefillAdder\n \n         has_inflight = self.current_inflight_req is not None\n         if self.current_inflight_req is not None:\n@@ -487,7 +489,8 @@ class ModelTpServer:\n             self.token_to_kv_pool,\n             self.tree_cache,\n         )\n-        self.waiting_queue = [x for x in self.waiting_queue if x not in can_run_list]\n+        can_run_set = set(can_run_list)\n+        self.waiting_queue = [x for x in self.waiting_queue if x not in can_run_set]\n         return new_batch\n \n     def forward_prefill_batch(self, batch: ScheduleBatch):\n@@ -931,7 +934,9 @@ def broadcast_recv_input(\n         else:\n             serialized_data = pickle.dumps(data)\n             size = len(serialized_data)\n-            tensor_data = torch.ByteTensor(list(serialized_data))\n+            # Avoid slow Python list conversion by leveraging numpy view\n+            np_arr = np.frombuffer(serialized_data, dtype=np.uint8)\n+            tensor_data = torch.from_numpy(np_arr)\n             tensor_size = torch.tensor([size], dtype=torch.long)\n \n             dist.broadcast(tensor_size, src=0, group=dist_group)\n@@ -948,6 +953,7 @@ def broadcast_recv_input(\n         tensor_data = torch.empty(size, dtype=torch.uint8)\n         dist.broadcast(tensor_data, src=0, group=dist_group)\n \n-        serialized_data = bytes(tensor_data.tolist())\n+        # Faster conversion back to bytes without building Python lists\n+        serialized_data = tensor_data.numpy().tobytes()\n         data = pickle.loads(serialized_data)\n         return data\n", "model_name_or_path": "gpt-5-2025-08-07"}
