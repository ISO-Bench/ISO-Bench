{"instance_id": "sgl-project__sglan-57eec0b", "model_patch": "diff --git a/python/sglang/srt/entrypoints/http_server.py b/python/sglang/srt/entrypoints/http_server.py\nindex 5a97072de..a3a3e925a 100644\n--- a/python/sglang/srt/entrypoints/http_server.py\n+++ b/python/sglang/srt/entrypoints/http_server.py\n@@ -21,7 +21,7 @@ import asyncio\n import dataclasses\n import json\n import logging\n-import multiprocessing as multiprocessing\n+import multiprocessing.connection as _mp_conn\n import os\n import threading\n import time\n@@ -33,7 +33,6 @@ setattr(threading, \"_register_atexit\", lambda *args, **kwargs: None)\n \n from contextlib import asynccontextmanager\n \n-import numpy as np\n import orjson\n import requests\n import uvicorn\n@@ -172,12 +171,17 @@ async def health_generate(request: Request) -> Response:\n             break\n \n     tic = time.time()\n+    deadline = tic + HEALTH_CHECK_TIMEOUT\n+    tm = _global_state.tokenizer_manager\n     task = asyncio.create_task(gen())\n-    while time.time() < tic + HEALTH_CHECK_TIMEOUT:\n+    while True:\n+        now = time.time()\n+        if now >= deadline:\n+            break\n         await asyncio.sleep(1)\n-        if _global_state.tokenizer_manager.last_receive_tstamp > tic:\n+        if tm.last_receive_tstamp > tic:\n             task.cancel()\n-            _global_state.tokenizer_manager.rid_to_state.pop(rid, None)\n+            tm.rid_to_state.pop(rid, None)\n             return Response(status_code=200)\n \n     task.cancel()\n@@ -229,19 +233,16 @@ async def generate_request(obj: GenerateReqInput, request: Request):\n     if obj.stream:\n \n         async def stream_results() -> AsyncIterator[bytes]:\n+            opt = orjson.OPT_NON_STR_KEYS\n             try:\n                 async for out in _global_state.tokenizer_manager.generate_request(\n                     obj, request\n                 ):\n-                    yield b\"data: \" + orjson.dumps(\n-                        out, option=orjson.OPT_NON_STR_KEYS\n-                    ) + b\"\\n\\n\"\n+                    yield b\"data: \" + orjson.dumps(out, option=opt) + b\"\\n\\n\"\n             except ValueError as e:\n                 out = {\"error\": {\"message\": str(e)}}\n                 logger.error(f\"Error: {e}\")\n-                yield b\"data: \" + orjson.dumps(\n-                    out, option=orjson.OPT_NON_STR_KEYS\n-                ) + b\"\\n\\n\"\n+                yield b\"data: \" + orjson.dumps(out, option=opt) + b\"\\n\\n\"\n             yield b\"data: [DONE]\\n\\n\"\n \n         return StreamingResponse(\n@@ -528,10 +529,8 @@ async def openai_v1_embeddings(raw_request: Request):\n @app.get(\"/v1/models\", response_class=ORJSONResponse)\n def available_models():\n     \"\"\"Show available models.\"\"\"\n-    served_model_names = [_global_state.tokenizer_manager.served_model_name]\n-    model_cards = []\n-    for served_model_name in served_model_names:\n-        model_cards.append(ModelCard(id=served_model_name, root=served_model_name))\n+    name = _global_state.tokenizer_manager.served_model_name\n+    model_cards = [ModelCard(id=name, root=name)]\n     return ModelList(data=model_cards)\n \n \n@@ -615,14 +614,13 @@ async def vertex_generate(vertex_req: VertexGenerateReqInput, raw_request: Reque\n \n \n def _create_error_response(e):\n-    return ORJSONResponse(\n-        {\"error\": {\"message\": str(e)}}, status_code=HTTPStatus.BAD_REQUEST\n-    )\n+    msg = str(e)\n+    return ORJSONResponse({\"error\": {\"message\": msg}}, status_code=HTTPStatus.BAD_REQUEST)\n \n \n def launch_server(\n     server_args: ServerArgs,\n-    pipe_finish_writer: Optional[multiprocessing.connection.Connection] = None,\n+    pipe_finish_writer: Optional[_mp_conn.Connection] = None,\n     launch_callback: Optional[Callable[[], None]] = None,\n ):\n     \"\"\"\n@@ -689,7 +687,7 @@ def launch_server(\n \n def _wait_and_warmup(\n     server_args: ServerArgs,\n-    pipe_finish_writer: Optional[multiprocessing.connection.Connection],\n+    pipe_finish_writer: Optional[_mp_conn.Connection],\n     image_token_text: str,\n     launch_callback: Optional[Callable[[], None]] = None,\n ):\n@@ -700,10 +698,11 @@ def _wait_and_warmup(\n \n     # Wait until the server is launched\n     success = False\n+    s = requests.Session()\n     for _ in range(120):\n         time.sleep(1)\n         try:\n-            res = requests.get(url + \"/get_model_info\", timeout=5, headers=headers)\n+            res = s.get(url + \"/get_model_info\", timeout=5, headers=headers)\n             assert res.status_code == 200, f\"{res=}, {res.text=}\"\n             success = True\n             break\n@@ -737,6 +736,8 @@ def _wait_and_warmup(\n     # Debug dumping\n     if server_args.debug_tensor_dump_input_file:\n         json_data.pop(\"text\", None)\n+        # Lazy import numpy only when debugging to avoid heavy import at startup\n+        import numpy as np\n         json_data[\"input_ids\"] = np.load(\n             server_args.debug_tensor_dump_input_file\n         ).tolist()\n", "model_name_or_path": "gpt-5-2025-08-07"}
