diff --git a/model_patch.diff b/model_patch.diff
new file mode 100644
index 0000000..e69de29
diff --git a/python/sglang/srt/distributed/parallel_state.py b/python/sglang/srt/distributed/parallel_state.py
index fdde7dd..c4e4857 100644
--- a/python/sglang/srt/distributed/parallel_state.py
+++ b/python/sglang/srt/distributed/parallel_state.py
@@ -539,20 +539,27 @@ class GroupCoordinator:
             dim += input_.dim()
         if self.xpu_communicator is not None and not self.xpu_communicator.disabled:
             return self.xpu_communicator.gather(input_, self.rank_in_group, dst, dim)
-        # Allocate output tensor.
         if self.rank_in_group == dst:
-            gather_list = [torch.empty_like(input_) for _ in range(world_size)]
+            # Allocate output tensor and receive directly into its views to avoid extra concat/copy
+            input_size = input_.size()
+            output_size = list(input_size)
+            output_size[dim] = input_size[dim] * world_size
+            output_tensor = torch.empty(tuple(output_size), dtype=input_.dtype, device=input_.device)
+            gather_list = [
+                output_tensor.narrow(dim, i * input_size[dim], input_size[dim])
+                for i in range(world_size)
+            ]
         else:
+            output_tensor = None
             gather_list = None
         # Gather.
         torch.distributed.gather(
             input_, gather_list, dst=self.ranks[dst], group=self.device_group
         )
         if self.rank_in_group == dst:
-            output_tensor = torch.cat(gather_list, dim=dim)
+            return output_tensor
         else:
-            output_tensor = None
-        return output_tensor
+            return None
 
     def broadcast(self, input_: torch.Tensor, src: int = 0):
         """Broadcast the input tensor.
@@ -622,17 +629,17 @@ class GroupCoordinator:
         )
 
         # Serialize object to tensor and get the size as well
-        object_tensor = torch.frombuffer(pickle.dumps(obj), dtype=torch.uint8)
-
-        size_tensor = torch.tensor(
-            [object_tensor.numel()], dtype=torch.long, device="cpu"
-        )
+        # Use highest protocol for speed/size and bytearray for efficient buffer view
+        b = pickle.dumps(obj, protocol=pickle.HIGHEST_PROTOCOL)
+        ba = bytearray(b)
+        size_tensor = torch.empty(1, dtype=torch.long, device="cpu")
+        size_tensor[0] = len(ba)
 
         # Send object size
-
         torch.distributed.send(size_tensor, dst=self.ranks[dst], group=self.cpu_group)
 
         # Send object
+        object_tensor = torch.frombuffer(ba, dtype=torch.uint8)
         torch.distributed.send(object_tensor, dst=self.ranks[dst], group=self.cpu_group)
 
         return None
@@ -669,7 +676,7 @@ class GroupCoordinator:
             rank_object == rank_size
         ), "Received object sender rank does not match the size sender rank."
 
-        obj = pickle.loads(object_tensor.numpy().tobytes())
+        obj = pickle.loads(memoryview(object_tensor.numpy()))
 
         return obj
 
@@ -1290,7 +1297,7 @@ def in_the_same_node_as(pg: ProcessGroup, source_rank: int = 0) -> List[bool]:
     world_size = torch.distributed.get_world_size(group=pg)
 
     # local tensor in each process to store the result
-    is_in_the_same_node = torch.tensor([0] * world_size, dtype=torch.int32)
+    is_in_the_same_node = torch.zeros(world_size, dtype=torch.int32)
 
     # global ranks of the processes in the group
     ranks = torch.distributed.get_process_group_ranks(pg)
