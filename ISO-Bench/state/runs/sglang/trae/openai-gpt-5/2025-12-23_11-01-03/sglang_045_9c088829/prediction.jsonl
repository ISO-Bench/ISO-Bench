{"instance_id": "/home/ubuntu/OmniPerf-Bench/sglan__repo-005aad3", "model_patch": "diff --git a/model_patch.diff b/model_patch.diff\nnew file mode 100644\nindex 0000000..e69de29\ndiff --git a/python/sglang/srt/distributed/parallel_state.py b/python/sglang/srt/distributed/parallel_state.py\nindex fdde7dd..c4e4857 100644\n--- a/python/sglang/srt/distributed/parallel_state.py\n+++ b/python/sglang/srt/distributed/parallel_state.py\n@@ -539,20 +539,27 @@ class GroupCoordinator:\n             dim += input_.dim()\n         if self.xpu_communicator is not None and not self.xpu_communicator.disabled:\n             return self.xpu_communicator.gather(input_, self.rank_in_group, dst, dim)\n-        # Allocate output tensor.\n         if self.rank_in_group == dst:\n-            gather_list = [torch.empty_like(input_) for _ in range(world_size)]\n+            # Allocate output tensor and receive directly into its views to avoid extra concat/copy\n+            input_size = input_.size()\n+            output_size = list(input_size)\n+            output_size[dim] = input_size[dim] * world_size\n+            output_tensor = torch.empty(tuple(output_size), dtype=input_.dtype, device=input_.device)\n+            gather_list = [\n+                output_tensor.narrow(dim, i * input_size[dim], input_size[dim])\n+                for i in range(world_size)\n+            ]\n         else:\n+            output_tensor = None\n             gather_list = None\n         # Gather.\n         torch.distributed.gather(\n             input_, gather_list, dst=self.ranks[dst], group=self.device_group\n         )\n         if self.rank_in_group == dst:\n-            output_tensor = torch.cat(gather_list, dim=dim)\n+            return output_tensor\n         else:\n-            output_tensor = None\n-        return output_tensor\n+            return None\n \n     def broadcast(self, input_: torch.Tensor, src: int = 0):\n         \"\"\"Broadcast the input tensor.\n@@ -622,17 +629,17 @@ class GroupCoordinator:\n         )\n \n         # Serialize object to tensor and get the size as well\n-        object_tensor = torch.frombuffer(pickle.dumps(obj), dtype=torch.uint8)\n-\n-        size_tensor = torch.tensor(\n-            [object_tensor.numel()], dtype=torch.long, device=\"cpu\"\n-        )\n+        # Use highest protocol for speed/size and bytearray for efficient buffer view\n+        b = pickle.dumps(obj, protocol=pickle.HIGHEST_PROTOCOL)\n+        ba = bytearray(b)\n+        size_tensor = torch.empty(1, dtype=torch.long, device=\"cpu\")\n+        size_tensor[0] = len(ba)\n \n         # Send object size\n-\n         torch.distributed.send(size_tensor, dst=self.ranks[dst], group=self.cpu_group)\n \n         # Send object\n+        object_tensor = torch.frombuffer(ba, dtype=torch.uint8)\n         torch.distributed.send(object_tensor, dst=self.ranks[dst], group=self.cpu_group)\n \n         return None\n@@ -669,7 +676,7 @@ class GroupCoordinator:\n             rank_object == rank_size\n         ), \"Received object sender rank does not match the size sender rank.\"\n \n-        obj = pickle.loads(object_tensor.numpy().tobytes())\n+        obj = pickle.loads(memoryview(object_tensor.numpy()))\n \n         return obj\n \n@@ -1290,7 +1297,7 @@ def in_the_same_node_as(pg: ProcessGroup, source_rank: int = 0) -> List[bool]:\n     world_size = torch.distributed.get_world_size(group=pg)\n \n     # local tensor in each process to store the result\n-    is_in_the_same_node = torch.tensor([0] * world_size, dtype=torch.int32)\n+    is_in_the_same_node = torch.zeros(world_size, dtype=torch.int32)\n \n     # global ranks of the processes in the group\n     ranks = torch.distributed.get_process_group_ranks(pg)\n", "model_name_or_path": "gpt-5-2025-08-07"}
