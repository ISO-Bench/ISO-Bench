{"instance_id": "/home/ubuntu/OmniPerf-Bench/sglan__repo-58d1082", "model_patch": "diff --git a/model_patch.diff b/model_patch.diff\nindex c76456a..e69de29 100644\n--- a/model_patch.diff\n+++ b/model_patch.diff\n@@ -1,228 +0,0 @@\n-diff --git a/benchmark/lora/launch_server.py b/benchmark/lora/launch_server.py\n-index 1fa4d71..b0877de 100644\n---- a/benchmark/lora/launch_server.py\n-+++ b/benchmark/lora/launch_server.py\n-@@ -11,18 +11,19 @@ LORA_PATH = {\n- def launch_server(args):\n-     base_path = LORA_PATH[\"base\"]\n-     lora_path = LORA_PATH[\"lora\"]\n--    max_loras_per_batch = 4\n- \n--    if args.base_only:\n--        cmd = f\"python -m sglang.launch_server --model {base_path} \"\n--    else:\n--        cmd = f\"python -m sglang.launch_server --model {base_path} --lora-paths \"\n--        for i in range(NUM_LORAS):\n--            lora_name = f\"lora{i}\"\n--            cmd += f\"{lora_name}={lora_path} \"\n--    cmd += f\"--disable-radix --disable-cuda-graph \"\n--    cmd += f\"--max-loras-per-batch {args.max_loras_per_batch} \"\n--    cmd += f\"--max-running-requests {args.max_running_requests}\"\n-+    parts = [\"python -m sglang.launch_server\", f\"--model {base_path}\"]\n-+    if not args.base_only:\n-+        num = args.num_loras if hasattr(args, \"num_loras\") and args.num_loras is not None else NUM_LORAS\n-+        loras = (f\"lora{i}={lora_path}\" for i in range(num))\n-+        parts.append(\"--lora-paths \" + \" \".join(loras))\n-+    parts.extend([\n-+        \"--disable-radix\",\n-+        \"--disable-cuda-graph\",\n-+        f\"--max-loras-per-batch {args.max_loras_per_batch}\",\n-+        f\"--max-running-requests {args.max_running_requests}\",\n-+    ])\n-+    cmd = \" \".join(parts)\n-     print(cmd)\n-     os.system(cmd)\n- \n-diff --git a/python/sglang/srt/lora/lora.py b/python/sglang/srt/lora/lora.py\n-index 379b233..c8d9e0e 100644\n---- a/python/sglang/srt/lora/lora.py\n-+++ b/python/sglang/srt/lora/lora.py\n-@@ -299,7 +299,8 @@ class LoRALayer(nn.Module):\n- \n-     def load_to_gpu(self):\n-         for name, weight in self.weights.items():\n--            self.weight_gpu[name] = weight.to(torch.float16).to(\"cuda\")\n-+            # merge dtype and device move; non_blocking may speed H2D when possible\n-+            self.weight_gpu[name] = weight.to(device=\"cuda\", dtype=torch.float16, non_blocking=True)\n- \n-     def offload_from_gpu(self):\n-         for name, weight in self.weights.items():\n-@@ -336,7 +337,7 @@ class LoRAAdapter(nn.Module):\n- \n-     def load_to_gpu(self):\n-         for name, weight in self.weights.items():\n--            self.weights_gpu[name] = weight.to(torch.float16).to(\"cuda\")\n-+            self.weights_gpu[name] = weight.to(device=\"cuda\", dtype=torch.float16, non_blocking=True)\n-         for layer in self.layers:\n-             layer.load_to_gpu()\n- \n-@@ -357,9 +358,10 @@ class LoRAAdapter(nn.Module):\n-             match = re.search(r\"layers\\.(\\d+)\\.\", name)\n-             if match is not None:\n-                 layer_id = int(match.group(1))\n--                self.layers[layer_id].weights[name] = loaded_weight.cpu()\n-+                # keep on CPU; avoid redundant copy if already CPU\n-+                self.layers[layer_id].weights[name] = loaded_weight.to(\"cpu\", copy=False)\n-             else:\n--                self.weights[name] = loaded_weight.cpu()\n-+                self.weights[name] = loaded_weight.to(\"cpu\", copy=False)\n- \n-         # stack kv_proj and gate_up_proj\n-         for i in range(self.base_hf_config.num_hidden_layers):\n-diff --git a/python/sglang/srt/lora/lora_manager.py b/python/sglang/srt/lora/lora_manager.py\n-index 59cd7e1..d4c5cd2 100644\n---- a/python/sglang/srt/lora/lora_manager.py\n-+++ b/python/sglang/srt/lora/lora_manager.py\n-@@ -105,6 +105,7 @@ class LoRAManager:\n-         self.load_config = load_config\n-         self.dtype = dtype\n- \n-+        # allocate once and reuse\n-         workspace_buffer = torch.empty(1 * 1024 * 1024, dtype=torch.int8, device=\"cuda\")\n-         self.segment_gemm = SegmentGEMMWrapper(workspace_buffer)\n- \n-@@ -113,15 +114,14 @@ class LoRAManager:\n-         self.init_lora_batch()\n- \n-     def match_target_modules(self, module_name):\n--        for target_module in self.target_modules:\n--            if module_name.split(\".\")[-1] == target_module:\n--                return True\n--        return False\n-+        last = module_name.split(\".\")[-1]\n-+        return any(last == target for target in self.target_modules)\n- \n-     def get_target_modules(self):\n-         modules = []\n-+        mt = self.match_target_modules\n-         for module_name, module in self.base_model.named_modules():\n--            if self.match_target_modules(module_name):\n-+            if mt(module_name):\n-                 modules.append((module_name, module))\n-         return modules\n- \n-@@ -137,10 +137,9 @@ class LoRAManager:\n-         self.configs = {}\n-         self.origin_target_modules = set()\n-         for name, path in self.lora_paths.items():\n--            self.configs[name] = LoRAConfig(path)\n--            self.origin_target_modules = set(self.origin_target_modules) | set(\n--                self.configs[name].target_modules\n--            )\n-+            conf = LoRAConfig(path)\n-+            self.configs[name] = conf\n-+            self.origin_target_modules |= set(conf.target_modules)\n-         if hasattr(self.base_model, \"get_module_name\"):\n-             self.target_modules = {\n-                 self.base_model.get_module_name(module)\n-@@ -148,13 +147,9 @@ class LoRAManager:\n-             }\n-         else:\n-             logger.warning(\n--                f\"WARNING: get_module_name() is not defined, \"\n--                f\"which is used to map config module name to model implementation module name.\"\n--                f\"Use the default one, but please check if it is correct for your model.\"\n-+                f\"WARNING: get_module_name() is not defined, which is used to map config module name to model implementation module name. Use the default one, but please check if it is correct for your model.\"\n-             )\n--            self.target_modules = {\n--                get_module_name(module) for module in self.origin_target_modules\n--            }\n-+            self.target_modules = {get_module_name(module) for module in self.origin_target_modules}\n-         self.target_weights = set(\n-             [get_stacked_name(module) for module in self.origin_target_modules]\n-         )\n-@@ -164,12 +159,9 @@ class LoRAManager:\n-         self.lora_id = {}\n-         for name in self.lora_paths.keys():\n-             self.lora_id[name] = len(self.loras)\n--            self.loras.append(\n--                LoRAAdapter(\n--                    name, self.configs[name], self.base_hf_config, self.load_config\n--                )\n--            )\n--            self.loras[-1].initialize_weights()\n-+            adp = LoRAAdapter(name, self.configs[name], self.base_hf_config, self.load_config)\n-+            self.loras.append(adp)\n-+            adp.initialize_weights()\n- \n-         # misc lora configs\n-         self.max_lora_dim = max([x.hf_config[\"r\"] for x in self.configs.values()])\n-@@ -181,9 +173,7 @@ class LoRAManager:\n-         # monkey patch to use the LoRA version\n-         self.lora_modules = []\n-         for module_name, module in self.get_target_modules():\n--            self.lora_modules.append(\n--                (module_name, self.set_lora_module(module_name, module))\n--            )\n-+            self.lora_modules.append((module_name, self.set_lora_module(module_name, module)))\n- \n-     def init_lora_memory_pool(self):\n-         # preallocate lora memory pool\n-@@ -196,13 +186,12 @@ class LoRAManager:\n-                 hidden_dim_A, _ = self.base_model.get_hidden_dim(module_A)\n-             else:\n-                 logger.warning(\n--                    f\"WARNING: get_hidden_dim() is not defined, \"\n--                    f\"which is used to get the hidden dim for different lora modules\"\n--                    f\"Use the default one, but please check if it is correct for your model.\"\n-+                    f\"WARNING: get_hidden_dim() is not defined, which is used to get the hidden dim for different lora modulesUse the default one, but please check if it is correct for your model.\"\n-                 )\n-                 hidden_dim_A, _ = get_hidden_dim(module_A, self.base_hf_config)\n-             c = self.loras[-1].get_stacked_multiply(module_A)\n-             if module_A not in self.A_buffer:\n-+                # use empty to avoid unnecessary zero fill\n-                 self.A_buffer[module_A] = [\n-                     torch.empty(\n-                         (\n-@@ -213,16 +202,14 @@ class LoRAManager:\n-                         dtype=self.dtype,\n-                         device=\"cuda\",\n-                     )\n--                    for i in range(num_layer)\n-+                    for _ in range(num_layer)\n-                 ]\n-             # init B tensor, column_major=True\n-             if hasattr(self.base_model, \"get_hidden_dim\"):\n-                 _, hidden_dim_B = self.base_model.get_hidden_dim(module_B)\n-             else:\n-                 logger.warning(\n--                    f\"WARNING: get_hidden_dim() is not defined, \"\n--                    f\"which is used to get the hidden dim for different lora modules\"\n--                    f\"Use the default one, but please check if it is correct for your model.\"\n-+                    f\"WARNING: get_hidden_dim() is not defined, which is used to get the hidden dim for different lora modulesUse the default one, but please check if it is correct for your model.\"\n-                 )\n-                 _, hidden_dim_B = get_hidden_dim(module_B, self.base_hf_config)\n-             c = self.loras[-1].get_stacked_multiply(module_B)\n-@@ -237,7 +224,7 @@ class LoRAManager:\n-                         dtype=self.dtype,\n-                         device=\"cuda\",\n-                     )\n--                    for i in range(num_layer)\n-+                    for _ in range(num_layer)\n-                 ]\n- \n-     def init_lora_batch(self):\n-@@ -254,7 +241,9 @@ class LoRAManager:\n-         if uid is None:\n-             for i in range(num_layer):\n-                 for k in self.A_buffer.keys():\n--                    self.A_buffer[k][i][buffer_id] *= 0\n-+                    # avoid in-place multiply by zero; simply zero via resize_ to new tensor then fill_(0) is costly.\n-+                    # Use .zero_() directly which is efficient.\n-+                    self.A_buffer[k][i][buffer_id].zero_()\n-             return\n- \n-         for i in range(num_layer):\n-@@ -292,10 +281,11 @@ class LoRAManager:\n- \n-         # setup lora in forward modules\n-         bs = forward_batch.batch_size\n-+        # avoid creating ones on CPU; ensure cuda dtype int32 as used later for seg lengths\n-         seg_lens = (\n-             forward_batch.extend_seq_lens\n-             if forward_batch.forward_mode.is_extend()\n--            else torch.ones(bs)\n-+            else torch.ones(bs, device=\"cuda\")\n-         )\n-         weight_indices = torch.empty((bs,), dtype=torch.int64, device=\"cuda\")\n-         for i, lora_path in enumerate(forward_batch.lora_paths):\n", "model_name_or_path": "gpt-5-2025-08-07"}
