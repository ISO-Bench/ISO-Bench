diff --git a/model_patch.diff b/model_patch.diff
new file mode 100644
index 000000000..e69de29bb
diff --git a/python/sglang/srt/managers/scheduler.py b/python/sglang/srt/managers/scheduler.py
index 1d1cf3688..535a6f5d3 100644
--- a/python/sglang/srt/managers/scheduler.py
+++ b/python/sglang/srt/managers/scheduler.py
@@ -75,6 +75,7 @@ from sglang.srt.utils import (
     kill_parent_process,
     set_random_seed,
     suppress_other_loggers,
+    get_arange_cache,
 )
 from sglang.utils import get_exception_traceback
 
@@ -306,6 +307,9 @@ class Scheduler:
         # This is an optimization to reduce the overhead of the prefill check.
         self.batch_is_full = False
 
+        # Pre-allocate and reuse tensor buffers where possible to reduce frequent small allocations
+        # This avoids repeated empty->fill_ pairs in hot paths and favors single-call creations.
+
         # Init watchdog thread
         self.watchdog_timeout = server_args.watchdog_timeout
         t = threading.Thread(target=self.watchdog_thread, daemon=True)
@@ -425,8 +429,9 @@ class Scheduler:
         else:
             num_tokens = local_batch.extend_num_tokens
 
-        local_num_tokens = torch.tensor([num_tokens], dtype=torch.int64)
-        global_num_tokens = torch.empty(self.tp_size, dtype=torch.int64)
+        local_num_tokens = torch.empty(1, dtype=torch.int64, device="cpu")
+        local_num_tokens[0] = num_tokens
+        global_num_tokens = torch.empty(self.tp_size, dtype=torch.int64, device="cpu")
         torch.distributed.all_gather_into_tensor(
             global_num_tokens,
             local_num_tokens,
@@ -441,14 +446,12 @@ class Scheduler:
 
             # Check forward mode for cuda graph
             if not self.server_args.disable_cuda_graph:
-                forward_mode_state = torch.tensor(
-                    (
-                        1
-                        if local_batch.forward_mode.is_decode()
-                        or local_batch.forward_mode.is_idle()
-                        else 0
-                    ),
-                    dtype=torch.int32,
+                forward_mode_state = torch.empty(1, dtype=torch.int32)
+                forward_mode_state[0] = (
+                    1
+                    if local_batch.forward_mode.is_decode()
+                    or local_batch.forward_mode.is_idle()
+                    else 0
                 )
                 torch.distributed.all_reduce(
                     forward_mode_state,
@@ -819,8 +822,9 @@ class Scheduler:
         can_run_list = adder.can_run_list
         if len(can_run_list) == 0:
             return None
+        can_run_set = set(can_run_list)
         self.waiting_queue = [
-            x for x in self.waiting_queue if x not in set(can_run_list)
+            x for x in self.waiting_queue if x not in can_run_set
         ]
 
         if adder.new_inflight_req is not None:
@@ -924,12 +928,12 @@ class Scheduler:
                 return
             else:
                 logits_output = None
+                batch_size = batch.batch_size()
                 if self.skip_tokenizer_init:
-                    next_token_ids = torch.full(
-                        (batch.batch_size(),), self.tokenizer.eos_token_id
-                    )
+                    next_token_ids = torch.full((batch_size,), self.tokenizer.eos_token_id, dtype=torch.long, device=self.device)
                 else:
-                    next_token_ids = torch.full((batch.batch_size(),), 0)
+                    next_token_ids = torch.empty(batch_size, dtype=torch.long, device=self.device)
+                    next_token_ids.zero_()
             batch.output_ids = next_token_ids
             ret = logits_output, next_token_ids, model_worker_batch.bid
         else:  # embedding or reward model
@@ -1287,7 +1291,8 @@ class Scheduler:
 
         if self.tp_size > 1:
             # Sync across TP ranks to make sure they have the same number of ready requests
-            tensor = torch.tensor(num_ready_reqs, dtype=torch.int32)
+            tensor = torch.empty(1, dtype=torch.int32)
+            tensor[0] = num_ready_reqs
             torch.distributed.all_reduce(
                 tensor, op=torch.distributed.ReduceOp.MAX, group=self.tp_cpu_group
             )
diff --git a/python/sglang/srt/utils.py b/python/sglang/srt/utils.py
index e947d1a92..7aefb07c7 100644
--- a/python/sglang/srt/utils.py
+++ b/python/sglang/srt/utils.py
@@ -61,6 +61,17 @@ show_time_cost = False
 time_infos = {}
 
 
+
+
+def _maybe_cuda_synchronize():
+    # Avoid unnecessary device sync on CPU or when CUDA is unavailable
+    try:
+        if torch.cuda.is_available():
+            torch.cuda.synchronize()
+    except Exception:
+        # Be resilient in environments without CUDA
+        pass
+
 def is_hip() -> bool:
     """Return whether it is HIP on the AMD ROCm platform."""
     return torch.version.hip is not None
@@ -89,33 +100,11 @@ def enable_show_time_cost():
     show_time_cost = True
 
 
-class TimeInfo:
-    def __init__(self, name, interval=0.1, color=0, indent=0):
-        self.name = name
-        self.interval = interval
-        self.color = color
-        self.indent = indent
-
-        self.acc_time = 0
-        self.last_acc_time = 0
-
-    def check(self):
-        if self.acc_time - self.last_acc_time > self.interval:
-            self.last_acc_time = self.acc_time
-            return True
-        return False
-
-    def pretty_print(self):
-        print(f"\x1b[{self.color}m", end="")
-        print("-" * self.indent * 2, end="")
-        print(f"{self.name}: {self.acc_time:.3f}s\x1b[0m")
-
-
 def mark_start(name, interval=0.1, color=0, indent=0):
     global time_infos, show_time_cost
     if not show_time_cost:
         return
-    torch.cuda.synchronize()
+    _maybe_cuda_synchronize()
     if time_infos.get(name, None) is None:
         time_infos[name] = TimeInfo(name, interval, color, indent)
     time_infos[name].acc_time -= time.time()
@@ -125,7 +114,7 @@ def mark_end(name):
     global time_infos, show_time_cost
     if not show_time_cost:
         return
-    torch.cuda.synchronize()
+    _maybe_cuda_synchronize()
     time_infos[name].acc_time += time.time()
     if time_infos[name].check():
         time_infos[name].pretty_print()
@@ -134,11 +123,11 @@ def mark_end(name):
 def calculate_time(show=False, min_cost_ms=0.0):
     def wrapper(func):
         def inner_func(*args, **kwargs):
-            torch.cuda.synchronize()
+            _maybe_cuda_synchronize()
             if show:
                 start_time = time.time()
             result = func(*args, **kwargs)
-            torch.cuda.synchronize()
+            _maybe_cuda_synchronize()
             if show:
                 cost_time = (time.time() - start_time) * 1000
                 if cost_time > min_cost_ms:
@@ -150,6 +139,18 @@ def calculate_time(show=False, min_cost_ms=0.0):
     return wrapper
 
 
+# Lightweight cache for small frequently used arange tensors to avoid repeated allocations
+_ARANGE_CACHE: Dict[Tuple[int, str], torch.Tensor] = {}
+
+def get_arange_cache(n: int, device: str = "cpu", dtype: torch.dtype = torch.long) -> torch.Tensor:
+    # Only cache small sizes to bound memory; key by device string for simplicity
+    key = (n, f"{device}:{dtype}")
+    t = _ARANGE_CACHE.get(key)
+    if t is None or t.device.type != device or t.dtype != dtype:
+        t = torch.arange(n, device=device, dtype=dtype)
+        _ARANGE_CACHE[key] = t
+    return t
+
 def get_available_gpu_memory(device, gpu_id, distributed=False):
     """
     Get available memory for cuda:gpu_id device.
@@ -687,21 +688,23 @@ def broadcast_pyobj(
 
     if rank == 0:
         if len(data) == 0:
-            tensor_size = torch.tensor([0], dtype=torch.long)
+            tensor_size = torch.empty(1, dtype=torch.long)
+            tensor_size[0] = 0
             dist.broadcast(tensor_size, src=0, group=dist_group)
         else:
             serialized_data = pickle.dumps(data)
             size = len(serialized_data)
-            tensor_data = torch.ByteTensor(
+            tensor_data = torch.from_numpy(
                 np.frombuffer(serialized_data, dtype=np.uint8)
             )
-            tensor_size = torch.tensor([size], dtype=torch.long)
+            tensor_size = torch.empty(1, dtype=torch.long)
+            tensor_size[0] = size
 
             dist.broadcast(tensor_size, src=0, group=dist_group)
             dist.broadcast(tensor_data, src=0, group=dist_group)
         return data
     else:
-        tensor_size = torch.tensor([0], dtype=torch.long)
+        tensor_size = torch.empty(1, dtype=torch.long)
         dist.broadcast(tensor_size, src=0, group=dist_group)
         size = tensor_size.item()
 
