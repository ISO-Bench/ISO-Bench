diff --git a/model_patch.diff b/model_patch.diff
new file mode 100644
index 000000000..e69de29bb
diff --git a/python/sglang/srt/layers/logits_processor.py b/python/sglang/srt/layers/logits_processor.py
index f95c30786..d57460ca2 100644
--- a/python/sglang/srt/layers/logits_processor.py
+++ b/python/sglang/srt/layers/logits_processor.py
@@ -1,5 +1,6 @@
 import torch
 from torch import nn
+
 from vllm.distributed import (
     get_tensor_model_parallel_world_size,
     tensor_model_parallel_all_gather,
@@ -13,6 +14,52 @@ class LogitsProcessor(nn.Module):
         super().__init__()
         self.config = config
         self.tp_size = get_tensor_model_parallel_world_size()
+        # Pre-allocate commonly used tensors to avoid repeated allocations
+        self._zero_tensor_cache = None
+        self._arange_cache = None
+        self._arange_cache_size = 0
+        # Buffer for shifted input_ids to avoid cat allocations
+        self._shift_buf = None
+        self._shift_buf_size = 0
+        self._shift_buf_dtype = None
+        self._shift_buf_device = None
+
+
+    def _get_zero_tensor(self):
+        """Get or create a cached zero tensor to avoid repeated allocations."""
+        if self._zero_tensor_cache is None:
+            self._zero_tensor_cache = torch.tensor([0], device="cuda")
+        return self._zero_tensor_cache
+
+
+    def _get_shifted_ids(self, input_ids, pad_last_value: int = 0):
+        """Return a view where out[i] = input_ids[i+1] and out[-1] = pad_last_value.
+        Reuse a cached buffer to avoid repeated allocations."""
+        n = input_ids.shape[0]
+        device = input_ids.device
+        dtype = input_ids.dtype
+        if (
+            self._shift_buf is None
+            or self._shift_buf_size < n
+            or self._shift_buf_device != device
+            or self._shift_buf_dtype != dtype
+        ):
+            self._shift_buf = torch.empty(n, device=device, dtype=dtype)
+            self._shift_buf_size = n
+            self._shift_buf_device = device
+            self._shift_buf_dtype = dtype
+        out = self._shift_buf[:n]
+        out[:-1] = input_ids[1:]
+        out[-1] = pad_last_value
+        return out
+
+    def _get_arange(self, size):
+        """Get or create a cached arange tensor to avoid repeated allocations."""
+        if self._arange_cache is None or self._arange_cache_size < size:
+            self._arange_cache = torch.arange(size, device="cuda")
+            self._arange_cache_size = size
+            return self._arange_cache
+        return self._arange_cache[:size]
 
     def _get_normalized_prompt_logprobs(
         self, prefill_token_logprobs, input_metadata: InputMetadata
@@ -21,10 +68,12 @@ class LogitsProcessor(nn.Module):
             prefill_token_logprobs, dim=0, dtype=torch.float32
         )
 
-        start = input_metadata.extend_start_loc.clone()
+        # Avoid clone by computing directly without modifying input_metadata
+        start = input_metadata.extend_start_loc
         end = start + input_metadata.extend_seq_lens - 2
-        start.clamp_(min=0, max=prefill_token_logprobs.shape[0] - 1)
-        end.clamp_(min=0, max=prefill_token_logprobs.shape[0] - 1)
+        # Use torch.clamp instead of clamp_ to avoid in-place modification
+        start = start.clamp(min=0, max=prefill_token_logprobs.shape[0] - 1)
+        end = end.clamp(min=0, max=prefill_token_logprobs.shape[0] - 1)
         sum_logp = (
             logprobs_cumsum[end]
             - logprobs_cumsum[start]
@@ -38,13 +87,24 @@ class LogitsProcessor(nn.Module):
 
     def _get_top_logprobs(self, all_logprobs, input_metadata: InputMetadata):
         if input_metadata.forward_mode == ForwardMode.DECODE:
+            # Batched top-k with max k to avoid per-row kernel launches
+            ks = input_metadata.top_logprobs_nums
+            if isinstance(ks, torch.Tensor):
+                k_max = int(ks.max().item())
+            else:
+                try:
+                    k_max = int(max(ks))
+                except Exception:
+                    k_max = int(ks)
+            if k_max <= 0:
+                return None, [[] for _ in range(all_logprobs.shape[0])]
+            t_vals, t_idx = all_logprobs.topk(k_max, dim=-1)
+            t_vals = t_vals.cpu().tolist()
+            t_idx = t_idx.cpu().tolist()
             decode_top_logprobs = []
             for i in range(all_logprobs.shape[0]):
-                k = input_metadata.top_logprobs_nums[i]
-                t = all_logprobs[i].topk(k)
-                v_cpu = t.values.cpu().tolist()
-                p_cpu = t.indices.cpu().tolist()
-                decode_top_logprobs.append(list(zip(v_cpu, p_cpu)))
+                k = ks[i]
+                decode_top_logprobs.append(list(zip(t_vals[i][:k], t_idx[i][:k])))
             return None, decode_top_logprobs
         else:
             prefill_top_logprobs, decode_top_logprobs = [], []
@@ -98,7 +158,8 @@ class LogitsProcessor(nn.Module):
                     all_logits = tensor_model_parallel_all_gather(all_logits)
                 all_logits = all_logits[:, : self.config.vocab_size]
 
-            all_logprobs = torch.log(torch.softmax(all_logits.float(), dim=-1) + 1e-6)
+            # Use log_softmax for better numerical stability and performance
+            all_logprobs = torch.nn.functional.log_softmax(all_logits.float(), dim=-1)
 
             prefill_top_logprobs, decode_top_logprobs = self._get_top_logprobs(
                 all_logprobs, input_metadata
@@ -119,9 +180,12 @@ class LogitsProcessor(nn.Module):
 
                 # Compute the logprobs and normalized logprobs for the prefill tokens.
                 # Note that we pad a zero at the end of each sequence for easy computation.
+                # Use cached tensors to avoid repeated allocations
+                arange_indices = self._get_arange(all_logprobs.shape[0])
+                shifted_ids = self._get_shifted_ids(input_ids, pad_last_value=0)
                 prefill_token_logprobs = all_logprobs[
-                    torch.arange(all_logprobs.shape[0], device="cuda"),
-                    torch.cat([input_ids[1:], torch.tensor([0], device="cuda")]),
+                    arange_indices,
+                    shifted_ids,
                 ]
 
                 normalized_prompt_logprobs = self._get_normalized_prompt_logprobs(
diff --git a/python/sglang/srt/managers/router/model_rpc.py b/python/sglang/srt/managers/router/model_rpc.py
index f283635c3..d8aaae1f8 100644
--- a/python/sglang/srt/managers/router/model_rpc.py
+++ b/python/sglang/srt/managers/router/model_rpc.py
@@ -103,8 +103,10 @@ class ModelRpcServer:
                 else server_args.max_prefill_num_token
             ),
         )
+        # Optimize tensor creation with explicit dtype and device
+        logit_bias_list = get_int_token_logit_bias(self.tokenizer, self.model_config.vocab_size)
         self.int_token_logit_bias = torch.tensor(
-            get_int_token_logit_bias(self.tokenizer, self.model_config.vocab_size)
+            logit_bias_list, dtype=torch.float32, device="cpu"
         )
         set_random_seed(server_args.random_seed)
         logger.info(
@@ -150,6 +152,18 @@ class ModelRpcServer:
         self.min_new_token_ratio = min(0.2 * server_args.schedule_conservativeness, 1.0)
         self.new_token_ratio_step = (0.0001, 0.05)  # (down, up)
 
+        # Cache for torch.arange to avoid repeated allocations
+        self._arange_cache = None
+        self._arange_cache_size = 0
+
+    def _get_arange(self, size, device="cuda"):
+        """Get or create a cached arange tensor to avoid repeated allocations."""
+        if self._arange_cache is None or self._arange_cache_size < size:
+            self._arange_cache = torch.arange(size, device=device)
+            self._arange_cache_size = size
+            return self._arange_cache
+        return self._arange_cache[:size]
+
     def flush_cache(self):
         if len(self.forward_queue) == 0 and (
             self.running_batch is None or len(self.running_batch.reqs) == 0
@@ -159,6 +173,9 @@ class ModelRpcServer:
             self.regex_fsm_cache.reset()
             self.req_to_token_pool.clear()
             self.token_to_kv_pool.clear()
+            # Clear arange cache
+            self._arange_cache = None
+            self._arange_cache_size = 0
             torch.cuda.empty_cache()
             logger.info("Cache flushed successfully!")
         else:
@@ -313,10 +330,8 @@ class ModelRpcServer:
         )
         if self.running_batch:
             available_size -= sum(
-                [
-                    (r.max_new_tokens() - len(r.output_ids)) * self.new_token_ratio
-                    for r in self.running_batch.reqs
-                ]
+                (r.max_new_tokens() - len(r.output_ids)) * self.new_token_ratio
+                for r in self.running_batch.reqs
             )
 
         for req in self.forward_queue:
@@ -436,8 +451,11 @@ class ModelRpcServer:
         reqs = batch.reqs
         last_token_logprobs = None
         if last_logprobs is not None:
+            # Use cached arange to avoid repeated allocations
+            arange_indices = self._get_arange(len(reqs), device=last_logprobs.device)
+            next_ids_t = torch.tensor(next_token_ids, device=last_logprobs.device)
             last_token_logprobs = (
-                last_logprobs[torch.arange(len(reqs)), next_token_ids].cpu().tolist()
+                last_logprobs[arange_indices, next_ids_t].cpu().tolist()
             )
 
         # Check finish condition
@@ -530,8 +548,11 @@ class ModelRpcServer:
         reqs = batch.reqs
         new_token_logprobs = None
         if last_logprobs is not None:
+            # Use cached arange to avoid repeated allocations
+            arange_indices = self._get_arange(len(reqs), device=last_logprobs.device)
+            next_ids_t = torch.tensor(next_token_ids, device=last_logprobs.device)
             new_token_logprobs = last_logprobs[
-                torch.arange(len(reqs)), next_token_ids
+                arange_indices, next_ids_t
             ].tolist()
 
         # Check finish condition
