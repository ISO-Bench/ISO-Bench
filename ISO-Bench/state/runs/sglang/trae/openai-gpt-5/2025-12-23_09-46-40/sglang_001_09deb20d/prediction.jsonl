{"instance_id": "/home/ubuntu/OmniPerf-Bench/sglan__repo-33b242d", "model_patch": "diff --git a/model_patch.diff b/model_patch.diff\nnew file mode 100644\nindex 000000000..e69de29bb\ndiff --git a/python/sglang/srt/layers/logits_processor.py b/python/sglang/srt/layers/logits_processor.py\nindex f95c30786..d57460ca2 100644\n--- a/python/sglang/srt/layers/logits_processor.py\n+++ b/python/sglang/srt/layers/logits_processor.py\n@@ -1,5 +1,6 @@\n import torch\n from torch import nn\n+\n from vllm.distributed import (\n     get_tensor_model_parallel_world_size,\n     tensor_model_parallel_all_gather,\n@@ -13,6 +14,52 @@ class LogitsProcessor(nn.Module):\n         super().__init__()\n         self.config = config\n         self.tp_size = get_tensor_model_parallel_world_size()\n+        # Pre-allocate commonly used tensors to avoid repeated allocations\n+        self._zero_tensor_cache = None\n+        self._arange_cache = None\n+        self._arange_cache_size = 0\n+        # Buffer for shifted input_ids to avoid cat allocations\n+        self._shift_buf = None\n+        self._shift_buf_size = 0\n+        self._shift_buf_dtype = None\n+        self._shift_buf_device = None\n+\n+\n+    def _get_zero_tensor(self):\n+        \"\"\"Get or create a cached zero tensor to avoid repeated allocations.\"\"\"\n+        if self._zero_tensor_cache is None:\n+            self._zero_tensor_cache = torch.tensor([0], device=\"cuda\")\n+        return self._zero_tensor_cache\n+\n+\n+    def _get_shifted_ids(self, input_ids, pad_last_value: int = 0):\n+        \"\"\"Return a view where out[i] = input_ids[i+1] and out[-1] = pad_last_value.\n+        Reuse a cached buffer to avoid repeated allocations.\"\"\"\n+        n = input_ids.shape[0]\n+        device = input_ids.device\n+        dtype = input_ids.dtype\n+        if (\n+            self._shift_buf is None\n+            or self._shift_buf_size < n\n+            or self._shift_buf_device != device\n+            or self._shift_buf_dtype != dtype\n+        ):\n+            self._shift_buf = torch.empty(n, device=device, dtype=dtype)\n+            self._shift_buf_size = n\n+            self._shift_buf_device = device\n+            self._shift_buf_dtype = dtype\n+        out = self._shift_buf[:n]\n+        out[:-1] = input_ids[1:]\n+        out[-1] = pad_last_value\n+        return out\n+\n+    def _get_arange(self, size):\n+        \"\"\"Get or create a cached arange tensor to avoid repeated allocations.\"\"\"\n+        if self._arange_cache is None or self._arange_cache_size < size:\n+            self._arange_cache = torch.arange(size, device=\"cuda\")\n+            self._arange_cache_size = size\n+            return self._arange_cache\n+        return self._arange_cache[:size]\n \n     def _get_normalized_prompt_logprobs(\n         self, prefill_token_logprobs, input_metadata: InputMetadata\n@@ -21,10 +68,12 @@ class LogitsProcessor(nn.Module):\n             prefill_token_logprobs, dim=0, dtype=torch.float32\n         )\n \n-        start = input_metadata.extend_start_loc.clone()\n+        # Avoid clone by computing directly without modifying input_metadata\n+        start = input_metadata.extend_start_loc\n         end = start + input_metadata.extend_seq_lens - 2\n-        start.clamp_(min=0, max=prefill_token_logprobs.shape[0] - 1)\n-        end.clamp_(min=0, max=prefill_token_logprobs.shape[0] - 1)\n+        # Use torch.clamp instead of clamp_ to avoid in-place modification\n+        start = start.clamp(min=0, max=prefill_token_logprobs.shape[0] - 1)\n+        end = end.clamp(min=0, max=prefill_token_logprobs.shape[0] - 1)\n         sum_logp = (\n             logprobs_cumsum[end]\n             - logprobs_cumsum[start]\n@@ -38,13 +87,24 @@ class LogitsProcessor(nn.Module):\n \n     def _get_top_logprobs(self, all_logprobs, input_metadata: InputMetadata):\n         if input_metadata.forward_mode == ForwardMode.DECODE:\n+            # Batched top-k with max k to avoid per-row kernel launches\n+            ks = input_metadata.top_logprobs_nums\n+            if isinstance(ks, torch.Tensor):\n+                k_max = int(ks.max().item())\n+            else:\n+                try:\n+                    k_max = int(max(ks))\n+                except Exception:\n+                    k_max = int(ks)\n+            if k_max <= 0:\n+                return None, [[] for _ in range(all_logprobs.shape[0])]\n+            t_vals, t_idx = all_logprobs.topk(k_max, dim=-1)\n+            t_vals = t_vals.cpu().tolist()\n+            t_idx = t_idx.cpu().tolist()\n             decode_top_logprobs = []\n             for i in range(all_logprobs.shape[0]):\n-                k = input_metadata.top_logprobs_nums[i]\n-                t = all_logprobs[i].topk(k)\n-                v_cpu = t.values.cpu().tolist()\n-                p_cpu = t.indices.cpu().tolist()\n-                decode_top_logprobs.append(list(zip(v_cpu, p_cpu)))\n+                k = ks[i]\n+                decode_top_logprobs.append(list(zip(t_vals[i][:k], t_idx[i][:k])))\n             return None, decode_top_logprobs\n         else:\n             prefill_top_logprobs, decode_top_logprobs = [], []\n@@ -98,7 +158,8 @@ class LogitsProcessor(nn.Module):\n                     all_logits = tensor_model_parallel_all_gather(all_logits)\n                 all_logits = all_logits[:, : self.config.vocab_size]\n \n-            all_logprobs = torch.log(torch.softmax(all_logits.float(), dim=-1) + 1e-6)\n+            # Use log_softmax for better numerical stability and performance\n+            all_logprobs = torch.nn.functional.log_softmax(all_logits.float(), dim=-1)\n \n             prefill_top_logprobs, decode_top_logprobs = self._get_top_logprobs(\n                 all_logprobs, input_metadata\n@@ -119,9 +180,12 @@ class LogitsProcessor(nn.Module):\n \n                 # Compute the logprobs and normalized logprobs for the prefill tokens.\n                 # Note that we pad a zero at the end of each sequence for easy computation.\n+                # Use cached tensors to avoid repeated allocations\n+                arange_indices = self._get_arange(all_logprobs.shape[0])\n+                shifted_ids = self._get_shifted_ids(input_ids, pad_last_value=0)\n                 prefill_token_logprobs = all_logprobs[\n-                    torch.arange(all_logprobs.shape[0], device=\"cuda\"),\n-                    torch.cat([input_ids[1:], torch.tensor([0], device=\"cuda\")]),\n+                    arange_indices,\n+                    shifted_ids,\n                 ]\n \n                 normalized_prompt_logprobs = self._get_normalized_prompt_logprobs(\ndiff --git a/python/sglang/srt/managers/router/model_rpc.py b/python/sglang/srt/managers/router/model_rpc.py\nindex f283635c3..d8aaae1f8 100644\n--- a/python/sglang/srt/managers/router/model_rpc.py\n+++ b/python/sglang/srt/managers/router/model_rpc.py\n@@ -103,8 +103,10 @@ class ModelRpcServer:\n                 else server_args.max_prefill_num_token\n             ),\n         )\n+        # Optimize tensor creation with explicit dtype and device\n+        logit_bias_list = get_int_token_logit_bias(self.tokenizer, self.model_config.vocab_size)\n         self.int_token_logit_bias = torch.tensor(\n-            get_int_token_logit_bias(self.tokenizer, self.model_config.vocab_size)\n+            logit_bias_list, dtype=torch.float32, device=\"cpu\"\n         )\n         set_random_seed(server_args.random_seed)\n         logger.info(\n@@ -150,6 +152,18 @@ class ModelRpcServer:\n         self.min_new_token_ratio = min(0.2 * server_args.schedule_conservativeness, 1.0)\n         self.new_token_ratio_step = (0.0001, 0.05)  # (down, up)\n \n+        # Cache for torch.arange to avoid repeated allocations\n+        self._arange_cache = None\n+        self._arange_cache_size = 0\n+\n+    def _get_arange(self, size, device=\"cuda\"):\n+        \"\"\"Get or create a cached arange tensor to avoid repeated allocations.\"\"\"\n+        if self._arange_cache is None or self._arange_cache_size < size:\n+            self._arange_cache = torch.arange(size, device=device)\n+            self._arange_cache_size = size\n+            return self._arange_cache\n+        return self._arange_cache[:size]\n+\n     def flush_cache(self):\n         if len(self.forward_queue) == 0 and (\n             self.running_batch is None or len(self.running_batch.reqs) == 0\n@@ -159,6 +173,9 @@ class ModelRpcServer:\n             self.regex_fsm_cache.reset()\n             self.req_to_token_pool.clear()\n             self.token_to_kv_pool.clear()\n+            # Clear arange cache\n+            self._arange_cache = None\n+            self._arange_cache_size = 0\n             torch.cuda.empty_cache()\n             logger.info(\"Cache flushed successfully!\")\n         else:\n@@ -313,10 +330,8 @@ class ModelRpcServer:\n         )\n         if self.running_batch:\n             available_size -= sum(\n-                [\n-                    (r.max_new_tokens() - len(r.output_ids)) * self.new_token_ratio\n-                    for r in self.running_batch.reqs\n-                ]\n+                (r.max_new_tokens() - len(r.output_ids)) * self.new_token_ratio\n+                for r in self.running_batch.reqs\n             )\n \n         for req in self.forward_queue:\n@@ -436,8 +451,11 @@ class ModelRpcServer:\n         reqs = batch.reqs\n         last_token_logprobs = None\n         if last_logprobs is not None:\n+            # Use cached arange to avoid repeated allocations\n+            arange_indices = self._get_arange(len(reqs), device=last_logprobs.device)\n+            next_ids_t = torch.tensor(next_token_ids, device=last_logprobs.device)\n             last_token_logprobs = (\n-                last_logprobs[torch.arange(len(reqs)), next_token_ids].cpu().tolist()\n+                last_logprobs[arange_indices, next_ids_t].cpu().tolist()\n             )\n \n         # Check finish condition\n@@ -530,8 +548,11 @@ class ModelRpcServer:\n         reqs = batch.reqs\n         new_token_logprobs = None\n         if last_logprobs is not None:\n+            # Use cached arange to avoid repeated allocations\n+            arange_indices = self._get_arange(len(reqs), device=last_logprobs.device)\n+            next_ids_t = torch.tensor(next_token_ids, device=last_logprobs.device)\n             new_token_logprobs = last_logprobs[\n-                torch.arange(len(reqs)), next_token_ids\n+                arange_indices, next_ids_t\n             ].tolist()\n \n         # Check finish condition\n", "model_name_or_path": "gpt-5-2025-08-07"}
