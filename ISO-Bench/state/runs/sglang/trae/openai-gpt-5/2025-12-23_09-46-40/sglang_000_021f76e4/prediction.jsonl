{"instance_id": "/home/ubuntu/OmniPerf-Bench/sglan__repo-777688b", "model_patch": "diff --git a/model_patch.diff b/model_patch.diff\nnew file mode 100644\nindex 000000000..e69de29bb\ndiff --git a/python/sglang/srt/lora/lora_manager.py b/python/sglang/srt/lora/lora_manager.py\nindex 45050df53..2fb941c30 100644\n--- a/python/sglang/srt/lora/lora_manager.py\n+++ b/python/sglang/srt/lora/lora_manager.py\n@@ -71,23 +71,29 @@ class LoRAManager:\n \n         self.init_loras()\n         self.init_lora_memory_pool()\n+        # Preallocate reusable buffers for per-batch LoRA metadata to avoid reallocations\n+        self._lora_ranks_buf_i32 = torch.empty(self.max_loras_per_batch, dtype=torch.int32, device=self.device)\n+        self._lora_ranks_buf_i64 = torch.empty(self.max_loras_per_batch, dtype=torch.int64, device=self.device)\n+        self._scalings_buf = torch.empty(self.max_loras_per_batch, dtype=torch.float32, device=self.device)\n \n     def init_cuda_graph_batch_info(self, max_bs_in_cuda_graph: int):\n         self.max_bs_in_cuda_graph = max_bs_in_cuda_graph\n         with torch.device(\"cuda\"):\n             self.cuda_graph_batch_info = LoRABatchInfo(\n                 bs=self.max_bs_in_cuda_graph,\n-                seg_lens=torch.zeros(self.max_bs_in_cuda_graph, dtype=torch.int32),\n-                seg_indptr=torch.zeros(\n+                seg_lens=torch.empty(self.max_bs_in_cuda_graph, dtype=torch.int32),\n+                seg_indptr=torch.empty(\n                     self.max_bs_in_cuda_graph + 1, dtype=torch.int32\n                 ),\n                 max_len=0,\n-                weight_indices=torch.zeros(\n+                weight_indices=torch.empty(\n                     self.max_bs_in_cuda_graph, dtype=torch.int32\n                 ),\n-                lora_ranks=torch.zeros(self.max_loras_per_batch, dtype=torch.int32),\n-                scalings=torch.zeros(self.max_loras_per_batch, dtype=torch.float),\n+                lora_ranks=torch.empty(self.max_loras_per_batch, dtype=torch.int32),\n+                scalings=torch.empty(self.max_loras_per_batch, dtype=torch.float32),\n             )\n+            # Cache a tensor of ones to avoid per-iteration fill_\n+            self._ones_cuda_graph = torch.ones(self.max_bs_in_cuda_graph, dtype=torch.int32, device=\"cuda\")\n \n     def init_loras(self):\n         # Config of each LoRA adapter\n@@ -167,7 +173,8 @@ class LoRAManager:\n             # Do in-place updates when CUDA graph is enabled and the batch forward mode\n             # could use CUDA graph.\n             self.cuda_graph_batch_info.bs = bs\n-            self.cuda_graph_batch_info.seg_lens[:bs].fill_(1)\n+            self.cuda_graph_batch_info.seg_lens[:bs].copy_(self._ones_cuda_graph[:bs])\n+            self.cuda_graph_batch_info.seg_indptr[0] = 0\n             torch.cumsum(\n                 self.cuda_graph_batch_info.seg_lens[:bs],\n                 dim=0,\n@@ -175,36 +182,38 @@ class LoRAManager:\n             )\n             self.cuda_graph_batch_info.max_len = 1\n \n+            lora_ranks = self._lora_ranks_buf_i32\n+            scalings = self._scalings_buf\n+            # Reset only the active indices; avoid reallocation\n+            lora_ranks.zero_()\n+            scalings.zero_()\n             for i, lora_path in enumerate(forward_batch.lora_paths):\n                 self.cuda_graph_batch_info.weight_indices[i] = (\n                     self.memory_pool.get_buffer_id(lora_path)\n                 )\n                 if lora_path is not None:\n                     lora = self.loras[lora_path]\n-                    self.cuda_graph_batch_info.lora_ranks[\n-                        self.cuda_graph_batch_info.weight_indices[i]\n-                    ] = lora.config.hf_config[\"r\"]\n-                    self.cuda_graph_batch_info.scalings[\n-                        self.cuda_graph_batch_info.weight_indices[i]\n-                    ] = lora.scaling\n+                    lora_ranks[self.cuda_graph_batch_info.weight_indices[i]] = lora.config.hf_config[\"r\"]\n+                    scalings[self.cuda_graph_batch_info.weight_indices[i]] = lora.scaling\n+            self.cuda_graph_batch_info.lora_ranks.copy_(lora_ranks)\n+            self.cuda_graph_batch_info.scalings.copy_(scalings)\n             batch_info = self.cuda_graph_batch_info\n         else:\n             seg_lens = (\n                 forward_batch.extend_seq_lens\n                 if forward_batch.forward_mode.is_extend()\n-                else torch.ones(bs, device=self.device)\n+                else torch.ones(bs, dtype=torch.int32, device=self.device)\n             )\n-            seg_indptr = torch.zeros((bs + 1,), dtype=torch.int32, device=self.device)\n+            seg_indptr = torch.empty((bs + 1,), dtype=torch.int32, device=self.device)\n+            seg_indptr[0] = 0\n             seg_indptr[1:] = torch.cumsum(seg_lens, dim=0)\n             max_len = int(torch.max(seg_lens))\n             weight_indices = torch.empty((bs,), dtype=torch.int64, device=self.device)\n \n-            lora_ranks = torch.zeros(\n-                (self.max_loras_per_batch,), dtype=torch.int64, device=\"cuda\"\n-            )\n-            scalings = torch.zeros(\n-                (self.max_loras_per_batch,), dtype=torch.float, device=\"cuda\"\n-            )\n+            lora_ranks = self._lora_ranks_buf_i64\n+            scalings = self._scalings_buf\n+            lora_ranks.zero_()\n+            scalings.zero_()\n             for i, lora_path in enumerate(forward_batch.lora_paths):\n                 weight_indices[i] = self.memory_pool.get_buffer_id(lora_path)\n                 if lora_path is not None:\n@@ -217,8 +226,8 @@ class LoRAManager:\n                 seg_indptr=seg_indptr,\n                 max_len=max_len,\n                 weight_indices=weight_indices,\n-                lora_ranks=lora_ranks,\n-                scalings=scalings,\n+                lora_ranks=lora_ranks.clone(),\n+                scalings=scalings.clone(),\n             )\n         self.lora_backend.set_batch_info(batch_info)\n \ndiff --git a/python/sglang/srt/lora/mem_pool.py b/python/sglang/srt/lora/mem_pool.py\nindex 8b8d21332..2014c65b8 100644\n--- a/python/sglang/srt/lora/mem_pool.py\n+++ b/python/sglang/srt/lora/mem_pool.py\n@@ -165,7 +165,7 @@ class LoRAMemoryPool:\n         if uid is None:\n             for i in range(self.num_layer):\n                 for k in self.A_buffer.keys():\n-                    self.A_buffer[k][i][buffer_id] = 0\n+                    self.A_buffer[k][i][buffer_id].zero_()\n             return\n \n         assert lora_adapter is not None\n", "model_name_or_path": "gpt-5-2025-08-07"}
