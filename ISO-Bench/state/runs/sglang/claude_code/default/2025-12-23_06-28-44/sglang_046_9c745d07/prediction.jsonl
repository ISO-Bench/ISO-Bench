{"instance_id": "/home/ubuntu/OmniPerf-Bench/sglan__repo-ebaa2f3", "model_patch": "diff --git a/python/sglang/srt/constrained/outlines_backend.py b/python/sglang/srt/constrained/outlines_backend.py\nindex cc68b97..fcbe132 100644\n--- a/python/sglang/srt/constrained/outlines_backend.py\n+++ b/python/sglang/srt/constrained/outlines_backend.py\n@@ -60,13 +60,17 @@ class OutlinesGrammar(BaseGrammarObject):\n         suffix_bytes = []\n         continuation_range = range(0x80, 0xC0)\n         cur_state = self.state\n-        while (\n-            len(jump_forward_bytes) and jump_forward_bytes[0][0] in continuation_range\n-        ):\n+        idx = 0\n+        while idx < len(jump_forward_bytes) and jump_forward_bytes[idx][0] in continuation_range:\n             # continuation bytes\n-            byte_edge = jump_forward_bytes.pop(0)\n+            byte_edge = jump_forward_bytes[idx]\n             suffix_bytes.append(byte_edge[0])\n             cur_state = byte_edge[1]\n+            idx += 1\n+\n+        # Remove processed bytes\n+        if idx > 0:\n+            del jump_forward_bytes[:idx]\n \n         suffix_tokens = [f\"<0x{hex(b)[2:].upper()}>\" for b in suffix_bytes]\n         suffix_ids = tokenizer.convert_tokens_to_ids(suffix_tokens)\n@@ -82,7 +86,8 @@ class OutlinesGrammar(BaseGrammarObject):\n         self.state = next_state\n \n     def fill_vocab_mask(self, vocab_mask: torch.Tensor):\n-        vocab_mask.fill_(1)\n+        # Set all positions to 1 (masked) first, then unmask allowed tokens\n+        vocab_mask[:] = 1\n         vocab_mask[self.guide.get_next_instruction(self.state).tokens] = 0\n \n     def copy(self):\ndiff --git a/python/sglang/srt/model_executor/model_runner.py b/python/sglang/srt/model_executor/model_runner.py\nindex 02750d5..e286c7d 100644\n--- a/python/sglang/srt/model_executor/model_runner.py\n+++ b/python/sglang/srt/model_executor/model_runner.py\n@@ -502,8 +502,8 @@ class ModelRunner:\n         \"\"\"We need to run a small matmul to init cublas. Otherwise, it will raise some errors later.\"\"\"\n         dtype = torch.float16\n         device = \"cuda\"\n-        a = torch.ones((16, 16), dtype=dtype, device=device)\n-        b = torch.ones((16, 16), dtype=dtype, device=device)\n+        a = torch.empty((16, 16), dtype=dtype, device=device)\n+        b = torch.empty((16, 16), dtype=dtype, device=device)\n         c = a @ b\n         return c\n \ndiff --git a/python/sglang/srt/sampling/sampling_batch_info.py b/python/sglang/srt/sampling/sampling_batch_info.py\nindex a341c2b..a9bceac 100644\n--- a/python/sglang/srt/sampling/sampling_batch_info.py\n+++ b/python/sglang/srt/sampling/sampling_batch_info.py\n@@ -53,19 +53,19 @@ class SamplingBatchInfo:\n             torch.tensor(\n                 [r.sampling_params.temperature for r in reqs],\n                 dtype=torch.float,\n+                device=device,\n             )\n             .view(-1, 1)\n-            .to(device, non_blocking=True)\n         )\n         top_ps = torch.tensor(\n-            [r.sampling_params.top_p for r in reqs], dtype=torch.float\n-        ).to(device, non_blocking=True)\n+            [r.sampling_params.top_p for r in reqs], dtype=torch.float, device=device\n+        )\n         top_ks = torch.tensor(\n-            [r.sampling_params.top_k for r in reqs], dtype=torch.int32\n-        ).to(device, non_blocking=True)\n+            [r.sampling_params.top_k for r in reqs], dtype=torch.int32, device=device\n+        )\n         min_ps = torch.tensor(\n-            [r.sampling_params.min_p for r in reqs], dtype=torch.float\n-        ).to(device, non_blocking=True)\n+            [r.sampling_params.min_p for r in reqs], dtype=torch.float, device=device\n+        )\n \n         ret = cls(\n             temperatures=temperatures,\n@@ -125,7 +125,7 @@ class SamplingBatchInfo:\n             else:\n                 if self.linear_penalties is None:\n                     bs = self.penalizer_orchestrator.batch.batch_size()\n-                    self.linear_penalties = torch.zeros(\n+                    self.linear_penalties = torch.empty(\n                         (bs, self.vocab_size),\n                         dtype=torch.float32,\n                         device=self.device,\n@@ -137,7 +137,7 @@ class SamplingBatchInfo:\n             self.vocab_mask = None\n             return\n \n-        self.vocab_mask = torch.zeros(\n+        self.vocab_mask = torch.empty(\n             len(self.temperatures),\n             self.vocab_size,\n             dtype=torch.bool,\n@@ -178,11 +178,10 @@ class SamplingBatchInfo:\n                 shape, dtype = lhs.shape[1:], lhs.dtype\n             else:\n                 shape, dtype = rhs.shape[1:], rhs.dtype\n-            with torch.dtype(dtype):\n-                if lhs is None:\n-                    lhs = torch.empty((bs1, *shape), device=device).fill_(default)\n-                if rhs is None:\n-                    rhs = torch.empty((bs2, *shape), device=device).fill_(default)\n+            if lhs is None:\n+                lhs = torch.full((bs1, *shape), default, dtype=dtype, device=device)\n+            if rhs is None:\n+                rhs = torch.full((bs2, *shape), default, dtype=dtype, device=device)\n             return torch.cat([lhs, rhs])\n \n         return None\n", "model_name_or_path": "gpt-5-2025-08-07"}
