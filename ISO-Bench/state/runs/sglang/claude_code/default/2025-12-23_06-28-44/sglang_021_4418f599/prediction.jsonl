{"instance_id": "/home/ubuntu/OmniPerf-Bench/sglan__repo-04f2abc", "model_patch": "diff --git a/python/sglang/srt/models/deepseek_v2.py b/python/sglang/srt/models/deepseek_v2.py\nindex b299c3037..70778f909 100644\n--- a/python/sglang/srt/models/deepseek_v2.py\n+++ b/python/sglang/srt/models/deepseek_v2.py\n@@ -323,8 +323,8 @@ class DeepseekV2MoE(nn.Module):\n         self, hidden_states: torch.Tensor, forward_mode: ForwardMode\n     ) -> torch.Tensor:\n         shared_output = None\n-        topk_idx = torch.full(\n-            (0, self.top_k), -1, dtype=torch.int, device=hidden_states.device\n+        topk_idx = torch.empty(\n+            (0, self.top_k), dtype=torch.int, device=hidden_states.device\n         )\n         topk_weights = torch.empty(\n             (0, self.top_k), dtype=torch.float32, device=hidden_states.device\n@@ -671,9 +671,7 @@ class DeepseekV2AttentionMLA(nn.Module):\n         k_pe = latent_cache[:, :, self.kv_lora_rank :]\n         q_pe, k_pe = self.rotary_emb(positions, q_pe, k_pe)\n         q[..., self.qk_nope_head_dim :] = q_pe\n-        k = torch.empty_like(q)\n-        k[..., : self.qk_nope_head_dim] = k_nope\n-        k[..., self.qk_nope_head_dim :] = k_pe\n+        k = torch.cat([k_nope, k_pe], dim=-1)\n \n         latent_cache[:, :, : self.kv_lora_rank] = kv_a.unsqueeze(1)\n         latent_cache[:, :, self.kv_lora_rank :] = k_pe\n@@ -974,17 +972,7 @@ class DeepseekV2AttentionMLA(nn.Module):\n             v = kv[..., self.qk_nope_head_dim :]\n             k_nope = kv[..., : self.qk_nope_head_dim]\n \n-            k = torch.empty(\n-                (\n-                    k_nope.shape[0],\n-                    self.num_local_heads,\n-                    self.qk_nope_head_dim + self.qk_rope_head_dim,\n-                ),\n-                dtype=v.dtype,\n-                device=v.device,\n-            )\n-            k[..., : self.qk_nope_head_dim] = k_nope\n-            k[..., self.qk_nope_head_dim :] = k_pe\n+            k = torch.cat([k_nope, k_pe], dim=-1)\n \n             output, lse = self.attn_mha(q, k, v, forward_batch, save_kv_cache=False)\n             lse = torch.transpose(lse, 0, 1).contiguous()\n@@ -1029,9 +1017,7 @@ class DeepseekV2AttentionMLA(nn.Module):\n \n         q_pe, k_pe = self.rotary_emb(positions, q_pe, k_pe)\n         q[..., self.qk_nope_head_dim :] = q_pe\n-        k = torch.empty_like(q)\n-        k[..., : self.qk_nope_head_dim] = k_nope\n-        k[..., self.qk_nope_head_dim :] = k_pe\n+        k = torch.cat([k_nope, k_pe], dim=-1)\n \n         latent_cache[:, :, : self.kv_lora_rank] = kv_a.unsqueeze(1)\n         latent_cache[:, :, self.kv_lora_rank :] = k_pe\n", "model_name_or_path": "gpt-5-2025-08-07"}
