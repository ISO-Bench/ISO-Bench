{"instance_id": "/home/ubuntu/OmniPerf-Bench/sglan__repo-87fafa0", "model_patch": "diff --git a/python/sglang/srt/layers/moe/ep_moe/kernels.py b/python/sglang/srt/layers/moe/ep_moe/kernels.py\nindex 30c9eb6a7..4a1a9d644 100644\n--- a/python/sglang/srt/layers/moe/ep_moe/kernels.py\n+++ b/python/sglang/srt/layers/moe/ep_moe/kernels.py\n@@ -145,7 +145,8 @@ def compute_seg_indptr_triton_kernel(reorder_topk_ids, seg_indptr, num_toks):\n \n def run_moe_ep_preproess(topk_ids: torch.Tensor, num_experts: int):\n     reorder_topk_ids, reorder_ids = torch.sort(topk_ids.view(-1), stable=True)\n-    seg_indptr = torch.zeros(num_experts + 1, device=topk_ids.device, dtype=torch.int64)\n+    seg_indptr = torch.empty(num_experts + 1, device=topk_ids.device, dtype=torch.int64)\n+    seg_indptr[0] = 0\n     src2dst = torch.empty(topk_ids.numel(), device=topk_ids.device, dtype=torch.int32)\n \n     compute_seg_indptr_triton_kernel[(num_experts,)](\n@@ -331,7 +332,6 @@ def post_reorder_triton_kernel(\n     topk_ids_ptr = topk_ids_ptr + src_idx * topk\n     topk_weights_ptr = topk_weights_ptr + src_idx * topk\n \n-    computed = False\n     store_ptr = output_ptr + src_idx * hidden_size\n     for start_offset in tl.range(0, hidden_size, BLOCK_SIZE):\n         offset = start_offset + tl.arange(0, BLOCK_SIZE)\n@@ -341,7 +341,6 @@ def post_reorder_triton_kernel(\n         for idx in range(topk):\n             expert_id = tl.load(topk_ids_ptr + idx)\n             if expert_id >= start_expert_id and expert_id <= end_expert_id:\n-                computed = True\n                 dst_idx = tl.load(src2dst_ptr + idx)\n                 weigh_scale = tl.load(topk_weights_ptr + idx).to(InDtype)\n                 load_ptr = down_output_ptr + dst_idx * hidden_size\n@@ -349,14 +348,6 @@ def post_reorder_triton_kernel(\n                 sum_vec += in_data * weigh_scale\n         tl.store(store_ptr + offset, sum_vec, mask=mask)\n \n-    if computed == False:\n-        for start_offset in tl.range(0, hidden_size, BLOCK_SIZE):\n-            offset = start_offset + tl.arange(0, BLOCK_SIZE)\n-            mask = offset < hidden_size\n-            tl.store(\n-                store_ptr + offset, tl.zeros([BLOCK_SIZE], dtype=InDtype), mask=mask\n-            )\n-\n \n @triton.jit\n def compute_m_range(\n@@ -529,7 +520,8 @@ def grouped_gemm_triton(\n         \"BLOCK_SIZE_K\": 128,\n     }\n \n-    m_num_tiles_indptr = torch.zeros(batch_size + 1, device=a.device, dtype=torch.int64)\n+    m_num_tiles_indptr = torch.empty(batch_size + 1, device=a.device, dtype=torch.int64)\n+    m_num_tiles_indptr[0] = 0\n     compute_m_num_tiles_indptr[(1,)](\n         m_num_tiles_indptr, seg_indptr, batch_size, config[\"BLOCK_SIZE_M\"]\n     )\ndiff --git a/python/sglang/srt/layers/moe/ep_moe/token_dispatcher.py b/python/sglang/srt/layers/moe/ep_moe/token_dispatcher.py\nindex 6b67f6cea..57ac3fbdf 100644\n--- a/python/sglang/srt/layers/moe/ep_moe/token_dispatcher.py\n+++ b/python/sglang/srt/layers/moe/ep_moe/token_dispatcher.py\n@@ -226,9 +226,10 @@ class DeepEPDispatcher:\n             reorder_topk_ids = torch.empty(\n                 (0,), device=hidden_states.device, dtype=torch.int64\n             )\n-            seg_indptr = torch.zeros(\n+            seg_indptr = torch.empty(\n                 (num_experts + 1,), device=hidden_states.device, dtype=torch.int64\n             )\n+            seg_indptr[:] = 0\n         return hidden_states, reorder_topk_ids, seg_indptr\n \n     def dispatch_normal(\n@@ -366,7 +367,7 @@ class DeepEPDispatcher:\n                     BLOCK_SIZE=512,\n                 )\n             else:\n-                output = torch.zeros(\n+                output = torch.empty(\n                     (0, hidden_states.shape[1]),\n                     device=hidden_states.device,\n                     dtype=hidden_states.dtype,\ndiff --git a/python/sglang/srt/managers/schedule_batch.py b/python/sglang/srt/managers/schedule_batch.py\nindex ab8b81602..2f9770d41 100644\n--- a/python/sglang/srt/managers/schedule_batch.py\n+++ b/python/sglang/srt/managers/schedule_batch.py\n@@ -921,14 +921,14 @@ class ScheduleBatch(ScheduleBatchDisaggregationDecodeMixin):\n         )\n \n         if not decoder_out_cache_loc:\n-            self.out_cache_loc = torch.zeros(0, dtype=torch.int64).to(\n+            self.out_cache_loc = torch.empty(0, dtype=torch.int64).to(\n                 self.device, non_blocking=True\n             )\n         else:\n             self.out_cache_loc = torch.cat(decoder_out_cache_loc)\n \n         if not encoder_out_cache_loc:\n-            self.encoder_out_cache_loc = torch.zeros(0, dtype=torch.int64).to(\n+            self.encoder_out_cache_loc = torch.empty(0, dtype=torch.int64).to(\n                 self.device, non_blocking=True\n             )\n         else:\ndiff --git a/python/sglang/srt/models/deepseek_v2.py b/python/sglang/srt/models/deepseek_v2.py\nindex 37760407b..3fc95484f 100644\n--- a/python/sglang/srt/models/deepseek_v2.py\n+++ b/python/sglang/srt/models/deepseek_v2.py\n@@ -276,8 +276,8 @@ class DeepseekV2MoE(nn.Module):\n         self, hidden_states: torch.Tensor, forward_mode: ForwardMode\n     ) -> torch.Tensor:\n         shared_output = None\n-        topk_idx = torch.full(\n-            (0, self.top_k), -1, dtype=torch.int, device=hidden_states.device\n+        topk_idx = torch.empty(\n+            (0, self.top_k), dtype=torch.int, device=hidden_states.device\n         )\n         topk_weights = torch.empty(\n             (0, self.top_k), dtype=torch.float32, device=hidden_states.device\n", "model_name_or_path": "gpt-5-2025-08-07"}
