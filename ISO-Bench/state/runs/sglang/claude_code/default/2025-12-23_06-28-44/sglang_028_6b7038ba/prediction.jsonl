{"instance_id": "/home/ubuntu/OmniPerf-Bench/sglan__repo-57eec0b", "model_patch": "diff --git a/python/sglang/srt/entrypoints/http_server.py b/python/sglang/srt/entrypoints/http_server.py\nindex 5a97072..2204d94 100644\n--- a/python/sglang/srt/entrypoints/http_server.py\n+++ b/python/sglang/srt/entrypoints/http_server.py\n@@ -197,12 +197,11 @@ async def health_generate(request: Request) -> Response:\n @app.get(\"/get_model_info\")\n async def get_model_info():\n     \"\"\"Get the model information.\"\"\"\n-    result = {\n+    return {\n         \"model_path\": _global_state.tokenizer_manager.model_path,\n         \"tokenizer_path\": _global_state.tokenizer_manager.server_args.tokenizer_path,\n         \"is_generation\": _global_state.tokenizer_manager.is_generation,\n     }\n-    return result\n \n \n @app.get(\"/get_server_info\")\n@@ -227,22 +226,23 @@ async def set_internal_state(obj: SetInternalStateReq, request: Request):\n async def generate_request(obj: GenerateReqInput, request: Request):\n     \"\"\"Handle a generate request.\"\"\"\n     if obj.stream:\n+        # Pre-compute static byte strings\n+        data_prefix = b\"data: \"\n+        data_suffix = b\"\\n\\n\"\n+        done_msg = b\"data: [DONE]\\n\\n\"\n \n         async def stream_results() -> AsyncIterator[bytes]:\n             try:\n                 async for out in _global_state.tokenizer_manager.generate_request(\n                     obj, request\n                 ):\n-                    yield b\"data: \" + orjson.dumps(\n-                        out, option=orjson.OPT_NON_STR_KEYS\n-                    ) + b\"\\n\\n\"\n+                    yield data_prefix + orjson.dumps(out, option=orjson.OPT_NON_STR_KEYS) + data_suffix\n             except ValueError as e:\n-                out = {\"error\": {\"message\": str(e)}}\n                 logger.error(f\"Error: {e}\")\n-                yield b\"data: \" + orjson.dumps(\n-                    out, option=orjson.OPT_NON_STR_KEYS\n-                ) + b\"\\n\\n\"\n-            yield b\"data: [DONE]\\n\\n\"\n+                yield data_prefix + orjson.dumps(\n+                    {\"error\": {\"message\": str(e)}}, option=orjson.OPT_NON_STR_KEYS\n+                ) + data_suffix\n+            yield done_msg\n \n         return StreamingResponse(\n             stream_results(),\n@@ -354,16 +354,10 @@ async def update_weights_from_disk(obj: UpdateWeightFromDiskReqInput, request: R\n         \"message\": message,\n         \"num_paused_requests\": num_paused_requests,\n     }\n-    if success:\n-        return ORJSONResponse(\n-            content,\n-            status_code=HTTPStatus.OK,\n-        )\n-    else:\n-        return ORJSONResponse(\n-            content,\n-            status_code=HTTPStatus.BAD_REQUEST,\n-        )\n+    return ORJSONResponse(\n+        content,\n+        status_code=HTTPStatus.OK if success else HTTPStatus.BAD_REQUEST,\n+    )\n \n \n @app.post(\"/init_weights_update_group\")\n@@ -374,11 +368,10 @@ async def init_weights_update_group(\n     success, message = await _global_state.tokenizer_manager.init_weights_update_group(\n         obj, request\n     )\n-    content = {\"success\": success, \"message\": message}\n-    if success:\n-        return ORJSONResponse(content, status_code=200)\n-    else:\n-        return ORJSONResponse(content, status_code=HTTPStatus.BAD_REQUEST)\n+    return ORJSONResponse(\n+        {\"success\": success, \"message\": message},\n+        status_code=200 if success else HTTPStatus.BAD_REQUEST\n+    )\n \n \n @app.post(\"/update_weights_from_distributed\")\n@@ -391,11 +384,10 @@ async def update_weights_from_distributed(\n             obj, request\n         )\n     )\n-    content = {\"success\": success, \"message\": message}\n-    if success:\n-        return ORJSONResponse(content, status_code=200)\n-    else:\n-        return ORJSONResponse(content, status_code=HTTPStatus.BAD_REQUEST)\n+    return ORJSONResponse(\n+        {\"success\": success, \"message\": message},\n+        status_code=200 if success else HTTPStatus.BAD_REQUEST\n+    )\n \n \n @app.api_route(\"/get_weights_by_name\", methods=[\"GET\", \"POST\"])\n@@ -403,10 +395,7 @@ async def get_weights_by_name(obj: GetWeightsByNameReqInput, request: Request):\n     \"\"\"Get model parameter by name.\"\"\"\n     try:\n         ret = await _global_state.tokenizer_manager.get_weights_by_name(obj, request)\n-        if ret is None:\n-            return _create_error_response(\"Get parameter by name failed\")\n-        else:\n-            return ORJSONResponse(ret, status_code=200)\n+        return _create_error_response(\"Get parameter by name failed\") if ret is None else ORJSONResponse(ret, status_code=200)\n     except Exception as e:\n         return _create_error_response(e)\n \n@@ -438,11 +427,9 @@ async def open_session(obj: OpenSessionReqInput, request: Request):\n     \"\"\"Open a session, and return its unique session id.\"\"\"\n     try:\n         session_id = await _global_state.tokenizer_manager.open_session(obj, request)\n-        if session_id is None:\n-            raise Exception(\n-                \"Failed to open the session. Check if a session with the same id is still open.\"\n-            )\n-        return session_id\n+        return session_id if session_id is not None else _create_error_response(\n+            \"Failed to open the session. Check if a session with the same id is still open.\"\n+        )\n     except Exception as e:\n         return _create_error_response(e)\n \n@@ -469,21 +456,20 @@ async def parse_function_call_request(obj: ParseFunctionCallReq, request: Reques\n     \"\"\"\n     A native API endpoint to parse function calls from a text.\n     \"\"\"\n-    # 1) Initialize the parser based on the request body\n+    # Initialize the parser based on the request body\n     parser = FunctionCallParser(tools=obj.tools, tool_call_parser=obj.tool_call_parser)\n \n-    # 2) Call the non-stream parsing method (non-stream)\n+    # Call the non-stream parsing method (non-stream)\n     normal_text, calls = parser.parse_non_stream(obj.text)\n \n-    # 3) Organize the response content\n-    response_data = {\n-        \"normal_text\": normal_text,\n-        \"calls\": [\n-            call.model_dump() for call in calls\n-        ],  # Convert pydantic objects to dictionaries\n-    }\n-\n-    return ORJSONResponse(content=response_data, status_code=200)\n+    # Organize the response content\n+    return ORJSONResponse(\n+        content={\n+            \"normal_text\": normal_text,\n+            \"calls\": [call.model_dump() for call in calls],\n+        },\n+        status_code=200\n+    )\n \n \n @app.post(\"/separate_reasoning\")\n@@ -491,19 +477,20 @@ async def separate_reasoning_request(obj: SeparateReasoningReqInput, request: Re\n     \"\"\"\n     A native API endpoint to separate reasoning from a text.\n     \"\"\"\n-    # 1) Initialize the parser based on the request body\n+    # Initialize the parser based on the request body\n     parser = ReasoningParser(model_type=obj.reasoning_parser)\n \n-    # 2) Call the non-stream parsing method (non-stream)\n+    # Call the non-stream parsing method (non-stream)\n     reasoning_text, normal_text = parser.parse_non_stream(obj.text)\n \n-    # 3) Organize the response content\n-    response_data = {\n-        \"reasoning_text\": reasoning_text,\n-        \"text\": normal_text,\n-    }\n-\n-    return ORJSONResponse(content=response_data, status_code=200)\n+    # Organize the response content\n+    return ORJSONResponse(\n+        content={\n+            \"reasoning_text\": reasoning_text,\n+            \"text\": normal_text,\n+        },\n+        status_code=200\n+    )\n \n \n ##### OpenAI-compatible API endpoints #####\n@@ -529,9 +516,7 @@ async def openai_v1_embeddings(raw_request: Request):\n def available_models():\n     \"\"\"Show available models.\"\"\"\n     served_model_names = [_global_state.tokenizer_manager.served_model_name]\n-    model_cards = []\n-    for served_model_name in served_model_names:\n-        model_cards.append(ModelCard(id=served_model_name, root=served_model_name))\n+    model_cards = [ModelCard(id=name, root=name) for name in served_model_names]\n     return ModelList(data=model_cards)\n \n \n@@ -593,18 +578,16 @@ async def sagemaker_chat_completions(raw_request: Request):\n async def vertex_generate(vertex_req: VertexGenerateReqInput, raw_request: Request):\n     if not vertex_req.instances:\n         return []\n+\n     inputs = {}\n+    first_instance = vertex_req.instances[0]\n     for input_key in (\"text\", \"input_ids\", \"input_embeds\"):\n-        if vertex_req.instances[0].get(input_key):\n-            inputs[input_key] = [\n-                instance.get(input_key) for instance in vertex_req.instances\n-            ]\n+        if first_instance.get(input_key):\n+            inputs[input_key] = [instance.get(input_key) for instance in vertex_req.instances]\n             break\n-    image_data = [\n-        instance.get(\"image_data\")\n-        for instance in vertex_req.instances\n-        if instance.get(\"image_data\") is not None\n-    ] or None\n+\n+    image_data = [instance.get(\"image_data\") for instance in vertex_req.instances if instance.get(\"image_data\") is not None] or None\n+\n     req = GenerateReqInput(\n         **inputs,\n         image_data=image_data,\n@@ -693,10 +676,8 @@ def _wait_and_warmup(\n     image_token_text: str,\n     launch_callback: Optional[Callable[[], None]] = None,\n ):\n-    headers = {}\n     url = server_args.url()\n-    if server_args.api_key:\n-        headers[\"Authorization\"] = f\"Bearer {server_args.api_key}\"\n+    headers = {\"Authorization\": f\"Bearer {server_args.api_key}\"} if server_args.api_key else {}\n \n     # Wait until the server is launched\n     success = False\n", "model_name_or_path": "gpt-5-2025-08-07"}
