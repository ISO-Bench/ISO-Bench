diff --git a/python/sglang/srt/layers/moe/topk.py b/python/sglang/srt/layers/moe/topk.py
index 8895e6be6..8dda13c64 100644
--- a/python/sglang/srt/layers/moe/topk.py
+++ b/python/sglang/srt/layers/moe/topk.py
@@ -48,15 +48,13 @@ def fused_topk_native(
     assert (
         hidden_states.shape[0] == gating_output.shape[0]
     ), f"Number of tokens mismatch, {hidden_states.shape=} vs {gating_output.shape=}"
-    M, _ = hidden_states.shape
-    topk_weights = torch.empty(
-        M, topk, dtype=torch.float32, device=hidden_states.device
-    )
-    topk_ids = torch.empty(M, topk, dtype=torch.int32, device=hidden_states.device)
+    # Removed unnecessary pre-allocation of topk_weights and topk_ids
+    # They are immediately overwritten by F.softmax and torch.topk
     topk_weights = F.softmax(gating_output.float(), dim=-1)
     topk_weights, topk_ids = torch.topk(topk_weights, topk, dim=-1)
     if renormalize:
-        topk_weights = topk_weights / topk_weights.sum(dim=-1, keepdim=True)
+        # Use in-place division for better performance
+        topk_weights.div_(topk_weights.sum(dim=-1, keepdim=True))
     return topk_weights, topk_ids
 
 
@@ -70,6 +68,7 @@ def fused_topk(
 
     M, _ = hidden_states.shape
 
+    # Pre-allocate output tensors for the kernel
     topk_weights = torch.empty(
         M, topk, dtype=torch.float32, device=hidden_states.device
     )
@@ -87,7 +86,8 @@ def fused_topk(
     del token_expert_indicies
 
     if renormalize:
-        topk_weights = topk_weights / topk_weights.sum(dim=-1, keepdim=True)
+        # Use in-place division for better performance
+        topk_weights.div_(topk_weights.sum(dim=-1, keepdim=True))
 
     return topk_weights, topk_ids
 
@@ -117,7 +117,9 @@ def grouped_topk(
     group_idx = torch.topk(group_scores, k=topk_group, dim=-1, sorted=False)[
         1
     ]  # [n, top_k_group]
-    group_mask = torch.zeros_like(group_scores)  # [n, n_group]
+    # Optimize memory allocation: use empty + fill for better performance
+    group_mask = torch.empty_like(group_scores)  # [n, n_group]
+    group_mask.fill_(0)
     group_mask.scatter_(1, group_idx, 1)  # [n, n_group]
     score_mask = (
         group_mask.unsqueeze(-1)
@@ -142,7 +144,8 @@ def grouped_topk(
             if n_share_experts_fusion == 0
             else topk_weights[:, :-1].sum(dim=-1, keepdim=True)
         )
-        topk_weights = topk_weights / topk_weights_sum
+        # Use in-place division for better performance
+        topk_weights.div_(topk_weights_sum)
 
     topk_weights, topk_ids = topk_weights.to(torch.float32), topk_ids.to(torch.int32)
     topk_ids = topk_ids_logical_to_physical(topk_ids, expert_location_dispatch_info)
@@ -177,7 +180,9 @@ def biased_grouped_topk_impl(
     group_idx = torch.topk(group_scores, k=topk_group, dim=-1, sorted=False)[
         1
     ]  # [n, top_k_group]
-    group_mask = torch.zeros_like(group_scores)  # [n, n_group]
+    # Optimize memory allocation: use empty + fill for better performance
+    group_mask = torch.empty_like(group_scores)  # [n, n_group]
+    group_mask.fill_(0)
     group_mask.scatter_(1, group_idx, 1)  # [n, n_group]
     score_mask = (
         group_mask.unsqueeze(-1)
@@ -206,7 +211,8 @@ def biased_grouped_topk_impl(
             if n_share_experts_fusion == 0
             else topk_weights[:, :-1].sum(dim=-1, keepdim=True)
         )
-        topk_weights = topk_weights / topk_weights_sum
+        # Use in-place division for better performance
+        topk_weights.div_(topk_weights_sum)
 
     topk_weights, topk_ids = topk_weights.to(torch.float32), topk_ids.to(torch.int32)
     topk_ids = topk_ids_logical_to_physical(topk_ids, expert_location_dispatch_info)
