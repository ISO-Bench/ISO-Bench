{"instance_id": "/home/ubuntu/OmniPerf-Bench/sglan__repo-d818966", "model_patch": "diff --git a/python/sglang/srt/layers/moe/topk.py b/python/sglang/srt/layers/moe/topk.py\nindex 8895e6be6..8dda13c64 100644\n--- a/python/sglang/srt/layers/moe/topk.py\n+++ b/python/sglang/srt/layers/moe/topk.py\n@@ -48,15 +48,13 @@ def fused_topk_native(\n     assert (\n         hidden_states.shape[0] == gating_output.shape[0]\n     ), f\"Number of tokens mismatch, {hidden_states.shape=} vs {gating_output.shape=}\"\n-    M, _ = hidden_states.shape\n-    topk_weights = torch.empty(\n-        M, topk, dtype=torch.float32, device=hidden_states.device\n-    )\n-    topk_ids = torch.empty(M, topk, dtype=torch.int32, device=hidden_states.device)\n+    # Removed unnecessary pre-allocation of topk_weights and topk_ids\n+    # They are immediately overwritten by F.softmax and torch.topk\n     topk_weights = F.softmax(gating_output.float(), dim=-1)\n     topk_weights, topk_ids = torch.topk(topk_weights, topk, dim=-1)\n     if renormalize:\n-        topk_weights = topk_weights / topk_weights.sum(dim=-1, keepdim=True)\n+        # Use in-place division for better performance\n+        topk_weights.div_(topk_weights.sum(dim=-1, keepdim=True))\n     return topk_weights, topk_ids\n \n \n@@ -70,6 +68,7 @@ def fused_topk(\n \n     M, _ = hidden_states.shape\n \n+    # Pre-allocate output tensors for the kernel\n     topk_weights = torch.empty(\n         M, topk, dtype=torch.float32, device=hidden_states.device\n     )\n@@ -87,7 +86,8 @@ def fused_topk(\n     del token_expert_indicies\n \n     if renormalize:\n-        topk_weights = topk_weights / topk_weights.sum(dim=-1, keepdim=True)\n+        # Use in-place division for better performance\n+        topk_weights.div_(topk_weights.sum(dim=-1, keepdim=True))\n \n     return topk_weights, topk_ids\n \n@@ -117,7 +117,9 @@ def grouped_topk(\n     group_idx = torch.topk(group_scores, k=topk_group, dim=-1, sorted=False)[\n         1\n     ]  # [n, top_k_group]\n-    group_mask = torch.zeros_like(group_scores)  # [n, n_group]\n+    # Optimize memory allocation: use empty + fill for better performance\n+    group_mask = torch.empty_like(group_scores)  # [n, n_group]\n+    group_mask.fill_(0)\n     group_mask.scatter_(1, group_idx, 1)  # [n, n_group]\n     score_mask = (\n         group_mask.unsqueeze(-1)\n@@ -142,7 +144,8 @@ def grouped_topk(\n             if n_share_experts_fusion == 0\n             else topk_weights[:, :-1].sum(dim=-1, keepdim=True)\n         )\n-        topk_weights = topk_weights / topk_weights_sum\n+        # Use in-place division for better performance\n+        topk_weights.div_(topk_weights_sum)\n \n     topk_weights, topk_ids = topk_weights.to(torch.float32), topk_ids.to(torch.int32)\n     topk_ids = topk_ids_logical_to_physical(topk_ids, expert_location_dispatch_info)\n@@ -177,7 +180,9 @@ def biased_grouped_topk_impl(\n     group_idx = torch.topk(group_scores, k=topk_group, dim=-1, sorted=False)[\n         1\n     ]  # [n, top_k_group]\n-    group_mask = torch.zeros_like(group_scores)  # [n, n_group]\n+    # Optimize memory allocation: use empty + fill for better performance\n+    group_mask = torch.empty_like(group_scores)  # [n, n_group]\n+    group_mask.fill_(0)\n     group_mask.scatter_(1, group_idx, 1)  # [n, n_group]\n     score_mask = (\n         group_mask.unsqueeze(-1)\n@@ -206,7 +211,8 @@ def biased_grouped_topk_impl(\n             if n_share_experts_fusion == 0\n             else topk_weights[:, :-1].sum(dim=-1, keepdim=True)\n         )\n-        topk_weights = topk_weights / topk_weights_sum\n+        # Use in-place division for better performance\n+        topk_weights.div_(topk_weights_sum)\n \n     topk_weights, topk_ids = topk_weights.to(torch.float32), topk_ids.to(torch.int32)\n     topk_ids = topk_ids_logical_to_physical(topk_ids, expert_location_dispatch_info)\n", "model_name_or_path": "gpt-5-2025-08-07"}
