{"instance_id": "/home/ubuntu/OmniPerf-Bench/sglan__repo-58d1082", "model_patch": "diff --git a/python/sglang/srt/lora/lora.py b/python/sglang/srt/lora/lora.py\nindex 379b233..567fc26 100644\n--- a/python/sglang/srt/lora/lora.py\n+++ b/python/sglang/srt/lora/lora.py\n@@ -100,6 +100,7 @@ class MergedColumnParallelLinearWithLoRA(ColumnParallelLinearWithLoRA):\n         self, base_layer: MergedColumnParallelLinear, segment_gemm, lora_rank, scaling\n     ) -> None:\n         super().__init__(base_layer, segment_gemm, lora_rank, scaling)\n+        self.lora_output_cache = None\n \n     def set_lora_info(self, A_buffer, B_buffer, bs, seq_lens, weight_indices):\n         self.set_lora = True\n@@ -120,7 +121,10 @@ class MergedColumnParallelLinearWithLoRA(ColumnParallelLinearWithLoRA):\n         )\n         # FIXME\n         assert lora_a_output.shape[-1] == self.lora_rank * 2\n-        lora_output = torch.empty_like(base_output)\n+        # Cache and reuse lora_output tensor\n+        if self.lora_output_cache is None or self.lora_output_cache.shape != base_output.shape:\n+            self.lora_output_cache = torch.empty_like(base_output)\n+        lora_output = self.lora_output_cache\n         output_dim = lora_output.shape[-1] // 2\n         for i in range(2):\n             left = output_dim * i\n@@ -143,6 +147,7 @@ class QKVParallelLinearWithLoRA(ColumnParallelLinearWithLoRA):\n         self, base_layer: QKVParallelLinear, segment_gemm, lora_rank, scaling\n     ) -> None:\n         super().__init__(base_layer, segment_gemm, lora_rank, scaling)\n+        self.lora_output_cache = None\n \n     def set_lora_info(\n         self, A_buffer_qkv, B_buffer_q, B_buffer_kv, bs, seq_lens, weight_indices\n@@ -165,7 +170,10 @@ class QKVParallelLinearWithLoRA(ColumnParallelLinearWithLoRA):\n             weight_indices=self.weight_indices,\n         )\n         # FIXME parallelize qkv\n-        lora_output = torch.empty_like(base_output)\n+        # Cache and reuse lora_output tensor\n+        if self.lora_output_cache is None or self.lora_output_cache.shape != base_output.shape:\n+            self.lora_output_cache = torch.empty_like(base_output)\n+        lora_output = self.lora_output_cache\n         # q\n         output_dim_q = self.B_buffer_q.shape[-2]\n         lora_output[:, :output_dim_q] = self.segment_gemm.run(\ndiff --git a/python/sglang/srt/lora/lora_manager.py b/python/sglang/srt/lora/lora_manager.py\nindex 59cd7e1..f0c5e7c 100644\n--- a/python/sglang/srt/lora/lora_manager.py\n+++ b/python/sglang/srt/lora/lora_manager.py\n@@ -243,6 +243,9 @@ class LoRAManager:\n     def init_lora_batch(self):\n         self.active_uids = set()  # set of active loras\n         self.buffer_id = {}  # lora uid -> idx in memory pool\n+        # Cache for seg_lens and weight_indices to avoid repeated allocations\n+        self.seg_lens_cache = {}\n+        self.weight_indices_cache = {}\n \n     def get_weight_name(self, name, idx):\n         for target_weight_name in self.target_weights:\n@@ -254,7 +257,7 @@ class LoRAManager:\n         if uid is None:\n             for i in range(num_layer):\n                 for k in self.A_buffer.keys():\n-                    self.A_buffer[k][i][buffer_id] *= 0\n+                    self.A_buffer[k][i][buffer_id].fill_(0)\n             return\n \n         for i in range(num_layer):\n@@ -292,12 +295,18 @@ class LoRAManager:\n \n         # setup lora in forward modules\n         bs = forward_batch.batch_size\n-        seg_lens = (\n-            forward_batch.extend_seq_lens\n-            if forward_batch.forward_mode.is_extend()\n-            else torch.ones(bs)\n-        )\n-        weight_indices = torch.empty((bs,), dtype=torch.int64, device=\"cuda\")\n+        if forward_batch.forward_mode.is_extend():\n+            seg_lens = forward_batch.extend_seq_lens\n+        else:\n+            # Cache and reuse seg_lens tensor\n+            if bs not in self.seg_lens_cache:\n+                self.seg_lens_cache[bs] = torch.ones(bs, device=\"cuda\")\n+            seg_lens = self.seg_lens_cache[bs]\n+\n+        # Cache and reuse weight_indices tensor\n+        if bs not in self.weight_indices_cache:\n+            self.weight_indices_cache[bs] = torch.empty((bs,), dtype=torch.int64, device=\"cuda\")\n+        weight_indices = self.weight_indices_cache[bs]\n         for i, lora_path in enumerate(forward_batch.lora_paths):\n             weight_indices[i] = self.buffer_id[lora_path]\n \n", "model_name_or_path": "gpt-5-2025-08-07"}
