{"instance_id": "/home/ubuntu/OmniPerf-Bench/sglan__repo-8879944", "model_patch": "diff --git a/python/sglang/srt/models/llama4.py b/python/sglang/srt/models/llama4.py\nindex d933f27..55cb108 100644\n--- a/python/sglang/srt/models/llama4.py\n+++ b/python/sglang/srt/models/llama4.py\n@@ -64,11 +64,12 @@ class Llama4MoE(nn.Module):\n         renormalize: bool,\n     ) -> Tuple[torch.Tensor, torch.Tensor]:\n         router_scores_aK, router_indices_aK = fast_topk(gating_output, topk, dim=-1)\n-        router_scores_aK = torch.sigmoid(router_scores_aK.float()).to(\n+        # Optimize by removing redundant view().reshape() and using in-place sigmoid\n+        router_scores_aK = torch.sigmoid_(router_scores_aK.float()).to(\n             hidden_states.dtype\n         )\n         return (\n-            router_scores_aK.view(-1).reshape(router_scores_aK.shape),\n+            router_scores_aK,\n             router_indices_aK.to(torch.int32),\n         )\n \n@@ -238,10 +239,13 @@ class Llama4Attention(nn.Module):\n         )\n \n     def _get_attn_scale(self, positions: torch.Tensor) -> torch.Tensor:\n+        # Optimize by reducing intermediate tensor allocations\n         floor = torch.floor((positions + 1.0) / self.floor_scale)\n-        attn_scale = torch.log(floor + 1.0) * self.attn_scale + 1.0\n-\n-        return attn_scale.unsqueeze(-1)\n+        floor.add_(1.0)\n+        torch.log_(floor)\n+        floor.mul_(self.attn_scale)\n+        floor.add_(1.0)\n+        return floor.unsqueeze(-1)\n \n     def forward(\n         self,\n@@ -257,10 +261,18 @@ class Llama4Attention(nn.Module):\n \n         if self.qk_norm is not None:\n             # TODO: support float\n-            q = q.reshape(-1, self.head_dim).contiguous().bfloat16()\n-            k = k.reshape(-1, self.head_dim).contiguous().bfloat16()\n-            q = self.qk_norm(q).to(q.dtype)\n-            k = self.qk_norm(k).to(k.dtype)\n+            # Optimize by storing original dtype and reducing reshape operations\n+            orig_dtype = q.dtype\n+            q = q.reshape(-1, self.head_dim)\n+            k = k.reshape(-1, self.head_dim)\n+            if q.dtype != torch.bfloat16:\n+                q = q.contiguous().bfloat16()\n+                k = k.contiguous().bfloat16()\n+            q = self.qk_norm(q)\n+            k = self.qk_norm(k)\n+            if orig_dtype != torch.bfloat16:\n+                q = q.to(orig_dtype)\n+                k = k.to(orig_dtype)\n             q = q.reshape(-1, self.q_size)\n             k = k.reshape(-1, self.kv_size)\n \n@@ -270,7 +282,8 @@ class Llama4Attention(nn.Module):\n         # https://arxiv.org/abs/2501.19399\n         if self.attn_temperature_tuning and not self.use_rope:\n             attn_scale = self._get_attn_scale(positions)\n-            q = (q * attn_scale).to(q.dtype)\n+            # Optimize by using in-place multiplication when possible\n+            q = q.mul(attn_scale)\n \n         attn_output = self.attn(q, k, v, forward_batch)\n         output, _ = self.o_proj(attn_output)\n@@ -435,7 +448,11 @@ class Llama4Model(nn.Module):\n         aux_hidden_states = []\n         for i in range(len(self.layers)):\n             if i in self.layers_to_capture:\n-                aux_hidden_states.append(hidden_states + residual)\n+                # Optimize by avoiding in-place addition for captures\n+                if residual is not None:\n+                    aux_hidden_states.append(hidden_states + residual)\n+                else:\n+                    aux_hidden_states.append(hidden_states)\n             layer = self.layers[i]\n             hidden_states, residual = layer(\n                 positions,\n", "model_name_or_path": "gpt-5-2025-08-07"}
