{"instance_id": "/home/ubuntu/OmniPerf-Bench/sglan__repo-8609e63", "model_patch": "diff --git a/python/sglang/srt/managers/scheduler.py b/python/sglang/srt/managers/scheduler.py\nindex 8253a30..cc8f795 100644\n--- a/python/sglang/srt/managers/scheduler.py\n+++ b/python/sglang/srt/managers/scheduler.py\n@@ -1242,8 +1242,9 @@ class Scheduler(\n         can_run_list: List[Req],\n         running_bs: int,\n     ):\n-        gap_latency = time.perf_counter() - self.last_prefill_stats_tic\n-        self.last_prefill_stats_tic = time.perf_counter()\n+        current_time = time.perf_counter()\n+        gap_latency = current_time - self.last_prefill_stats_tic\n+        self.last_prefill_stats_tic = current_time\n         self.last_input_throughput = self.last_prefill_tokens / gap_latency\n         self.last_prefill_tokens = adder.log_input_tokens\n \n@@ -1295,8 +1296,9 @@ class Scheduler(\n     ):\n         batch = running_batch or self.running_batch\n \n-        gap_latency = time.perf_counter() - self.last_decode_stats_tic\n-        self.last_decode_stats_tic = time.perf_counter()\n+        current_time = time.perf_counter()\n+        gap_latency = current_time - self.last_decode_stats_tic\n+        self.last_decode_stats_tic = current_time\n         self.last_gen_throughput = self.num_generated_tokens / gap_latency\n         self.num_generated_tokens = 0\n         num_running_reqs = len(batch.reqs)\n@@ -1402,12 +1404,8 @@ class Scheduler(\n     def coordinate_spec_dp_attn_batch(self, new_batch: Optional[ScheduleBatch]):\n         \"\"\"Coordinate the DP attention batch.\"\"\"\n \n-        local_info = torch.tensor(\n-            [\n-                (new_batch is not None),\n-            ],\n-            dtype=torch.int64,\n-        )\n+        local_info = torch.empty(1, dtype=torch.int64)\n+        local_info[0] = (new_batch is not None)\n         global_info = torch.empty(\n             (self.server_args.dp_size, self.attn_tp_size, 1),\n             dtype=torch.int64,\n@@ -1579,8 +1577,9 @@ class Scheduler(\n             for req in can_run_list:\n                 req.queue_time_end = time.perf_counter()\n \n+        can_run_set = set(can_run_list)\n         self.waiting_queue = [\n-            x for x in self.waiting_queue if x not in set(can_run_list)\n+            x for x in self.waiting_queue if x not in can_run_set\n         ]\n \n         if adder.new_chunked_req is not None:\n@@ -1842,21 +1841,19 @@ class Scheduler(\n \n         tbo_preparer = TboDPAttentionPreparer()\n \n-        local_info = torch.tensor(\n-            [\n-                num_tokens,\n-                can_cuda_graph,\n-                num_tokens_for_logprob,\n-                is_extend_in_batch,\n-                *tbo_preparer.prepare_all_gather(\n-                    local_batch,\n-                    deepep_mode,\n-                    enable_deepep_moe,\n-                    enable_two_batch_overlap,\n-                ),\n-            ],\n-            dtype=torch.int64,\n+        tbo_values = tbo_preparer.prepare_all_gather(\n+            local_batch,\n+            deepep_mode,\n+            enable_deepep_moe,\n+            enable_two_batch_overlap,\n         )\n+        local_info = torch.empty(6, dtype=torch.int64)\n+        local_info[0] = num_tokens\n+        local_info[1] = can_cuda_graph\n+        local_info[2] = num_tokens_for_logprob\n+        local_info[3] = is_extend_in_batch\n+        local_info[4] = tbo_values[0]\n+        local_info[5] = tbo_values[1]\n         global_info = torch.empty(\n             (dp_size, attn_tp_size, 6),\n             dtype=torch.int64,\n@@ -1947,7 +1944,9 @@ class Scheduler(\n \n         if tp_size > 1:\n             # Sync across TP ranks to make sure they have the same number of ready requests\n-            tensor = torch.tensor([num_ready_reqs, num_timeout_reqs], dtype=torch.int32)\n+            tensor = torch.empty(2, dtype=torch.int32)\n+            tensor[0] = num_ready_reqs\n+            tensor[1] = num_timeout_reqs\n             torch.distributed.all_reduce(\n                 tensor, op=torch.distributed.ReduceOp.MAX, group=tp_group\n             )\n", "model_name_or_path": "gpt-5-2025-08-07"}
