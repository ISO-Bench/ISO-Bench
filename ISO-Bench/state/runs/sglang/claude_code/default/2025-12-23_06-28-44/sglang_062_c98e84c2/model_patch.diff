diff --git a/python/sglang/srt/layers/sampler.py b/python/sglang/srt/layers/sampler.py
index ad7f0a1..a98b6b8 100644
--- a/python/sglang/srt/layers/sampler.py
+++ b/python/sglang/srt/layers/sampler.py
@@ -39,9 +39,8 @@ class Sampler(nn.Module):
 
         if torch.any(torch.isnan(probs)):
             logger.warning("Detected errors during sampling! NaN in the probability.")
-            probs = torch.where(
-                torch.isnan(probs), torch.full_like(probs, 1e-10), probs
-            )
+            # Optimize: Use scalar instead of full_like to avoid tensor allocation
+            probs = torch.where(torch.isnan(probs), 1e-10, probs)
 
         if global_server_args_dict["sampling_backend"] == "flashinfer":
             max_top_k_round, batch_size = 32, probs.shape[0]
@@ -87,14 +86,24 @@ def top_k_top_p_min_p_sampling_from_probs_torch(
 ):
     """A top-k, top-p and min-p sampling implementation with native pytorch operations."""
     probs_sort, probs_idx = probs.sort(dim=-1, descending=True)
+
+    # Optimize: Create position mask once and reuse
+    batch_size, vocab_size = probs_sort.shape
+    positions = torch.arange(vocab_size, device=probs_sort.device).unsqueeze(0)
+
+    # Optimize: Combine filtering operations to reduce memory access patterns
+    # Calculate cumsum for top-p filtering
     probs_sum = torch.cumsum(probs_sort, dim=-1)
-    min_p_thresholds = probs_sort[:, 0] * min_ps
-    probs_sort[(probs_sum - probs_sort) > top_ps.view(-1, 1)] = 0.0
-    probs_sort[
-        torch.arange(0, probs.shape[-1], device=probs.device).view(1, -1)
-        >= top_ks.view(-1, 1)
-    ] = 0.0
-    probs_sort[probs_sort < min_p_thresholds.view(-1, 1)] = 0.0
+
+    # Apply all filters: top-p, top-k, and min-p
+    top_p_mask = (probs_sum - probs_sort) > top_ps.view(-1, 1)
+    top_k_mask = positions >= top_ks.view(-1, 1)
+    min_p_mask = probs_sort < (probs_sort[:, 0:1] * min_ps.view(-1, 1))
+
+    # Combine masks and apply once to reduce memory writes
+    combined_mask = top_p_mask | top_k_mask | min_p_mask
+    probs_sort[combined_mask] = 0.0
+
     probs_sort.div_(probs_sort.max(dim=-1, keepdim=True)[0])
     sampled_index = torch.multinomial(probs_sort, num_samples=1)
     batch_next_token_ids = torch.gather(probs_idx, dim=1, index=sampled_index).view(-1)
