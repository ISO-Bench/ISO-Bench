{"instance_id": "/home/ubuntu/OmniPerf-Bench/sglan__repo-9c064bf", "model_patch": "diff --git a/python/sglang/srt/layers/sampler.py b/python/sglang/srt/layers/sampler.py\nindex ad7f0a1..a98b6b8 100644\n--- a/python/sglang/srt/layers/sampler.py\n+++ b/python/sglang/srt/layers/sampler.py\n@@ -39,9 +39,8 @@ class Sampler(nn.Module):\n \n         if torch.any(torch.isnan(probs)):\n             logger.warning(\"Detected errors during sampling! NaN in the probability.\")\n-            probs = torch.where(\n-                torch.isnan(probs), torch.full_like(probs, 1e-10), probs\n-            )\n+            # Optimize: Use scalar instead of full_like to avoid tensor allocation\n+            probs = torch.where(torch.isnan(probs), 1e-10, probs)\n \n         if global_server_args_dict[\"sampling_backend\"] == \"flashinfer\":\n             max_top_k_round, batch_size = 32, probs.shape[0]\n@@ -87,14 +86,24 @@ def top_k_top_p_min_p_sampling_from_probs_torch(\n ):\n     \"\"\"A top-k, top-p and min-p sampling implementation with native pytorch operations.\"\"\"\n     probs_sort, probs_idx = probs.sort(dim=-1, descending=True)\n+\n+    # Optimize: Create position mask once and reuse\n+    batch_size, vocab_size = probs_sort.shape\n+    positions = torch.arange(vocab_size, device=probs_sort.device).unsqueeze(0)\n+\n+    # Optimize: Combine filtering operations to reduce memory access patterns\n+    # Calculate cumsum for top-p filtering\n     probs_sum = torch.cumsum(probs_sort, dim=-1)\n-    min_p_thresholds = probs_sort[:, 0] * min_ps\n-    probs_sort[(probs_sum - probs_sort) > top_ps.view(-1, 1)] = 0.0\n-    probs_sort[\n-        torch.arange(0, probs.shape[-1], device=probs.device).view(1, -1)\n-        >= top_ks.view(-1, 1)\n-    ] = 0.0\n-    probs_sort[probs_sort < min_p_thresholds.view(-1, 1)] = 0.0\n+\n+    # Apply all filters: top-p, top-k, and min-p\n+    top_p_mask = (probs_sum - probs_sort) > top_ps.view(-1, 1)\n+    top_k_mask = positions >= top_ks.view(-1, 1)\n+    min_p_mask = probs_sort < (probs_sort[:, 0:1] * min_ps.view(-1, 1))\n+\n+    # Combine masks and apply once to reduce memory writes\n+    combined_mask = top_p_mask | top_k_mask | min_p_mask\n+    probs_sort[combined_mask] = 0.0\n+\n     probs_sort.div_(probs_sort.max(dim=-1, keepdim=True)[0])\n     sampled_index = torch.multinomial(probs_sort, num_samples=1)\n     batch_next_token_ids = torch.gather(probs_idx, dim=1, index=sampled_index).view(-1)\n", "model_name_or_path": "gpt-5-2025-08-07"}
