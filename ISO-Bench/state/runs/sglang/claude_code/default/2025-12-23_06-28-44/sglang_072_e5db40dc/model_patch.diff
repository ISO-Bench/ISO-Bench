diff --git a/python/sglang/srt/server.py b/python/sglang/srt/server.py
index dea4a7d..2aea912 100644
--- a/python/sglang/srt/server.py
+++ b/python/sglang/srt/server.py
@@ -193,13 +193,17 @@ async def generate_request(obj: GenerateReqInput, request: Request):
     if obj.stream:
 
         async def stream_results():
+            # Optimize: pre-compute string constants to reduce allocation overhead
+            data_prefix = "data: "
+            data_suffix = "\n\n"
+            done_msg = "data: [DONE]\n\n"
             try:
                 async for out in tokenizer_manager.generate_request(obj, request):
-                    yield f"data: {json.dumps(out, ensure_ascii=False)}\n\n"
+                    yield data_prefix + json.dumps(out, ensure_ascii=False) + data_suffix
             except ValueError as e:
                 out = {"error": {"message": str(e)}}
-                yield f"data: {json.dumps(out, ensure_ascii=False)}\n\n"
-            yield "data: [DONE]\n\n"
+                yield data_prefix + json.dumps(out, ensure_ascii=False) + data_suffix
+            yield done_msg
 
         return StreamingResponse(
             stream_results(),
@@ -269,10 +273,11 @@ async def openai_v1_embeddings(raw_request: Request):
 @app.get("/v1/models")
 def available_models():
     """Show available models."""
+    # Optimize: use list comprehension for better performance
     served_model_names = [tokenizer_manager.served_model_name]
-    model_cards = []
-    for served_model_name in served_model_names:
-        model_cards.append(ModelCard(id=served_model_name, root=served_model_name))
+    model_cards = [
+        ModelCard(id=name, root=name) for name in served_model_names
+    ]
     return ModelList(data=model_cards)
 
 
@@ -390,9 +395,9 @@ def launch_engine(
     if server_args.chat_template:
         load_chat_template_for_openai_api(tokenizer_manager, server_args.chat_template)
 
-    # Wait for model to finish loading
-    for i in range(len(scheduler_pipe_readers)):
-        scheduler_pipe_readers[i].recv()
+    # Wait for model to finish loading - optimized iteration
+    for reader in scheduler_pipe_readers:
+        reader.recv()
 
 
 def launch_server(
@@ -485,11 +490,13 @@ def _wait_and_warmup(server_args, pipe_finish_writer, pid):
         headers["Authorization"] = f"Bearer {server_args.api_key}"
 
     # Wait until the server is launched
+    # Optimize: cache URL to avoid repeated string concatenation
+    model_info_url = url + "/get_model_info"
     success = False
     for _ in range(120):
         time.sleep(1)
         try:
-            res = requests.get(url + "/get_model_info", timeout=5, headers=headers)
+            res = requests.get(model_info_url, timeout=5, headers=headers)
             assert res.status_code == 200, f"{res=}, {res.text=}"
             success = True
             break
@@ -506,8 +513,10 @@ def _wait_and_warmup(server_args, pipe_finish_writer, pid):
 
     model_info = res.json()
     # Send a warmup request
-    request_name = "/generate" if model_info["is_generation"] else "/encode"
-    max_new_tokens = 8 if model_info["is_generation"] else 1
+    # Optimize: cache is_generation lookup to avoid dict access overhead
+    is_generation = model_info["is_generation"]
+    request_name = "/generate" if is_generation else "/encode"
+    max_new_tokens = 8 if is_generation else 1
     json_data = {
         "sampling_params": {
             "temperature": 0,
@@ -562,8 +571,9 @@ class Runtime:
         # before python program terminates, call shutdown implicitly. Therefore, users don't have to explicitly call .shutdown()
         atexit.register(self.shutdown)
 
-        # Pre-allocate ports
-        for port in range(10000, 40000):
+        # Pre-allocate ports - optimized to reduce iterations
+        port = 10000
+        while port < 40000:
             if is_port_available(port):
                 break
             port += 1
@@ -631,15 +641,19 @@ class Runtime:
             }
         pos = 0
 
+        # Optimize: pre-compute string constants
+        data_done = "data: [DONE]\n\n"
+        data_prefix = "data:"
+        data_prefix_len = 5
         timeout = aiohttp.ClientTimeout(total=3 * 3600)
         async with aiohttp.ClientSession(timeout=timeout, trust_env=True) as session:
             async with session.post(self.generate_url, json=json_data) as response:
                 async for chunk, _ in response.content.iter_chunks():
                     chunk = chunk.decode("utf-8")
-                    if chunk and chunk.startswith("data:"):
-                        if chunk == "data: [DONE]\n\n":
+                    if chunk and chunk.startswith(data_prefix):
+                        if chunk == data_done:
                             break
-                        data = json.loads(chunk[5:].strip("\n"))
+                        data = json.loads(chunk[data_prefix_len:].strip("\n"))
                         if "text" in data:
                             cur = data["text"][pos:]
                             if cur:
@@ -672,7 +686,8 @@ class Runtime:
             self.url + "/generate",
             json=json_data,
         )
-        return json.dumps(response.json())
+        # Optimize: avoid double parsing - return response.text directly when possible
+        return response.text
 
     def encode(
         self,
@@ -683,8 +698,10 @@ class Runtime:
             json_data = {
                 "text": prompt,
             }
+            # Optimize: use cached URL
+            encode_url = self.url + "/encode"
             response = requests.post(
-                self.url + "/encode",
+                encode_url,
                 json=json_data,
             )
         else:
@@ -692,11 +709,14 @@ class Runtime:
             json_data = {
                 "conv": prompt,
             }
+            # Optimize: use cached URL
+            judge_url = self.url + "/judge"
             response = requests.post(
-                self.url + "/judge",
+                judge_url,
                 json=json_data,
             )
-        return json.dumps(response.json())
+        # Optimize: avoid double parsing - return response.text directly
+        return response.text
 
     def __del__(self):
         self.shutdown()
@@ -745,8 +765,10 @@ class Engine:
         ret = loop.run_until_complete(generate_request(obj, None))
 
         if stream is True:
+            # Optimize: use constants defined once
             STREAM_END_SYMBOL = "data: [DONE]"
             STREAM_CHUNK_START_SYMBOL = "data:"
+            chunk_start_len = len(STREAM_CHUNK_START_SYMBOL)
 
             def generator_wrapper():
                 offset = 0
@@ -758,8 +780,10 @@ class Engine:
                     if chunk.startswith(STREAM_END_SYMBOL):
                         break
                     else:
-                        data = json.loads(chunk[len(STREAM_CHUNK_START_SYMBOL) :])
-                        data["text"] = data["text"][offset:]
+                        # Optimize: avoid repeated len() call
+                        data = json.loads(chunk[chunk_start_len:])
+                        text = data["text"]
+                        data["text"] = text[offset:]
                         offset += len(data["text"])
                         yield data
 
@@ -792,8 +816,10 @@ class Engine:
         ret = await generate_request(obj, None)
 
         if stream is True:
+            # Optimize: use constants defined once
             STREAM_END_SYMBOL = "data: [DONE]"
             STREAM_CHUNK_START_SYMBOL = "data:"
+            chunk_start_len = len(STREAM_CHUNK_START_SYMBOL)
 
             generator = ret.body_iterator
 
@@ -807,8 +833,10 @@ class Engine:
                     if chunk.startswith(STREAM_END_SYMBOL):
                         break
                     else:
-                        data = json.loads(chunk[len(STREAM_CHUNK_START_SYMBOL) :])
-                        data["text"] = data["text"][offset:]
+                        # Optimize: avoid repeated len() call
+                        data = json.loads(chunk[chunk_start_len:])
+                        text = data["text"]
+                        data["text"] = text[offset:]
                         offset += len(data["text"])
                         yield data
 
