{"instance_id": "/home/ubuntu/OmniPerf-Bench/sglan__repo-136c6e0", "model_patch": "diff --git a/python/sglang/bench_serving.py b/python/sglang/bench_serving.py\nindex 04c2202..0bdee6a 100644\n--- a/python/sglang/bench_serving.py\n+++ b/python/sglang/bench_serving.py\n@@ -231,22 +231,25 @@ async def async_request_openai_completions(\n                             # NOTE: Some completion API might have a last\n                             # usage summary response without a token so we\n                             # want to check a token was generated\n-                            if data[\"choices\"][0][\"text\"]:\n-                                timestamp = time.perf_counter()\n-                                # First token\n-                                if ttft == 0.0:\n-                                    ttft = time.perf_counter() - st\n-                                    output.ttft = ttft\n+                            choices = data.get(\"choices\")\n+                            if choices and len(choices) > 0:\n+                                text = choices[0].get(\"text\")\n+                                if text:\n+                                    timestamp = time.perf_counter()\n+                                    # First token\n+                                    if ttft == 0.0:\n+                                        ttft = time.perf_counter() - st\n+                                        output.ttft = ttft\n \n-                                # Decoding phase\n-                                else:\n-                                    output.itl.append(timestamp - most_recent_timestamp)\n+                                    # Decoding phase\n+                                    else:\n+                                        output.itl.append(timestamp - most_recent_timestamp)\n \n-                                most_recent_timestamp = timestamp\n-                                generated_text += data[\"choices\"][0][\"text\"]\n-                                output_len = (data.get(\"usage\") or {}).get(\n-                                    \"completion_tokens\", output_len\n-                                )\n+                                    most_recent_timestamp = timestamp\n+                                    generated_text += text\n+                                    usage = data.get(\"usage\")\n+                                    if usage:\n+                                        output_len = usage.get(\"completion_tokens\", output_len)\n \n                     output.generated_text = generated_text\n                     output.success = True\n@@ -444,19 +447,22 @@ async def async_request_truss(\n                             # NOTE: Some completion API might have a last\n                             # usage summary response without a token so we\n                             # want to check a token was generated\n-                            if data[\"choices\"][0][\"text\"]:\n-                                timestamp = time.perf_counter()\n-                                # First token\n-                                if ttft == 0.0:\n-                                    ttft = time.perf_counter() - st\n-                                    output.ttft = ttft\n+                            choices = data.get(\"choices\")\n+                            if choices and len(choices) > 0:\n+                                text = choices[0].get(\"text\")\n+                                if text:\n+                                    timestamp = time.perf_counter()\n+                                    # First token\n+                                    if ttft == 0.0:\n+                                        ttft = time.perf_counter() - st\n+                                        output.ttft = ttft\n \n-                                # Decoding phase\n-                                else:\n-                                    output.itl.append(timestamp - most_recent_timestamp)\n+                                    # Decoding phase\n+                                    else:\n+                                        output.itl.append(timestamp - most_recent_timestamp)\n \n-                                most_recent_timestamp = timestamp\n-                                generated_text += data[\"choices\"][0][\"text\"]\n+                                    most_recent_timestamp = timestamp\n+                                    generated_text += text\n \n                     output.generated_text = generated_text\n                     output.success = True\n@@ -531,10 +537,12 @@ async def async_request_sglang_generate(\n                             # NOTE: Some completion API might have a last\n                             # usage summary response without a token so we\n                             # want to check a token was generated\n-                            if \"text\" in data and data[\"text\"]:\n+                            text = data.get(\"text\")\n+                            if text:\n                                 timestamp = time.perf_counter()\n-                                generated_text = data[\"text\"]\n-                                output_len = data[\"meta_info\"][\"completion_tokens\"]\n+                                generated_text = text\n+                                meta_info = data.get(\"meta_info\", {})\n+                                output_len = meta_info.get(\"completion_tokens\", output_len)\n \n                                 # First token\n                                 if ttft == 0.0:\n@@ -1115,8 +1123,10 @@ def sample_random_requests(\n \n def gen_prompt(tokenizer, token_num):\n     \"\"\"Generate a random prompt of specified token length using tokenizer vocabulary.\"\"\"\n-    all_available_tokens = list(tokenizer.get_vocab().values())\n-    selected_tokens = random.choices(all_available_tokens, k=token_num)\n+    vocab = tokenizer.get_vocab()\n+    vocab_size = len(vocab)\n+    # Use random.choices directly on range instead of creating a full list\n+    selected_tokens = random.choices(range(vocab_size), k=token_num)\n     return tokenizer.decode(selected_tokens)\n \n \ndiff --git a/python/sglang/srt/entrypoints/http_server_engine.py b/python/sglang/srt/entrypoints/http_server_engine.py\nindex b2edf1a..ce379ef 100644\n--- a/python/sglang/srt/entrypoints/http_server_engine.py\n+++ b/python/sglang/srt/entrypoints/http_server_engine.py\n@@ -115,20 +115,28 @@ class HttpServerEngineAdapter(EngineBase):\n         lora_path=None,\n         custom_logit_processor=None,\n     ):\n-        payload = {\n-            \"text\": prompt,\n-            \"sampling_params\": sampling_params,\n-            \"input_ids\": input_ids,\n-            \"image_data\": image_data,\n-            \"return_logprob\": return_logprob,\n-            \"logprob_start_len\": logprob_start_len,\n-            \"top_logprobs_num\": top_logprobs_num,\n-            \"token_ids_logprob\": token_ids_logprob,\n-            \"lora_path\": lora_path,\n-            \"custom_logit_processor\": custom_logit_processor,\n-        }\n-        # Filter out None values\n-        payload = {k: v for k, v in payload.items() if v is not None}\n+        # Build payload directly without None values for better performance\n+        payload = {}\n+        if prompt is not None:\n+            payload[\"text\"] = prompt\n+        if sampling_params is not None:\n+            payload[\"sampling_params\"] = sampling_params\n+        if input_ids is not None:\n+            payload[\"input_ids\"] = input_ids\n+        if image_data is not None:\n+            payload[\"image_data\"] = image_data\n+        if return_logprob is not None:\n+            payload[\"return_logprob\"] = return_logprob\n+        if logprob_start_len is not None:\n+            payload[\"logprob_start_len\"] = logprob_start_len\n+        if top_logprobs_num is not None:\n+            payload[\"top_logprobs_num\"] = top_logprobs_num\n+        if token_ids_logprob is not None:\n+            payload[\"token_ids_logprob\"] = token_ids_logprob\n+        if lora_path is not None:\n+            payload[\"lora_path\"] = lora_path\n+        if custom_logit_processor is not None:\n+            payload[\"custom_logit_processor\"] = custom_logit_processor\n \n         return self._make_request(\"generate\", payload)\n \ndiff --git a/python/sglang/srt/utils.py b/python/sglang/srt/utils.py\nindex 052e732..8635113 100644\n--- a/python/sglang/srt/utils.py\n+++ b/python/sglang/srt/utils.py\n@@ -2354,7 +2354,7 @@ def is_fa3_default_architecture(hf_config):\n # Can be more general if it is used in multiple places (keep it simple and thus not general now)\n class BumpAllocator:\n     def __init__(self, buffer_size: int, dtype, device):\n-        self._buffer = torch.zeros((buffer_size,), dtype=dtype, device=device)\n+        self._buffer = torch.empty((buffer_size,), dtype=dtype, device=device)\n         self._pointer = 0\n \n     def allocate(self, size: int):\n", "model_name_or_path": "gpt-5-2025-08-07"}
