{"instance_id": "/home/ubuntu/OmniPerf-Bench/sglan__repo-1e495e0", "model_patch": "diff --git a/python/sglang/srt/model_executor/cuda_graph_runner.py b/python/sglang/srt/model_executor/cuda_graph_runner.py\nindex 4459213..8c6e018 100644\n--- a/python/sglang/srt/model_executor/cuda_graph_runner.py\n+++ b/python/sglang/srt/model_executor/cuda_graph_runner.py\n@@ -106,30 +106,32 @@ class CudaGraphRunner:\n \n         # Common inputs\n         self.max_bs = max_batch_size_to_capture\n-        self.input_ids = torch.zeros((self.max_bs,), dtype=torch.int32, device=\"cuda\")\n-        self.req_pool_indices = torch.zeros(\n+        self.input_ids = torch.empty((self.max_bs,), dtype=torch.int32, device=\"cuda\")\n+        self.req_pool_indices = torch.empty(\n             (self.max_bs,), dtype=torch.int32, device=\"cuda\"\n         )\n-        self.seq_lens = torch.zeros((self.max_bs,), dtype=torch.int32, device=\"cuda\")\n-        self.position_ids_offsets = torch.ones(\n+        self.seq_lens = torch.empty((self.max_bs,), dtype=torch.int32, device=\"cuda\")\n+        self.position_ids_offsets = torch.empty(\n             (self.max_bs,), dtype=torch.int32, device=\"cuda\"\n         )\n-        self.out_cache_loc = torch.zeros(\n+        self.position_ids_offsets.fill_(1)\n+        self.out_cache_loc = torch.empty(\n             (self.max_bs,), dtype=torch.int32, device=\"cuda\"\n         )\n \n         # FlashInfer inputs\n-        self.flashinfer_kv_indptr = torch.zeros(\n+        self.flashinfer_kv_indptr = torch.empty(\n             (self.max_bs + 1,), dtype=torch.int32, device=\"cuda\"\n         )\n-        self.flashinfer_kv_indices = torch.zeros(\n+        self.flashinfer_kv_indices = torch.empty(\n             (self.max_bs * model_runner.model_config.context_len,),\n             dtype=torch.int32,\n             device=\"cuda\",\n         )\n-        self.flashinfer_kv_last_page_len = torch.ones(\n+        self.flashinfer_kv_last_page_len = torch.empty(\n             (self.max_bs,), dtype=torch.int32, device=\"cuda\"\n         )\n+        self.flashinfer_kv_last_page_len.fill_(1)\n         if model_runner.sliding_window_size is None:\n             self.flashinfer_workspace_buffer = (\n                 self.model_runner.flashinfer_workspace_buffer\ndiff --git a/python/sglang/srt/model_executor/forward_batch_info.py b/python/sglang/srt/model_executor/forward_batch_info.py\nindex a443b11..8c4180a 100644\n--- a/python/sglang/srt/model_executor/forward_batch_info.py\n+++ b/python/sglang/srt/model_executor/forward_batch_info.py\n@@ -149,7 +149,8 @@ class InputMetadata:\n             ]\n             self.extend_seq_lens = torch.tensor(extend_lens_cpu, device=\"cuda\")\n             self.extend_prefix_lens = torch.tensor(batch.prefix_lens_cpu, device=\"cuda\")\n-            self.extend_start_loc = torch.zeros_like(self.seq_lens)\n+            self.extend_start_loc = torch.empty_like(self.seq_lens)\n+            self.extend_start_loc[0] = 0\n             self.extend_start_loc[1:] = torch.cumsum(self.extend_seq_lens[:-1], dim=0)\n             self.extend_no_prefix = all(l == 0 for l in batch.prefix_lens_cpu)\n \n@@ -221,7 +222,8 @@ class InputMetadata:\n     def init_triton_args(self, batch: ScheduleBatch):\n         \"\"\"Init auxiliary variables for triton attention backend.\"\"\"\n         self.triton_max_seq_len = int(torch.max(self.seq_lens))\n-        self.triton_start_loc = torch.zeros_like(self.seq_lens, dtype=torch.int32)\n+        self.triton_start_loc = torch.empty_like(self.seq_lens, dtype=torch.int32)\n+        self.triton_start_loc[0] = 0\n         self.triton_start_loc[1:] = torch.cumsum(self.seq_lens[:-1], dim=0)\n \n         if self.forward_mode == ForwardMode.DECODE:\n@@ -321,7 +323,8 @@ def update_flashinfer_indices(\n         else:\n             paged_kernel_lens = seq_lens\n \n-        kv_indptr = torch.zeros((batch_size + 1,), dtype=torch.int32, device=\"cuda\")\n+        kv_indptr = torch.empty((batch_size + 1,), dtype=torch.int32, device=\"cuda\")\n+        kv_indptr[0] = 0\n         kv_indptr[1:] = torch.cumsum(paged_kernel_lens, dim=0)\n \n         kv_indices = torch.empty(kv_indptr[-1], dtype=torch.int32, device=\"cuda\")\n@@ -356,7 +359,8 @@ def update_flashinfer_indices(\n             )\n         else:\n             # extend part\n-            qo_indptr = torch.zeros((batch_size + 1,), dtype=torch.int32, device=\"cuda\")\n+            qo_indptr = torch.empty((batch_size + 1,), dtype=torch.int32, device=\"cuda\")\n+            qo_indptr[0] = 0\n             qo_indptr[1:] = torch.cumsum(seq_lens - prefix_lens, dim=0)\n \n             if flashinfer_use_ragged:\n@@ -402,7 +406,8 @@ def update_flashinfer_indices(\n \n             kv_start_idx = seq_lens - paged_kernel_lens\n \n-            kv_indptr = torch.zeros((batch_size + 1,), dtype=torch.int32, device=\"cuda\")\n+            kv_indptr = torch.empty((batch_size + 1,), dtype=torch.int32, device=\"cuda\")\n+            kv_indptr[0] = 0\n             kv_indptr[1:] = torch.cumsum(paged_kernel_lens, dim=0)\n \n             kv_indices = torch.empty(kv_indptr[-1], dtype=torch.int32, device=\"cuda\")\n@@ -435,9 +440,10 @@ def update_flashinfer_indices(\n                 )\n             else:\n                 # extend part\n-                qo_indptr = torch.zeros(\n+                qo_indptr = torch.empty(\n                     (batch_size + 1,), dtype=torch.int32, device=\"cuda\"\n                 )\n+                qo_indptr[0] = 0\n                 qo_indptr[1:] = torch.cumsum(seq_lens - prefix_lens, dim=0)\n \n                 model_runner.flashinfer_prefill_wrapper_paged[wrapper_id].end_forward()\ndiff --git a/python/sglang/srt/model_executor/model_runner.py b/python/sglang/srt/model_executor/model_runner.py\nindex 09b3c71..0d70a81 100644\n--- a/python/sglang/srt/model_executor/model_runner.py\n+++ b/python/sglang/srt/model_executor/model_runner.py\n@@ -415,8 +415,8 @@ class ModelRunner:\n         \"\"\"We need to run a small matmul to init cublas. Otherwise, it will raise some errors later.\"\"\"\n         dtype = torch.float16\n         device = \"cuda\"\n-        a = torch.ones((16, 16), dtype=dtype, device=device)\n-        b = torch.ones((16, 16), dtype=dtype, device=device)\n+        a = torch.empty((16, 16), dtype=dtype, device=device)\n+        b = torch.empty((16, 16), dtype=dtype, device=device)\n         c = a @ b\n         return c\n \n", "model_name_or_path": "gpt-5-2025-08-07"}
