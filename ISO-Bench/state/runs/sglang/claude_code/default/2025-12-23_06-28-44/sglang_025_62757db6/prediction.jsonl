{"instance_id": "/home/ubuntu/OmniPerf-Bench/sglan__repo-73fa2d4", "model_patch": "diff --git a/python/sglang/srt/managers/schedule_batch.py b/python/sglang/srt/managers/schedule_batch.py\nindex 2489abd..30039f5 100644\n--- a/python/sglang/srt/managers/schedule_batch.py\n+++ b/python/sglang/srt/managers/schedule_batch.py\n@@ -414,9 +414,10 @@ class ScheduleBatch:\n         for i in range(bs):\n             if reqs[i].sampling_params.dtype == \"int\":\n                 if self.logit_bias is None:\n-                    self.logit_bias = torch.zeros(\n+                    self.logit_bias = torch.empty(\n                         (bs, vocab_size), dtype=torch.float32, device=device\n                     )\n+                    self.logit_bias.fill_(0.0)\n                 self.logit_bias[i][: len(int_token_logit_bias)] = int_token_logit_bias\n \n     def prepare_for_extend(self, vocab_size: int, int_token_logit_bias: torch.Tensor):\n@@ -452,7 +453,8 @@ class ScheduleBatch:\n             self.input_ids = torch.tensor(sum(input_ids, []), dtype=torch.int32)\n             self.req_pool_indices = torch.tensor(req_pool_indices_cpu)\n             self.seq_lens = torch.tensor(seq_lens, dtype=torch.int32)\n-            self.position_ids_offsets = torch.zeros((bs,), dtype=torch.int64)\n+            self.position_ids_offsets = torch.empty((bs,), dtype=torch.int64)\n+            self.position_ids_offsets.fill_(0)\n \n         self.extend_num_tokens = extend_num_tokens\n         self.out_cache_loc = out_cache_loc\n@@ -711,13 +713,15 @@ class ScheduleBatch:\n                 else other.logit_bias.shape[1]\n             )\n             if self.logit_bias is None:\n-                self.logit_bias = torch.zeros(\n+                self.logit_bias = torch.empty(\n                     (len(self.reqs), vocab_size), dtype=torch.float32, device=\"cuda\"\n                 )\n+                self.logit_bias.fill_(0.0)\n             if other.logit_bias is None:\n-                other.logit_bias = torch.zeros(\n+                other.logit_bias = torch.empty(\n                     (len(other.reqs), vocab_size), dtype=torch.float32, device=\"cuda\"\n                 )\n+                other.logit_bias.fill_(0.0)\n             self.logit_bias = torch.concat([self.logit_bias, other.logit_bias])\n \n     def sample(self, logits: torch.Tensor):\n@@ -733,7 +737,7 @@ class ScheduleBatch:\n             allowed_mask = torch.empty_like(logits[0], dtype=torch.bool)\n             for i, req in enumerate(self.reqs):\n                 if req.regex_fsm is not None:\n-                    allowed_mask.zero_()\n+                    allowed_mask.fill_(False)\n                     allowed_mask[\n                         req.regex_fsm.get_next_instruction(req.regex_fsm_state).tokens\n                     ] = 1\n@@ -793,10 +797,12 @@ def top_k_top_p_sampling_from_probs_torch(\n     try:\n         sampled_index = torch.multinomial(probs_sort, num_samples=1)\n     except RuntimeError:\n-        batch_next_token_ids = torch.zeros(\n+        batch_next_token_ids = torch.empty(\n             (probs_sort.shape[0],), dtype=torch.int32, device=probs.device\n         )\n-        success = torch.zeros(probs.shape[0], dtype=torch.bool, device=probs.device)\n+        batch_next_token_ids.fill_(0)\n+        success = torch.empty(probs.shape[0], dtype=torch.bool, device=probs.device)\n+        success.fill_(False)\n         return batch_next_token_ids, success\n \n     batch_next_token_ids = torch.gather(probs_idx, dim=1, index=sampled_index).view(-1)\ndiff --git a/python/sglang/srt/mem_cache/radix_cache.py b/python/sglang/srt/mem_cache/radix_cache.py\nindex c238120..49e30c2 100644\n--- a/python/sglang/srt/mem_cache/radix_cache.py\n+++ b/python/sglang/srt/mem_cache/radix_cache.py\n@@ -78,7 +78,7 @@ class RadixCache(BasePrefixCache):\n         if value:\n             value = torch.concat(value)\n         else:\n-            value = torch.tensor([], dtype=torch.int32)\n+            value = torch.empty(0, dtype=torch.int32)\n         return value, last_node[0]\n \n     def insert(self, key, value=None):\n", "model_name_or_path": "gpt-5-2025-08-07"}
