diff --git a/python/sglang/srt/managers/schedule_batch.py b/python/sglang/srt/managers/schedule_batch.py
index 2489abd..30039f5 100644
--- a/python/sglang/srt/managers/schedule_batch.py
+++ b/python/sglang/srt/managers/schedule_batch.py
@@ -414,9 +414,10 @@ class ScheduleBatch:
         for i in range(bs):
             if reqs[i].sampling_params.dtype == "int":
                 if self.logit_bias is None:
-                    self.logit_bias = torch.zeros(
+                    self.logit_bias = torch.empty(
                         (bs, vocab_size), dtype=torch.float32, device=device
                     )
+                    self.logit_bias.fill_(0.0)
                 self.logit_bias[i][: len(int_token_logit_bias)] = int_token_logit_bias
 
     def prepare_for_extend(self, vocab_size: int, int_token_logit_bias: torch.Tensor):
@@ -452,7 +453,8 @@ class ScheduleBatch:
             self.input_ids = torch.tensor(sum(input_ids, []), dtype=torch.int32)
             self.req_pool_indices = torch.tensor(req_pool_indices_cpu)
             self.seq_lens = torch.tensor(seq_lens, dtype=torch.int32)
-            self.position_ids_offsets = torch.zeros((bs,), dtype=torch.int64)
+            self.position_ids_offsets = torch.empty((bs,), dtype=torch.int64)
+            self.position_ids_offsets.fill_(0)
 
         self.extend_num_tokens = extend_num_tokens
         self.out_cache_loc = out_cache_loc
@@ -711,13 +713,15 @@ class ScheduleBatch:
                 else other.logit_bias.shape[1]
             )
             if self.logit_bias is None:
-                self.logit_bias = torch.zeros(
+                self.logit_bias = torch.empty(
                     (len(self.reqs), vocab_size), dtype=torch.float32, device="cuda"
                 )
+                self.logit_bias.fill_(0.0)
             if other.logit_bias is None:
-                other.logit_bias = torch.zeros(
+                other.logit_bias = torch.empty(
                     (len(other.reqs), vocab_size), dtype=torch.float32, device="cuda"
                 )
+                other.logit_bias.fill_(0.0)
             self.logit_bias = torch.concat([self.logit_bias, other.logit_bias])
 
     def sample(self, logits: torch.Tensor):
@@ -733,7 +737,7 @@ class ScheduleBatch:
             allowed_mask = torch.empty_like(logits[0], dtype=torch.bool)
             for i, req in enumerate(self.reqs):
                 if req.regex_fsm is not None:
-                    allowed_mask.zero_()
+                    allowed_mask.fill_(False)
                     allowed_mask[
                         req.regex_fsm.get_next_instruction(req.regex_fsm_state).tokens
                     ] = 1
@@ -793,10 +797,12 @@ def top_k_top_p_sampling_from_probs_torch(
     try:
         sampled_index = torch.multinomial(probs_sort, num_samples=1)
     except RuntimeError:
-        batch_next_token_ids = torch.zeros(
+        batch_next_token_ids = torch.empty(
             (probs_sort.shape[0],), dtype=torch.int32, device=probs.device
         )
-        success = torch.zeros(probs.shape[0], dtype=torch.bool, device=probs.device)
+        batch_next_token_ids.fill_(0)
+        success = torch.empty(probs.shape[0], dtype=torch.bool, device=probs.device)
+        success.fill_(False)
         return batch_next_token_ids, success
 
     batch_next_token_ids = torch.gather(probs_idx, dim=1, index=sampled_index).view(-1)
diff --git a/python/sglang/srt/mem_cache/radix_cache.py b/python/sglang/srt/mem_cache/radix_cache.py
index c238120..49e30c2 100644
--- a/python/sglang/srt/mem_cache/radix_cache.py
+++ b/python/sglang/srt/mem_cache/radix_cache.py
@@ -78,7 +78,7 @@ class RadixCache(BasePrefixCache):
         if value:
             value = torch.concat(value)
         else:
-            value = torch.tensor([], dtype=torch.int32)
+            value = torch.empty(0, dtype=torch.int32)
         return value, last_node[0]
 
     def insert(self, key, value=None):
