{"instance_id": "/home/ubuntu/OmniPerf-Bench/sglan__repo-2cea614", "model_patch": "diff --git a/python/sglang/srt/layers/logits_processor.py b/python/sglang/srt/layers/logits_processor.py\nindex 53d6620..f5f5c1e 100644\n--- a/python/sglang/srt/layers/logits_processor.py\n+++ b/python/sglang/srt/layers/logits_processor.py\n@@ -21,10 +21,10 @@ class LogitsProcessor(nn.Module):\n             prefill_token_logprobs, dim=0, dtype=torch.float32\n         )\n \n-        start = input_metadata.extend_start_loc.clone()\n+        start = input_metadata.extend_start_loc\n         end = start + input_metadata.extend_seq_lens - 2\n-        start.clamp_(min=0, max=prefill_token_logprobs.shape[0] - 1)\n-        end.clamp_(min=0, max=prefill_token_logprobs.shape[0] - 1)\n+        start = start.clamp(min=0, max=prefill_token_logprobs.shape[0] - 1)\n+        end = end.clamp(min=0, max=prefill_token_logprobs.shape[0] - 1)\n         sum_logp = (\n             logprobs_cumsum[end]\n             - logprobs_cumsum[start]\ndiff --git a/python/sglang/srt/managers/router/infer_batch.py b/python/sglang/srt/managers/router/infer_batch.py\nindex dbe9437..c68c78b 100644\n--- a/python/sglang/srt/managers/router/infer_batch.py\n+++ b/python/sglang/srt/managers/router/infer_batch.py\n@@ -250,7 +250,8 @@ class Batch:\n \n             seq_lens.append(prefix_lens[-1] + extend_lens[-1])\n \n-        position_ids_offsets = torch.zeros((bs,), dtype=torch.int32, device=device)\n+        position_ids_offsets = torch.empty((bs,), dtype=torch.int32, device=device)\n+        position_ids_offsets.zero_()\n \n         # Alloc mem\n         seq_lens, prefix_lens = np.array(seq_lens), np.array(prefix_lens)\n@@ -277,9 +278,10 @@ class Batch:\n         for i in range(bs):\n             if reqs[i].sampling_params.dtype == \"int\":\n                 if logit_bias is None:\n-                    logit_bias = torch.zeros(\n+                    logit_bias = torch.empty(\n                         (bs, vocab_size), dtype=torch.float32, device=device\n                     )\n+                    logit_bias.zero_()\n                 logit_bias[i] = int_token_logit_bias\n \n         # Set fields\n@@ -495,13 +497,15 @@ class Batch:\n                 else other.logit_bias.shape[1]\n             )\n             if self.logit_bias is None:\n-                self.logit_bias = torch.zeros(\n+                self.logit_bias = torch.empty(\n                     (len(self.reqs), vocab_size), dtype=torch.float32, device=\"cuda\"\n                 )\n+                self.logit_bias.zero_()\n             if other.logit_bias is None:\n-                other.logit_bias = torch.zeros(\n+                other.logit_bias = torch.empty(\n                     (len(other.reqs), vocab_size), dtype=torch.float32, device=\"cuda\"\n                 )\n+                other.logit_bias.zero_()\n             self.logit_bias = torch.concat([self.logit_bias, other.logit_bias])\n \n     def sample(self, logits: torch.Tensor):\n@@ -513,7 +517,7 @@ class Batch:\n \n         has_regex = any(req.regex_fsm is not None for req in self.reqs)\n         if has_regex:\n-            allowed_mask = torch.empty_like(logits[0], dtype=torch.bool)\n+            allowed_mask = torch.empty(logits.shape[1], dtype=torch.bool, device=logits.device)\n             for i, req in enumerate(self.reqs):\n                 if req.regex_fsm is not None:\n                     allowed_mask.zero_()\ndiff --git a/python/sglang/srt/managers/router/model_runner.py b/python/sglang/srt/managers/router/model_runner.py\nindex 34b1607..affd8bb 100644\n--- a/python/sglang/srt/managers/router/model_runner.py\n+++ b/python/sglang/srt/managers/router/model_runner.py\n@@ -69,9 +69,10 @@ class InputMetadata:\n             BatchPrefillWithPagedKVCacheWrapper,\n         )\n \n-        self.kv_indptr = torch.zeros(\n+        self.kv_indptr = torch.empty(\n             (self.batch_size + 1,), dtype=torch.int32, device=\"cuda\"\n         )\n+        self.kv_indptr[0] = 0\n         self.kv_indptr[1:] = torch.cumsum(self.seq_lens, dim=0)\n         self.kv_last_page_len = torch.ones(\n             (self.batch_size,), dtype=torch.int32, device=\"cuda\"\n@@ -95,9 +96,10 @@ class InputMetadata:\n             self.forward_mode == ForwardMode.PREFILL\n             or self.forward_mode == ForwardMode.EXTEND\n         ):\n-            self.qo_indptr = torch.zeros(\n+            self.qo_indptr = torch.empty(\n                 (self.batch_size + 1,), dtype=torch.int32, device=\"cuda\"\n             )\n+            self.qo_indptr[0] = 0\n             self.qo_indptr[1:] = torch.cumsum(self.extend_seq_lens, dim=0)\n             self.prefill_wrapper = BatchPrefillWithPagedKVCacheWrapper(\n                 workspace_buffer, \"NHD\"\n", "model_name_or_path": "gpt-5-2025-08-07"}
