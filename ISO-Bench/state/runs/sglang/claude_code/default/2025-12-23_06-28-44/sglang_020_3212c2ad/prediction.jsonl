{"instance_id": "/home/ubuntu/OmniPerf-Bench/sglan__repo-5347567", "model_patch": "diff --git a/python/sglang/srt/managers/mm_utils.py b/python/sglang/srt/managers/mm_utils.py\nindex 13ca29c54..a46535bca 100644\n--- a/python/sglang/srt/managers/mm_utils.py\n+++ b/python/sglang/srt/managers/mm_utils.py\n@@ -231,7 +231,7 @@ def _get_precomputed_embedding(\n             raise NotImplementedError(\n                 \"MM inputs where only some items are precomputed.\"\n             )\n-        result = torch.concat(precomputed_embeddings)\n+        result = torch.cat(precomputed_embeddings)\n         # some models embedding is 3-dim, reshape it to 2-dim (similar to get_embedding_chunk)\n         result = result.reshape(-1, result.shape[-1])\n         return result\n@@ -286,7 +286,7 @@ def _get_chunked_prefill_embedding(\n         embedding_list.append(embedding_per_req_chunk)\n     if len(embedding_list) == 0:\n         return None\n-    return torch.concat(embedding_list, dim=0)\n+    return torch.cat(embedding_list, dim=0)\n \n \n def _get_multimodal_mask(\n@@ -433,7 +433,8 @@ def embed_mm_inputs(\n                 device=input_ids.device,\n             )\n             # calculate per request items length offset\n-            items_size = torch.zeros(len(mm_inputs_list) + 1, dtype=int)\n+            items_size = torch.empty(len(mm_inputs_list) + 1, dtype=int)\n+            items_size[0] = 0\n             items_offsets = []\n             for i, mm_inputs in enumerate(mm_inputs_list):\n                 mm_items = [\n@@ -591,7 +592,7 @@ def get_multimodal_data_bounds(\n     valid_mm_data_nums = min(len(data_start_tokens_cpu), len(data_end_tokens_cpu))\n \n     if valid_mm_data_nums == 0:\n-        return torch.zeros((0, 2), device=input_ids.device)\n+        return torch.empty((0, 2), dtype=torch.int64, device=input_ids.device)\n \n     # Filter out pairs where start_token >= end_token\n     valid_pairs = []\n@@ -602,7 +603,7 @@ def get_multimodal_data_bounds(\n             valid_pairs.append((start_token + 1, end_token - 1))\n \n     if not valid_pairs:\n-        return torch.zeros((0, 2), device=input_ids.device)\n+        return torch.empty((0, 2), dtype=torch.int64, device=input_ids.device)\n \n     # Convert valid pairs to tensor\n     valid_pairs_tensor = torch.as_tensor(valid_pairs, device=input_ids.device)\n@@ -624,7 +625,7 @@ def tensor_hash(tensor_list) -> int:\n         tensor_list = [\n             x.flatten() if isinstance(x, torch.Tensor) else x for x in tensor_list\n         ]\n-        tensor = torch.concat(tensor_list)\n+        tensor = torch.cat(tensor_list)\n     if tensor.is_cuda:\n         return gpu_tensor_hash(tensor)\n     tensor = tensor.detach().contiguous()\ndiff --git a/python/sglang/srt/managers/schedule_batch.py b/python/sglang/srt/managers/schedule_batch.py\nindex ad8bcf119..963a8258e 100644\n--- a/python/sglang/srt/managers/schedule_batch.py\n+++ b/python/sglang/srt/managers/schedule_batch.py\n@@ -1096,16 +1096,12 @@ class ScheduleBatch(ScheduleBatchDisaggregationDecodeMixin):\n         )\n \n         if not decoder_out_cache_loc:\n-            self.out_cache_loc = torch.zeros(0, dtype=torch.int64).to(\n-                self.device, non_blocking=True\n-            )\n+            self.out_cache_loc = torch.empty(0, dtype=torch.int64, device=self.device)\n         else:\n             self.out_cache_loc = torch.cat(decoder_out_cache_loc)\n \n         if not encoder_out_cache_loc:\n-            self.encoder_out_cache_loc = torch.zeros(0, dtype=torch.int64).to(\n-                self.device, non_blocking=True\n-            )\n+            self.encoder_out_cache_loc = torch.empty(0, dtype=torch.int64, device=self.device)\n         else:\n             self.encoder_out_cache_loc = torch.cat(encoder_out_cache_loc)\n \n", "model_name_or_path": "gpt-5-2025-08-07"}
