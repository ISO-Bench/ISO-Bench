{"instance_id": "/home/ubuntu/OmniPerf-Bench/sglan__repo-3d7f7a4", "model_patch": "diff --git a/python/sglang/srt/layers/attention/flashattention_backend.py b/python/sglang/srt/layers/attention/flashattention_backend.py\nindex 2f974ea9a..496acb82e 100644\n--- a/python/sglang/srt/layers/attention/flashattention_backend.py\n+++ b/python/sglang/srt/layers/attention/flashattention_backend.py\n@@ -1131,20 +1131,20 @@ class FlashAttentionBackend(AttentionBackend):\n         \"\"\"\n         # This is being used by normal decode and draft decode when topk == 1\n         self.decode_cuda_graph_metadata = {\n-            \"cache_seqlens\": torch.zeros(max_bs, dtype=torch.int32, device=self.device),\n+            \"cache_seqlens\": torch.empty(max_bs, dtype=torch.int32, device=self.device),\n             \"cu_seqlens_q\": torch.arange(\n                 0, max_bs + 1, dtype=torch.int32, device=self.device\n             ),\n-            \"cu_seqlens_k\": torch.zeros(\n+            \"cu_seqlens_k\": torch.empty(\n                 max_bs + 1, dtype=torch.int32, device=self.device\n             ),\n-            \"page_table\": torch.zeros(\n+            \"page_table\": torch.empty(\n                 max_bs,\n                 (self.max_context_len + self.page_size - 1) // self.page_size,\n                 dtype=torch.int32,\n                 device=self.device,\n             ),\n-            \"page_table_draft_decode\": torch.zeros(\n+            \"page_table_draft_decode\": torch.empty(\n                 max_bs,\n                 (self.max_context_len + self.page_size - 1) // self.page_size,\n                 dtype=torch.int32,\n@@ -1168,13 +1168,13 @@ class FlashAttentionBackend(AttentionBackend):\n             max_pages_per_block = (attn_chunk_size + page_size - 1) // page_size\n \n             self.decode_cuda_graph_local_attn_metadata = {\n-                \"local_query_start_loc\": torch.zeros(\n+                \"local_query_start_loc\": torch.empty(\n                     max_virtual_batches + 1, dtype=torch.int32, device=self.device\n                 ),\n-                \"local_seqused_k\": torch.zeros(\n+                \"local_seqused_k\": torch.empty(\n                     max_virtual_batches, dtype=torch.int32, device=self.device\n                 ),\n-                \"local_block_table\": torch.zeros(\n+                \"local_block_table\": torch.empty(\n                     max_virtual_batches,\n                     max_pages_per_block,\n                     dtype=torch.int32,\n@@ -1185,7 +1185,7 @@ class FlashAttentionBackend(AttentionBackend):\n         # This is used by draft decode's first half of metadata when topk > 1\n         if self.topk > 1:\n             self.draft_decode_metadata_topk_normal = {\n-                \"cache_seqlens\": torch.zeros(\n+                \"cache_seqlens\": torch.empty(\n                     max_bs, dtype=torch.int32, device=self.device\n                 ),\n                 \"cu_seqlens_q\": torch.arange(\n@@ -1195,10 +1195,10 @@ class FlashAttentionBackend(AttentionBackend):\n                     dtype=torch.int32,\n                     device=self.device,\n                 ),\n-                \"cu_seqlens_k\": torch.zeros(\n+                \"cu_seqlens_k\": torch.empty(\n                     max_bs + 1, dtype=torch.int32, device=self.device\n                 ),\n-                \"page_table\": torch.zeros(\n+                \"page_table\": torch.empty(\n                     max_bs,\n                     self.max_context_len,\n                     dtype=torch.int32,\n@@ -1228,7 +1228,7 @@ class FlashAttentionBackend(AttentionBackend):\n                     dtype=torch.int32,\n                     device=self.device,\n                 ),\n-                \"page_table\": torch.zeros(\n+                \"page_table\": torch.empty(\n                     max_bs * self.topk,\n                     decode_length,\n                     dtype=torch.int32,\n@@ -1241,7 +1241,7 @@ class FlashAttentionBackend(AttentionBackend):\n             and self.speculative_num_draft_tokens > 0\n         ):\n             self.target_verify_metadata = {\n-                \"cache_seqlens\": torch.zeros(\n+                \"cache_seqlens\": torch.empty(\n                     max_bs, dtype=torch.int32, device=self.device\n                 ),\n                 \"cu_seqlens_q\": torch.arange(\n@@ -1251,10 +1251,10 @@ class FlashAttentionBackend(AttentionBackend):\n                     dtype=torch.int32,\n                     device=self.device,\n                 ),\n-                \"cu_seqlens_k\": torch.zeros(\n+                \"cu_seqlens_k\": torch.empty(\n                     max_bs + 1, dtype=torch.int32, device=self.device\n                 ),\n-                \"page_table\": torch.zeros(\n+                \"page_table\": torch.empty(\n                     max_bs,\n                     (self.max_context_len + self.page_size - 1) // self.page_size,\n                     dtype=torch.int32,\n@@ -1267,7 +1267,7 @@ class FlashAttentionBackend(AttentionBackend):\n \n         if self.topk > 1:\n             self.target_verify_metadata_topk_normal = {\n-                \"cache_seqlens\": torch.zeros(\n+                \"cache_seqlens\": torch.empty(\n                     max_bs, dtype=torch.int32, device=self.device\n                 ),\n                 \"cu_seqlens_q\": torch.arange(\n@@ -1277,10 +1277,10 @@ class FlashAttentionBackend(AttentionBackend):\n                     dtype=torch.int32,\n                     device=self.device,\n                 ),\n-                \"cu_seqlens_k\": torch.zeros(\n+                \"cu_seqlens_k\": torch.empty(\n                     max_bs + 1, dtype=torch.int32, device=self.device\n                 ),\n-                \"page_table\": torch.zeros(\n+                \"page_table\": torch.empty(\n                     max_bs,\n                     self.max_context_len,\n                     dtype=torch.int32,\n@@ -1289,12 +1289,12 @@ class FlashAttentionBackend(AttentionBackend):\n             }\n \n             self.target_verify_metadata_topk_expand = {\n-                \"cache_seqlens\": torch.zeros(\n+                \"cache_seqlens\": torch.empty(\n                     max_bs * self.speculative_num_draft_tokens,\n                     dtype=torch.int32,\n                     device=self.device,\n                 ),\n-                \"cu_seqlens_k\": torch.zeros(\n+                \"cu_seqlens_k\": torch.empty(\n                     max_bs * self.speculative_num_draft_tokens + 1,\n                     dtype=torch.int32,\n                     device=self.device,\n@@ -1305,7 +1305,7 @@ class FlashAttentionBackend(AttentionBackend):\n                     dtype=torch.int32,\n                     device=self.device,\n                 ),\n-                \"page_table\": torch.zeros(\n+                \"page_table\": torch.empty(\n                     max_bs * self.speculative_num_draft_tokens,\n                     self.speculative_num_draft_tokens,\n                     dtype=torch.int32,\n@@ -1314,16 +1314,16 @@ class FlashAttentionBackend(AttentionBackend):\n             }\n \n         self.encoder_metadata = {\n-            \"encoder_page_table\": torch.zeros(\n+            \"encoder_page_table\": torch.empty(\n                 max_bs,\n                 self.max_context_len,\n                 dtype=torch.int32,\n                 device=self.device,\n             ),\n-            \"encoder_lens_int32\": torch.zeros(\n+            \"encoder_lens_int32\": torch.empty(\n                 max_bs, dtype=torch.int32, device=self.device\n             ),\n-            \"encoder_cu_seqlens_k\": torch.zeros(\n+            \"encoder_cu_seqlens_k\": torch.empty(\n                 max_bs + 1, dtype=torch.int32, device=self.device\n             ),\n         }\n", "model_name_or_path": "gpt-5-2025-08-07"}
