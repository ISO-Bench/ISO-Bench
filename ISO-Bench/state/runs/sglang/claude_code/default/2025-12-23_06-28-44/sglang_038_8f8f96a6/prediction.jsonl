{"instance_id": "/home/ubuntu/OmniPerf-Bench/sglan__repo-05b3bf5", "model_patch": "diff --git a/python/sglang/srt/layers/sampler.py b/python/sglang/srt/layers/sampler.py\nindex 9ae5801..9008856 100644\n--- a/python/sglang/srt/layers/sampler.py\n+++ b/python/sglang/srt/layers/sampler.py\n@@ -80,7 +80,7 @@ class Sampler(nn.Module):\n \n                 if not torch.all(success):\n                     logger.warning(\"Detected errors during sampling!\")\n-                    batch_next_token_ids = torch.zeros_like(batch_next_token_ids)\n+                    batch_next_token_ids = torch.empty_like(batch_next_token_ids).fill_(0)\n             elif global_server_args_dict[\"sampling_backend\"] == \"pytorch\":\n                 # A slower fallback implementation with torch native operations.\n                 batch_next_token_ids = top_k_top_p_min_p_sampling_from_probs_torch(\ndiff --git a/python/sglang/srt/managers/schedule_batch.py b/python/sglang/srt/managers/schedule_batch.py\nindex fac008d..6541549 100644\n--- a/python/sglang/srt/managers/schedule_batch.py\n+++ b/python/sglang/srt/managers/schedule_batch.py\n@@ -543,9 +543,7 @@ class ScheduleBatch:\n                     or len(req.prefix_indices) >= im.num_image_tokens\n                 )\n \n-        self.encoder_lens = torch.tensor(self.encoder_lens_cpu, dtype=torch.int32).to(\n-            self.device, non_blocking=True\n-        )\n+        self.encoder_lens = torch.tensor(self.encoder_lens_cpu, dtype=torch.int32, device=self.device)\n \n         # Strip encoder infos\n         pt = 0\n@@ -574,24 +572,16 @@ class ScheduleBatch:\n             pt += req.extend_input_len\n \n         # Reassign\n-        self.input_ids = torch.tensor(sum(input_ids, []), dtype=torch.int32).to(\n-            self.device, non_blocking=True\n-        )\n-        self.seq_lens = torch.tensor(seq_lens, dtype=torch.int32).to(\n-            self.device, non_blocking=True\n-        )\n+        self.input_ids = torch.tensor(sum(input_ids, []), dtype=torch.int32, device=self.device)\n+        self.seq_lens = torch.tensor(seq_lens, dtype=torch.int32, device=self.device)\n \n         if not decoder_out_cache_loc:\n-            self.out_cache_loc = torch.empty(0, dtype=torch.int32).to(\n-                self.device, non_blocking=True\n-            )\n+            self.out_cache_loc = torch.empty(0, dtype=torch.int32, device=self.device)\n         else:\n             self.out_cache_loc = torch.cat(decoder_out_cache_loc)\n \n         if not encoder_out_cache_loc:\n-            self.encoder_out_cache_loc = torch.empty(0, dtype=torch.int32).to(\n-                self.device, non_blocking=True\n-            )\n+            self.encoder_out_cache_loc = torch.empty(0, dtype=torch.int32, device=self.device)\n         else:\n             self.encoder_out_cache_loc = torch.cat(encoder_out_cache_loc)\n \n@@ -645,15 +635,9 @@ class ScheduleBatch:\n             pt += req.extend_input_len\n \n         # Set fields\n-        self.input_ids = torch.tensor(sum(input_ids, []), dtype=torch.int32).to(\n-            self.device, non_blocking=True\n-        )\n-        self.req_pool_indices = torch.tensor(req_pool_indices, dtype=torch.int32).to(\n-            self.device, non_blocking=True\n-        )\n-        self.seq_lens = torch.tensor(seq_lens, dtype=torch.int32).to(\n-            self.device, non_blocking=True\n-        )\n+        self.input_ids = torch.tensor(sum(input_ids, []), dtype=torch.int32, device=self.device)\n+        self.req_pool_indices = torch.tensor(req_pool_indices, dtype=torch.int32, device=self.device)\n+        self.seq_lens = torch.tensor(seq_lens, dtype=torch.int32, device=self.device)\n \n         self.out_cache_loc = out_cache_loc\n \n@@ -926,9 +910,7 @@ class ScheduleBatch:\n             self.encoder_lens_cpu = [self.encoder_lens_cpu[i] for i in keep_indices]\n \n         self.reqs = [self.reqs[i] for i in keep_indices]\n-        new_indices = torch.tensor(keep_indices, dtype=torch.int32).to(\n-            self.device, non_blocking=True\n-        )\n+        new_indices = torch.tensor(keep_indices, dtype=torch.int32, device=self.device)\n         self.req_pool_indices = self.req_pool_indices[new_indices]\n         self.seq_lens = self.seq_lens[new_indices]\n         self.out_cache_loc = None\ndiff --git a/python/sglang/srt/sampling/penaltylib/penalizers/min_new_tokens.py b/python/sglang/srt/sampling/penaltylib/penalizers/min_new_tokens.py\nindex c9e0f07..9006150 100644\n--- a/python/sglang/srt/sampling/penaltylib/penalizers/min_new_tokens.py\n+++ b/python/sglang/srt/sampling/penaltylib/penalizers/min_new_tokens.py\n@@ -43,11 +43,11 @@ class BatchedMinNewTokensPenalizer(_BatchedPenalizer):\n             batch_first=True,\n             padding_value=self.orchestrator.vocab_size,\n         )\n-        self.stop_token_penalties = torch.zeros(\n+        self.stop_token_penalties = torch.empty(\n             size=(self.orchestrator.batch_size(), self.orchestrator.vocab_size + 1),\n             dtype=torch.float32,\n             device=self.orchestrator.device,\n-        ).scatter_add_(\n+        ).fill_(0).scatter_add_(\n             dim=1,\n             index=padded_stop_token_ids,\n             src=torch.full_like(\n", "model_name_or_path": "gpt-5-2025-08-07"}
