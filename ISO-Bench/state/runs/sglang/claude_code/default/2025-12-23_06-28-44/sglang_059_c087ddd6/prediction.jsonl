{"instance_id": "/home/ubuntu/OmniPerf-Bench/sglan__repo-f4a8987", "model_patch": "diff --git a/benchmark/kernels/fused_moe_triton/benchmark_ep_pre_reorder_triton.py b/benchmark/kernels/fused_moe_triton/benchmark_ep_pre_reorder_triton.py\nnew file mode 100644\nindex 0000000..d25a456\n--- /dev/null\n+++ b/benchmark/kernels/fused_moe_triton/benchmark_ep_pre_reorder_triton.py\n@@ -0,0 +1,189 @@\n+import argparse\n+import time\n+\n+import torch\n+import triton\n+\n+from sglang.srt.layers.moe.ep_moe.kernels import (\n+    run_moe_ep_preproess,\n+    pre_reorder_triton_kernel,\n+    post_reorder_triton_kernel,\n+)\n+\n+\n+def benchmark_ep_preprocess(\n+    num_tokens: int,\n+    num_experts: int,\n+    topk: int,\n+    num_iters: int = 100,\n+):\n+    \"\"\"Benchmark the EP MoE preprocessing kernel.\"\"\"\n+    # Create input data\n+    topk_ids = torch.randint(\n+        0, num_experts, (num_tokens, topk), dtype=torch.int32, device=\"cuda\"\n+    )\n+\n+    # Warmup\n+    for _ in range(10):\n+        reorder_topk_ids, src2dst, seg_indptr = run_moe_ep_preproess(\n+            topk_ids, num_experts\n+        )\n+\n+    # Benchmark\n+    torch.cuda.synchronize()\n+    start = time.time()\n+\n+    for _ in range(num_iters):\n+        reorder_topk_ids, src2dst, seg_indptr = run_moe_ep_preproess(\n+            topk_ids, num_experts\n+        )\n+\n+    torch.cuda.synchronize()\n+    duration = time.time() - start\n+\n+    avg_time = duration / num_iters\n+    print(\n+        f\"EP Preprocess: num_tokens={num_tokens}, num_experts={num_experts}, \"\n+        f\"topk={topk}, avg_time={avg_time*1000:.4f}ms\"\n+    )\n+    return avg_time\n+\n+\n+def benchmark_reorder_kernels(\n+    num_tokens: int,\n+    hidden_size: int,\n+    num_experts: int,\n+    topk: int,\n+    num_iters: int = 100,\n+):\n+    \"\"\"Benchmark the pre and post reorder kernels.\"\"\"\n+    # Create input data\n+    topk_ids = torch.randint(\n+        0, num_experts, (num_tokens, topk), dtype=torch.int32, device=\"cuda\"\n+    )\n+    topk_weights = torch.rand(num_tokens, topk, dtype=torch.float32, device=\"cuda\")\n+    input_tensor = torch.randn(num_tokens, hidden_size, dtype=torch.float16, device=\"cuda\")\n+\n+    # Preprocess to get indices\n+    reorder_topk_ids, src2dst, seg_indptr = run_moe_ep_preproess(topk_ids, num_experts)\n+\n+    # Allocate output tensors\n+    gateup_input = torch.empty(\n+        topk_ids.numel(), hidden_size, dtype=torch.float16, device=\"cuda\"\n+    )\n+    output = torch.empty(num_tokens, hidden_size, dtype=torch.float16, device=\"cuda\")\n+\n+    BLOCK_SIZE = 128\n+\n+    # Warmup\n+    for _ in range(10):\n+        pre_reorder_triton_kernel[(num_tokens,)](\n+            input_tensor,\n+            gateup_input,\n+            src2dst,\n+            topk_ids,\n+            None,\n+            0,\n+            num_experts - 1,\n+            topk,\n+            hidden_size,\n+            BLOCK_SIZE,\n+        )\n+\n+    # Benchmark pre-reorder\n+    torch.cuda.synchronize()\n+    start = time.time()\n+\n+    for _ in range(num_iters):\n+        pre_reorder_triton_kernel[(num_tokens,)](\n+            input_tensor,\n+            gateup_input,\n+            src2dst,\n+            topk_ids,\n+            None,\n+            0,\n+            num_experts - 1,\n+            topk,\n+            hidden_size,\n+            BLOCK_SIZE,\n+        )\n+\n+    torch.cuda.synchronize()\n+    duration = time.time() - start\n+\n+    avg_time = duration / num_iters\n+    print(\n+        f\"Pre-Reorder: num_tokens={num_tokens}, hidden_size={hidden_size}, \"\n+        f\"avg_time={avg_time*1000:.4f}ms\"\n+    )\n+\n+    # Benchmark post-reorder\n+    down_output = torch.randn(\n+        topk_ids.numel(), hidden_size, dtype=torch.float16, device=\"cuda\"\n+    )\n+\n+    # Warmup\n+    for _ in range(10):\n+        post_reorder_triton_kernel[(num_tokens,)](\n+            down_output,\n+            output,\n+            src2dst,\n+            topk_ids,\n+            topk_weights,\n+            0,\n+            num_experts - 1,\n+            topk,\n+            hidden_size,\n+            BLOCK_SIZE,\n+        )\n+\n+    torch.cuda.synchronize()\n+    start = time.time()\n+\n+    for _ in range(num_iters):\n+        post_reorder_triton_kernel[(num_tokens,)](\n+            down_output,\n+            output,\n+            src2dst,\n+            topk_ids,\n+            topk_weights,\n+            0,\n+            num_experts - 1,\n+            topk,\n+            hidden_size,\n+            BLOCK_SIZE,\n+        )\n+\n+    torch.cuda.synchronize()\n+    duration = time.time() - start\n+\n+    avg_time = duration / num_iters\n+    print(\n+        f\"Post-Reorder: num_tokens={num_tokens}, hidden_size={hidden_size}, \"\n+        f\"avg_time={avg_time*1000:.4f}ms\"\n+    )\n+\n+\n+if __name__ == \"__main__\":\n+    parser = argparse.ArgumentParser()\n+    parser.add_argument(\"--num-tokens\", type=int, default=4096)\n+    parser.add_argument(\"--hidden-size\", type=int, default=4096)\n+    parser.add_argument(\"--num-experts\", type=int, default=64)\n+    parser.add_argument(\"--topk\", type=int, default=2)\n+    parser.add_argument(\"--num-iters\", type=int, default=100)\n+    args = parser.parse_args()\n+\n+    print(\"Benchmarking EP MoE Preprocessing and Reordering Kernels\")\n+    print(\"=\" * 60)\n+\n+    benchmark_ep_preprocess(\n+        args.num_tokens, args.num_experts, args.topk, args.num_iters\n+    )\n+\n+    benchmark_reorder_kernels(\n+        args.num_tokens,\n+        args.hidden_size,\n+        args.num_experts,\n+        args.topk,\n+        args.num_iters,\n+    )\ndiff --git a/python/sglang/srt/layers/moe/ep_moe/kernels.py b/python/sglang/srt/layers/moe/ep_moe/kernels.py\nindex 8c00552..1261766 100644\n--- a/python/sglang/srt/layers/moe/ep_moe/kernels.py\n+++ b/python/sglang/srt/layers/moe/ep_moe/kernels.py\n@@ -121,7 +121,7 @@ def deepep_run_moe_deep_preprocess(topk_ids: torch.Tensor, num_experts: int):\n     )\n     torch.searchsorted(reorder_topk_ids, expert_ids, out=seg_indptr)\n     num_minus_one = seg_indptr[0]\n-    seg_indptr = seg_indptr - num_minus_one\n+    seg_indptr.sub_(num_minus_one)\n \n     BLOCK_SIZE = 512\n     grid = (triton.cdiv(topk_ids.numel(), BLOCK_SIZE),)\n@@ -151,7 +151,8 @@ def compute_seg_indptr_triton_kernel(reorder_topk_ids, seg_indptr, num_toks):\n \n def run_moe_ep_preproess(topk_ids: torch.Tensor, num_experts: int):\n     reorder_topk_ids, reorder_ids = torch.sort(topk_ids.view(-1), stable=True)\n-    seg_indptr = torch.zeros(num_experts + 1, device=topk_ids.device, dtype=torch.int64)\n+    seg_indptr = torch.empty(num_experts + 1, device=topk_ids.device, dtype=torch.int64)\n+    seg_indptr[0] = 0\n     src2dst = torch.empty(topk_ids.numel(), device=topk_ids.device, dtype=torch.int32)\n \n     compute_seg_indptr_triton_kernel[(num_experts,)](\n@@ -479,7 +480,6 @@ def post_reorder_triton_kernel(\n     topk_ids_ptr = topk_ids_ptr + src_idx * topk\n     topk_weights_ptr = topk_weights_ptr + src_idx * topk\n \n-    computed = False\n     store_ptr = output_ptr + src_idx * hidden_size\n     for start_offset in tl.range(0, hidden_size, BLOCK_SIZE):\n         offset = start_offset + tl.arange(0, BLOCK_SIZE)\n@@ -489,7 +489,6 @@ def post_reorder_triton_kernel(\n         for idx in range(topk):\n             expert_id = tl.load(topk_ids_ptr + idx)\n             if expert_id >= start_expert_id and expert_id <= end_expert_id:\n-                computed = True\n                 dst_idx = tl.load(src2dst_ptr + idx)\n                 weigh_scale = tl.load(topk_weights_ptr + idx).to(InDtype)\n                 load_ptr = down_output_ptr + dst_idx * hidden_size\n@@ -497,14 +496,6 @@ def post_reorder_triton_kernel(\n                 sum_vec += in_data * weigh_scale\n         tl.store(store_ptr + offset, sum_vec, mask=mask)\n \n-    if computed == False:\n-        for start_offset in tl.range(0, hidden_size, BLOCK_SIZE):\n-            offset = start_offset + tl.arange(0, BLOCK_SIZE)\n-            mask = offset < hidden_size\n-            tl.store(\n-                store_ptr + offset, tl.zeros([BLOCK_SIZE], dtype=InDtype), mask=mask\n-            )\n-\n \n @triton.jit\n def compute_m_range(\n@@ -679,7 +670,8 @@ def grouped_gemm_triton(\n         \"BLOCK_SIZE_K\": 128,\n     }\n \n-    m_num_tiles_indptr = torch.zeros(batch_size + 1, device=a.device, dtype=torch.int64)\n+    m_num_tiles_indptr = torch.empty(batch_size + 1, device=a.device, dtype=torch.int64)\n+    m_num_tiles_indptr[0] = 0\n     compute_m_num_tiles_indptr[(1,)](\n         m_num_tiles_indptr, seg_indptr, batch_size, config[\"BLOCK_SIZE_M\"]\n     )\n", "model_name_or_path": "gpt-5-2025-08-07"}
