{"instance_id": "/home/ubuntu/OmniPerf-Bench/sglan__repo-ad506a4", "model_patch": "diff --git a/python/sglang/srt/layers/attention/flashattention_backend.py b/python/sglang/srt/layers/attention/flashattention_backend.py\nindex 4e85432..614e92f 100644\n--- a/python/sglang/srt/layers/attention/flashattention_backend.py\n+++ b/python/sglang/srt/layers/attention/flashattention_backend.py\n@@ -1130,20 +1130,20 @@ class FlashAttentionBackend(AttentionBackend):\n \n         # This is being used by normal decode and draft decode when topk == 1\n         self.decode_cuda_graph_metadata = {\n-            \"cache_seqlens\": torch.zeros(max_bs, dtype=torch.int32, device=self.device),\n+            \"cache_seqlens\": torch.empty(max_bs, dtype=torch.int32, device=self.device),\n             \"cu_seqlens_q\": torch.arange(\n                 0, max_bs + 1, dtype=torch.int32, device=self.device\n             ),\n-            \"cu_seqlens_k\": torch.zeros(\n+            \"cu_seqlens_k\": torch.empty(\n                 max_bs + 1, dtype=torch.int32, device=self.device\n             ),\n-            \"page_table\": torch.zeros(\n+            \"page_table\": torch.empty(\n                 max_bs,\n                 (self.max_context_len + self.page_size - 1) // self.page_size,\n                 dtype=torch.int32,\n                 device=self.device,\n             ),\n-            \"page_table_draft_decode\": torch.zeros(\n+            \"page_table_draft_decode\": torch.empty(\n                 max_bs,\n                 (self.max_context_len + self.page_size - 1) // self.page_size,\n                 dtype=torch.int32,\n@@ -1157,7 +1157,7 @@ class FlashAttentionBackend(AttentionBackend):\n         # This is used by draft decode's first half of metadata when topk > 1\n         if self.topk > 1:\n             self.draft_decode_metadata_topk_normal = {\n-                \"cache_seqlens\": torch.zeros(\n+                \"cache_seqlens\": torch.empty(\n                     max_bs, dtype=torch.int32, device=self.device\n                 ),\n                 \"cu_seqlens_q\": torch.arange(\n@@ -1167,10 +1167,10 @@ class FlashAttentionBackend(AttentionBackend):\n                     dtype=torch.int32,\n                     device=self.device,\n                 ),\n-                \"cu_seqlens_k\": torch.zeros(\n+                \"cu_seqlens_k\": torch.empty(\n                     max_bs + 1, dtype=torch.int32, device=self.device\n                 ),\n-                \"page_table\": torch.zeros(\n+                \"page_table\": torch.empty(\n                     max_bs,\n                     self.max_context_len,\n                     dtype=torch.int32,\n@@ -1200,7 +1200,7 @@ class FlashAttentionBackend(AttentionBackend):\n                     dtype=torch.int32,\n                     device=self.device,\n                 ),\n-                \"page_table\": torch.zeros(\n+                \"page_table\": torch.empty(\n                     max_bs * self.topk,\n                     decode_length,\n                     dtype=torch.int32,\n@@ -1213,7 +1213,7 @@ class FlashAttentionBackend(AttentionBackend):\n             and self.speculative_num_draft_tokens > 0\n         ):\n             self.target_verify_metadata = {\n-                \"cache_seqlens\": torch.zeros(\n+                \"cache_seqlens\": torch.empty(\n                     max_bs, dtype=torch.int32, device=self.device\n                 ),\n                 \"cu_seqlens_q\": torch.arange(\n@@ -1223,10 +1223,10 @@ class FlashAttentionBackend(AttentionBackend):\n                     dtype=torch.int32,\n                     device=self.device,\n                 ),\n-                \"cu_seqlens_k\": torch.zeros(\n+                \"cu_seqlens_k\": torch.empty(\n                     max_bs + 1, dtype=torch.int32, device=self.device\n                 ),\n-                \"page_table\": torch.zeros(\n+                \"page_table\": torch.empty(\n                     max_bs,\n                     (self.max_context_len + self.page_size - 1) // self.page_size,\n                     dtype=torch.int32,\n@@ -1239,7 +1239,7 @@ class FlashAttentionBackend(AttentionBackend):\n \n         if self.topk > 1:\n             self.target_verify_metadata_topk_normal = {\n-                \"cache_seqlens\": torch.zeros(\n+                \"cache_seqlens\": torch.empty(\n                     max_bs, dtype=torch.int32, device=self.device\n                 ),\n                 \"cu_seqlens_q\": torch.arange(\n@@ -1249,10 +1249,10 @@ class FlashAttentionBackend(AttentionBackend):\n                     dtype=torch.int32,\n                     device=self.device,\n                 ),\n-                \"cu_seqlens_k\": torch.zeros(\n+                \"cu_seqlens_k\": torch.empty(\n                     max_bs + 1, dtype=torch.int32, device=self.device\n                 ),\n-                \"page_table\": torch.zeros(\n+                \"page_table\": torch.empty(\n                     max_bs,\n                     self.max_context_len,\n                     dtype=torch.int32,\n@@ -1261,12 +1261,12 @@ class FlashAttentionBackend(AttentionBackend):\n             }\n \n             self.target_verify_metadata_topk_expand = {\n-                \"cache_seqlens\": torch.zeros(\n+                \"cache_seqlens\": torch.empty(\n                     max_bs * self.speculative_num_draft_tokens,\n                     dtype=torch.int32,\n                     device=self.device,\n                 ),\n-                \"cu_seqlens_k\": torch.zeros(\n+                \"cu_seqlens_k\": torch.empty(\n                     max_bs * self.speculative_num_draft_tokens + 1,\n                     dtype=torch.int32,\n                     device=self.device,\n@@ -1277,7 +1277,7 @@ class FlashAttentionBackend(AttentionBackend):\n                     dtype=torch.int32,\n                     device=self.device,\n                 ),\n-                \"page_table\": torch.zeros(\n+                \"page_table\": torch.empty(\n                     max_bs * self.speculative_num_draft_tokens,\n                     self.speculative_num_draft_tokens,\n                     dtype=torch.int32,\n@@ -1286,16 +1286,16 @@ class FlashAttentionBackend(AttentionBackend):\n             }\n \n         self.encoder_metadata = {\n-            \"encoder_page_table\": torch.zeros(\n+            \"encoder_page_table\": torch.empty(\n                 max_bs,\n                 self.max_context_len,\n                 dtype=torch.int32,\n                 device=self.device,\n             ),\n-            \"encoder_lens_int32\": torch.zeros(\n+            \"encoder_lens_int32\": torch.empty(\n                 max_bs, dtype=torch.int32, device=self.device\n             ),\n-            \"encoder_cu_seqlens_k\": torch.zeros(\n+            \"encoder_cu_seqlens_k\": torch.empty(\n                 max_bs + 1, dtype=torch.int32, device=self.device\n             ),\n         }\n", "model_name_or_path": "gpt-5-2025-08-07"}
