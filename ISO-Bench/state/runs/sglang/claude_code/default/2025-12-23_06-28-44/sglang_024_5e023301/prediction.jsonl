{"instance_id": "/home/ubuntu/OmniPerf-Bench/sglan__repo-fa7d7fd", "model_patch": "diff --git a/python/sglang/srt/layers/quantization/fp8_utils.py b/python/sglang/srt/layers/quantization/fp8_utils.py\nindex aeab9d4..b118283 100644\n--- a/python/sglang/srt/layers/quantization/fp8_utils.py\n+++ b/python/sglang/srt/layers/quantization/fp8_utils.py\n@@ -146,7 +146,7 @@ def apply_w8a8_block_fp8_linear(\n         q_input, x_scale = per_token_group_quant_fp8(\n             input_2d, block_size[1], column_major_scales=False\n         )\n-        output = torch.zeros(\n+        output = torch.empty(\n             [q_input.shape[0], weight.shape[0]],\n             dtype=input.dtype,\n             device=q_input.device,\ndiff --git a/python/sglang/srt/models/deepseek_v2.py b/python/sglang/srt/models/deepseek_v2.py\nindex 339aaad..8dc8b34 100644\n--- a/python/sglang/srt/models/deepseek_v2.py\n+++ b/python/sglang/srt/models/deepseek_v2.py\n@@ -344,8 +344,8 @@ class DeepseekV2MoE(nn.Module):\n                 routed_scaling_factor=self.routed_scaling_factor,\n             )\n         else:\n-            topk_idx = torch.full(\n-                (0, self.top_k), -1, dtype=torch.int, device=hidden_states.device\n+            topk_idx = torch.empty(\n+                (0, self.top_k), dtype=torch.int, device=hidden_states.device\n             )\n             topk_weights = torch.empty(\n                 (0, self.top_k), dtype=torch.float32, device=hidden_states.device\n@@ -781,8 +781,19 @@ class DeepseekV2AttentionMLA(nn.Module):\n                 q_nope_out, k_nope, k_nope, forward_batch, q_rope=q_pe, k_rope=k_pe\n             )\n         else:\n-            q = torch.cat([q_nope_out, q_pe], dim=-1)\n-            k = torch.cat([k_nope, k_pe], dim=-1)\n+            # Optimize: pre-allocate and use slice assignment instead of torch.cat\n+            q = torch.empty(\n+                (q_nope_out.shape[0], q_nope_out.shape[1], self.kv_lora_rank + self.qk_rope_head_dim),\n+                dtype=q_nope_out.dtype, device=q_nope_out.device\n+            )\n+            q[..., :self.kv_lora_rank] = q_nope_out\n+            q[..., self.kv_lora_rank:] = q_pe\n+            k = torch.empty(\n+                (k_nope.shape[0], k_nope.shape[1], self.kv_lora_rank + self.qk_rope_head_dim),\n+                dtype=k_nope.dtype, device=k_nope.device\n+            )\n+            k[..., :self.kv_lora_rank] = k_nope\n+            k[..., self.kv_lora_rank:] = k_pe\n             attn_output = self.attn_mqa(q, k, k_nope, forward_batch)\n         attn_output = attn_output.view(-1, self.num_local_heads, self.kv_lora_rank)\n \n", "model_name_or_path": "gpt-5-2025-08-07"}
