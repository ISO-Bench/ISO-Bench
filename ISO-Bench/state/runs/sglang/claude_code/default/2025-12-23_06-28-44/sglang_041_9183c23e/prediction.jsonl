{"instance_id": "/home/ubuntu/OmniPerf-Bench/sglan__repo-148254d", "model_patch": "diff --git a/python/sglang/srt/constrained/outlines_backend.py b/python/sglang/srt/constrained/outlines_backend.py\nindex 4820d47..55df525 100644\n--- a/python/sglang/srt/constrained/outlines_backend.py\n+++ b/python/sglang/srt/constrained/outlines_backend.py\n@@ -83,7 +83,7 @@ class OutlinesGrammar(BaseGrammarObject):\n     def allocate_vocab_mask(\n         self, vocab_size: int, batch_size: int, device\n     ) -> torch.Tensor:\n-        return torch.zeros(batch_size, vocab_size, dtype=torch.bool, device=device)\n+        return torch.empty(batch_size, vocab_size, dtype=torch.bool, device=device)\n \n     @staticmethod\n     def move_vocab_mask(vocab_mask: torch.Tensor, device) -> torch.Tensor:\ndiff --git a/python/sglang/srt/lora/lora_manager.py b/python/sglang/srt/lora/lora_manager.py\nindex 0449e25..7c287b9 100644\n--- a/python/sglang/srt/lora/lora_manager.py\n+++ b/python/sglang/srt/lora/lora_manager.py\n@@ -300,7 +300,8 @@ class LoRAManager:\n             else torch.ones(bs, device=\"cuda\")\n         )\n         # FIXME: reuse the data rather than recompute\n-        seg_indptr = torch.zeros((bs + 1,), dtype=torch.int32, device=\"cuda\")\n+        seg_indptr = torch.empty((bs + 1,), dtype=torch.int32, device=\"cuda\")\n+        seg_indptr[0] = 0\n         seg_indptr[1:] = torch.cumsum(seg_lens, dim=0)\n         weight_indices = torch.empty((bs,), dtype=torch.int64, device=\"cuda\")\n         for i, lora_path in enumerate(forward_batch.lora_paths):\ndiff --git a/python/sglang/srt/managers/schedule_batch.py b/python/sglang/srt/managers/schedule_batch.py\nindex 64cc9de..32ee8a9 100644\n--- a/python/sglang/srt/managers/schedule_batch.py\n+++ b/python/sglang/srt/managers/schedule_batch.py\n@@ -695,14 +695,14 @@ class ScheduleBatch:\n         )\n \n         if not decoder_out_cache_loc:\n-            self.out_cache_loc = torch.zeros(0, dtype=torch.int32).to(\n+            self.out_cache_loc = torch.empty(0, dtype=torch.int32).to(\n                 self.device, non_blocking=True\n             )\n         else:\n             self.out_cache_loc = torch.cat(decoder_out_cache_loc)\n \n         if not encoder_out_cache_loc:\n-            self.encoder_out_cache_loc = torch.zeros(0, dtype=torch.int32).to(\n+            self.encoder_out_cache_loc = torch.empty(0, dtype=torch.int32).to(\n                 self.device, non_blocking=True\n             )\n         else:\ndiff --git a/python/sglang/srt/model_executor/cuda_graph_runner.py b/python/sglang/srt/model_executor/cuda_graph_runner.py\nindex d045605..75433c9 100644\n--- a/python/sglang/srt/model_executor/cuda_graph_runner.py\n+++ b/python/sglang/srt/model_executor/cuda_graph_runner.py\n@@ -183,14 +183,14 @@ class CudaGraphRunner:\n \n         # Common inputs\n         with torch.device(\"cuda\"):\n-            self.input_ids = torch.zeros((self.max_num_token,), dtype=torch.int32)\n-            self.req_pool_indices = torch.zeros((self.max_bs,), dtype=torch.int32)\n+            self.input_ids = torch.empty((self.max_num_token,), dtype=torch.int32)\n+            self.req_pool_indices = torch.empty((self.max_bs,), dtype=torch.int32)\n             self.seq_lens = torch.full(\n                 (self.max_bs,), self.seq_len_fill_value, dtype=torch.int32\n             )\n-            self.out_cache_loc = torch.zeros((self.max_num_token,), dtype=torch.int32)\n-            self.positions = torch.zeros((self.max_num_token,), dtype=torch.int64)\n-            self.mrope_positions = torch.zeros((3, self.max_bs), dtype=torch.int32)\n+            self.out_cache_loc = torch.empty((self.max_num_token,), dtype=torch.int32)\n+            self.positions = torch.empty((self.max_num_token,), dtype=torch.int64)\n+            self.mrope_positions = torch.empty((3, self.max_bs), dtype=torch.int32)\n \n             if self.is_encoder_decoder:\n                 # NOTE: encoder_lens can influence the full_text_row_masked_out_mask tensor when doing mixed batch\n@@ -201,7 +201,7 @@ class CudaGraphRunner:\n                 self.encoder_lens = None\n \n             if self.enable_dp_attention:\n-                self.gathered_buffer = torch.zeros(\n+                self.gathered_buffer = torch.empty(\n                     (\n                         self.max_bs * self.tp_size,\n                         self.model_runner.model_config.hidden_size,\ndiff --git a/python/sglang/srt/model_executor/forward_batch_info.py b/python/sglang/srt/model_executor/forward_batch_info.py\nindex 2b5ee09..498000d 100644\n--- a/python/sglang/srt/model_executor/forward_batch_info.py\n+++ b/python/sglang/srt/model_executor/forward_batch_info.py\n@@ -269,7 +269,7 @@ class ForwardBatch:\n \n         if ret.global_num_tokens is not None:\n             max_len = max(ret.global_num_tokens)\n-            ret.gathered_buffer = torch.zeros(\n+            ret.gathered_buffer = torch.empty(\n                 (max_len * model_runner.tp_size, model_runner.model_config.hidden_size),\n                 dtype=model_runner.dtype,\n                 device=device,\ndiff --git a/python/sglang/srt/sampling/penaltylib/orchestrator.py b/python/sglang/srt/sampling/penaltylib/orchestrator.py\nindex 9c393d1..9e57112 100644\n--- a/python/sglang/srt/sampling/penaltylib/orchestrator.py\n+++ b/python/sglang/srt/sampling/penaltylib/orchestrator.py\n@@ -192,11 +192,11 @@ class _TokenIDs:\n                 batch_first=True,\n                 padding_value=self.orchestrator.vocab_size,\n             )\n-            self.cached_counts = torch.zeros(\n+            self.cached_counts = torch.empty(\n                 size=(self.orchestrator.batch_size(), self.orchestrator.vocab_size + 1),\n                 dtype=torch.int64,\n                 device=self.orchestrator.device,\n-            ).scatter_add_(\n+            ).fill_(0).scatter_add_(\n                 dim=1,\n                 index=padded_token_ids,\n                 src=torch.ones_like(padded_token_ids),\n@@ -206,10 +206,10 @@ class _TokenIDs:\n         else:\n             # TODO: optimize this part. We do not need to create this big tensor every time.\n             # We can directly apply the results on the logits.\n-            self.cached_counts = torch.zeros(\n+            self.cached_counts = torch.empty(\n                 size=(self.orchestrator.batch_size(), self.orchestrator.vocab_size),\n                 device=self.orchestrator.device,\n-            )\n+            ).fill_(0)\n             self.cached_counts[\n                 torch.arange(len(token_ids), device=self.orchestrator.device), token_ids\n             ] = 1\ndiff --git a/python/sglang/srt/sampling/penaltylib/penalizers/min_new_tokens.py b/python/sglang/srt/sampling/penaltylib/penalizers/min_new_tokens.py\nindex 0e27c7e..06e73cd 100644\n--- a/python/sglang/srt/sampling/penaltylib/penalizers/min_new_tokens.py\n+++ b/python/sglang/srt/sampling/penaltylib/penalizers/min_new_tokens.py\n@@ -46,11 +46,11 @@ class BatchedMinNewTokensPenalizer(_BatchedPenalizer):\n             batch_first=True,\n             padding_value=self.orchestrator.vocab_size,\n         )\n-        self.stop_token_penalties = torch.zeros(\n+        self.stop_token_penalties = torch.empty(\n             size=(self.orchestrator.batch_size(), self.orchestrator.vocab_size + 1),\n             dtype=torch.float32,\n             device=self.orchestrator.device,\n-        ).scatter_add_(\n+        ).fill_(0.0).scatter_add_(\n             dim=1,\n             index=padded_stop_token_ids,\n             src=torch.full_like(\n@@ -63,11 +63,11 @@ class BatchedMinNewTokensPenalizer(_BatchedPenalizer):\n             :, : self.orchestrator.vocab_size\n         ]\n \n-        self.len_output_tokens = torch.zeros(\n+        self.len_output_tokens = torch.empty(\n             size=(self.orchestrator.batch_size(), 1),\n             dtype=torch.int32,\n             device=self.orchestrator.device,\n-        )\n+        ).fill_(0)\n \n     def _teardown(self):\n         self.min_new_tokens = None\ndiff --git a/python/sglang/srt/sampling/sampling_batch_info.py b/python/sglang/srt/sampling/sampling_batch_info.py\nindex 3a46b22..c55fe95 100644\n--- a/python/sglang/srt/sampling/sampling_batch_info.py\n+++ b/python/sglang/srt/sampling/sampling_batch_info.py\n@@ -144,7 +144,7 @@ class SamplingBatchInfo:\n             else:\n                 if self.linear_penalties is None:\n                     bs = self.penalizer_orchestrator.batch.batch_size()\n-                    self.linear_penalties = torch.zeros(\n+                    self.linear_penalties = torch.empty(\n                         (bs, self.vocab_size),\n                         dtype=torch.float32,\n                         device=self.device,\n@@ -208,9 +208,9 @@ class SamplingBatchInfo:\n                 shape, dtype = rhs.shape[1:], rhs.dtype\n             with torch.dtype(dtype):\n                 if lhs is None:\n-                    lhs = torch.empty((bs1, *shape), device=device).fill_(default)\n+                    lhs = torch.full((bs1, *shape), default, device=device)\n                 if rhs is None:\n-                    rhs = torch.empty((bs2, *shape), device=device).fill_(default)\n+                    rhs = torch.full((bs2, *shape), default, device=device)\n             return torch.cat([lhs, rhs])\n \n         return None\n", "model_name_or_path": "gpt-5-2025-08-07"}
