diff --git a/python/sglang/srt/constrained/outlines_backend.py b/python/sglang/srt/constrained/outlines_backend.py
index 4820d47..55df525 100644
--- a/python/sglang/srt/constrained/outlines_backend.py
+++ b/python/sglang/srt/constrained/outlines_backend.py
@@ -83,7 +83,7 @@ class OutlinesGrammar(BaseGrammarObject):
     def allocate_vocab_mask(
         self, vocab_size: int, batch_size: int, device
     ) -> torch.Tensor:
-        return torch.zeros(batch_size, vocab_size, dtype=torch.bool, device=device)
+        return torch.empty(batch_size, vocab_size, dtype=torch.bool, device=device)
 
     @staticmethod
     def move_vocab_mask(vocab_mask: torch.Tensor, device) -> torch.Tensor:
diff --git a/python/sglang/srt/lora/lora_manager.py b/python/sglang/srt/lora/lora_manager.py
index 0449e25..7c287b9 100644
--- a/python/sglang/srt/lora/lora_manager.py
+++ b/python/sglang/srt/lora/lora_manager.py
@@ -300,7 +300,8 @@ class LoRAManager:
             else torch.ones(bs, device="cuda")
         )
         # FIXME: reuse the data rather than recompute
-        seg_indptr = torch.zeros((bs + 1,), dtype=torch.int32, device="cuda")
+        seg_indptr = torch.empty((bs + 1,), dtype=torch.int32, device="cuda")
+        seg_indptr[0] = 0
         seg_indptr[1:] = torch.cumsum(seg_lens, dim=0)
         weight_indices = torch.empty((bs,), dtype=torch.int64, device="cuda")
         for i, lora_path in enumerate(forward_batch.lora_paths):
diff --git a/python/sglang/srt/managers/schedule_batch.py b/python/sglang/srt/managers/schedule_batch.py
index 64cc9de..32ee8a9 100644
--- a/python/sglang/srt/managers/schedule_batch.py
+++ b/python/sglang/srt/managers/schedule_batch.py
@@ -695,14 +695,14 @@ class ScheduleBatch:
         )
 
         if not decoder_out_cache_loc:
-            self.out_cache_loc = torch.zeros(0, dtype=torch.int32).to(
+            self.out_cache_loc = torch.empty(0, dtype=torch.int32).to(
                 self.device, non_blocking=True
             )
         else:
             self.out_cache_loc = torch.cat(decoder_out_cache_loc)
 
         if not encoder_out_cache_loc:
-            self.encoder_out_cache_loc = torch.zeros(0, dtype=torch.int32).to(
+            self.encoder_out_cache_loc = torch.empty(0, dtype=torch.int32).to(
                 self.device, non_blocking=True
             )
         else:
diff --git a/python/sglang/srt/model_executor/cuda_graph_runner.py b/python/sglang/srt/model_executor/cuda_graph_runner.py
index d045605..75433c9 100644
--- a/python/sglang/srt/model_executor/cuda_graph_runner.py
+++ b/python/sglang/srt/model_executor/cuda_graph_runner.py
@@ -183,14 +183,14 @@ class CudaGraphRunner:
 
         # Common inputs
         with torch.device("cuda"):
-            self.input_ids = torch.zeros((self.max_num_token,), dtype=torch.int32)
-            self.req_pool_indices = torch.zeros((self.max_bs,), dtype=torch.int32)
+            self.input_ids = torch.empty((self.max_num_token,), dtype=torch.int32)
+            self.req_pool_indices = torch.empty((self.max_bs,), dtype=torch.int32)
             self.seq_lens = torch.full(
                 (self.max_bs,), self.seq_len_fill_value, dtype=torch.int32
             )
-            self.out_cache_loc = torch.zeros((self.max_num_token,), dtype=torch.int32)
-            self.positions = torch.zeros((self.max_num_token,), dtype=torch.int64)
-            self.mrope_positions = torch.zeros((3, self.max_bs), dtype=torch.int32)
+            self.out_cache_loc = torch.empty((self.max_num_token,), dtype=torch.int32)
+            self.positions = torch.empty((self.max_num_token,), dtype=torch.int64)
+            self.mrope_positions = torch.empty((3, self.max_bs), dtype=torch.int32)
 
             if self.is_encoder_decoder:
                 # NOTE: encoder_lens can influence the full_text_row_masked_out_mask tensor when doing mixed batch
@@ -201,7 +201,7 @@ class CudaGraphRunner:
                 self.encoder_lens = None
 
             if self.enable_dp_attention:
-                self.gathered_buffer = torch.zeros(
+                self.gathered_buffer = torch.empty(
                     (
                         self.max_bs * self.tp_size,
                         self.model_runner.model_config.hidden_size,
diff --git a/python/sglang/srt/model_executor/forward_batch_info.py b/python/sglang/srt/model_executor/forward_batch_info.py
index 2b5ee09..498000d 100644
--- a/python/sglang/srt/model_executor/forward_batch_info.py
+++ b/python/sglang/srt/model_executor/forward_batch_info.py
@@ -269,7 +269,7 @@ class ForwardBatch:
 
         if ret.global_num_tokens is not None:
             max_len = max(ret.global_num_tokens)
-            ret.gathered_buffer = torch.zeros(
+            ret.gathered_buffer = torch.empty(
                 (max_len * model_runner.tp_size, model_runner.model_config.hidden_size),
                 dtype=model_runner.dtype,
                 device=device,
diff --git a/python/sglang/srt/sampling/penaltylib/orchestrator.py b/python/sglang/srt/sampling/penaltylib/orchestrator.py
index 9c393d1..9e57112 100644
--- a/python/sglang/srt/sampling/penaltylib/orchestrator.py
+++ b/python/sglang/srt/sampling/penaltylib/orchestrator.py
@@ -192,11 +192,11 @@ class _TokenIDs:
                 batch_first=True,
                 padding_value=self.orchestrator.vocab_size,
             )
-            self.cached_counts = torch.zeros(
+            self.cached_counts = torch.empty(
                 size=(self.orchestrator.batch_size(), self.orchestrator.vocab_size + 1),
                 dtype=torch.int64,
                 device=self.orchestrator.device,
-            ).scatter_add_(
+            ).fill_(0).scatter_add_(
                 dim=1,
                 index=padded_token_ids,
                 src=torch.ones_like(padded_token_ids),
@@ -206,10 +206,10 @@ class _TokenIDs:
         else:
             # TODO: optimize this part. We do not need to create this big tensor every time.
             # We can directly apply the results on the logits.
-            self.cached_counts = torch.zeros(
+            self.cached_counts = torch.empty(
                 size=(self.orchestrator.batch_size(), self.orchestrator.vocab_size),
                 device=self.orchestrator.device,
-            )
+            ).fill_(0)
             self.cached_counts[
                 torch.arange(len(token_ids), device=self.orchestrator.device), token_ids
             ] = 1
diff --git a/python/sglang/srt/sampling/penaltylib/penalizers/min_new_tokens.py b/python/sglang/srt/sampling/penaltylib/penalizers/min_new_tokens.py
index 0e27c7e..06e73cd 100644
--- a/python/sglang/srt/sampling/penaltylib/penalizers/min_new_tokens.py
+++ b/python/sglang/srt/sampling/penaltylib/penalizers/min_new_tokens.py
@@ -46,11 +46,11 @@ class BatchedMinNewTokensPenalizer(_BatchedPenalizer):
             batch_first=True,
             padding_value=self.orchestrator.vocab_size,
         )
-        self.stop_token_penalties = torch.zeros(
+        self.stop_token_penalties = torch.empty(
             size=(self.orchestrator.batch_size(), self.orchestrator.vocab_size + 1),
             dtype=torch.float32,
             device=self.orchestrator.device,
-        ).scatter_add_(
+        ).fill_(0.0).scatter_add_(
             dim=1,
             index=padded_stop_token_ids,
             src=torch.full_like(
@@ -63,11 +63,11 @@ class BatchedMinNewTokensPenalizer(_BatchedPenalizer):
             :, : self.orchestrator.vocab_size
         ]
 
-        self.len_output_tokens = torch.zeros(
+        self.len_output_tokens = torch.empty(
             size=(self.orchestrator.batch_size(), 1),
             dtype=torch.int32,
             device=self.orchestrator.device,
-        )
+        ).fill_(0)
 
     def _teardown(self):
         self.min_new_tokens = None
diff --git a/python/sglang/srt/sampling/sampling_batch_info.py b/python/sglang/srt/sampling/sampling_batch_info.py
index 3a46b22..c55fe95 100644
--- a/python/sglang/srt/sampling/sampling_batch_info.py
+++ b/python/sglang/srt/sampling/sampling_batch_info.py
@@ -144,7 +144,7 @@ class SamplingBatchInfo:
             else:
                 if self.linear_penalties is None:
                     bs = self.penalizer_orchestrator.batch.batch_size()
-                    self.linear_penalties = torch.zeros(
+                    self.linear_penalties = torch.empty(
                         (bs, self.vocab_size),
                         dtype=torch.float32,
                         device=self.device,
@@ -208,9 +208,9 @@ class SamplingBatchInfo:
                 shape, dtype = rhs.shape[1:], rhs.dtype
             with torch.dtype(dtype):
                 if lhs is None:
-                    lhs = torch.empty((bs1, *shape), device=device).fill_(default)
+                    lhs = torch.full((bs1, *shape), default, device=device)
                 if rhs is None:
-                    rhs = torch.empty((bs2, *shape), device=device).fill_(default)
+                    rhs = torch.full((bs2, *shape), default, device=device)
             return torch.cat([lhs, rhs])
 
         return None
