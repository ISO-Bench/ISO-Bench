{"instance_id": "/home/ubuntu/OmniPerf-Bench/sglan__repo-9f2c956", "model_patch": "diff --git a/python/sglang/srt/layers/dp_attention.py b/python/sglang/srt/layers/dp_attention.py\nindex 2cc399ab7..7a22db619 100644\n--- a/python/sglang/srt/layers/dp_attention.py\n+++ b/python/sglang/srt/layers/dp_attention.py\n@@ -129,7 +129,7 @@ def get_dp_local_info(forward_batch: ForwardBatch):\n     if forward_batch.dp_local_start_pos is None:\n         cumtokens = torch.cumsum(forward_batch.global_num_tokens_gpu, dim=0)\n         if dp_rank == 0:\n-            local_start_pos = torch.zeros_like(cumtokens[0])\n+            local_start_pos = torch.tensor(0, dtype=cumtokens.dtype, device=cumtokens.device)\n         else:\n             local_start_pos = cumtokens[dp_rank - 1]\n         local_num_tokens = forward_batch.global_num_tokens_gpu[dp_rank]\ndiff --git a/python/sglang/srt/layers/logits_processor.py b/python/sglang/srt/layers/logits_processor.py\nindex 4958c6d04..915a5a16c 100644\n--- a/python/sglang/srt/layers/logits_processor.py\n+++ b/python/sglang/srt/layers/logits_processor.py\n@@ -171,13 +171,15 @@ class LogitsMetadata:\n         cumtokens = torch.cumsum(self.global_num_tokens_for_logprob_gpu, dim=0)\n         dp_rank = get_attention_dp_rank()\n         if dp_rank == 0:\n-            dp_local_start_pos = torch.zeros_like(\n-                self.global_num_tokens_for_logprob_gpu[0]\n+            dp_local_start_pos = torch.tensor(\n+                0,\n+                dtype=self.global_num_tokens_for_logprob_gpu.dtype,\n+                device=self.global_num_tokens_for_logprob_gpu.device\n             )\n         else:\n             dp_local_start_pos = cumtokens[dp_rank - 1]\n         dp_local_num_tokens = self.global_num_tokens_for_logprob_gpu[dp_rank]\n-        gathered_buffer = torch.zeros(\n+        gathered_buffer = torch.empty(\n             (\n                 sum(self.global_num_tokens_for_logprob_cpu),\n                 hidden_states.shape[1],\ndiff --git a/python/sglang/srt/managers/schedule_batch.py b/python/sglang/srt/managers/schedule_batch.py\nindex a797a7f3a..052f9d00b 100644\n--- a/python/sglang/srt/managers/schedule_batch.py\n+++ b/python/sglang/srt/managers/schedule_batch.py\n@@ -1024,14 +1024,14 @@ class ScheduleBatch(ScheduleBatchDisaggregationDecodeMixin):\n         )\n \n         if not decoder_out_cache_loc:\n-            self.out_cache_loc = torch.zeros(0, dtype=torch.int64).to(\n+            self.out_cache_loc = torch.empty(0, dtype=torch.int64).to(\n                 self.device, non_blocking=True\n             )\n         else:\n             self.out_cache_loc = torch.cat(decoder_out_cache_loc)\n \n         if not encoder_out_cache_loc:\n-            self.encoder_out_cache_loc = torch.zeros(0, dtype=torch.int64).to(\n+            self.encoder_out_cache_loc = torch.empty(0, dtype=torch.int64).to(\n                 self.device, non_blocking=True\n             )\n         else:\ndiff --git a/python/sglang/srt/models/llama.py b/python/sglang/srt/models/llama.py\nindex ab884ad9d..0b22570fe 100644\n--- a/python/sglang/srt/models/llama.py\n+++ b/python/sglang/srt/models/llama.py\n@@ -655,7 +655,7 @@ class LlamaForCausalLM(nn.Module):\n             else:\n                 weight = param.data\n             if tp_size > 1 and (\"o_proj\" in name or \"down_proj\" in name):\n-                gathered_weights = [torch.zeros_like(weight) for _ in range(tp_size)]\n+                gathered_weights = [torch.empty_like(weight) for _ in range(tp_size)]\n                 torch.distributed.all_gather(gathered_weights, weight)\n                 weight = torch.cat(gathered_weights, dim=1)\n             return weight.cpu().to(torch.float32).numpy().tolist()[:truncate_size]\n", "model_name_or_path": "gpt-5-2025-08-07"}
