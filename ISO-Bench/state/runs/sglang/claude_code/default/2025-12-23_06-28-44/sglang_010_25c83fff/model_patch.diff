diff --git a/python/sglang/srt/layers/dp_attention.py b/python/sglang/srt/layers/dp_attention.py
index 2cc399ab7..7a22db619 100644
--- a/python/sglang/srt/layers/dp_attention.py
+++ b/python/sglang/srt/layers/dp_attention.py
@@ -129,7 +129,7 @@ def get_dp_local_info(forward_batch: ForwardBatch):
     if forward_batch.dp_local_start_pos is None:
         cumtokens = torch.cumsum(forward_batch.global_num_tokens_gpu, dim=0)
         if dp_rank == 0:
-            local_start_pos = torch.zeros_like(cumtokens[0])
+            local_start_pos = torch.tensor(0, dtype=cumtokens.dtype, device=cumtokens.device)
         else:
             local_start_pos = cumtokens[dp_rank - 1]
         local_num_tokens = forward_batch.global_num_tokens_gpu[dp_rank]
diff --git a/python/sglang/srt/layers/logits_processor.py b/python/sglang/srt/layers/logits_processor.py
index 4958c6d04..915a5a16c 100644
--- a/python/sglang/srt/layers/logits_processor.py
+++ b/python/sglang/srt/layers/logits_processor.py
@@ -171,13 +171,15 @@ class LogitsMetadata:
         cumtokens = torch.cumsum(self.global_num_tokens_for_logprob_gpu, dim=0)
         dp_rank = get_attention_dp_rank()
         if dp_rank == 0:
-            dp_local_start_pos = torch.zeros_like(
-                self.global_num_tokens_for_logprob_gpu[0]
+            dp_local_start_pos = torch.tensor(
+                0,
+                dtype=self.global_num_tokens_for_logprob_gpu.dtype,
+                device=self.global_num_tokens_for_logprob_gpu.device
             )
         else:
             dp_local_start_pos = cumtokens[dp_rank - 1]
         dp_local_num_tokens = self.global_num_tokens_for_logprob_gpu[dp_rank]
-        gathered_buffer = torch.zeros(
+        gathered_buffer = torch.empty(
             (
                 sum(self.global_num_tokens_for_logprob_cpu),
                 hidden_states.shape[1],
diff --git a/python/sglang/srt/managers/schedule_batch.py b/python/sglang/srt/managers/schedule_batch.py
index a797a7f3a..052f9d00b 100644
--- a/python/sglang/srt/managers/schedule_batch.py
+++ b/python/sglang/srt/managers/schedule_batch.py
@@ -1024,14 +1024,14 @@ class ScheduleBatch(ScheduleBatchDisaggregationDecodeMixin):
         )
 
         if not decoder_out_cache_loc:
-            self.out_cache_loc = torch.zeros(0, dtype=torch.int64).to(
+            self.out_cache_loc = torch.empty(0, dtype=torch.int64).to(
                 self.device, non_blocking=True
             )
         else:
             self.out_cache_loc = torch.cat(decoder_out_cache_loc)
 
         if not encoder_out_cache_loc:
-            self.encoder_out_cache_loc = torch.zeros(0, dtype=torch.int64).to(
+            self.encoder_out_cache_loc = torch.empty(0, dtype=torch.int64).to(
                 self.device, non_blocking=True
             )
         else:
diff --git a/python/sglang/srt/models/llama.py b/python/sglang/srt/models/llama.py
index ab884ad9d..0b22570fe 100644
--- a/python/sglang/srt/models/llama.py
+++ b/python/sglang/srt/models/llama.py
@@ -655,7 +655,7 @@ class LlamaForCausalLM(nn.Module):
             else:
                 weight = param.data
             if tp_size > 1 and ("o_proj" in name or "down_proj" in name):
-                gathered_weights = [torch.zeros_like(weight) for _ in range(tp_size)]
+                gathered_weights = [torch.empty_like(weight) for _ in range(tp_size)]
                 torch.distributed.all_gather(gathered_weights, weight)
                 weight = torch.cat(gathered_weights, dim=1)
             return weight.cpu().to(torch.float32).numpy().tolist()[:truncate_size]
