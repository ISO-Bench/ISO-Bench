diff --git a/python/sglang/srt/managers/router/infer_batch.py b/python/sglang/srt/managers/router/infer_batch.py
index 88f6031..df8d757 100644
--- a/python/sglang/srt/managers/router/infer_batch.py
+++ b/python/sglang/srt/managers/router/infer_batch.py
@@ -238,7 +238,7 @@ class Batch:
                 logit_bias[i] = int_token_logit_bias
 
         # Set fields
-        self.input_ids = torch.tensor(
+        self.input_ids = torch.as_tensor(
             flatten_input_ids, dtype=torch.int32, device=device
         )
         self.pixel_values = [r.pixel_values for r in reqs]
@@ -247,29 +247,29 @@ class Batch:
             r.image_offset - p_len for r, p_len in zip(reqs, prefix_lens)
         ]
         self.req_pool_indices = req_pool_indices
-        self.seq_lens = torch.tensor(seq_lens, dtype=torch.int32, device=device)
-        self.prefix_lens = torch.tensor(prefix_lens, dtype=torch.int32, device=device)
+        self.seq_lens = torch.from_numpy(seq_lens).to(dtype=torch.int32, device=device)
+        self.prefix_lens = torch.from_numpy(prefix_lens).to(dtype=torch.int32, device=device)
         self.position_ids_offsets = position_ids_offsets
         self.extend_num_tokens = extend_num_tokens
         self.out_cache_loc = out_cache_loc
 
-        self.temperatures = torch.tensor(
+        self.temperatures = torch.as_tensor(
             [r.sampling_params.temperature for r in reqs],
             dtype=torch.float,
             device=device,
         ).view(-1, 1)
-        self.top_ps = torch.tensor(
+        self.top_ps = torch.as_tensor(
             [r.sampling_params.top_p for r in reqs], dtype=torch.float, device=device
         ).view(-1, 1)
-        self.top_ks = torch.tensor(
+        self.top_ks = torch.as_tensor(
             [r.sampling_params.top_k for r in reqs], dtype=torch.int, device=device
         ).view(-1, 1)
-        self.frequency_penalties = torch.tensor(
+        self.frequency_penalties = torch.as_tensor(
             [r.sampling_params.frequency_penalty for r in reqs],
             dtype=torch.float,
             device=device,
         )
-        self.presence_penalties = torch.tensor(
+        self.presence_penalties = torch.as_tensor(
             [r.sampling_params.presence_penalty for r in reqs],
             dtype=torch.float,
             device=device,
@@ -366,7 +366,7 @@ class Batch:
             input_ids = [
                 r.output_ids[-1] if r.output_ids else r.input_ids[-1] for r in self.reqs
             ]
-        self.input_ids = torch.tensor(input_ids, dtype=torch.int32, device="cuda")
+        self.input_ids = torch.as_tensor(input_ids, dtype=torch.int32, device="cuda")
         self.seq_lens.add_(1)
         self.prefix_lens = None
 
@@ -394,7 +394,7 @@ class Batch:
 
     def filter_batch(self, unfinished_indices: List[int]):
         self.reqs = [self.reqs[i] for i in unfinished_indices]
-        new_indices = torch.tensor(unfinished_indices, dtype=torch.int32, device="cuda")
+        new_indices = torch.as_tensor(unfinished_indices, dtype=torch.int32, device="cuda")
         self.seq_lens = self.seq_lens[new_indices]
         self.input_ids = None
         self.req_pool_indices = self.req_pool_indices[new_indices]
@@ -447,10 +447,9 @@ class Batch:
 
         has_regex = any(req.regex_fsm is not None for req in self.reqs)
         if has_regex:
-            allowed_mask = torch.empty_like(logits[0], dtype=torch.bool)
+            allowed_mask = torch.zeros_like(logits[0], dtype=torch.bool)
             for i, req in enumerate(self.reqs):
                 if req.regex_fsm is not None:
-                    allowed_mask.zero_()
                     allowed_mask[
                         req.regex_fsm.allowed_token_ids(req.regex_fsm_state)
                     ] = 1
diff --git a/python/sglang/srt/managers/router/model_rpc.py b/python/sglang/srt/managers/router/model_rpc.py
index 8b7adf9..55e7364 100644
--- a/python/sglang/srt/managers/router/model_rpc.py
+++ b/python/sglang/srt/managers/router/model_rpc.py
@@ -85,7 +85,7 @@ class ModelRpcServer(rpyc.Service):
             self.model_config.context_len,
             self.max_total_num_token // 6 if server_args.max_prefill_num_token is None else server_args.max_prefill_num_token,
         )
-        self.int_token_logit_bias = torch.tensor(
+        self.int_token_logit_bias = torch.as_tensor(
             get_int_token_logit_bias(self.tokenizer, self.model_config.vocab_size)
         )
         set_random_seed(server_args.random_seed)
