{"instance_id": "/home/ubuntu/OmniPerf-Bench/sglan__repo-7360067", "model_patch": "diff --git a/python/sglang/bench_serving.py b/python/sglang/bench_serving.py\nindex 9b27221..56d8b59 100644\n--- a/python/sglang/bench_serving.py\n+++ b/python/sglang/bench_serving.py\n@@ -637,7 +637,8 @@ def sample_sharegpt_requests(\n \n     # Filter out sequences that are too long or too short\n     filtered_dataset: List[Tuple[str, int, int]] = []\n-    for i in range(len(dataset)):\n+    dataset_len = len(dataset)\n+    for i in range(dataset_len):\n         if len(filtered_dataset) == num_requests:\n             break\n \n@@ -690,13 +691,17 @@ def sample_random_requests(\n     dataset_path: str,\n     random_sample: bool = True,\n ) -> List[Tuple[str, int, int]]:\n+    # Pre-compute range bounds to avoid repeated calculations\n+    min_input_len = max(int(input_len * range_ratio), 1)\n+    min_output_len = int(output_len * range_ratio)\n+\n     input_lens = np.random.randint(\n-        max(int(input_len * range_ratio), 1),\n+        min_input_len,\n         input_len + 1,\n         size=num_prompts,\n     )\n     output_lens = np.random.randint(\n-        int(output_len * range_ratio),\n+        min_output_len,\n         output_len + 1,\n         size=num_prompts,\n     )\n@@ -771,8 +776,10 @@ def sample_random_requests(\n \n def gen_prompt(tokenizer, token_num):\n     \"\"\"Generate a random prompt of specified token length using tokenizer vocabulary.\"\"\"\n-    all_available_tokens = list(tokenizer.get_vocab().values())\n-    selected_tokens = random.choices(all_available_tokens, k=token_num)\n+    # Cache the token list to avoid repeated list() calls\n+    if not hasattr(tokenizer, '_cached_vocab_values'):\n+        tokenizer._cached_vocab_values = list(tokenizer.get_vocab().values())\n+    selected_tokens = random.choices(tokenizer._cached_vocab_values, k=token_num)\n     return tokenizer.decode(selected_tokens)\n \n \n@@ -815,9 +822,10 @@ def sample_generated_shared_prefix_requests(\n         system_prompt = gen_prompt(tokenizer, system_prompt_len)\n         system_prompts.append(system_prompt)\n \n-    # Generate questions\n+    # Generate questions (pre-compute total count)\n+    total_questions = num_groups * prompts_per_group\n     questions = []\n-    for _ in range(num_groups * prompts_per_group):\n+    for _ in range(total_questions):\n         question = gen_prompt(tokenizer, question_len)\n         questions.append(question)\n \n@@ -890,6 +898,8 @@ def calculate_metrics(\n     tokenizer: PreTrainedTokenizerBase,\n     backend: str,\n ) -> Tuple[BenchmarkMetrics, List[int]]:\n+    # Pre-allocate lists with known size for better performance\n+    num_outputs = len(outputs)\n     output_lens: List[int] = []\n     retokenized_output_lens: List[int] = []\n     total_input = 0\n@@ -898,21 +908,23 @@ def calculate_metrics(\n     tpots: List[float] = []\n     ttfts: List[float] = []\n     e2e_latencies: List[float] = []\n-    for i in range(len(outputs)):\n-        if outputs[i].success:\n-            output_len = outputs[i].output_len\n+\n+    # Use enumerate to avoid repeated indexing\n+    for i, output in enumerate(outputs):\n+        if output.success:\n+            output_len = output.output_len\n             output_lens.append(output_len)\n             retokenized_output_len = len(\n-                tokenizer.encode(outputs[i].generated_text, add_special_tokens=False)\n+                tokenizer.encode(output.generated_text, add_special_tokens=False)\n             )\n             retokenized_output_lens.append(retokenized_output_len)\n             total_input += input_requests[i][1]\n             if output_len > 1:\n-                tpots.append((outputs[i].latency - outputs[i].ttft) / (output_len - 1))\n-            itls += outputs[i].itl\n-            ttfts.append(outputs[i].ttft)\n+                tpots.append((output.latency - output.ttft) / (output_len - 1))\n+            itls.extend(output.itl)  # Use extend instead of += for better performance\n+            ttfts.append(output.ttft)\n \n-            e2e_latencies.append(outputs[i].latency)\n+            e2e_latencies.append(output.latency)\n \n             completed += 1\n         else:\ndiff --git a/python/sglang/test/test_utils.py b/python/sglang/test/test_utils.py\nindex 6bcacb4..6e4f58b 100644\n--- a/python/sglang/test/test_utils.py\n+++ b/python/sglang/test/test_utils.py\n@@ -510,6 +510,7 @@ class TestFile:\n def run_unittest_files(files: List[TestFile], timeout_per_file: float):\n     tic = time.time()\n     success = True\n+    num_files = len(files)\n \n     for i, file in enumerate(files):\n         filename, estimated_time = file.name, file.estimated_time\n@@ -520,7 +521,7 @@ def run_unittest_files(files: List[TestFile], timeout_per_file: float):\n \n             filename = os.path.join(os.getcwd(), filename)\n             print(\n-                f\".\\n.\\nBegin ({i}/{len(files) - 1}):\\npython3 {filename}\\n.\\n.\\n\",\n+                f\".\\n.\\nBegin ({i}/{num_files - 1}):\\npython3 {filename}\\n.\\n.\\n\",\n                 flush=True,\n             )\n             tic = time.time()\n@@ -532,7 +533,7 @@ def run_unittest_files(files: List[TestFile], timeout_per_file: float):\n             elapsed = time.time() - tic\n \n             print(\n-                f\".\\n.\\nEnd ({i}/{len(files) - 1}):\\n{filename=}, {elapsed=:.0f}, {estimated_time=}\\n.\\n.\\n\",\n+                f\".\\n.\\nEnd ({i}/{num_files - 1}):\\n{filename=}, {elapsed=:.0f}, {estimated_time=}\\n.\\n.\\n\",\n                 flush=True,\n             )\n             return process.returncode\n@@ -554,10 +555,11 @@ def run_unittest_files(files: List[TestFile], timeout_per_file: float):\n             success = False\n             break\n \n+    elapsed_total = time.time() - tic\n     if success:\n-        print(f\"Success. Time elapsed: {time.time() - tic:.2f}s\", flush=True)\n+        print(f\"Success. Time elapsed: {elapsed_total:.2f}s\", flush=True)\n     else:\n-        print(f\"Fail. Time elapsed: {time.time() - tic:.2f}s\", flush=True)\n+        print(f\"Fail. Time elapsed: {elapsed_total:.2f}s\", flush=True)\n \n     return 0 if success else -1\n \n@@ -582,36 +584,37 @@ def get_benchmark_args(\n     seed: int = 0,\n     pd_seperated: bool = False,\n ):\n-    return SimpleNamespace(\n-        backend=\"sglang\",\n-        base_url=base_url,\n-        host=None,\n-        port=None,\n-        dataset_name=dataset_name,\n-        dataset_path=dataset_path,\n-        model=None,\n-        tokenizer=tokenizer,\n-        num_prompts=num_prompts,\n-        sharegpt_output_len=sharegpt_output_len,\n-        sharegpt_context_len=sharegpt_context_len,\n-        random_input_len=random_input_len,\n-        random_output_len=random_output_len,\n-        random_range_ratio=0.0,\n-        request_rate=request_rate,\n-        multi=None,\n-        output_file=None,\n-        disable_tqdm=False,\n-        disable_stream=disable_stream,\n-        return_logprob=False,\n-        seed=seed,\n-        disable_ignore_eos=disable_ignore_eos,\n-        extra_request_body=None,\n-        apply_chat_template=False,\n-        profile=None,\n-        lora_name=None,\n-        prompt_suffix=\"\",\n-        pd_seperated=pd_seperated,\n-    )\n+    # Construct namespace directly with dict for faster initialization\n+    return SimpleNamespace(**{\n+        \"backend\": \"sglang\",\n+        \"base_url\": base_url,\n+        \"host\": None,\n+        \"port\": None,\n+        \"dataset_name\": dataset_name,\n+        \"dataset_path\": dataset_path,\n+        \"model\": None,\n+        \"tokenizer\": tokenizer,\n+        \"num_prompts\": num_prompts,\n+        \"sharegpt_output_len\": sharegpt_output_len,\n+        \"sharegpt_context_len\": sharegpt_context_len,\n+        \"random_input_len\": random_input_len,\n+        \"random_output_len\": random_output_len,\n+        \"random_range_ratio\": 0.0,\n+        \"request_rate\": request_rate,\n+        \"multi\": None,\n+        \"output_file\": None,\n+        \"disable_tqdm\": False,\n+        \"disable_stream\": disable_stream,\n+        \"return_logprob\": False,\n+        \"seed\": seed,\n+        \"disable_ignore_eos\": disable_ignore_eos,\n+        \"extra_request_body\": None,\n+        \"apply_chat_template\": False,\n+        \"profile\": None,\n+        \"lora_name\": None,\n+        \"prompt_suffix\": \"\",\n+        \"pd_seperated\": pd_seperated,\n+    })\n \n \n def run_bench_serving(\n@@ -803,18 +806,23 @@ def run_bench_one_batch_server(\n def lcs(X, Y):\n     m = len(X)\n     n = len(Y)\n-    L = [[0] * (n + 1) for _ in range(m + 1)]\n-\n-    for i in range(m + 1):\n-        for j in range(n + 1):\n-            if i == 0 or j == 0:\n-                L[i][j] = 0\n-            elif X[i - 1] == Y[j - 1]:\n-                L[i][j] = L[i - 1][j - 1] + 1\n+    # Optimize memory: use two rows instead of full 2D array\n+    # Previous row and current row\n+    prev_row = [0] * (n + 1)\n+    curr_row = [0] * (n + 1)\n+\n+    for i in range(1, m + 1):\n+        for j in range(1, n + 1):\n+            if X[i - 1] == Y[j - 1]:\n+                curr_row[j] = prev_row[j - 1] + 1\n             else:\n-                L[i][j] = max(L[i - 1][j], L[i][j - 1])\n+                curr_row[j] = max(prev_row[j], curr_row[j - 1])\n+        # Swap rows for next iteration\n+        prev_row, curr_row = curr_row, prev_row\n+        # Clear curr_row for reuse\n+        curr_row = [0] * (n + 1)\n \n-    return L[m][n]\n+    return prev_row[n]\n \n \n def calculate_rouge_l(output_strs_list1, output_strs_list2):\n@@ -823,10 +831,14 @@ def calculate_rouge_l(output_strs_list1, output_strs_list2):\n \n     for s1, s2 in zip(output_strs_list1, output_strs_list2):\n         lcs_len = lcs(s1, s2)\n-        precision = lcs_len / len(s1) if len(s1) > 0 else 0\n-        recall = lcs_len / len(s2) if len(s2) > 0 else 0\n-        if precision + recall > 0:\n-            fmeasure = (2 * precision * recall) / (precision + recall)\n+        len_s1 = len(s1)\n+        len_s2 = len(s2)\n+        # Avoid repeated len() calls\n+        precision = lcs_len / len_s1 if len_s1 > 0 else 0\n+        recall = lcs_len / len_s2 if len_s2 > 0 else 0\n+        precision_plus_recall = precision + recall\n+        if precision_plus_recall > 0:\n+            fmeasure = (2 * precision * recall) / precision_plus_recall\n         else:\n             fmeasure = 0.0\n         rouge_l_scores.append(fmeasure)\n", "model_name_or_path": "gpt-5-2025-08-07"}
