diff --git a/python/sglang/srt/mem_cache/memory_pool.py b/python/sglang/srt/mem_cache/memory_pool.py
index e307367..df50fb4 100644
--- a/python/sglang/srt/mem_cache/memory_pool.py
+++ b/python/sglang/srt/mem_cache/memory_pool.py
@@ -59,7 +59,7 @@ class ReqToTokenPool:
         self.max_context_len = max_context_len
         self.device = device
         with memory_saver_adapter.region():
-            self.req_to_token = torch.zeros(
+            self.req_to_token = torch.empty(
                 (size, max_context_len), dtype=torch.int32, device=device
             )
         self.free_slots = list(range(size))
@@ -245,7 +245,7 @@ class MHATokenToKVPool(BaseTokenToKVPool):
     @debug_timing
     def transfer(self, indices, flat_data):
         # transfer prepared data from host to device
-        flat_data = flat_data.to(device=self.device, non_blocking=False)
+        flat_data = flat_data.to(device=self.device, non_blocking=True)
         k_data, v_data = flat_data[0], flat_data[1]
         for i in range(self.layer_num):
             self.k_buffer[i][indices] = k_data[i]
@@ -486,9 +486,10 @@ class MLATokenToKVPoolHost:
         )
 
         # Initialize memory states and tracking structures.
-        self.mem_state = torch.zeros(
+        self.mem_state = torch.empty(
             (self.size,), dtype=torch.uint8, device=self.device
         )
+        self.mem_state.fill_(0)
         self.free_slots = torch.arange(self.size, dtype=torch.int32)
         self.can_use_mem_size = self.size
 
@@ -502,7 +503,7 @@ class MLATokenToKVPoolHost:
     def transfer(self, indices, flat_data):
         # backup prepared data from device to host
         self.kv_buffer[:, :, indices] = flat_data.to(
-            device=self.device, non_blocking=False
+            device=self.device, non_blocking=True
         )
 
     @synchronized
