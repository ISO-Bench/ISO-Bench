{"instance_id": "/home/ubuntu/OmniPerf-Bench/sglan__repo-2b3bdc9", "model_patch": "diff --git a/python/sglang/srt/managers/tokenizer_manager.py b/python/sglang/srt/managers/tokenizer_manager.py\nindex a391dd7..2152cdb 100644\n--- a/python/sglang/srt/managers/tokenizer_manager.py\n+++ b/python/sglang/srt/managers/tokenizer_manager.py\n@@ -558,16 +558,17 @@ class TokenizerManager:\n     ):\n         batch_size = obj.batch_size\n \n-        generators = []\n-        rids = []\n+        # Optimize: Pre-allocate lists with known size to avoid dynamic resizing\n+        generators = [None] * batch_size\n+        rids = [None] * batch_size\n         if getattr(obj, \"parallel_sample_num\", 1) == 1:\n             # Send all requests\n             for i in range(batch_size):\n                 tmp_obj = obj[i]\n                 tokenized_obj = await self._tokenize_one_request(tmp_obj)\n                 self._send_one_request(tmp_obj, tokenized_obj, created_time)\n-                generators.append(self._wait_one_response(tmp_obj, request))\n-                rids.append(tmp_obj.rid)\n+                generators[i] = self._wait_one_response(tmp_obj, request)\n+                rids[i] = tmp_obj.rid\n         else:\n             # FIXME: When using batch and parallel_sample_num together, the perf is not optimal.\n             if batch_size > 128:\n@@ -595,14 +596,20 @@ class TokenizerManager:\n                 await self._wait_one_response(tmp_obj, request).__anext__()\n \n             # Expand requests, assign new rids for them, and send them\n+            # Optimize: Pre-allocate with exact size needed\n+            total_samples = batch_size * obj.parallel_sample_num\n+            generators = [None] * total_samples\n+            rids = [None] * total_samples\n+            idx = 0\n             for i in range(batch_size):\n                 for _ in range(obj.parallel_sample_num):\n                     tmp_obj = copy.copy(objs[i])\n                     tokenized_obj = copy.copy(tokenized_objs[i])\n                     tokenized_obj.rid = tmp_obj.regenerate_rid()\n                     self._send_one_request(tmp_obj, tokenized_obj, created_time)\n-                    generators.append(self._wait_one_response(tmp_obj, request))\n-                    rids.append(tmp_obj.rid)\n+                    generators[idx] = self._wait_one_response(tmp_obj, request)\n+                    rids[idx] = tmp_obj.rid\n+                    idx += 1\n \n         # Wait for all requests\n         is_stream = hasattr(obj, \"stream\") and obj.stream\n@@ -931,16 +938,21 @@ class TokenizerManager:\n             BatchStrOut, BatchEmbeddingOut, BatchMultimodalOut, BatchTokenIDOut\n         ],\n     ):\n+        # Optimize: Cache method references to avoid repeated attribute lookups\n+        rid_to_state_get = self.rid_to_state.get\n+        finished_reasons = recv_obj.finished_reasons\n+        prompt_tokens = recv_obj.prompt_tokens\n+\n         for i, rid in enumerate(recv_obj.rids):\n-            state = self.rid_to_state.get(rid, None)\n+            state = rid_to_state_get(rid, None)\n             if state is None:\n                 continue\n \n             # Build meta_info and return value\n             meta_info = {\n                 \"id\": rid,\n-                \"finish_reason\": recv_obj.finished_reasons[i],\n-                \"prompt_tokens\": recv_obj.prompt_tokens[i],\n+                \"finish_reason\": finished_reasons[i],\n+                \"prompt_tokens\": prompt_tokens[i],\n             }\n \n             if getattr(state.obj, \"return_logprob\", False):\n@@ -1060,14 +1072,14 @@ class TokenizerManager:\n         decode_to_text: bool,\n     ):\n         if not decode_to_text:\n-            return [\n-                (logprob, token_id, None)\n-                for logprob, token_id in zip(token_logprobs_val, token_logprobs_idx)\n-            ]\n+            # Optimize: Direct tuple construction without intermediate list comprehension\n+            return tuple((logprob, token_id, None)\n+                        for logprob, token_id in zip(token_logprobs_val, token_logprobs_idx))\n         else:\n             assert self.tokenizer is not None\n             token_texts = self.tokenizer.batch_decode(token_logprobs_idx)\n-            return list(zip(token_logprobs_val, token_logprobs_idx, token_texts))\n+            # Optimize: Use tuple instead of list for immutable return\n+            return tuple(zip(token_logprobs_val, token_logprobs_idx, token_texts))\n \n     def detokenize_top_logprobs_tokens(\n         self,\n@@ -1077,16 +1089,14 @@ class TokenizerManager:\n     ):\n         # TODO: The current implementation only batches the detokenization for top-k tokens per single position.\n         # We should batch all top-k tokens in all positions.\n-        ret = []\n-        for i in range(len(token_logprobs_val)):\n+        # Optimize: Pre-allocate list with known size to avoid dynamic resizing\n+        n = len(token_logprobs_val)\n+        ret = [None] * n\n+        for i in range(n):\n             if token_logprobs_val[i]:\n-                ret.append(\n-                    self.detokenize_logprob_tokens(\n-                        token_logprobs_val[i], token_logprobs_idx[i], decode_to_text\n-                    )\n+                ret[i] = self.detokenize_logprob_tokens(\n+                    token_logprobs_val[i], token_logprobs_idx[i], decode_to_text\n                 )\n-            else:\n-                ret.append(None)\n         return ret\n \n     def collect_metrics(self, state: ReqState, recv_obj: BatchStrOut, i: int):\n@@ -1195,7 +1205,9 @@ class _Communicator(Generic[T]):\n         self._sender = sender\n         self._fan_out = fan_out\n         self._result_event: Optional[asyncio.Event] = None\n+        # Optimize: Pre-allocate list with expected size to avoid dynamic resizing\n         self._result_values: Optional[List[T]] = None\n+        self._result_count: int = 0\n         self._ready_queue: Deque[asyncio.Future] = deque()\n \n     async def __call__(self, obj):\n@@ -1210,17 +1222,25 @@ class _Communicator(Generic[T]):\n             self._sender.send_pyobj(obj)\n \n         self._result_event = asyncio.Event()\n-        self._result_values = []\n+        # Optimize: Pre-allocate list with exact size to avoid resizing\n+        self._result_values = [None] * self._fan_out if self._fan_out > 1 else []\n+        self._result_count = 0\n         await self._result_event.wait()\n         result_values = self._result_values\n         self._result_event = self._result_values = None\n+        self._result_count = 0\n \n-        if len(self._ready_queue) > 0:\n+        if self._ready_queue:\n             self._ready_queue.popleft().set()\n \n         return result_values\n \n     def handle_recv(self, recv_obj: T):\n-        self._result_values.append(recv_obj)\n-        if len(self._result_values) == self._fan_out:\n+        # Optimize: Use counter and index assignment for pre-allocated lists\n+        if self._fan_out > 1:\n+            self._result_values[self._result_count] = recv_obj\n+        else:\n+            self._result_values.append(recv_obj)\n+        self._result_count += 1\n+        if self._result_count == self._fan_out:\n             self._result_event.set()\ndiff --git a/python/sglang/srt/server_args.py b/python/sglang/srt/server_args.py\nindex e1768b5..cb6e994 100644\n--- a/python/sglang/srt/server_args.py\n+++ b/python/sglang/srt/server_args.py\n@@ -1201,8 +1201,10 @@ class ServerArgs:\n         args.tp_size = args.tensor_parallel_size\n         args.dp_size = args.data_parallel_size\n         args.ep_size = args.expert_parallel_size\n-        attrs = [attr.name for attr in dataclasses.fields(cls)]\n-        return cls(**{attr: getattr(args, attr) for attr in attrs})\n+        # Optimize: Build dict directly without intermediate list\n+        fields = dataclasses.fields(cls)\n+        kwargs = {field.name: getattr(args, field.name) for field in fields}\n+        return cls(**kwargs)\n \n     def url(self):\n         if is_valid_ipv6_address(self.host):\n@@ -1227,14 +1229,13 @@ class ServerArgs:\n         assert self.gpu_id_step >= 1, \"gpu_id_step must be positive\"\n \n         if isinstance(self.lora_paths, list):\n+            # Optimize: Build dict directly using dict comprehension\n             lora_paths = self.lora_paths\n-            self.lora_paths = {}\n-            for lora_path in lora_paths:\n-                if \"=\" in lora_path:\n-                    name, path = lora_path.split(\"=\", 1)\n-                    self.lora_paths[name] = path\n-                else:\n-                    self.lora_paths[lora_path] = lora_path\n+            self.lora_paths = {\n+                (parts[0] if len(parts := lora_path.split(\"=\", 1)) == 2 else lora_path):\n+                (parts[1] if len(parts) == 2 else lora_path)\n+                for lora_path in lora_paths\n+            }\n \n \n def prepare_server_args(argv: List[str]) -> ServerArgs:\n", "model_name_or_path": "gpt-5-2025-08-07"}
