{"instance_id": "/home/ubuntu/OmniPerf-Bench/sglan__repo-e3bed74", "model_patch": "diff --git a/python/sglang/srt/layers/moe/topk.py b/python/sglang/srt/layers/moe/topk.py\nindex a7af87144..a7baf034a 100644\n--- a/python/sglang/srt/layers/moe/topk.py\n+++ b/python/sglang/srt/layers/moe/topk.py\n@@ -111,7 +111,8 @@ def grouped_topk(\n     group_idx = torch.topk(group_scores, k=topk_group, dim=-1, sorted=False)[\n         1\n     ]  # [n, top_k_group]\n-    group_mask = torch.zeros_like(group_scores)  # [n, n_group]\n+    group_mask = torch.empty_like(group_scores)  # [n, n_group]\n+    group_mask.zero_()\n     group_mask.scatter_(1, group_idx, 1)  # [n, n_group]\n     score_mask = (\n         group_mask.unsqueeze(-1)\n@@ -166,7 +167,8 @@ def biased_grouped_topk_impl(\n     group_idx = torch.topk(group_scores, k=topk_group, dim=-1, sorted=False)[\n         1\n     ]  # [n, top_k_group]\n-    group_mask = torch.zeros_like(group_scores)  # [n, n_group]\n+    group_mask = torch.empty_like(group_scores)  # [n, n_group]\n+    group_mask.zero_()\n     group_mask.scatter_(1, group_idx, 1)  # [n, n_group]\n     score_mask = (\n         group_mask.unsqueeze(-1)\ndiff --git a/python/sglang/srt/model_executor/cuda_graph_runner.py b/python/sglang/srt/model_executor/cuda_graph_runner.py\nindex e88022beb..c365b2fa5 100644\n--- a/python/sglang/srt/model_executor/cuda_graph_runner.py\n+++ b/python/sglang/srt/model_executor/cuda_graph_runner.py\n@@ -232,23 +232,23 @@ class CudaGraphRunner:\n \n         # Graph inputs\n         with torch.device(\"cuda\"):\n-            self.input_ids = torch.zeros((self.max_num_token,), dtype=torch.int64)\n-            self.req_pool_indices = torch.zeros((self.max_bs,), dtype=torch.int32)\n+            self.input_ids = torch.empty((self.max_num_token,), dtype=torch.int64)\n+            self.req_pool_indices = torch.empty((self.max_bs,), dtype=torch.int32)\n             self.seq_lens = torch.full(\n                 (self.max_bs,), self.seq_len_fill_value, dtype=torch.int32\n             )\n-            self.out_cache_loc = torch.zeros((self.max_num_token,), dtype=torch.int64)\n-            self.positions = torch.zeros((self.max_num_token,), dtype=torch.int64)\n-            self.mrope_positions = torch.zeros((3, self.max_bs), dtype=torch.int64)\n+            self.out_cache_loc = torch.empty((self.max_num_token,), dtype=torch.int64)\n+            self.positions = torch.empty((self.max_num_token,), dtype=torch.int64)\n+            self.mrope_positions = torch.empty((3, self.max_bs), dtype=torch.int64)\n \n             # pipeline parallelism\n             if self.pp_size > 1:\n                 self.pp_proxy_tensors = {\n-                    \"hidden_states\": torch.zeros(\n+                    \"hidden_states\": torch.empty(\n                         (self.max_bs, self.model_runner.model_config.hidden_size),\n                         dtype=torch.bfloat16,\n                     ),\n-                    \"residual\": torch.zeros(\n+                    \"residual\": torch.empty(\n                         (self.max_bs, self.model_runner.model_config.hidden_size),\n                         dtype=torch.bfloat16,\n                     ),\n@@ -259,7 +259,7 @@ class CudaGraphRunner:\n                 model_runner.spec_algorithm.is_eagle3()\n                 and not model_runner.is_draft_worker\n             ):\n-                self.hidden_states = torch.zeros(\n+                self.hidden_states = torch.empty(\n                     (\n                         self.max_num_token,\n                         3 * self.model_runner.model_config.hidden_size,\n@@ -268,7 +268,7 @@ class CudaGraphRunner:\n                 )\n                 self.model_runner.model.set_eagle3_layers_to_capture()\n             elif model_runner.spec_algorithm.is_eagle():\n-                self.hidden_states = torch.zeros(\n+                self.hidden_states = torch.empty(\n                     (self.max_num_token, self.model_runner.model_config.hidden_size),\n                     dtype=self.model_runner.dtype,\n                 )\n@@ -282,14 +282,14 @@ class CudaGraphRunner:\n                 self.encoder_lens = None\n             if self.enable_dp_attention or self.enable_sp_layernorm:\n                 # TODO(ch-wan): SP layernorm should use a different logic to manage gathered_buffer\n-                self.gathered_buffer = torch.zeros(\n+                self.gathered_buffer = torch.empty(\n                     (\n                         self.max_bs * self.dp_size * self.num_tokens_per_bs,\n                         self.model_runner.model_config.hidden_size,\n                     ),\n                     dtype=self.model_runner.dtype,\n                 )\n-                self.global_num_tokens_gpu = torch.zeros(\n+                self.global_num_tokens_gpu = torch.empty(\n                     (self.dp_size,), dtype=torch.int32\n                 )\n \n@@ -548,7 +548,6 @@ class CudaGraphRunner:\n         bs = self.capture_bs[index]\n         if bs != raw_bs:\n             self.seq_lens.fill_(1)\n-            self.out_cache_loc.zero_()\n \n         # Common inputs\n         self.input_ids[:raw_num_token].copy_(forward_batch.input_ids)\n@@ -632,7 +631,7 @@ class CudaGraphRunner:\n             else:\n                 spec_info = EagleVerifyInput(\n                     draft_token=None,\n-                    custom_mask=torch.zeros(\n+                    custom_mask=torch.empty(\n                         (num_tokens * self.model_runner.model_config.context_len),\n                         dtype=torch.bool,\n                         device=\"cuda\",\ndiff --git a/python/sglang/srt/model_executor/forward_batch_info.py b/python/sglang/srt/model_executor/forward_batch_info.py\nindex 5018f92d5..817c721e4 100644\n--- a/python/sglang/srt/model_executor/forward_batch_info.py\n+++ b/python/sglang/srt/model_executor/forward_batch_info.py\n@@ -305,7 +305,7 @@ class ForwardBatch:\n             ).to(device, non_blocking=True)\n \n             sum_len = sum(batch.global_num_tokens)\n-            ret.gathered_buffer = torch.zeros(\n+            ret.gathered_buffer = torch.empty(\n                 (sum_len, model_runner.model_config.hidden_size),\n                 dtype=model_runner.dtype,\n                 device=device,\n@@ -683,7 +683,8 @@ def compute_position_torch(\n         ],\n         axis=0,\n     )\n-    extend_start_loc = torch.zeros_like(extend_seq_lens)\n+    extend_start_loc = torch.empty_like(extend_seq_lens)\n+    extend_start_loc[0] = 0\n     extend_start_loc[1:] = torch.cumsum(extend_seq_lens[:-1], dim=0)\n     return positions.to(torch.int64), extend_start_loc\n \n", "model_name_or_path": "gpt-5-2025-08-07"}
