{"instance_id": "/home/ubuntu/OmniPerf-Bench/sglan__repo-5d264a9", "model_patch": "diff --git a/python/sglang/srt/layers/fused_moe.py b/python/sglang/srt/layers/fused_moe.py\nindex 7dddabb..b9d6006 100644\n--- a/python/sglang/srt/layers/fused_moe.py\n+++ b/python/sglang/srt/layers/fused_moe.py\n@@ -207,12 +207,12 @@ def moe_align_block_size(\n     sorted_ids = torch.empty(\n         (max_num_tokens_padded,), dtype=torch.int32, device=topk_ids.device\n     )\n-    sorted_ids.fill_(topk_ids.numel())\n     max_num_m_blocks = triton.cdiv(max_num_tokens_padded, block_size)\n     expert_ids = torch.empty(\n         (max_num_m_blocks,), dtype=torch.int32, device=topk_ids.device\n     )\n     num_tokens_post_pad = torch.empty((1), dtype=torch.int32, device=topk_ids.device)\n+    sorted_ids.fill_(topk_ids.numel())\n     ops.moe_align_block_size(\n         topk_ids, num_experts, block_size, sorted_ids, expert_ids, num_tokens_post_pad\n     )\ndiff --git a/python/sglang/srt/managers/controller/cuda_graph_runner.py b/python/sglang/srt/managers/controller/cuda_graph_runner.py\nindex 2e37e55..0f48176 100644\n--- a/python/sglang/srt/managers/controller/cuda_graph_runner.py\n+++ b/python/sglang/srt/managers/controller/cuda_graph_runner.py\n@@ -23,18 +23,18 @@ class CudaGraphRunner:\n \n         # Common inputs\n         self.max_bs = max_batch_size_to_capture\n-        self.input_ids = torch.zeros((self.max_bs,), dtype=torch.int32, device=\"cuda\")\n-        self.req_pool_indices = torch.zeros((self.max_bs,), dtype=torch.int32, device=\"cuda\")\n+        self.input_ids = torch.empty((self.max_bs,), dtype=torch.int32, device=\"cuda\")\n+        self.req_pool_indices = torch.empty((self.max_bs,), dtype=torch.int32, device=\"cuda\")\n         self.seq_lens = torch.ones((self.max_bs,), dtype=torch.int32, device=\"cuda\")\n-        self.position_ids_offsets = torch.zeros((self.max_bs,), dtype=torch.int32, device=\"cuda\")\n-        self.out_cache_loc = torch.zeros((self.max_bs,), dtype=torch.int32, device=\"cuda\")\n+        self.position_ids_offsets = torch.empty((self.max_bs,), dtype=torch.int32, device=\"cuda\")\n+        self.out_cache_loc = torch.empty((self.max_bs,), dtype=torch.int32, device=\"cuda\")\n \n         # FlashInfer inputs\n         self.flashinfer_workspace_buffer = self.model_runner.flashinfer_workspace_buffers[0]\n-        self.flashinfer_kv_indptr = torch.zeros(\n+        self.flashinfer_kv_indptr = torch.empty(\n             (self.max_bs + 1,), dtype=torch.int32, device=\"cuda\"\n         )\n-        self.flashinfer_kv_indices = torch.zeros(\n+        self.flashinfer_kv_indices = torch.empty(\n             (self.max_bs * model_runner.model_config.context_len,), dtype=torch.int32, device=\"cuda\"\n         )\n         self.flashinfer_kv_last_page_len = torch.ones(\ndiff --git a/python/sglang/srt/managers/controller/infer_batch.py b/python/sglang/srt/managers/controller/infer_batch.py\nindex d89e978..ee66718 100644\n--- a/python/sglang/srt/managers/controller/infer_batch.py\n+++ b/python/sglang/srt/managers/controller/infer_batch.py\n@@ -345,7 +345,7 @@ class Batch:\n \n             seq_lens.append(prefix_lens[-1] + extend_lens[-1])\n \n-        position_ids_offsets = torch.zeros((bs,), dtype=torch.int32, device=device)\n+        position_ids_offsets = torch.empty((bs,), dtype=torch.int32, device=device)\n \n         # Allocate memory\n         seq_lens, prefix_lens = np.array(seq_lens), np.array(prefix_lens)\n@@ -781,7 +781,8 @@ class InputMetadata:\n                 device=\"cuda\",\n             )\n             extend_seq_lens = seq_lens - prefix_lens\n-            extend_start_loc = torch.zeros_like(seq_lens)\n+            extend_start_loc = torch.empty_like(seq_lens)\n+            extend_start_loc[0] = 0\n             extend_start_loc[1:] = torch.cumsum(extend_seq_lens[:-1], dim=0)\n             extend_no_prefix = torch.all(prefix_lens == 0)\n             total_num_tokens = int(torch.sum(seq_lens))\n@@ -827,9 +828,10 @@ def init_flashinfer_args(forward_mode, model_runner, req_pool_indices, seq_lens,\n     else:\n         paged_kernel_lens = prefix_lens\n \n-    kv_indptr = torch.zeros(\n+    kv_indptr = torch.empty(\n         (batch_size + 1,), dtype=torch.int32, device=\"cuda\"\n     )\n+    kv_indptr[0] = 0\n     kv_indptr[1:] = torch.cumsum(paged_kernel_lens, dim=0)\n     req_pool_indices_cpu = req_pool_indices.cpu().numpy()\n     paged_kernel_lens_cpu = paged_kernel_lens.cpu().numpy()\n@@ -859,9 +861,10 @@ def init_flashinfer_args(forward_mode, model_runner, req_pool_indices, seq_lens,\n         )\n     else:\n         # extend part\n-        qo_indptr = torch.zeros(\n+        qo_indptr = torch.empty(\n             (batch_size + 1,), dtype=torch.int32, device=\"cuda\"\n         )\n+        qo_indptr[0] = 0\n         qo_indptr[1:] = torch.cumsum(seq_lens - prefix_lens, dim=0)\n \n         model_runner.flashinfer_prefill_wrapper_ragged.end_forward()\n@@ -890,7 +893,8 @@ def init_flashinfer_args(forward_mode, model_runner, req_pool_indices, seq_lens,\n def init_triton_args(forward_mode, seq_lens, prefix_lens):\n     batch_size = len(seq_lens)\n     max_seq_len = int(torch.max(seq_lens))\n-    start_loc = torch.zeros((batch_size,), dtype=torch.int32, device=\"cuda\")\n+    start_loc = torch.empty((batch_size,), dtype=torch.int32, device=\"cuda\")\n+    start_loc[0] = 0\n     start_loc[1:] = torch.cumsum(seq_lens[:-1], dim=0)\n \n     if forward_mode == ForwardMode.DECODE:\ndiff --git a/python/sglang/srt/memory_pool.py b/python/sglang/srt/memory_pool.py\nindex 245e6ef..9b78636 100644\n--- a/python/sglang/srt/memory_pool.py\n+++ b/python/sglang/srt/memory_pool.py\n@@ -41,7 +41,7 @@ class TokenToKVPool:\n         self.size = size\n         # mem_state is the reference counter.\n         # We also add one slot. This slot is used for writing dummy output from padded tokens.\n-        self.mem_state = torch.zeros((self.size + 1,), dtype=torch.int16, device=\"cuda\")\n+        self.mem_state = torch.empty((self.size + 1,), dtype=torch.int16, device=\"cuda\")\n         self.total_ref_ct = 0\n \n         # [size, key/value, head_num, head_dim] for each layer\n", "model_name_or_path": "gpt-5-2025-08-07"}
