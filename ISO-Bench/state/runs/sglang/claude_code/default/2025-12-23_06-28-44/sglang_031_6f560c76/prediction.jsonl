{"instance_id": "/home/ubuntu/OmniPerf-Bench/sglan__repo-cd68723", "model_patch": "diff --git a/python/sglang/srt/managers/router/infer_batch.py b/python/sglang/srt/managers/router/infer_batch.py\nindex 00ada29..8d5f841 100644\n--- a/python/sglang/srt/managers/router/infer_batch.py\n+++ b/python/sglang/srt/managers/router/infer_batch.py\n@@ -199,7 +199,8 @@ class Batch:\n \n             seq_lens.append(prefix_lens[-1] + extend_lens[-1])\n \n-        position_ids_offsets = torch.zeros((bs,), dtype=torch.int32, device=device)\n+        position_ids_offsets = torch.empty((bs,), dtype=torch.int32, device=device)\n+        position_ids_offsets.fill_(0)\n \n         # Alloc mem\n         seq_lens, prefix_lens = np.array(seq_lens), np.array(prefix_lens)\n@@ -222,10 +223,12 @@ class Batch:\n             pt += extend_lens[i]\n \n         # Handle logit bias\n-        logit_bias = torch.zeros((bs, vocab_size), dtype=torch.float32, device=device)\n+        logit_bias = torch.empty((bs, vocab_size), dtype=torch.float32, device=device)\n         for i in range(bs):\n             if reqs[i].sampling_params.dtype == \"int\":\n                 logit_bias[i] = int_token_logit_bias\n+            else:\n+                logit_bias[i].fill_(0)\n \n         # Set fields\n         self.input_ids = torch.tensor(\n@@ -437,9 +440,11 @@ class Batch:\n \n         has_regex = any(req.regex_fsm is not None for req in self.reqs)\n         if has_regex:\n-            allowed_mask = torch.empty_like(logits[0], dtype=torch.bool)\n+            allowed_mask = None\n             for i, req in enumerate(self.reqs):\n                 if req.regex_fsm is not None:\n+                    if allowed_mask is None:\n+                        allowed_mask = torch.empty_like(logits[0], dtype=torch.bool)\n                     allowed_mask.zero_()\n                     allowed_mask[\n                         req.regex_fsm.allowed_token_ids(req.regex_fsm_state)\ndiff --git a/python/sglang/srt/managers/router/model_runner.py b/python/sglang/srt/managers/router/model_runner.py\nindex c85ec53..fdd215b 100644\n--- a/python/sglang/srt/managers/router/model_runner.py\n+++ b/python/sglang/srt/managers/router/model_runner.py\n@@ -86,9 +86,10 @@ class InputMetadata:\n     decode_wrapper = None\n \n     def init_flashinfer_args(self, tp_size):\n-        self.kv_indptr = torch.zeros(\n+        self.kv_indptr = torch.empty(\n             (self.batch_size + 1,), dtype=torch.int32, device=\"cuda\"\n         )\n+        self.kv_indptr[0] = 0\n         self.kv_indptr[1:] = torch.cumsum(self.seq_lens, dim=0)\n         self.kv_indices = torch.cat(\n             [\n@@ -112,9 +113,10 @@ class InputMetadata:\n             self.forward_mode == ForwardMode.PREFILL\n             or self.forward_mode == ForwardMode.EXTEND\n         ):\n-            self.qo_indptr = torch.zeros(\n+            self.qo_indptr = torch.empty(\n                 (self.batch_size + 1,), dtype=torch.int32, device=\"cuda\"\n             )\n+            self.qo_indptr[0] = 0\n             self.qo_indptr[1:] = torch.cumsum(self.extend_seq_lens, dim=0)\n             self.prefill_wrapper = BatchPrefillWithPagedKVCacheWrapper()\n             self.prefill_wrapper.begin_forward(\n@@ -139,7 +141,8 @@ class InputMetadata:\n \n     def init_extend_args(self):\n         self.extend_seq_lens = self.seq_lens - self.prefix_lens\n-        self.extend_start_loc = torch.zeros_like(self.seq_lens)\n+        self.extend_start_loc = torch.empty_like(self.seq_lens)\n+        self.extend_start_loc[0] = 0\n         self.extend_start_loc[1:] = torch.cumsum(self.extend_seq_lens[:-1], dim=0)\n         self.max_extend_len = int(torch.max(self.extend_seq_lens))\n \n@@ -159,7 +162,8 @@ class InputMetadata:\n         return_logprob=False,\n     ):\n         batch_size = len(req_pool_indices)\n-        start_loc = torch.zeros((batch_size,), dtype=torch.int32, device=\"cuda\")\n+        start_loc = torch.empty((batch_size,), dtype=torch.int32, device=\"cuda\")\n+        start_loc[0] = 0\n         start_loc[1:] = torch.cumsum(seq_lens[:-1], dim=0)\n         total_num_tokens = int(torch.sum(seq_lens))\n         max_seq_len = int(torch.max(seq_lens))\n", "model_name_or_path": "gpt-5-2025-08-07"}
