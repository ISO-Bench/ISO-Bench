{"instance_id": "/home/ubuntu/OmniPerf-Bench/sglan__repo-48efec7", "model_patch": "diff --git a/python/sglang/srt/entrypoints/http_server.py b/python/sglang/srt/entrypoints/http_server.py\nindex 32a11c1..d0b0aa4 100644\n--- a/python/sglang/srt/entrypoints/http_server.py\n+++ b/python/sglang/srt/entrypoints/http_server.py\n@@ -228,19 +228,18 @@ async def generate_request(obj: GenerateReqInput, request: Request):\n     if obj.stream:\n \n         async def stream_results() -> AsyncIterator[bytes]:\n+            data_prefix = b\"data: \"\n+            data_suffix = b\"\\n\\n\"\n+            orjson_option = orjson.OPT_NON_STR_KEYS\n             try:\n                 async for out in _global_state.tokenizer_manager.generate_request(\n                     obj, request\n                 ):\n-                    yield b\"data: \" + orjson.dumps(\n-                        out, option=orjson.OPT_NON_STR_KEYS\n-                    ) + b\"\\n\\n\"\n+                    yield data_prefix + orjson.dumps(out, option=orjson_option) + data_suffix\n             except ValueError as e:\n                 out = {\"error\": {\"message\": str(e)}}\n                 logger.error(f\"Error: {e}\")\n-                yield b\"data: \" + orjson.dumps(\n-                    out, option=orjson.OPT_NON_STR_KEYS\n-                ) + b\"\\n\\n\"\n+                yield data_prefix + orjson.dumps(out, option=orjson_option) + data_suffix\n             yield b\"data: [DONE]\\n\\n\"\n \n         return StreamingResponse(\n@@ -330,16 +329,8 @@ async def update_weights_from_disk(obj: UpdateWeightFromDiskReqInput, request: R\n         \"message\": message,\n         \"num_paused_requests\": num_paused_requests,\n     }\n-    if success:\n-        return ORJSONResponse(\n-            content,\n-            status_code=HTTPStatus.OK,\n-        )\n-    else:\n-        return ORJSONResponse(\n-            content,\n-            status_code=HTTPStatus.BAD_REQUEST,\n-        )\n+    status_code = HTTPStatus.OK if success else HTTPStatus.BAD_REQUEST\n+    return ORJSONResponse(content, status_code=status_code)\n \n \n @app.post(\"/init_weights_update_group\")\n@@ -351,10 +342,8 @@ async def init_weights_update_group(\n         obj, request\n     )\n     content = {\"success\": success, \"message\": message}\n-    if success:\n-        return ORJSONResponse(content, status_code=200)\n-    else:\n-        return ORJSONResponse(content, status_code=HTTPStatus.BAD_REQUEST)\n+    status_code = 200 if success else HTTPStatus.BAD_REQUEST\n+    return ORJSONResponse(content, status_code=status_code)\n \n \n @app.post(\"/update_weights_from_distributed\")\n@@ -368,10 +357,8 @@ async def update_weights_from_distributed(\n         )\n     )\n     content = {\"success\": success, \"message\": message}\n-    if success:\n-        return ORJSONResponse(content, status_code=200)\n-    else:\n-        return ORJSONResponse(content, status_code=HTTPStatus.BAD_REQUEST)\n+    status_code = 200 if success else HTTPStatus.BAD_REQUEST\n+    return ORJSONResponse(content, status_code=status_code)\n \n \n @app.api_route(\"/get_weights_by_name\", methods=[\"GET\", \"POST\"])\n@@ -505,9 +492,10 @@ async def openai_v1_embeddings(raw_request: Request):\n def available_models():\n     \"\"\"Show available models.\"\"\"\n     served_model_names = [_global_state.tokenizer_manager.served_model_name]\n-    model_cards = []\n-    for served_model_name in served_model_names:\n-        model_cards.append(ModelCard(id=served_model_name, root=served_model_name))\n+    model_cards = [\n+        ModelCard(id=served_model_name, root=served_model_name)\n+        for served_model_name in served_model_names\n+    ]\n     return ModelList(data=model_cards)\n \n \n@@ -570,14 +558,15 @@ async def vertex_generate(vertex_req: VertexGenerateReqInput, raw_request: Reque\n     if not vertex_req.instances:\n         return []\n     inputs = {}\n+    first_instance = vertex_req.instances[0]\n     for input_key in (\"text\", \"input_ids\", \"input_embeds\"):\n-        if vertex_req.instances[0].get(input_key):\n+        if first_instance.get(input_key):\n             inputs[input_key] = [\n-                instance.get(input_key) for instance in vertex_req.instances\n+                instance[input_key] for instance in vertex_req.instances\n             ]\n             break\n     image_data = [\n-        instance.get(\"image_data\")\n+        instance[\"image_data\"]\n         for instance in vertex_req.instances\n         if instance.get(\"image_data\") is not None\n     ] or None\ndiff --git a/test/srt/test_input_embeddings.py b/test/srt/test_input_embeddings.py\nindex 015aabe..6a135e8 100644\n--- a/test/srt/test_input_embeddings.py\n+++ b/test/srt/test_input_embeddings.py\n@@ -35,12 +35,12 @@ class TestInputEmbeds(unittest.TestCase):\n         \"\"\"Generate input embeddings for a given text.\"\"\"\n         input_ids = self.tokenizer(text, return_tensors=\"pt\")[\"input_ids\"]\n         embeddings = self.ref_model.get_input_embeddings()(input_ids)\n-        return embeddings.squeeze().tolist()  # Convert tensor to a list for API use\n+        return embeddings.squeeze(0).tolist()  # Convert tensor to a list for API use\n \n     def send_request(self, payload):\n         \"\"\"Send a POST request to the API and return the response.\"\"\"\n         response = requests.post(\n-            self.base_url + \"/generate\",\n+            f\"{self.base_url}/generate\",\n             json=payload,\n             timeout=30,  # Set a reasonable timeout for the API request\n         )\n", "model_name_or_path": "gpt-5-2025-08-07"}
