{"instance_id": "/home/ubuntu/OmniPerf-Bench/sglan__repo-a4d6d6f", "model_patch": "diff --git a/docker/Dockerfile.rocm b/docker/Dockerfile.rocm\nindex 84ea69cc0..e988d2daa 100644\n--- a/docker/Dockerfile.rocm\n+++ b/docker/Dockerfile.rocm\n@@ -33,14 +33,10 @@ RUN git clone ${SGL_REPO} \\\n RUN cp -r /sgl-workspace/sglang /sglang\n RUN python -m pip cache purge\n \n-RUN pip install IPython \\\n-    && pip install orjson \\\n-    && pip install python-multipart \\\n-    && pip install torchao \\\n-    && pip install pybind11\n+RUN pip install --no-cache-dir IPython orjson python-multipart torchao pybind11\n \n-RUN pip uninstall -y triton\n-RUN git clone ${TRITON_REPO} \\\n+RUN pip uninstall -y triton \\\n+    && git clone ${TRITON_REPO} \\\n     && cd triton \\\n     && git checkout ${TRITON_COMMIT} \\\n     && cd python \\\ndiff --git a/python/sglang/srt/layers/moe/fused_moe_triton/fused_moe.py b/python/sglang/srt/layers/moe/fused_moe_triton/fused_moe.py\nindex cbacd90c0..eeb71c998 100644\n--- a/python/sglang/srt/layers/moe/fused_moe_triton/fused_moe.py\n+++ b/python/sglang/srt/layers/moe/fused_moe_triton/fused_moe.py\n@@ -263,10 +263,9 @@ def moe_align_block_size(\n         by block_size for proper block matrix operations.\n     \"\"\"\n     max_num_tokens_padded = topk_ids.numel() + num_experts * (block_size - 1)\n-    sorted_ids = torch.empty(\n-        (max_num_tokens_padded,), dtype=torch.int32, device=topk_ids.device\n+    sorted_ids = torch.full(\n+        (max_num_tokens_padded,), topk_ids.numel(), dtype=torch.int32, device=topk_ids.device\n     )\n-    sorted_ids.fill_(topk_ids.numel())\n     max_num_m_blocks = triton.cdiv(max_num_tokens_padded, block_size)\n     expert_ids = torch.empty(\n         (max_num_m_blocks,), dtype=torch.int32, device=topk_ids.device\n@@ -784,7 +783,7 @@ def fused_experts_impl(\n     else:\n         out_hidden_states = torch.empty_like(hidden_states)\n \n-    for chunk in range((num_tokens // CHUNK_SIZE) + 1):\n+    for chunk in range(triton.cdiv(num_tokens, CHUNK_SIZE)):\n         begin_chunk_idx, end_chunk_idx = (\n             chunk * CHUNK_SIZE,\n             min((chunk + 1) * CHUNK_SIZE, num_tokens),\n@@ -801,7 +800,7 @@ def fused_experts_impl(\n             # so the cache size and config are already set correctly and\n             # do not need to be adjusted.\n             intermediate_cache1 = intermediate_cache1[:tokens_in_chunk]\n-            intermediate_cache2 = intermediate_cache2[:tokens_in_chunk]\n+            intermediate_cache2 = intermediate_cache2[:tokens_in_chunk * topk_ids.shape[1]]\n             intermediate_cache3 = intermediate_cache3[:tokens_in_chunk]\n             config = get_config_func(tokens_in_chunk)\n \n@@ -855,7 +854,7 @@ def fused_experts_impl(\n         )\n \n         torch.sum(\n-            intermediate_cache3.view(*intermediate_cache3.shape),\n+            intermediate_cache3,\n             dim=1,\n             out=out_hidden_states[begin_chunk_idx:end_chunk_idx],\n         )\n", "model_name_or_path": "gpt-5-2025-08-07"}
