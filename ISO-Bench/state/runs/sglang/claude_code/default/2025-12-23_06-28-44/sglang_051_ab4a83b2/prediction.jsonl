{"instance_id": "/home/ubuntu/OmniPerf-Bench/sglan__repo-62f15ee", "model_patch": "diff --git a/python/sglang/srt/managers/policy_scheduler.py b/python/sglang/srt/managers/policy_scheduler.py\nindex 04169e8..845bc22 100644\n--- a/python/sglang/srt/managers/policy_scheduler.py\n+++ b/python/sglang/srt/managers/policy_scheduler.py\n@@ -68,9 +68,8 @@ class PolicyScheduler:\n             for req in waiting_queue:\n                 last_node_to_reqs[req.last_node].append(req)\n \n-            node_to_weight = defaultdict(int)\n-            for node in last_node_to_reqs:\n-                node_to_weight[node] = len(last_node_to_reqs[node])\n+            # Optimize: use dict comprehension instead of loop\n+            node_to_weight = defaultdict(int, {node: len(reqs) for node, reqs in last_node_to_reqs.items()})\n             self.calc_weight(self.tree_cache.root_node, node_to_weight)\n \n             waiting_queue.clear()\n@@ -97,8 +96,8 @@ class PolicyScheduler:\n         last_node_to_reqs: Dict,\n         q: List,\n     ):\n-        childs = [child for child in cur_node.children.values()]\n-        childs.sort(key=lambda x: -node_to_priority[x])\n+        # Optimize: use sorted() directly instead of list comprehension + sort\n+        childs = sorted(cur_node.children.values(), key=lambda x: -node_to_priority[x])\n         for child in childs:\n             self.get_dfs_priority(child, node_to_priority, last_node_to_reqs, q)\n         q.extend(last_node_to_reqs[cur_node])\ndiff --git a/python/sglang/srt/managers/tp_worker.py b/python/sglang/srt/managers/tp_worker.py\nindex 8fc03b8..6d1a29f 100644\n--- a/python/sglang/srt/managers/tp_worker.py\n+++ b/python/sglang/srt/managers/tp_worker.py\n@@ -487,7 +487,9 @@ class ModelTpServer:\n             self.token_to_kv_pool,\n             self.tree_cache,\n         )\n-        self.waiting_queue = [x for x in self.waiting_queue if x not in can_run_list]\n+        # Optimize: use set for O(1) lookup instead of O(n) list lookup\n+        can_run_set = set(can_run_list)\n+        self.waiting_queue = [x for x in self.waiting_queue if x not in can_run_set]\n         return new_batch\n \n     def forward_prefill_batch(self, batch: ScheduleBatch):\n@@ -931,7 +933,8 @@ def broadcast_recv_input(\n         else:\n             serialized_data = pickle.dumps(data)\n             size = len(serialized_data)\n-            tensor_data = torch.ByteTensor(list(serialized_data))\n+            # Optimize: use frombuffer instead of list conversion\n+            tensor_data = torch.frombuffer(serialized_data, dtype=torch.uint8).clone()\n             tensor_size = torch.tensor([size], dtype=torch.long)\n \n             dist.broadcast(tensor_size, src=0, group=dist_group)\n@@ -948,6 +951,7 @@ def broadcast_recv_input(\n         tensor_data = torch.empty(size, dtype=torch.uint8)\n         dist.broadcast(tensor_data, src=0, group=dist_group)\n \n-        serialized_data = bytes(tensor_data.tolist())\n+        # Optimize: use numpy tobytes instead of list conversion\n+        serialized_data = tensor_data.cpu().numpy().tobytes()\n         data = pickle.loads(serialized_data)\n         return data\n", "model_name_or_path": "gpt-5-2025-08-07"}
