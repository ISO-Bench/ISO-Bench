{"instance_id": "/home/ubuntu/OmniPerf-Bench/sglan__repo-db45276", "model_patch": "diff --git a/python/sglang/srt/layers/attention/flashattention_backend.py b/python/sglang/srt/layers/attention/flashattention_backend.py\nindex 62604fe..2d36711 100644\n--- a/python/sglang/srt/layers/attention/flashattention_backend.py\n+++ b/python/sglang/srt/layers/attention/flashattention_backend.py\n@@ -341,13 +341,11 @@ class FlashAttentionBackend(AttentionBackend):\n                 ).T\n \n                 for idx, single_seq_len in enumerate(seq_lens_with_decode):\n-                    real_bsz_start_idx = idx\n-                    real_bsz_end_idx = idx + 1\n                     metadata.page_table[\n-                        real_bsz_start_idx:real_bsz_end_idx,\n+                        idx:idx + 1,\n                         (single_seq_len - (self.step_id + 1)) : single_seq_len,\n                     ] = cache_loc[\n-                        real_bsz_start_idx:real_bsz_end_idx, : (self.step_id + 1)\n+                        idx:idx + 1, : (self.step_id + 1)\n                     ]\n             else:  # Normal Decode without Spec Decoding\n                 metadata.cache_seqlens_int32 = seqlens_in_batch.to(torch.int32)\n@@ -720,13 +718,13 @@ class FlashAttentionBackend(AttentionBackend):\n         \"\"\"\n         self.decode_cuda_graph_metadata = {\n             # Page table for token mapping (batch_size, max_context_len)\n-            \"page_table\": torch.zeros(\n+            \"page_table\": torch.empty(\n                 max_bs,\n                 (self.max_context_len + self.page_size - 1) // self.page_size,\n                 dtype=torch.int32,\n                 device=self.device,\n             ),\n-            \"page_table_draft_decode\": torch.zeros(\n+            \"page_table_draft_decode\": torch.empty(\n                 max_bs,\n                 (self.max_context_len + self.page_size - 1) // self.page_size,\n                 dtype=torch.int32,\n@@ -735,27 +733,27 @@ class FlashAttentionBackend(AttentionBackend):\n             \"strided_indices\": torch.arange(\n                 0, self.max_context_len, self.page_size, device=self.device\n             ),\n-            \"cache_seqlens\": torch.zeros(max_bs, dtype=torch.int32, device=self.device),\n+            \"cache_seqlens\": torch.empty(max_bs, dtype=torch.int32, device=self.device),\n             \"cu_seqlens_q\": torch.arange(\n                 0, max_bs + 128, dtype=torch.int32, device=self.device\n             ),\n-            \"cu_seqlens_k\": torch.zeros(\n+            \"cu_seqlens_k\": torch.empty(\n                 max_bs + 128, dtype=torch.int32, device=self.device\n             ),\n         }\n \n         self.target_verify_metadata = {\n-            \"page_table\": torch.zeros(\n+            \"page_table\": torch.empty(\n                 max_bs,\n                 (self.max_context_len + self.page_size - 1) // self.page_size,\n                 dtype=torch.int32,\n                 device=self.device,\n             ),\n-            \"cache_seqlens\": torch.zeros(max_bs, dtype=torch.int32, device=self.device),\n-            \"cu_seqlens_q\": torch.zeros(\n+            \"cache_seqlens\": torch.empty(max_bs, dtype=torch.int32, device=self.device),\n+            \"cu_seqlens_q\": torch.empty(\n                 max_bs + 128, dtype=torch.int32, device=self.device\n             ),\n-            \"cu_seqlens_k\": torch.zeros(\n+            \"cu_seqlens_k\": torch.empty(\n                 max_bs + 128, dtype=torch.int32, device=self.device\n             ),\n             \"max_seqlen_q\": 0,\n@@ -890,8 +888,6 @@ class FlashAttentionBackend(AttentionBackend):\n                     (seq_lens + (self.step_id + 1)).to(torch.int32)\n                 )\n \n-                metadata.max_seq_len_k = seq_lens_cpu.max().item() + (self.step_id + 1)\n-\n                 metadata.cu_seqlens_k.copy_(\n                     torch.nn.functional.pad(\n                         torch.cumsum(\n", "model_name_or_path": "gpt-5-2025-08-07"}
