diff --git a/python/sglang/srt/layers/attention/flashattention_backend.py b/python/sglang/srt/layers/attention/flashattention_backend.py
index 62604fe..2d36711 100644
--- a/python/sglang/srt/layers/attention/flashattention_backend.py
+++ b/python/sglang/srt/layers/attention/flashattention_backend.py
@@ -341,13 +341,11 @@ class FlashAttentionBackend(AttentionBackend):
                 ).T
 
                 for idx, single_seq_len in enumerate(seq_lens_with_decode):
-                    real_bsz_start_idx = idx
-                    real_bsz_end_idx = idx + 1
                     metadata.page_table[
-                        real_bsz_start_idx:real_bsz_end_idx,
+                        idx:idx + 1,
                         (single_seq_len - (self.step_id + 1)) : single_seq_len,
                     ] = cache_loc[
-                        real_bsz_start_idx:real_bsz_end_idx, : (self.step_id + 1)
+                        idx:idx + 1, : (self.step_id + 1)
                     ]
             else:  # Normal Decode without Spec Decoding
                 metadata.cache_seqlens_int32 = seqlens_in_batch.to(torch.int32)
@@ -720,13 +718,13 @@ class FlashAttentionBackend(AttentionBackend):
         """
         self.decode_cuda_graph_metadata = {
             # Page table for token mapping (batch_size, max_context_len)
-            "page_table": torch.zeros(
+            "page_table": torch.empty(
                 max_bs,
                 (self.max_context_len + self.page_size - 1) // self.page_size,
                 dtype=torch.int32,
                 device=self.device,
             ),
-            "page_table_draft_decode": torch.zeros(
+            "page_table_draft_decode": torch.empty(
                 max_bs,
                 (self.max_context_len + self.page_size - 1) // self.page_size,
                 dtype=torch.int32,
@@ -735,27 +733,27 @@ class FlashAttentionBackend(AttentionBackend):
             "strided_indices": torch.arange(
                 0, self.max_context_len, self.page_size, device=self.device
             ),
-            "cache_seqlens": torch.zeros(max_bs, dtype=torch.int32, device=self.device),
+            "cache_seqlens": torch.empty(max_bs, dtype=torch.int32, device=self.device),
             "cu_seqlens_q": torch.arange(
                 0, max_bs + 128, dtype=torch.int32, device=self.device
             ),
-            "cu_seqlens_k": torch.zeros(
+            "cu_seqlens_k": torch.empty(
                 max_bs + 128, dtype=torch.int32, device=self.device
             ),
         }
 
         self.target_verify_metadata = {
-            "page_table": torch.zeros(
+            "page_table": torch.empty(
                 max_bs,
                 (self.max_context_len + self.page_size - 1) // self.page_size,
                 dtype=torch.int32,
                 device=self.device,
             ),
-            "cache_seqlens": torch.zeros(max_bs, dtype=torch.int32, device=self.device),
-            "cu_seqlens_q": torch.zeros(
+            "cache_seqlens": torch.empty(max_bs, dtype=torch.int32, device=self.device),
+            "cu_seqlens_q": torch.empty(
                 max_bs + 128, dtype=torch.int32, device=self.device
             ),
-            "cu_seqlens_k": torch.zeros(
+            "cu_seqlens_k": torch.empty(
                 max_bs + 128, dtype=torch.int32, device=self.device
             ),
             "max_seqlen_q": 0,
@@ -890,8 +888,6 @@ class FlashAttentionBackend(AttentionBackend):
                     (seq_lens + (self.step_id + 1)).to(torch.int32)
                 )
 
-                metadata.max_seq_len_k = seq_lens_cpu.max().item() + (self.step_id + 1)
-
                 metadata.cu_seqlens_k.copy_(
                     torch.nn.functional.pad(
                         torch.cumsum(
