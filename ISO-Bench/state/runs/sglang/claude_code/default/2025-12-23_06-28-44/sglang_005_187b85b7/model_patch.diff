diff --git a/python/sglang/srt/mem_cache/memory_pool.py b/python/sglang/srt/mem_cache/memory_pool.py
index c01807f1b..302cc6e50 100644
--- a/python/sglang/srt/mem_cache/memory_pool.py
+++ b/python/sglang/srt/mem_cache/memory_pool.py
@@ -64,7 +64,7 @@ class ReqToTokenPool:
         self.max_context_len = max_context_len
         self.device = device
         with memory_saver_adapter.region(GPU_MEMORY_TYPE_KV_CACHE):
-            self.req_to_token = torch.zeros(
+            self.req_to_token = torch.empty(
                 (size, max_context_len), dtype=torch.int32, device=device
             )
         self.free_slots = list(range(size))
@@ -303,7 +303,7 @@ class MHATokenToKVPool(KVCache):
                 # [size, head_num, head_dim] for each layer
                 # The padded slot 0 is used for writing dummy outputs from padded tokens.
                 self.k_buffer = [
-                    torch.zeros(
+                    torch.empty(
                         (self.size + self.page_size, self.head_num, self.head_dim),
                         dtype=self.store_dtype,
                         device=self.device,
@@ -311,7 +311,7 @@ class MHATokenToKVPool(KVCache):
                     for _ in range(self.layer_num)
                 ]
                 self.v_buffer = [
-                    torch.zeros(
+                    torch.empty(
                         (self.size + self.page_size, self.head_num, self.head_dim),
                         dtype=self.store_dtype,
                         device=self.device,
@@ -620,7 +620,7 @@ class MLATokenToKVPool(KVCache):
             ):
                 # The padded slot 0 is used for writing dummy outputs from padded tokens.
                 self.kv_buffer = [
-                    torch.zeros(
+                    torch.empty(
                         (size + page_size, 1, kv_lora_rank + qk_rope_head_dim),
                         dtype=self.store_dtype,
                         device=device,
@@ -758,13 +758,13 @@ class DoubleSparseTokenToKVPool(KVCache):
         with self.memory_saver_adapter.region(GPU_MEMORY_TYPE_KV_CACHE):
             # [size, head_num, head_dim] for each layer
             self.k_buffer = [
-                torch.zeros(
+                torch.empty(
                     (size + page_size, head_num, head_dim), dtype=dtype, device=device
                 )
                 for _ in range(layer_num)
             ]
             self.v_buffer = [
-                torch.zeros(
+                torch.empty(
                     (size + page_size, head_num, head_dim), dtype=dtype, device=device
                 )
                 for _ in range(layer_num)
@@ -772,7 +772,7 @@ class DoubleSparseTokenToKVPool(KVCache):
 
             # [size, head_num, heavy_channel_num] for each layer
             self.label_buffer = [
-                torch.zeros(
+                torch.empty(
                     (size + 1, head_num, heavy_channel_num), dtype=dtype, device=device
                 )
                 for _ in range(layer_num)
