{"instance_id": "/home/ubuntu/OmniPerf-Bench/sglan__repo-ceba0ce", "model_patch": "diff --git a/python/sglang/srt/mem_cache/memory_pool.py b/python/sglang/srt/mem_cache/memory_pool.py\nindex c01807f1b..302cc6e50 100644\n--- a/python/sglang/srt/mem_cache/memory_pool.py\n+++ b/python/sglang/srt/mem_cache/memory_pool.py\n@@ -64,7 +64,7 @@ class ReqToTokenPool:\n         self.max_context_len = max_context_len\n         self.device = device\n         with memory_saver_adapter.region(GPU_MEMORY_TYPE_KV_CACHE):\n-            self.req_to_token = torch.zeros(\n+            self.req_to_token = torch.empty(\n                 (size, max_context_len), dtype=torch.int32, device=device\n             )\n         self.free_slots = list(range(size))\n@@ -303,7 +303,7 @@ class MHATokenToKVPool(KVCache):\n                 # [size, head_num, head_dim] for each layer\n                 # The padded slot 0 is used for writing dummy outputs from padded tokens.\n                 self.k_buffer = [\n-                    torch.zeros(\n+                    torch.empty(\n                         (self.size + self.page_size, self.head_num, self.head_dim),\n                         dtype=self.store_dtype,\n                         device=self.device,\n@@ -311,7 +311,7 @@ class MHATokenToKVPool(KVCache):\n                     for _ in range(self.layer_num)\n                 ]\n                 self.v_buffer = [\n-                    torch.zeros(\n+                    torch.empty(\n                         (self.size + self.page_size, self.head_num, self.head_dim),\n                         dtype=self.store_dtype,\n                         device=self.device,\n@@ -620,7 +620,7 @@ class MLATokenToKVPool(KVCache):\n             ):\n                 # The padded slot 0 is used for writing dummy outputs from padded tokens.\n                 self.kv_buffer = [\n-                    torch.zeros(\n+                    torch.empty(\n                         (size + page_size, 1, kv_lora_rank + qk_rope_head_dim),\n                         dtype=self.store_dtype,\n                         device=device,\n@@ -758,13 +758,13 @@ class DoubleSparseTokenToKVPool(KVCache):\n         with self.memory_saver_adapter.region(GPU_MEMORY_TYPE_KV_CACHE):\n             # [size, head_num, head_dim] for each layer\n             self.k_buffer = [\n-                torch.zeros(\n+                torch.empty(\n                     (size + page_size, head_num, head_dim), dtype=dtype, device=device\n                 )\n                 for _ in range(layer_num)\n             ]\n             self.v_buffer = [\n-                torch.zeros(\n+                torch.empty(\n                     (size + page_size, head_num, head_dim), dtype=dtype, device=device\n                 )\n                 for _ in range(layer_num)\n@@ -772,7 +772,7 @@ class DoubleSparseTokenToKVPool(KVCache):\n \n             # [size, head_num, heavy_channel_num] for each layer\n             self.label_buffer = [\n-                torch.zeros(\n+                torch.empty(\n                     (size + 1, head_num, heavy_channel_num), dtype=dtype, device=device\n                 )\n                 for _ in range(layer_num)\n", "model_name_or_path": "gpt-5-2025-08-07"}
