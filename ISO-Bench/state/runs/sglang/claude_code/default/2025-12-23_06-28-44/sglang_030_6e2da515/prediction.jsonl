{"instance_id": "/home/ubuntu/OmniPerf-Bench/sglan__repo-e9a47f4", "model_patch": "diff --git a/benchmark/benchmark_batch/benchmark_batch.py b/benchmark/benchmark_batch/benchmark_batch.py\nindex 15ef0ab..2ed7246 100644\n--- a/benchmark/benchmark_batch/benchmark_batch.py\n+++ b/benchmark/benchmark_batch/benchmark_batch.py\n@@ -43,20 +43,21 @@ def generate_random_prompt(index, tokenizer_dir, num_tokens):\n def prepare_all_prompts(num_requests, batch_size, num_tokens, tokenizer_dir):\n     \"\"\"Generate prompts for all requests in parallel.\"\"\"\n     total_prompts = num_requests * batch_size\n-    all_prompts = [None] * total_prompts\n+    all_prompts = []\n     max_workers = min(os.cpu_count() or 1, total_prompts)\n \n     with ProcessPoolExecutor(max_workers=max_workers) as executor:\n-        futures = [\n-            executor.submit(generate_random_prompt, i, tokenizer_dir, num_tokens)\n+        futures = {\n+            executor.submit(generate_random_prompt, i, tokenizer_dir, num_tokens): i\n             for i in range(total_prompts)\n-        ]\n+        }\n+        all_prompts = [None] * total_prompts\n         for future in tqdm(\n             concurrent.futures.as_completed(futures),\n             total=total_prompts,\n             desc=\"Generating prompts\",\n         ):\n-            index = futures.index(future)\n+            index = futures[future]\n             all_prompts[index] = future.result()\n \n     batched_prompts = [\ndiff --git a/benchmark/benchmark_batch/benchmark_tokenizer.py b/benchmark/benchmark_batch/benchmark_tokenizer.py\nindex c00bfb8..ec4d707 100644\n--- a/benchmark/benchmark_batch/benchmark_tokenizer.py\n+++ b/benchmark/benchmark_batch/benchmark_tokenizer.py\n@@ -16,7 +16,7 @@ NUM_RUNS = 5  # Number of runs for each batch size to get reliable measurements\n def generate_random_prompts(num_prompts, num_tokens, tokenizer):\n     \"\"\"Generate random prompts with specified token count.\"\"\"\n     vocab_size = tokenizer.vocab_size\n-    all_prompts = []\n+    all_prompts = [None] * num_prompts\n \n     print(f\"Generating {num_prompts} random prompts with {num_tokens} tokens each...\")\n     for i in range(num_prompts):\n@@ -31,7 +31,7 @@ def generate_random_prompts(num_prompts, num_tokens, tokenizer):\n         prompt = f\"Prompt {i}: {random_text}\"\n         tokens = tokenizer.encode(prompt)\n         print(f\"  Prompt {i}: {len(tokens)} tokens\")\n-        all_prompts.append(prompt)\n+        all_prompts[i] = prompt\n \n     return all_prompts\n \n@@ -40,7 +40,7 @@ def benchmark_sequential_vs_batch(prompts, batch_size, tokenizer):\n     \"\"\"Compare sequential vs batch tokenization for a given batch size.\"\"\"\n \n     # Sequential tokenization using encode()\n-    sequential_times = []\n+    sequential_times = [0.0] * NUM_RUNS\n     for run in range(NUM_RUNS):\n         batch_prompts = prompts[:batch_size]  # Use same prompts for fair comparison\n \n@@ -48,17 +48,17 @@ def benchmark_sequential_vs_batch(prompts, batch_size, tokenizer):\n         for prompt in batch_prompts:\n             tokens = tokenizer.encode(prompt)\n         sequential_time = (time.time() - start_time) * 1000\n-        sequential_times.append(sequential_time)\n+        sequential_times[run] = sequential_time\n \n     # Batch tokenization using tokenizer()\n-    batch_times = []\n+    batch_times = [0.0] * NUM_RUNS\n     for run in range(NUM_RUNS):\n         batch_prompts = prompts[:batch_size]  # Use same prompts for fair comparison\n \n         start_time = time.time()\n         tokens = tokenizer(batch_prompts)\n         batch_time = (time.time() - start_time) * 1000\n-        batch_times.append(batch_time)\n+        batch_times[run] = batch_time\n \n     return {\n         \"batch_size\": batch_size,\ndiff --git a/benchmark/kernels/scheduler_batch/benchmark_write_req_to_token_pool_triton.py b/benchmark/kernels/scheduler_batch/benchmark_write_req_to_token_pool_triton.py\nindex 1ce43c8..1115508 100644\n--- a/benchmark/kernels/scheduler_batch/benchmark_write_req_to_token_pool_triton.py\n+++ b/benchmark/kernels/scheduler_batch/benchmark_write_req_to_token_pool_triton.py\n@@ -116,7 +116,7 @@ def test_write_req_to_token_pool():\n     extend_len = 14\n \n     # Initialize input tensors\n-    req_to_token = torch.zeros(\n+    req_to_token = torch.empty(\n         (max_batch, max_context_len), dtype=torch.int32, device=\"cuda\"\n     )\n     req_pool_indices = torch.tensor([42], dtype=torch.int32, device=\"cuda\")\n@@ -175,7 +175,7 @@ def test_write_req_to_token_pool():\n     extend_lens_list = [14, 20, 30]\n     total_extend_len = sum(extend_lens_list)\n \n-    req_to_token = torch.zeros(\n+    req_to_token = torch.empty(\n         (max_batch, max_context_len), dtype=torch.int32, device=\"cuda\"\n     )\n     req_pool_indices = torch.tensor([42, 100, 200], dtype=torch.int32, device=\"cuda\")\n@@ -251,7 +251,7 @@ def get_benchmark():\n         extend_lens_list = [extend_len] * batch_size\n         total_extend_len = sum(extend_lens_list)\n \n-        req_to_token = torch.zeros(\n+        req_to_token = torch.empty(\n             (max_batch, max_context_len), dtype=torch.int32, device=\"cuda\"\n         )\n         req_pool_indices = torch.arange(batch_size, dtype=torch.int32, device=\"cuda\")\n", "model_name_or_path": "gpt-5-2025-08-07"}
