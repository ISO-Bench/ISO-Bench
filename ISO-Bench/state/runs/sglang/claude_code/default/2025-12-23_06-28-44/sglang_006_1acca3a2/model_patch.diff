diff --git a/python/sglang/srt/layers/attention/flashattention_backend.py b/python/sglang/srt/layers/attention/flashattention_backend.py
index 9579b19f2..a79b354ee 100644
--- a/python/sglang/srt/layers/attention/flashattention_backend.py
+++ b/python/sglang/srt/layers/attention/flashattention_backend.py
@@ -386,12 +386,11 @@ class FlashAttentionBackend(AttentionBackend):
 
                     metadata_expand = FlashAttentionMetadata()
                     decode_length = self.speculative_step_id + 1
-                    metadata_expand.cache_seqlens_int32 = torch.full(
+                    metadata_expand.cache_seqlens_int32 = torch.empty(
                         (seqlens_in_batch.numel() * self.topk,),
-                        decode_length,
                         device=device,
                         dtype=torch.int32,
-                    )
+                    ).fill_(decode_length)
                     metadata_expand.max_seq_len_q = 1
                     metadata_expand.max_seq_len_k = self.speculative_step_id + 1
                     metadata_expand.cu_seqlens_q = torch.arange(
@@ -1130,20 +1129,20 @@ class FlashAttentionBackend(AttentionBackend):
 
         # This is being used by normal decode and draft decode when topk == 1
         self.decode_cuda_graph_metadata = {
-            "cache_seqlens": torch.zeros(max_bs, dtype=torch.int32, device=self.device),
+            "cache_seqlens": torch.empty(max_bs, dtype=torch.int32, device=self.device),
             "cu_seqlens_q": torch.arange(
                 0, max_bs + 1, dtype=torch.int32, device=self.device
             ),
-            "cu_seqlens_k": torch.zeros(
+            "cu_seqlens_k": torch.empty(
                 max_bs + 1, dtype=torch.int32, device=self.device
             ),
-            "page_table": torch.zeros(
+            "page_table": torch.empty(
                 max_bs,
                 (self.max_context_len + self.page_size - 1) // self.page_size,
                 dtype=torch.int32,
                 device=self.device,
             ),
-            "page_table_draft_decode": torch.zeros(
+            "page_table_draft_decode": torch.empty(
                 max_bs,
                 (self.max_context_len + self.page_size - 1) // self.page_size,
                 dtype=torch.int32,
@@ -1157,7 +1156,7 @@ class FlashAttentionBackend(AttentionBackend):
         # This is used by draft decode's first half of metadata when topk > 1
         if self.topk > 1:
             self.draft_decode_metadata_topk_normal = {
-                "cache_seqlens": torch.zeros(
+                "cache_seqlens": torch.empty(
                     max_bs, dtype=torch.int32, device=self.device
                 ),
                 "cu_seqlens_q": torch.arange(
@@ -1167,10 +1166,10 @@ class FlashAttentionBackend(AttentionBackend):
                     dtype=torch.int32,
                     device=self.device,
                 ),
-                "cu_seqlens_k": torch.zeros(
+                "cu_seqlens_k": torch.empty(
                     max_bs + 1, dtype=torch.int32, device=self.device
                 ),
-                "page_table": torch.zeros(
+                "page_table": torch.empty(
                     max_bs,
                     self.max_context_len,
                     dtype=torch.int32,
@@ -1181,12 +1180,11 @@ class FlashAttentionBackend(AttentionBackend):
             # This is used by draft decode's second half of metadata when topk > 1
             decode_length = self.speculative_step_id + 1
             self.draft_decode_metadata_topk_expand = {
-                "cache_seqlens": torch.full(
+                "cache_seqlens": torch.empty(
                     (max_bs * self.topk,),
-                    decode_length,
                     device=self.device,
                     dtype=torch.int32,
-                ),
+                ).fill_(decode_length),
                 "cu_seqlens_q": torch.arange(
                     0,
                     max_bs * self.topk + 1,
@@ -1200,7 +1198,7 @@ class FlashAttentionBackend(AttentionBackend):
                     dtype=torch.int32,
                     device=self.device,
                 ),
-                "page_table": torch.zeros(
+                "page_table": torch.empty(
                     max_bs * self.topk,
                     decode_length,
                     dtype=torch.int32,
@@ -1213,7 +1211,7 @@ class FlashAttentionBackend(AttentionBackend):
             and self.speculative_num_draft_tokens > 0
         ):
             self.target_verify_metadata = {
-                "cache_seqlens": torch.zeros(
+                "cache_seqlens": torch.empty(
                     max_bs, dtype=torch.int32, device=self.device
                 ),
                 "cu_seqlens_q": torch.arange(
@@ -1223,10 +1221,10 @@ class FlashAttentionBackend(AttentionBackend):
                     dtype=torch.int32,
                     device=self.device,
                 ),
-                "cu_seqlens_k": torch.zeros(
+                "cu_seqlens_k": torch.empty(
                     max_bs + 1, dtype=torch.int32, device=self.device
                 ),
-                "page_table": torch.zeros(
+                "page_table": torch.empty(
                     max_bs,
                     (self.max_context_len + self.page_size - 1) // self.page_size,
                     dtype=torch.int32,
@@ -1239,7 +1237,7 @@ class FlashAttentionBackend(AttentionBackend):
 
         if self.topk > 1:
             self.target_verify_metadata_topk_normal = {
-                "cache_seqlens": torch.zeros(
+                "cache_seqlens": torch.empty(
                     max_bs, dtype=torch.int32, device=self.device
                 ),
                 "cu_seqlens_q": torch.arange(
@@ -1249,10 +1247,10 @@ class FlashAttentionBackend(AttentionBackend):
                     dtype=torch.int32,
                     device=self.device,
                 ),
-                "cu_seqlens_k": torch.zeros(
+                "cu_seqlens_k": torch.empty(
                     max_bs + 1, dtype=torch.int32, device=self.device
                 ),
-                "page_table": torch.zeros(
+                "page_table": torch.empty(
                     max_bs,
                     self.max_context_len,
                     dtype=torch.int32,
@@ -1261,12 +1259,12 @@ class FlashAttentionBackend(AttentionBackend):
             }
 
             self.target_verify_metadata_topk_expand = {
-                "cache_seqlens": torch.zeros(
+                "cache_seqlens": torch.empty(
                     max_bs * self.speculative_num_draft_tokens,
                     dtype=torch.int32,
                     device=self.device,
                 ),
-                "cu_seqlens_k": torch.zeros(
+                "cu_seqlens_k": torch.empty(
                     max_bs * self.speculative_num_draft_tokens + 1,
                     dtype=torch.int32,
                     device=self.device,
@@ -1277,7 +1275,7 @@ class FlashAttentionBackend(AttentionBackend):
                     dtype=torch.int32,
                     device=self.device,
                 ),
-                "page_table": torch.zeros(
+                "page_table": torch.empty(
                     max_bs * self.speculative_num_draft_tokens,
                     self.speculative_num_draft_tokens,
                     dtype=torch.int32,
@@ -1286,16 +1284,16 @@ class FlashAttentionBackend(AttentionBackend):
             }
 
         self.encoder_metadata = {
-            "encoder_page_table": torch.zeros(
+            "encoder_page_table": torch.empty(
                 max_bs,
                 self.max_context_len,
                 dtype=torch.int32,
                 device=self.device,
             ),
-            "encoder_lens_int32": torch.zeros(
+            "encoder_lens_int32": torch.empty(
                 max_bs, dtype=torch.int32, device=self.device
             ),
-            "encoder_cu_seqlens_k": torch.zeros(
+            "encoder_cu_seqlens_k": torch.empty(
                 max_bs + 1, dtype=torch.int32, device=self.device
             ),
         }
