{"instance_id": "/home/ubuntu/OmniPerf-Bench/sglan__repo-6ea1e6a", "model_patch": "diff --git a/python/sglang/srt/layers/attention/flashattention_backend.py b/python/sglang/srt/layers/attention/flashattention_backend.py\nindex 9579b19f2..a79b354ee 100644\n--- a/python/sglang/srt/layers/attention/flashattention_backend.py\n+++ b/python/sglang/srt/layers/attention/flashattention_backend.py\n@@ -386,12 +386,11 @@ class FlashAttentionBackend(AttentionBackend):\n \n                     metadata_expand = FlashAttentionMetadata()\n                     decode_length = self.speculative_step_id + 1\n-                    metadata_expand.cache_seqlens_int32 = torch.full(\n+                    metadata_expand.cache_seqlens_int32 = torch.empty(\n                         (seqlens_in_batch.numel() * self.topk,),\n-                        decode_length,\n                         device=device,\n                         dtype=torch.int32,\n-                    )\n+                    ).fill_(decode_length)\n                     metadata_expand.max_seq_len_q = 1\n                     metadata_expand.max_seq_len_k = self.speculative_step_id + 1\n                     metadata_expand.cu_seqlens_q = torch.arange(\n@@ -1130,20 +1129,20 @@ class FlashAttentionBackend(AttentionBackend):\n \n         # This is being used by normal decode and draft decode when topk == 1\n         self.decode_cuda_graph_metadata = {\n-            \"cache_seqlens\": torch.zeros(max_bs, dtype=torch.int32, device=self.device),\n+            \"cache_seqlens\": torch.empty(max_bs, dtype=torch.int32, device=self.device),\n             \"cu_seqlens_q\": torch.arange(\n                 0, max_bs + 1, dtype=torch.int32, device=self.device\n             ),\n-            \"cu_seqlens_k\": torch.zeros(\n+            \"cu_seqlens_k\": torch.empty(\n                 max_bs + 1, dtype=torch.int32, device=self.device\n             ),\n-            \"page_table\": torch.zeros(\n+            \"page_table\": torch.empty(\n                 max_bs,\n                 (self.max_context_len + self.page_size - 1) // self.page_size,\n                 dtype=torch.int32,\n                 device=self.device,\n             ),\n-            \"page_table_draft_decode\": torch.zeros(\n+            \"page_table_draft_decode\": torch.empty(\n                 max_bs,\n                 (self.max_context_len + self.page_size - 1) // self.page_size,\n                 dtype=torch.int32,\n@@ -1157,7 +1156,7 @@ class FlashAttentionBackend(AttentionBackend):\n         # This is used by draft decode's first half of metadata when topk > 1\n         if self.topk > 1:\n             self.draft_decode_metadata_topk_normal = {\n-                \"cache_seqlens\": torch.zeros(\n+                \"cache_seqlens\": torch.empty(\n                     max_bs, dtype=torch.int32, device=self.device\n                 ),\n                 \"cu_seqlens_q\": torch.arange(\n@@ -1167,10 +1166,10 @@ class FlashAttentionBackend(AttentionBackend):\n                     dtype=torch.int32,\n                     device=self.device,\n                 ),\n-                \"cu_seqlens_k\": torch.zeros(\n+                \"cu_seqlens_k\": torch.empty(\n                     max_bs + 1, dtype=torch.int32, device=self.device\n                 ),\n-                \"page_table\": torch.zeros(\n+                \"page_table\": torch.empty(\n                     max_bs,\n                     self.max_context_len,\n                     dtype=torch.int32,\n@@ -1181,12 +1180,11 @@ class FlashAttentionBackend(AttentionBackend):\n             # This is used by draft decode's second half of metadata when topk > 1\n             decode_length = self.speculative_step_id + 1\n             self.draft_decode_metadata_topk_expand = {\n-                \"cache_seqlens\": torch.full(\n+                \"cache_seqlens\": torch.empty(\n                     (max_bs * self.topk,),\n-                    decode_length,\n                     device=self.device,\n                     dtype=torch.int32,\n-                ),\n+                ).fill_(decode_length),\n                 \"cu_seqlens_q\": torch.arange(\n                     0,\n                     max_bs * self.topk + 1,\n@@ -1200,7 +1198,7 @@ class FlashAttentionBackend(AttentionBackend):\n                     dtype=torch.int32,\n                     device=self.device,\n                 ),\n-                \"page_table\": torch.zeros(\n+                \"page_table\": torch.empty(\n                     max_bs * self.topk,\n                     decode_length,\n                     dtype=torch.int32,\n@@ -1213,7 +1211,7 @@ class FlashAttentionBackend(AttentionBackend):\n             and self.speculative_num_draft_tokens > 0\n         ):\n             self.target_verify_metadata = {\n-                \"cache_seqlens\": torch.zeros(\n+                \"cache_seqlens\": torch.empty(\n                     max_bs, dtype=torch.int32, device=self.device\n                 ),\n                 \"cu_seqlens_q\": torch.arange(\n@@ -1223,10 +1221,10 @@ class FlashAttentionBackend(AttentionBackend):\n                     dtype=torch.int32,\n                     device=self.device,\n                 ),\n-                \"cu_seqlens_k\": torch.zeros(\n+                \"cu_seqlens_k\": torch.empty(\n                     max_bs + 1, dtype=torch.int32, device=self.device\n                 ),\n-                \"page_table\": torch.zeros(\n+                \"page_table\": torch.empty(\n                     max_bs,\n                     (self.max_context_len + self.page_size - 1) // self.page_size,\n                     dtype=torch.int32,\n@@ -1239,7 +1237,7 @@ class FlashAttentionBackend(AttentionBackend):\n \n         if self.topk > 1:\n             self.target_verify_metadata_topk_normal = {\n-                \"cache_seqlens\": torch.zeros(\n+                \"cache_seqlens\": torch.empty(\n                     max_bs, dtype=torch.int32, device=self.device\n                 ),\n                 \"cu_seqlens_q\": torch.arange(\n@@ -1249,10 +1247,10 @@ class FlashAttentionBackend(AttentionBackend):\n                     dtype=torch.int32,\n                     device=self.device,\n                 ),\n-                \"cu_seqlens_k\": torch.zeros(\n+                \"cu_seqlens_k\": torch.empty(\n                     max_bs + 1, dtype=torch.int32, device=self.device\n                 ),\n-                \"page_table\": torch.zeros(\n+                \"page_table\": torch.empty(\n                     max_bs,\n                     self.max_context_len,\n                     dtype=torch.int32,\n@@ -1261,12 +1259,12 @@ class FlashAttentionBackend(AttentionBackend):\n             }\n \n             self.target_verify_metadata_topk_expand = {\n-                \"cache_seqlens\": torch.zeros(\n+                \"cache_seqlens\": torch.empty(\n                     max_bs * self.speculative_num_draft_tokens,\n                     dtype=torch.int32,\n                     device=self.device,\n                 ),\n-                \"cu_seqlens_k\": torch.zeros(\n+                \"cu_seqlens_k\": torch.empty(\n                     max_bs * self.speculative_num_draft_tokens + 1,\n                     dtype=torch.int32,\n                     device=self.device,\n@@ -1277,7 +1275,7 @@ class FlashAttentionBackend(AttentionBackend):\n                     dtype=torch.int32,\n                     device=self.device,\n                 ),\n-                \"page_table\": torch.zeros(\n+                \"page_table\": torch.empty(\n                     max_bs * self.speculative_num_draft_tokens,\n                     self.speculative_num_draft_tokens,\n                     dtype=torch.int32,\n@@ -1286,16 +1284,16 @@ class FlashAttentionBackend(AttentionBackend):\n             }\n \n         self.encoder_metadata = {\n-            \"encoder_page_table\": torch.zeros(\n+            \"encoder_page_table\": torch.empty(\n                 max_bs,\n                 self.max_context_len,\n                 dtype=torch.int32,\n                 device=self.device,\n             ),\n-            \"encoder_lens_int32\": torch.zeros(\n+            \"encoder_lens_int32\": torch.empty(\n                 max_bs, dtype=torch.int32, device=self.device\n             ),\n-            \"encoder_cu_seqlens_k\": torch.zeros(\n+            \"encoder_cu_seqlens_k\": torch.empty(\n                 max_bs + 1, dtype=torch.int32, device=self.device\n             ),\n         }\n", "model_name_or_path": "gpt-5-2025-08-07"}
