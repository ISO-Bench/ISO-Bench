{"instance_id": "/home/ubuntu/OmniPerf-Bench/sglan__repo-33b242d", "model_patch": "diff --git a/python/sglang/srt/layers/logits_processor.py b/python/sglang/srt/layers/logits_processor.py\nindex f95c30786..1319b902e 100644\n--- a/python/sglang/srt/layers/logits_processor.py\n+++ b/python/sglang/srt/layers/logits_processor.py\n@@ -13,6 +13,24 @@ class LogitsProcessor(nn.Module):\n         super().__init__()\n         self.config = config\n         self.tp_size = get_tensor_model_parallel_world_size()\n+        # Pre-allocate commonly used tensors to avoid repeated allocations\n+        self._zero_tensor_cache = None\n+        self._arange_cache = None\n+        self._arange_cache_size = 0\n+\n+    def _get_zero_tensor(self):\n+        \"\"\"Get or create a cached zero tensor to avoid repeated allocations.\"\"\"\n+        if self._zero_tensor_cache is None:\n+            self._zero_tensor_cache = torch.tensor([0], device=\"cuda\")\n+        return self._zero_tensor_cache\n+\n+    def _get_arange(self, size):\n+        \"\"\"Get or create a cached arange tensor to avoid repeated allocations.\"\"\"\n+        if self._arange_cache is None or self._arange_cache_size < size:\n+            self._arange_cache = torch.arange(size, device=\"cuda\")\n+            self._arange_cache_size = size\n+            return self._arange_cache\n+        return self._arange_cache[:size]\n \n     def _get_normalized_prompt_logprobs(\n         self, prefill_token_logprobs, input_metadata: InputMetadata\n@@ -21,10 +39,12 @@ class LogitsProcessor(nn.Module):\n             prefill_token_logprobs, dim=0, dtype=torch.float32\n         )\n \n-        start = input_metadata.extend_start_loc.clone()\n+        # Avoid clone by computing directly without modifying input_metadata\n+        start = input_metadata.extend_start_loc\n         end = start + input_metadata.extend_seq_lens - 2\n-        start.clamp_(min=0, max=prefill_token_logprobs.shape[0] - 1)\n-        end.clamp_(min=0, max=prefill_token_logprobs.shape[0] - 1)\n+        # Use torch.clamp instead of clamp_ to avoid in-place modification\n+        start = start.clamp(min=0, max=prefill_token_logprobs.shape[0] - 1)\n+        end = end.clamp(min=0, max=prefill_token_logprobs.shape[0] - 1)\n         sum_logp = (\n             logprobs_cumsum[end]\n             - logprobs_cumsum[start]\n@@ -98,7 +118,8 @@ class LogitsProcessor(nn.Module):\n                     all_logits = tensor_model_parallel_all_gather(all_logits)\n                 all_logits = all_logits[:, : self.config.vocab_size]\n \n-            all_logprobs = torch.log(torch.softmax(all_logits.float(), dim=-1) + 1e-6)\n+            # Use log_softmax for better numerical stability and performance\n+            all_logprobs = torch.nn.functional.log_softmax(all_logits.float(), dim=-1)\n \n             prefill_top_logprobs, decode_top_logprobs = self._get_top_logprobs(\n                 all_logprobs, input_metadata\n@@ -119,9 +140,12 @@ class LogitsProcessor(nn.Module):\n \n                 # Compute the logprobs and normalized logprobs for the prefill tokens.\n                 # Note that we pad a zero at the end of each sequence for easy computation.\n+                # Use cached tensors to avoid repeated allocations\n+                arange_indices = self._get_arange(all_logprobs.shape[0])\n+                zero_tensor = self._get_zero_tensor()\n                 prefill_token_logprobs = all_logprobs[\n-                    torch.arange(all_logprobs.shape[0], device=\"cuda\"),\n-                    torch.cat([input_ids[1:], torch.tensor([0], device=\"cuda\")]),\n+                    arange_indices,\n+                    torch.cat([input_ids[1:], zero_tensor]),\n                 ]\n \n                 normalized_prompt_logprobs = self._get_normalized_prompt_logprobs(\ndiff --git a/python/sglang/srt/managers/router/model_rpc.py b/python/sglang/srt/managers/router/model_rpc.py\nindex f283635c3..b5af5e9a8 100644\n--- a/python/sglang/srt/managers/router/model_rpc.py\n+++ b/python/sglang/srt/managers/router/model_rpc.py\n@@ -103,8 +103,10 @@ class ModelRpcServer:\n                 else server_args.max_prefill_num_token\n             ),\n         )\n+        # Optimize tensor creation with explicit dtype and device\n+        logit_bias_list = get_int_token_logit_bias(self.tokenizer, self.model_config.vocab_size)\n         self.int_token_logit_bias = torch.tensor(\n-            get_int_token_logit_bias(self.tokenizer, self.model_config.vocab_size)\n+            logit_bias_list, dtype=torch.float32, device=\"cpu\"\n         )\n         set_random_seed(server_args.random_seed)\n         logger.info(\n@@ -150,6 +152,18 @@ class ModelRpcServer:\n         self.min_new_token_ratio = min(0.2 * server_args.schedule_conservativeness, 1.0)\n         self.new_token_ratio_step = (0.0001, 0.05)  # (down, up)\n \n+        # Cache for torch.arange to avoid repeated allocations\n+        self._arange_cache = None\n+        self._arange_cache_size = 0\n+\n+    def _get_arange(self, size, device=\"cuda\"):\n+        \"\"\"Get or create a cached arange tensor to avoid repeated allocations.\"\"\"\n+        if self._arange_cache is None or self._arange_cache_size < size:\n+            self._arange_cache = torch.arange(size, device=device)\n+            self._arange_cache_size = size\n+            return self._arange_cache\n+        return self._arange_cache[:size]\n+\n     def flush_cache(self):\n         if len(self.forward_queue) == 0 and (\n             self.running_batch is None or len(self.running_batch.reqs) == 0\n@@ -159,6 +173,9 @@ class ModelRpcServer:\n             self.regex_fsm_cache.reset()\n             self.req_to_token_pool.clear()\n             self.token_to_kv_pool.clear()\n+            # Clear arange cache\n+            self._arange_cache = None\n+            self._arange_cache_size = 0\n             torch.cuda.empty_cache()\n             logger.info(\"Cache flushed successfully!\")\n         else:\n@@ -436,8 +453,10 @@ class ModelRpcServer:\n         reqs = batch.reqs\n         last_token_logprobs = None\n         if last_logprobs is not None:\n+            # Use cached arange to avoid repeated allocations\n+            arange_indices = self._get_arange(len(reqs), device=last_logprobs.device)\n             last_token_logprobs = (\n-                last_logprobs[torch.arange(len(reqs)), next_token_ids].cpu().tolist()\n+                last_logprobs[arange_indices, next_token_ids].cpu().tolist()\n             )\n \n         # Check finish condition\n@@ -530,8 +549,10 @@ class ModelRpcServer:\n         reqs = batch.reqs\n         new_token_logprobs = None\n         if last_logprobs is not None:\n+            # Use cached arange to avoid repeated allocations\n+            arange_indices = self._get_arange(len(reqs), device=last_logprobs.device)\n             new_token_logprobs = last_logprobs[\n-                torch.arange(len(reqs)), next_token_ids\n+                arange_indices, next_token_ids\n             ].tolist()\n \n         # Check finish condition\n", "model_name_or_path": "gpt-5-2025-08-07"}
