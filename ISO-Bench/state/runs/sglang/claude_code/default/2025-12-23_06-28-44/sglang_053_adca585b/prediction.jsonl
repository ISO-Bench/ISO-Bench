{"instance_id": "/home/ubuntu/OmniPerf-Bench/sglan__repo-39d9044", "model_patch": "diff --git a/python/sglang/srt/models/deepseek_v2.py b/python/sglang/srt/models/deepseek_v2.py\nindex 42ae9d2..982a55a 100644\n--- a/python/sglang/srt/models/deepseek_v2.py\n+++ b/python/sglang/srt/models/deepseek_v2.py\n@@ -297,8 +297,8 @@ class DeepseekV2MoE(nn.Module):\n         self, hidden_states: torch.Tensor, forward_mode: ForwardMode\n     ) -> torch.Tensor:\n         shared_output = None\n-        topk_idx = torch.full(\n-            (0, self.top_k), -1, dtype=torch.int, device=hidden_states.device\n+        topk_idx = torch.empty(\n+            (0, self.top_k), dtype=torch.int, device=hidden_states.device\n         )\n         topk_weights = torch.empty(\n             (0, self.top_k), dtype=torch.float32, device=hidden_states.device\n@@ -815,7 +815,7 @@ class DeepseekV2AttentionMLA(nn.Module):\n             # TODO(kernel): add bmm_fp8 for torch.float8_e4m3fnuz\n             q_nope_out = torch.bmm(\n                 q_nope.to(torch.bfloat16).transpose(0, 1),\n-                self.w_kc.to(torch.bfloat16) * self.w_scale,\n+                self.w_kc_bf16_scaled,\n             )\n         elif self.w_kc.dtype == torch.float8_e4m3fn:\n             q_nope_val, q_nope_scale = input_to_float8(\n@@ -846,7 +846,7 @@ class DeepseekV2AttentionMLA(nn.Module):\n             # TODO(kernel): add bmm_fp8 for torch.float8_e4m3fnuz\n             attn_bmm_output = torch.bmm(\n                 attn_output.to(torch.bfloat16).transpose(0, 1),\n-                self.w_vc.to(torch.bfloat16) * self.w_scale,\n+                self.w_vc_bf16_scaled,\n             )\n         elif self.w_vc.dtype == torch.float8_e4m3fn:\n             attn_output_val, attn_output_scale = input_to_float8(\n@@ -893,7 +893,7 @@ class DeepseekV2AttentionMLA(nn.Module):\n             # TODO(kernel): add bmm_fp8 for torch.float8_e4m3fnuz\n             q_nope_out = torch.bmm(\n                 q_nope.to(torch.bfloat16).transpose(0, 1),\n-                self.w_kc.to(torch.bfloat16) * self.w_scale,\n+                self.w_kc_bf16_scaled,\n             )\n         elif self.w_kc.dtype == torch.float8_e4m3fn:\n             q_nope_val, q_nope_scale = input_to_float8(\n@@ -989,7 +989,7 @@ class DeepseekV2AttentionMLA(nn.Module):\n             # TODO(kernel): add bmm_fp8 for torch.float8_e4m3fnuz\n             attn_bmm_output = torch.bmm(\n                 attn_output.to(torch.bfloat16).transpose(0, 1),\n-                self.w_vc.to(torch.bfloat16) * self.w_scale,\n+                self.w_vc_bf16_scaled,\n             )\n         elif self.w_vc.dtype == torch.float8_e4m3fn:\n             attn_output_val, attn_output_scale = input_to_float8(\n@@ -1461,7 +1461,7 @@ class DeepseekV2ForCausalLM(nn.Module):\n                 w_kc, w_vc = w.unflatten(\n                     0, (-1, self_attn.qk_nope_head_dim + self_attn.v_head_dim)\n                 ).split([self_attn.qk_nope_head_dim, self_attn.v_head_dim], dim=1)\n-                self_attn.w_kc = w_kc.transpose(1, 2).contiguous().transpose(1, 2)\n+                self_attn.w_kc = w_kc.contiguous()\n                 self_attn.w_vc = w_vc.contiguous().transpose(1, 2)\n                 if (\n                     hasattr(self_attn.kv_b_proj, \"weight_scale\")\n@@ -1471,6 +1471,14 @@ class DeepseekV2ForCausalLM(nn.Module):\n                     if _is_hip:\n                         self_attn.w_scale *= 2.0\n \n+                # Pre-compute weight conversions for float8_e4m3fnuz to avoid repeated conversions\n+                if self_attn.w_kc.dtype == torch.float8_e4m3fnuz:\n+                    self_attn.w_kc_bf16_scaled = self_attn.w_kc.to(torch.bfloat16) * self_attn.w_scale\n+                    self_attn.w_vc_bf16_scaled = self_attn.w_vc.to(torch.bfloat16) * self_attn.w_scale\n+                else:\n+                    self_attn.w_kc_bf16_scaled = None\n+                    self_attn.w_vc_bf16_scaled = None\n+\n     def load_weights(self, weights: Iterable[Tuple[str, torch.Tensor]]):\n         stacked_params_mapping = [\n             # (param_name, shard_name, shard_id)\n", "model_name_or_path": "gpt-5-2025-08-07"}
