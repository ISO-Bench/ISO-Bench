diff --git a/python/sglang/srt/managers/scheduler.py b/python/sglang/srt/managers/scheduler.py
index 1d1cf3688..c0e0a4a8b 100644
--- a/python/sglang/srt/managers/scheduler.py
+++ b/python/sglang/srt/managers/scheduler.py
@@ -425,8 +425,9 @@ class Scheduler:
         else:
             num_tokens = local_batch.extend_num_tokens
 
-        local_num_tokens = torch.tensor([num_tokens], dtype=torch.int64)
-        global_num_tokens = torch.empty(self.tp_size, dtype=torch.int64)
+        local_num_tokens = torch.empty(1, dtype=torch.int64, device="cpu")
+        local_num_tokens[0] = num_tokens
+        global_num_tokens = torch.empty(self.tp_size, dtype=torch.int64, device="cpu")
         torch.distributed.all_gather_into_tensor(
             global_num_tokens,
             local_num_tokens,
@@ -441,14 +442,12 @@ class Scheduler:
 
             # Check forward mode for cuda graph
             if not self.server_args.disable_cuda_graph:
-                forward_mode_state = torch.tensor(
-                    (
-                        1
-                        if local_batch.forward_mode.is_decode()
-                        or local_batch.forward_mode.is_idle()
-                        else 0
-                    ),
-                    dtype=torch.int32,
+                forward_mode_state = torch.empty(1, dtype=torch.int32)
+                forward_mode_state[0] = (
+                    1
+                    if local_batch.forward_mode.is_decode()
+                    or local_batch.forward_mode.is_idle()
+                    else 0
                 )
                 torch.distributed.all_reduce(
                     forward_mode_state,
@@ -819,8 +818,9 @@ class Scheduler:
         can_run_list = adder.can_run_list
         if len(can_run_list) == 0:
             return None
+        can_run_set = set(can_run_list)
         self.waiting_queue = [
-            x for x in self.waiting_queue if x not in set(can_run_list)
+            x for x in self.waiting_queue if x not in can_run_set
         ]
 
         if adder.new_inflight_req is not None:
@@ -924,12 +924,12 @@ class Scheduler:
                 return
             else:
                 logits_output = None
+                batch_size = batch.batch_size()
                 if self.skip_tokenizer_init:
-                    next_token_ids = torch.full(
-                        (batch.batch_size(),), self.tokenizer.eos_token_id
-                    )
+                    next_token_ids = torch.empty(batch_size, dtype=torch.long, device=self.device)
+                    next_token_ids.fill_(self.tokenizer.eos_token_id)
                 else:
-                    next_token_ids = torch.full((batch.batch_size(),), 0)
+                    next_token_ids = torch.zeros(batch_size, dtype=torch.long, device=self.device)
             batch.output_ids = next_token_ids
             ret = logits_output, next_token_ids, model_worker_batch.bid
         else:  # embedding or reward model
@@ -1287,7 +1287,8 @@ class Scheduler:
 
         if self.tp_size > 1:
             # Sync across TP ranks to make sure they have the same number of ready requests
-            tensor = torch.tensor(num_ready_reqs, dtype=torch.int32)
+            tensor = torch.empty(1, dtype=torch.int32)
+            tensor[0] = num_ready_reqs
             torch.distributed.all_reduce(
                 tensor, op=torch.distributed.ReduceOp.MAX, group=self.tp_cpu_group
             )
diff --git a/python/sglang/srt/utils.py b/python/sglang/srt/utils.py
index e947d1a92..646892360 100644
--- a/python/sglang/srt/utils.py
+++ b/python/sglang/srt/utils.py
@@ -687,21 +687,23 @@ def broadcast_pyobj(
 
     if rank == 0:
         if len(data) == 0:
-            tensor_size = torch.tensor([0], dtype=torch.long)
+            tensor_size = torch.empty(1, dtype=torch.long)
+            tensor_size[0] = 0
             dist.broadcast(tensor_size, src=0, group=dist_group)
         else:
             serialized_data = pickle.dumps(data)
             size = len(serialized_data)
-            tensor_data = torch.ByteTensor(
+            tensor_data = torch.from_numpy(
                 np.frombuffer(serialized_data, dtype=np.uint8)
             )
-            tensor_size = torch.tensor([size], dtype=torch.long)
+            tensor_size = torch.empty(1, dtype=torch.long)
+            tensor_size[0] = size
 
             dist.broadcast(tensor_size, src=0, group=dist_group)
             dist.broadcast(tensor_data, src=0, group=dist_group)
         return data
     else:
-        tensor_size = torch.tensor([0], dtype=torch.long)
+        tensor_size = torch.empty(1, dtype=torch.long)
         dist.broadcast(tensor_size, src=0, group=dist_group)
         size = tensor_size.item()
 
