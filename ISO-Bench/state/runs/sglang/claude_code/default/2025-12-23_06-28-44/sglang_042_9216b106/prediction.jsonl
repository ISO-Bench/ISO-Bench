{"instance_id": "/home/ubuntu/OmniPerf-Bench/sglan__repo-da19434", "model_patch": "diff --git a/python/sglang/srt/managers/router/model_rpc.py b/python/sglang/srt/managers/router/model_rpc.py\nindex 883bb12..88a1935 100644\n--- a/python/sglang/srt/managers/router/model_rpc.py\n+++ b/python/sglang/srt/managers/router/model_rpc.py\n@@ -100,7 +100,8 @@ class ModelRpcServer:\n                 else server_args.max_prefill_num_token\n             ),\n         )\n-        self.int_token_logit_bias = torch.tensor(\n+        # Optimize: Use torch.as_tensor instead of torch.tensor to avoid unnecessary copy\n+        self.int_token_logit_bias = torch.as_tensor(\n             get_int_token_logit_bias(self.tokenizer, self.model_config.vocab_size)\n         )\n         set_random_seed(server_args.random_seed)\n@@ -309,11 +310,10 @@ class ModelRpcServer:\n             self.token_to_kv_pool.available_size() + self.tree_cache.evictable_size()\n         )\n         if self.running_batch:\n+            # Optimize: Use generator expression instead of list comprehension\n             available_size -= sum(\n-                [\n-                    (r.max_new_tokens() - len(r.output_ids)) * self.new_token_ratio\n-                    for r in self.running_batch.reqs\n-                ]\n+                (r.max_new_tokens() - len(r.output_ids)) * self.new_token_ratio\n+                for r in self.running_batch.reqs\n             )\n \n         for req in self.forward_queue:\n@@ -332,19 +332,19 @@ class ModelRpcServer:\n                 if req.image_offset is not None:\n                     req.image_offset += 1\n \n+            # Optimize: Cache repeated calculations\n+            req_extend_len = req.extend_input_len\n+            req_max_tokens = req.max_new_tokens()\n+            req_total_tokens = req_extend_len + req_max_tokens\n+\n             if (\n-                req.extend_input_len + req.max_new_tokens() + new_batch_total_tokens\n-                < available_size\n-                and req.extend_input_len + new_batch_input_tokens\n-                < self.max_prefill_num_token\n+                req_total_tokens + new_batch_total_tokens < available_size\n+                and req_extend_len + new_batch_input_tokens < self.max_prefill_num_token\n             ):\n                 delta = self.tree_cache.inc_ref_counter(req.last_node)\n                 available_size += delta\n \n-                if not (\n-                    req.extend_input_len + req.max_new_tokens() + new_batch_total_tokens\n-                    < available_size\n-                ):\n+                if not (req_total_tokens + new_batch_total_tokens < available_size):\n                     # Undo the insertion\n                     delta = self.tree_cache.dec_ref_counter(req.last_node)\n                     available_size += delta\n@@ -352,10 +352,8 @@ class ModelRpcServer:\n                     # Add this request to the running batch\n                     self.token_to_kv_pool.add_refs(req.prefix_indices)\n                     can_run_list.append(req)\n-                    new_batch_total_tokens += (\n-                        req.extend_input_len + req.max_new_tokens()\n-                    )\n-                    new_batch_input_tokens += req.extend_input_len\n+                    new_batch_total_tokens += req_total_tokens\n+                    new_batch_input_tokens += req_extend_len\n \n         if len(can_run_list) == 0:\n             return None\n@@ -393,7 +391,9 @@ class ModelRpcServer:\n             self.token_to_kv_pool,\n             self.tree_cache,\n         )\n-        self.forward_queue = [x for x in self.forward_queue if x not in can_run_list]\n+        # Optimize: Use set for O(1) membership check instead of O(n)\n+        can_run_set = set(can_run_list)\n+        self.forward_queue = [x for x in self.forward_queue if x not in can_run_set]\n         return new_batch\n \n     def forward_fill_batch(self, batch: Batch):\n@@ -412,12 +412,13 @@ class ModelRpcServer:\n                 normalized_prompt_logprobs,\n                 last_logprobs,\n             ) = self.model_runner.forward(batch, ForwardMode.EXTEND)\n-            if prefill_token_logprobs is not None:\n-                prefill_token_logprobs = prefill_token_logprobs.cpu().tolist()\n-                normalized_prompt_logprobs = normalized_prompt_logprobs.cpu().tolist()\n-\n+            # Optimize: Move tensor to CPU once and convert to list\n             next_token_ids, _ = batch.sample(logits)\n-            next_token_ids = next_token_ids.cpu().tolist()\n+            next_token_ids = next_token_ids.tolist() if next_token_ids.device.type == 'cpu' else next_token_ids.cpu().tolist()\n+\n+            if prefill_token_logprobs is not None:\n+                prefill_token_logprobs = prefill_token_logprobs.tolist() if prefill_token_logprobs.device.type == 'cpu' else prefill_token_logprobs.cpu().tolist()\n+                normalized_prompt_logprobs = normalized_prompt_logprobs.tolist() if normalized_prompt_logprobs.device.type == 'cpu' else normalized_prompt_logprobs.cpu().tolist()\n         else:\n             next_token_ids = [self.tokenizer.eos_token_id] * len(batch.reqs)\n             (\n@@ -431,9 +432,9 @@ class ModelRpcServer:\n         reqs = batch.reqs\n         last_token_logprobs = None\n         if last_logprobs is not None:\n-            last_token_logprobs = (\n-                last_logprobs[torch.arange(len(reqs)), next_token_ids].cpu().tolist()\n-            )\n+            # Optimize: Use direct indexing without creating arange tensor\n+            last_token_logprobs_tensor = last_logprobs[range(len(reqs)), next_token_ids]\n+            last_token_logprobs = last_token_logprobs_tensor.tolist() if last_token_logprobs_tensor.device.type == 'cpu' else last_token_logprobs_tensor.cpu().tolist()\n \n         # Check finish condition\n         pt = 0\n@@ -519,15 +520,15 @@ class ModelRpcServer:\n             last_logprobs,\n         ) = self.model_runner.forward(batch, ForwardMode.DECODE)\n         next_token_ids, _ = batch.sample(logits)\n-        next_token_ids = next_token_ids.cpu().tolist()\n+        # Optimize: Avoid unnecessary CPU transfer if already on CPU\n+        next_token_ids = next_token_ids.tolist() if next_token_ids.device.type == 'cpu' else next_token_ids.cpu().tolist()\n \n         # Only batch transfer the selected logprobs of the next token to CPU to reduce overhead.\n         reqs = batch.reqs\n         new_token_logprobs = None\n         if last_logprobs is not None:\n-            new_token_logprobs = last_logprobs[\n-                torch.arange(len(reqs)), next_token_ids\n-            ].tolist()\n+            # Optimize: Use direct indexing without creating arange tensor\n+            new_token_logprobs = last_logprobs[range(len(reqs)), next_token_ids].tolist()\n \n         # Check finish condition\n         for i, (req, next_token_id) in enumerate(zip(reqs, next_token_ids)):\n@@ -615,7 +616,8 @@ class ModelRpcServer:\n         # Remove finished reqs\n         if finished_indices:\n             # Update radix cache\n-            req_pool_indices_cpu = batch.req_pool_indices.cpu().tolist()\n+            # Optimize: Avoid unnecessary CPU transfer if already on CPU\n+            req_pool_indices_cpu = batch.req_pool_indices.tolist() if batch.req_pool_indices.device.type == 'cpu' else batch.req_pool_indices.cpu().tolist()\n             for i in finished_indices:\n                 req = batch.reqs[i]\n                 req_pool_idx = req_pool_indices_cpu[i]\n", "model_name_or_path": "gpt-5-2025-08-07"}
