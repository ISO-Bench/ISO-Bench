diff --git a/python/sglang/srt/managers/router/model_rpc.py b/python/sglang/srt/managers/router/model_rpc.py
index 883bb12..88a1935 100644
--- a/python/sglang/srt/managers/router/model_rpc.py
+++ b/python/sglang/srt/managers/router/model_rpc.py
@@ -100,7 +100,8 @@ class ModelRpcServer:
                 else server_args.max_prefill_num_token
             ),
         )
-        self.int_token_logit_bias = torch.tensor(
+        # Optimize: Use torch.as_tensor instead of torch.tensor to avoid unnecessary copy
+        self.int_token_logit_bias = torch.as_tensor(
             get_int_token_logit_bias(self.tokenizer, self.model_config.vocab_size)
         )
         set_random_seed(server_args.random_seed)
@@ -309,11 +310,10 @@ class ModelRpcServer:
             self.token_to_kv_pool.available_size() + self.tree_cache.evictable_size()
         )
         if self.running_batch:
+            # Optimize: Use generator expression instead of list comprehension
             available_size -= sum(
-                [
-                    (r.max_new_tokens() - len(r.output_ids)) * self.new_token_ratio
-                    for r in self.running_batch.reqs
-                ]
+                (r.max_new_tokens() - len(r.output_ids)) * self.new_token_ratio
+                for r in self.running_batch.reqs
             )
 
         for req in self.forward_queue:
@@ -332,19 +332,19 @@ class ModelRpcServer:
                 if req.image_offset is not None:
                     req.image_offset += 1
 
+            # Optimize: Cache repeated calculations
+            req_extend_len = req.extend_input_len
+            req_max_tokens = req.max_new_tokens()
+            req_total_tokens = req_extend_len + req_max_tokens
+
             if (
-                req.extend_input_len + req.max_new_tokens() + new_batch_total_tokens
-                < available_size
-                and req.extend_input_len + new_batch_input_tokens
-                < self.max_prefill_num_token
+                req_total_tokens + new_batch_total_tokens < available_size
+                and req_extend_len + new_batch_input_tokens < self.max_prefill_num_token
             ):
                 delta = self.tree_cache.inc_ref_counter(req.last_node)
                 available_size += delta
 
-                if not (
-                    req.extend_input_len + req.max_new_tokens() + new_batch_total_tokens
-                    < available_size
-                ):
+                if not (req_total_tokens + new_batch_total_tokens < available_size):
                     # Undo the insertion
                     delta = self.tree_cache.dec_ref_counter(req.last_node)
                     available_size += delta
@@ -352,10 +352,8 @@ class ModelRpcServer:
                     # Add this request to the running batch
                     self.token_to_kv_pool.add_refs(req.prefix_indices)
                     can_run_list.append(req)
-                    new_batch_total_tokens += (
-                        req.extend_input_len + req.max_new_tokens()
-                    )
-                    new_batch_input_tokens += req.extend_input_len
+                    new_batch_total_tokens += req_total_tokens
+                    new_batch_input_tokens += req_extend_len
 
         if len(can_run_list) == 0:
             return None
@@ -393,7 +391,9 @@ class ModelRpcServer:
             self.token_to_kv_pool,
             self.tree_cache,
         )
-        self.forward_queue = [x for x in self.forward_queue if x not in can_run_list]
+        # Optimize: Use set for O(1) membership check instead of O(n)
+        can_run_set = set(can_run_list)
+        self.forward_queue = [x for x in self.forward_queue if x not in can_run_set]
         return new_batch
 
     def forward_fill_batch(self, batch: Batch):
@@ -412,12 +412,13 @@ class ModelRpcServer:
                 normalized_prompt_logprobs,
                 last_logprobs,
             ) = self.model_runner.forward(batch, ForwardMode.EXTEND)
-            if prefill_token_logprobs is not None:
-                prefill_token_logprobs = prefill_token_logprobs.cpu().tolist()
-                normalized_prompt_logprobs = normalized_prompt_logprobs.cpu().tolist()
-
+            # Optimize: Move tensor to CPU once and convert to list
             next_token_ids, _ = batch.sample(logits)
-            next_token_ids = next_token_ids.cpu().tolist()
+            next_token_ids = next_token_ids.tolist() if next_token_ids.device.type == 'cpu' else next_token_ids.cpu().tolist()
+
+            if prefill_token_logprobs is not None:
+                prefill_token_logprobs = prefill_token_logprobs.tolist() if prefill_token_logprobs.device.type == 'cpu' else prefill_token_logprobs.cpu().tolist()
+                normalized_prompt_logprobs = normalized_prompt_logprobs.tolist() if normalized_prompt_logprobs.device.type == 'cpu' else normalized_prompt_logprobs.cpu().tolist()
         else:
             next_token_ids = [self.tokenizer.eos_token_id] * len(batch.reqs)
             (
@@ -431,9 +432,9 @@ class ModelRpcServer:
         reqs = batch.reqs
         last_token_logprobs = None
         if last_logprobs is not None:
-            last_token_logprobs = (
-                last_logprobs[torch.arange(len(reqs)), next_token_ids].cpu().tolist()
-            )
+            # Optimize: Use direct indexing without creating arange tensor
+            last_token_logprobs_tensor = last_logprobs[range(len(reqs)), next_token_ids]
+            last_token_logprobs = last_token_logprobs_tensor.tolist() if last_token_logprobs_tensor.device.type == 'cpu' else last_token_logprobs_tensor.cpu().tolist()
 
         # Check finish condition
         pt = 0
@@ -519,15 +520,15 @@ class ModelRpcServer:
             last_logprobs,
         ) = self.model_runner.forward(batch, ForwardMode.DECODE)
         next_token_ids, _ = batch.sample(logits)
-        next_token_ids = next_token_ids.cpu().tolist()
+        # Optimize: Avoid unnecessary CPU transfer if already on CPU
+        next_token_ids = next_token_ids.tolist() if next_token_ids.device.type == 'cpu' else next_token_ids.cpu().tolist()
 
         # Only batch transfer the selected logprobs of the next token to CPU to reduce overhead.
         reqs = batch.reqs
         new_token_logprobs = None
         if last_logprobs is not None:
-            new_token_logprobs = last_logprobs[
-                torch.arange(len(reqs)), next_token_ids
-            ].tolist()
+            # Optimize: Use direct indexing without creating arange tensor
+            new_token_logprobs = last_logprobs[range(len(reqs)), next_token_ids].tolist()
 
         # Check finish condition
         for i, (req, next_token_id) in enumerate(zip(reqs, next_token_ids)):
@@ -615,7 +616,8 @@ class ModelRpcServer:
         # Remove finished reqs
         if finished_indices:
             # Update radix cache
-            req_pool_indices_cpu = batch.req_pool_indices.cpu().tolist()
+            # Optimize: Avoid unnecessary CPU transfer if already on CPU
+            req_pool_indices_cpu = batch.req_pool_indices.tolist() if batch.req_pool_indices.device.type == 'cpu' else batch.req_pool_indices.cpu().tolist()
             for i in finished_indices:
                 req = batch.reqs[i]
                 req_pool_idx = req_pool_indices_cpu[i]
