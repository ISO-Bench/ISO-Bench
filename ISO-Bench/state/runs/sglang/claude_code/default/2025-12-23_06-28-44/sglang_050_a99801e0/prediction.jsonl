{"instance_id": "/home/ubuntu/OmniPerf-Bench/sglan__repo-4c60523", "model_patch": "diff --git a/python/sglang/srt/mem_cache/allocator.py b/python/sglang/srt/mem_cache/allocator.py\nindex 7dd488e..d6574d8 100644\n--- a/python/sglang/srt/mem_cache/allocator.py\n+++ b/python/sglang/srt/mem_cache/allocator.py\n@@ -175,12 +175,14 @@ class SWATokenToKVPoolAllocator(BaseTokenToKVPoolAllocator):\n             device,\n             kvcache.swa_kv_pool,\n         )\n-        self.full_to_swa_index_mapping = torch.empty(\n+        # Allocate with zeros directly to avoid fill_ operation in clear()\n+        self.full_to_swa_index_mapping = torch.zeros(\n             size + size_swa + 1,\n             dtype=torch.int64,\n             device=device,\n         )\n-        self.clear()\n+        self.is_not_in_free_group = True\n+        self.free_group = []\n \n         self._kvcache.full_to_swa_index_mapping = self.full_to_swa_index_mapping\n \n@@ -255,7 +257,8 @@ class SWATokenToKVPoolAllocator(BaseTokenToKVPoolAllocator):\n     def clear(self):\n         self.swa_attn_allocator.clear()\n         self.full_attn_allocator.clear()\n-        self.full_to_swa_index_mapping.fill_(0)\n+        # Use slice assignment with zero tensor instead of fill_ for better performance\n+        self.full_to_swa_index_mapping[:] = 0\n         self.is_in_free_group = False\n         self.free_group = []\n \n@@ -411,6 +414,8 @@ class PagedTokenToKVPoolAllocator(BaseTokenToKVPoolAllocator):\n         self.num_pages = size // page_size\n         self.debug_mode = get_bool_env_var(\"SGLANG_DEBUG_MEMORY_POOL\")\n         self.ret_values = torch.empty((), dtype=torch.int64, device=self.device)\n+        # Cache page offset tensor to avoid repeated allocation\n+        self._page_offset_cache = torch.arange(self.page_size, device=self.device)\n         self.clear()\n \n     def alloc(self, need_size: int):\n@@ -427,9 +432,9 @@ class PagedTokenToKVPoolAllocator(BaseTokenToKVPoolAllocator):\n         out_pages = self.free_pages[:num_pages]\n         self.free_pages = self.free_pages[num_pages:]\n \n+        # Use cached page offset tensor to avoid repeated allocation\n         out_indices = (\n-            out_pages[:, None] * self.page_size\n-            + torch.arange(self.page_size, device=self.device)\n+            out_pages[:, None] * self.page_size + self._page_offset_cache\n         ).reshape(-1)\n \n         return out_indices\n@@ -541,6 +546,7 @@ def alloc_extend_kernel_ascend(\n     out_indices,\n     page_size,\n     device,\n+    pos_in_page=None,\n ):\n     extend_lens = seq_lens - prefix_lens\n     end_pos = torch.cumsum(extend_lens, 0)\n@@ -554,7 +560,9 @@ def alloc_extend_kernel_ascend(\n     need_page = num_new_pages - num_full_new_pages\n     end_new_pages = torch.cumsum(num_new_pages, 0)\n     start_new_pages = end_new_pages - num_new_pages\n-    pos_in_page = torch.arange(page_size, device=device, dtype=torch.int32)\n+    # Reuse pos_in_page if provided to avoid repeated allocation\n+    if pos_in_page is None:\n+        pos_in_page = torch.arange(page_size, device=device, dtype=torch.int32)\n     for i in range(len(prefix_lens)):\n         num1 = (\n             min(\n@@ -620,6 +628,8 @@ class AscendPagedTokenToKVPoolAllocator(PagedTokenToKVPoolAllocator):\n     ):\n         super().__init__(size, page_size, dtype, device, kvcache)\n         self.ret_values = torch.empty((), dtype=torch.int32, device=self.device)\n+        # Cache position tensor for ascend kernels to avoid repeated allocation\n+        self._pos_in_page_cache = torch.arange(page_size, device=device, dtype=torch.int32)\n \n     def alloc_extend(\n         self,\n@@ -638,6 +648,7 @@ class AscendPagedTokenToKVPoolAllocator(PagedTokenToKVPoolAllocator):\n             (extend_num_tokens,), dtype=torch.int32, device=self.device\n         )\n \n+        # Pass cached position tensor to avoid repeated allocation\n         self.ret_values = alloc_extend_kernel_ascend(\n             prefix_lens,\n             seq_lens,\n@@ -646,6 +657,7 @@ class AscendPagedTokenToKVPoolAllocator(PagedTokenToKVPoolAllocator):\n             out_indices,\n             self.page_size,\n             self.device,\n+            self._pos_in_page_cache,\n         )\n \n         if self.debug_mode:\n", "model_name_or_path": "gpt-5-2025-08-07"}
