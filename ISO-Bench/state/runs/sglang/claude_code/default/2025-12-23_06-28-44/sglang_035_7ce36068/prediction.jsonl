{"instance_id": "/home/ubuntu/OmniPerf-Bench/sglan__repo-efb099c", "model_patch": "diff --git a/python/sglang/srt/managers/tp_worker_overlap_thread.py b/python/sglang/srt/managers/tp_worker_overlap_thread.py\nindex 5d78b97..cfa99bc 100644\n--- a/python/sglang/srt/managers/tp_worker_overlap_thread.py\n+++ b/python/sglang/srt/managers/tp_worker_overlap_thread.py\n@@ -55,6 +55,12 @@ class TpModelWorkerClient:\n             (self.max_running_requests * 5,), dtype=torch.int32, device=self.device\n         )\n \n+        # Pre-allocate tensors for token ID operations to reduce allocations\n+        self.max_batch_size = self.max_running_requests\n+        self._arange_cache = torch.empty(\n+            (self.max_batch_size,), dtype=torch.int32, device=self.device\n+        )\n+\n         # Launch a thread\n         self.input_queue = Queue()\n         self.output_queue = Queue()\n@@ -88,13 +94,13 @@ class TpModelWorkerClient:\n         while True:\n             model_worker_batch, future_token_ids_ct = self.input_queue.get()\n \n-            # Resolve future tokens in the input\n+            # Resolve future tokens in the input - optimized in-place operation\n             input_ids = model_worker_batch.input_ids\n-            input_ids[:] = torch.where(\n-                input_ids < 0,\n-                self.future_token_ids_map[torch.clamp(-input_ids, min=0)],\n-                input_ids,\n-            )\n+            # Use masked indexing to avoid torch.where allocation\n+            negative_mask = input_ids < 0\n+            if negative_mask.any():\n+                negative_indices = torch.clamp(-input_ids[negative_mask], min=0)\n+                input_ids[negative_mask] = self.future_token_ids_map[negative_indices]\n \n             # Run forward\n             logits_output, next_token_ids = self.worker.forward_batch_generation(\n@@ -103,15 +109,19 @@ class TpModelWorkerClient:\n \n             # Update the future token ids map\n             bs = len(model_worker_batch.seq_lens)\n-            future_next_token_ids = torch.arange(\n-                -(future_token_ids_ct + bs),\n-                -(future_token_ids_ct),\n-                dtype=torch.int32,\n-                device=self.device,\n-            )\n-            self.future_token_ids_map[-future_next_token_ids] = next_token_ids.to(\n-                torch.int32\n-            )\n+            # Reuse pre-allocated cache instead of torch.arange\n+            future_next_token_ids = self._arange_cache[:bs]\n+            start_idx = -(future_token_ids_ct + bs)\n+            end_idx = -future_token_ids_ct\n+            torch.arange(start_idx, end_idx, out=future_next_token_ids, dtype=torch.int32)\n+\n+            # Avoid unnecessary dtype conversion if already int32\n+            if next_token_ids.dtype == torch.int32:\n+                self.future_token_ids_map[-future_next_token_ids] = next_token_ids\n+            else:\n+                self.future_token_ids_map[-future_next_token_ids] = next_token_ids.to(\n+                    torch.int32\n+                )\n \n             # Set the result\n             next_token_ids = next_token_ids.tolist()\n@@ -126,14 +136,13 @@ class TpModelWorkerClient:\n         # Push a new batch to the queue\n         self.input_queue.put((model_worker_batch.copy(), self.future_token_ids_ct))\n \n-        # Allocate output future objects\n+        # Allocate output future objects - reuse cache\n         bs = len(model_worker_batch.seq_lens)\n-        future_next_token_ids = torch.arange(\n-            -(self.future_token_ids_ct + bs),\n-            -(self.future_token_ids_ct),\n-            dtype=torch.int32,\n-            device=self.device,\n-        )\n+        future_next_token_ids = self._arange_cache[:bs].clone()\n+        start_idx = -(self.future_token_ids_ct + bs)\n+        end_idx = -self.future_token_ids_ct\n+        torch.arange(start_idx, end_idx, out=future_next_token_ids, dtype=torch.int32)\n+\n         self.future_token_ids_ct = (\n             self.future_token_ids_ct + bs\n         ) % self.future_token_ids_limit\n", "model_name_or_path": "gpt-5-2025-08-07"}
