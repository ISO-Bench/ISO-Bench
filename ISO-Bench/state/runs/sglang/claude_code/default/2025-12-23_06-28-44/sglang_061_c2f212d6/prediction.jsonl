{"instance_id": "/home/ubuntu/OmniPerf-Bench/sglan__repo-e2cdc8a", "model_patch": "diff --git a/benchmark/kernels/minmax-text-01-lighting_attention/benchmark_lighting_attention_decode.py b/benchmark/kernels/minmax-text-01-lighting_attention/benchmark_lighting_attention_decode.py\nindex 1a2036d..2bf0198 100644\n--- a/benchmark/kernels/minmax-text-01-lighting_attention/benchmark_lighting_attention_decode.py\n+++ b/benchmark/kernels/minmax-text-01-lighting_attention/benchmark_lighting_attention_decode.py\n@@ -10,6 +10,9 @@ import triton\n import triton.language as tl\n from einops import rearrange\n \n+# Global variable for eval mode\n+do_eval = False\n+\n \n @triton.jit\n def _decode_kernel(\n@@ -66,21 +69,33 @@ def lightning_attn_decode(q, k, v, kv, s):\n     d_padded = next_power_of_2(d)\n     e_padded = next_power_of_2(e)\n \n-    # Pad inputs\n-    q_padded = F.pad(q, (0, d_padded - d))\n-    k_padded = F.pad(k, (0, d_padded - d))\n-    v_padded = F.pad(v, (0, e_padded - e))\n-    kv_padded = F.pad(kv, (0, e_padded - e, 0, d_padded - d))\n+    # Fast path: if no padding needed, use inputs directly\n+    if d == d_padded and e == e_padded:\n+        q_padded = q.contiguous()\n+        k_padded = k.contiguous()\n+        v_padded = v.contiguous()\n+        kv_padded = kv.contiguous().to(torch.float32)\n+        s = s.contiguous()\n+        o_padded = torch.empty(b, h, n, e_padded, dtype=v.dtype, device=v.device)\n+    else:\n+        # Pre-allocate padded tensors and copy data (avoid F.pad overhead)\n+        # torch.empty creates contiguous tensors by default, no need for extra .contiguous()\n+        q_padded = torch.empty(b, h, n, d_padded, dtype=q.dtype, device=q.device)\n+        q_padded[..., :d] = q\n+\n+        k_padded = torch.empty(b, h, n, d_padded, dtype=k.dtype, device=k.device)\n+        k_padded[..., :d] = k\n+\n+        v_padded = torch.empty(b, h, n, e_padded, dtype=v.dtype, device=v.device)\n+        v_padded[..., :e] = v\n+\n+        kv_padded = torch.empty(b, h, d_padded, e_padded, dtype=torch.float32, device=kv.device)\n+        kv_padded[..., :d, :e] = kv.to(torch.float32)\n \n-    # Ensure inputs are contiguous\n-    q_padded = q_padded.contiguous()\n-    k_padded = k_padded.contiguous()\n-    v_padded = v_padded.contiguous()\n-    kv_padded = kv_padded.contiguous().to(torch.float32)\n-    s = s.contiguous()\n+        s = s.contiguous()\n \n-    # Create output tensor (padded)\n-    o_padded = torch.empty(b, h, n, e_padded, dtype=v.dtype, device=v.device)\n+        # Create output tensor (padded)\n+        o_padded = torch.empty(b, h, n, e_padded, dtype=v.dtype, device=v.device)\n \n     # Launch kernel\n     grid = (b * h, 1)\n@@ -182,37 +197,20 @@ class MiniMaxText01LightningAttention(nn.Module):\n \n         # decode mode\n         kv = past_key_value  # [b, h, d, e]\n-        output = []\n+        # Pre-allocate output tensor to avoid list appends and concat\n+        output = torch.empty(b, self.num_heads, n, self.head_dim, dtype=q.dtype, device=q.device)\n+\n         for i in range(n):\n-            # kv: [b, h, d, e]\n-            # ratio: [h, 1, 1]\n-            # k: [b, h, n, d]\n-            # v: [b, h, n, e]\n-            # k[:, :, i : i + 1]: [b, h, 1, d]\n-            # v[:, :, i : i + 1]: [b, h, 1, e]\n-            # ratio * kv: [b, h, d, e]\n-            # torch.einsum(\n-            #     \"... n d, ... n e -> ... d e\",\n-            #     k[:, :, i : i + 1],\n-            #     v[:, :, i : i + 1],\n-            # )\n-            # [b, h, d, e] + [b, h, d, e] -> [b, h, d, e]\n-            kv = ratio * kv + torch.einsum(\n-                \"... n d, ... n e -> ... d e\",\n-                k[:, :, i : i + 1],\n-                v[:, :, i : i + 1],\n-            )\n-            # q[:, :, i : i + 1]: [b, h, 1, d]\n-            # kv.to(q.dtype): [b, h, d, e]\n-            # torch.einsum(\n-            #     \"... n e, ... e d -> ... n d\", q[:, :, i : i + 1], kv.to(q.dtype)\n-            # )\n-            # [b, h, 1, d] * [b, h, d, e] -> [b, h, 1, e]\n-            qkv = torch.einsum(\n-                \"... n e, ... e d -> ... n d\", q[:, :, i : i + 1], kv.to(q.dtype)\n-            )\n-            output.append(qkv)\n-        output = torch.concat(output, dim=-2)\n+            # Use torch.bmm for better performance than einsum\n+            # kv update: ratio * kv + k[i]^T @ v[i]\n+            k_i = k[:, :, i : i + 1]  # [b, h, 1, d]\n+            v_i = v[:, :, i : i + 1]  # [b, h, 1, e]\n+\n+            # Optimize: use matmul instead of einsum for better performance\n+            kv = ratio * kv + k_i.transpose(-2, -1) @ v_i\n+\n+            # q[i] @ kv: [b, h, 1, d] @ [b, h, d, e] -> [b, h, 1, e]\n+            output[:, :, i : i + 1] = q[:, :, i : i + 1] @ kv.to(q.dtype)\n \n         # reshape\n         output = rearrange(output, \"b h n d -> b n (h d)\")\n", "model_name_or_path": "gpt-5-2025-08-07"}
