{"instance_id": "/home/ubuntu/OmniPerf-Bench/sglan__repo-ae6a5b2", "model_patch": "diff --git a/python/sglang/srt/models/deepseek_v2.py b/python/sglang/srt/models/deepseek_v2.py\nindex 29f18f0ef..3e586e677 100644\n--- a/python/sglang/srt/models/deepseek_v2.py\n+++ b/python/sglang/srt/models/deepseek_v2.py\n@@ -528,9 +528,8 @@ class DeepseekV2MoE(nn.Module):\n         final_hidden_states = state.pop(\"hidden_states_after_combine\")\n \n         if (shared_output := state.pop(\"shared_output\")) is not None:\n-            x = shared_output\n-            x.add_(final_hidden_states, alpha=self.routed_scaling_factor)\n-            final_hidden_states = x\n+            shared_output.add_(final_hidden_states, alpha=self.routed_scaling_factor)\n+            final_hidden_states = shared_output\n         else:\n             final_hidden_states *= self.routed_scaling_factor\n \n@@ -984,8 +983,28 @@ class DeepseekV2AttentionMLA(nn.Module):\n                 q_nope_out, k_nope, k_nope, forward_batch, q_rope=q_pe, k_rope=k_pe\n             )\n         else:\n-            q = torch.cat([q_nope_out, q_pe], dim=-1)\n-            k = torch.cat([k_nope, k_pe], dim=-1)\n+            # Preallocate and fill instead of concatenation for better performance\n+            q_nope_dim = q_nope_out.shape[-1]\n+            q_pe_dim = q_pe.shape[-1]\n+            k_nope_dim = k_nope.shape[-1]\n+            k_pe_dim = k_pe.shape[-1]\n+\n+            q = torch.empty(\n+                (*q_nope_out.shape[:-1], q_nope_dim + q_pe_dim),\n+                dtype=q_nope_out.dtype,\n+                device=q_nope_out.device\n+            )\n+            q[..., :q_nope_dim] = q_nope_out\n+            q[..., q_nope_dim:] = q_pe\n+\n+            k = torch.empty(\n+                (*k_nope.shape[:-1], k_nope_dim + k_pe_dim),\n+                dtype=k_nope.dtype,\n+                device=k_nope.device\n+            )\n+            k[..., :k_nope_dim] = k_nope\n+            k[..., k_nope_dim:] = k_pe\n+\n             attn_output = self.attn_mqa(q, k, k_nope, forward_batch)\n         attn_output = attn_output.view(-1, self.num_local_heads, self.kv_lora_rank)\n \n@@ -1829,7 +1848,7 @@ class DeepseekV2ForCausalLM(nn.Module):\n                 0, (-1, self_attn.qk_nope_head_dim + self_attn.v_head_dim)\n             ).split([self_attn.qk_nope_head_dim, self_attn.v_head_dim], dim=1)\n             if not use_deep_gemm_bmm:\n-                self_attn.w_kc = w_kc.transpose(1, 2).contiguous().transpose(1, 2)\n+                self_attn.w_kc = w_kc.contiguous()\n                 self_attn.w_vc = w_vc.contiguous().transpose(1, 2)\n                 if (\n                     hasattr(self_attn.kv_b_proj, \"weight_scale\")\ndiff --git a/python/sglang/srt/two_batch_overlap.py b/python/sglang/srt/two_batch_overlap.py\nindex 6b0241f40..30d2465e5 100644\n--- a/python/sglang/srt/two_batch_overlap.py\n+++ b/python/sglang/srt/two_batch_overlap.py\n@@ -324,7 +324,7 @@ class TboForwardBatchPreparer:\n         # TODO improve, e.g. unify w/ `init_raw`\n         if global_server_args_dict[\"moe_dense_tp_size\"] == 1:\n             sum_len = end_token_index - start_token_index\n-            gathered_buffer = torch.zeros(\n+            gathered_buffer = torch.empty(\n                 (sum_len, batch.gathered_buffer.shape[1]),\n                 dtype=batch.gathered_buffer.dtype,\n                 device=batch.gathered_buffer.device,\n", "model_name_or_path": "gpt-5-2025-08-07"}
