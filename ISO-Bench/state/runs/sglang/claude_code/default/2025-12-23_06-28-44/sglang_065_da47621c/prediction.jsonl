{"instance_id": "/home/ubuntu/OmniPerf-Bench/sglan__repo-22a6b9f", "model_patch": "diff --git a/python/sglang/srt/layers/moe/topk.py b/python/sglang/srt/layers/moe/topk.py\nindex f5dceac..6823a0d 100644\n--- a/python/sglang/srt/layers/moe/topk.py\n+++ b/python/sglang/srt/layers/moe/topk.py\n@@ -49,11 +49,6 @@ def fused_topk_native(\n     assert (\n         hidden_states.shape[0] == gating_output.shape[0]\n     ), f\"Number of tokens mismatch, {hidden_states.shape=} vs {gating_output.shape=}\"\n-    M, _ = hidden_states.shape\n-    topk_weights = torch.empty(\n-        M, topk, dtype=torch.float32, device=hidden_states.device\n-    )\n-    topk_ids = torch.empty(M, topk, dtype=torch.int32, device=hidden_states.device)\n     topk_weights = F.softmax(gating_output.float(), dim=-1)\n     topk_weights, topk_ids = torch.topk(topk_weights, topk, dim=-1)\n     if renormalize:\n@@ -145,7 +140,7 @@ def grouped_topk(\n         .expand(num_token, num_expert_group, scores.shape[-1] // num_expert_group)\n         .reshape(num_token, -1)\n     )  # [n, e]\n-    tmp_scores = scores.masked_fill(~score_mask.bool(), 0.0)  # [n, e]\n+    tmp_scores = scores.masked_fill(score_mask == 0, 0.0)  # [n, e]\n     topk_weights, topk_ids = torch.topk(tmp_scores, k=topk, dim=-1, sorted=False)\n     if num_fused_shared_experts:\n         topk_ids[:, -1] = torch.randint(\n@@ -206,7 +201,7 @@ def biased_grouped_topk_impl(\n         .reshape(num_token, -1)\n     )  # [n, e]\n     tmp_scores = scores_for_choice.masked_fill(\n-        ~score_mask.bool(), float(\"-inf\")\n+        score_mask == 0, float(\"-inf\")\n     )  # [n, e]\n     _, topk_ids = torch.topk(tmp_scores, k=topk, dim=-1, sorted=False)\n     topk_weights = scores.gather(1, topk_ids)\n@@ -245,8 +240,16 @@ def _mask_topk_ids_padded_region(\n ):\n     if num_token_non_padded is None:\n         return\n-    indices = torch.arange(0, topk_ids.shape[0], device=topk_ids.device)\n-    topk_ids[indices >= num_token_non_padded, :] = -1\n+    # Optimize by avoiding arange allocation and using direct slicing\n+    if num_token_non_padded.numel() == 1:\n+        # Scalar tensor case - use direct slicing\n+        pad_start = num_token_non_padded.item()\n+        if pad_start < topk_ids.shape[0]:\n+            topk_ids[pad_start:, :] = -1\n+    else:\n+        # Fallback to original implementation for non-scalar cases\n+        indices = torch.arange(0, topk_ids.shape[0], device=topk_ids.device)\n+        topk_ids[indices >= num_token_non_padded, :] = -1\n \n \n def biased_grouped_topk(\n", "model_name_or_path": "gpt-5-2025-08-07"}
