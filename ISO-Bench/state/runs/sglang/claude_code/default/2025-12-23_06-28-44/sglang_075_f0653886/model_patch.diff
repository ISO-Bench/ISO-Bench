diff --git a/python/sglang/srt/layers/moe/ep_moe/token_dispatcher.py b/python/sglang/srt/layers/moe/ep_moe/token_dispatcher.py
index 4d165db..e006254 100644
--- a/python/sglang/srt/layers/moe/ep_moe/token_dispatcher.py
+++ b/python/sglang/srt/layers/moe/ep_moe/token_dispatcher.py
@@ -400,7 +400,7 @@ class _DeepEPDispatcherImplNormal(_DeepEPDispatcherImplBase):
                     BLOCK_SIZE=512,
                 )
             else:
-                output = torch.zeros(
+                output = torch.empty(
                     (0, hidden_states.shape[1]),
                     device=hidden_states.device,
                     dtype=hidden_states.dtype,
diff --git a/python/sglang/srt/layers/moe/topk.py b/python/sglang/srt/layers/moe/topk.py
index 4c065e4..64f412e 100644
--- a/python/sglang/srt/layers/moe/topk.py
+++ b/python/sglang/srt/layers/moe/topk.py
@@ -44,10 +44,6 @@ def fused_topk_native(
         hidden_states.shape[0] == gating_output.shape[0]
     ), f"Number of tokens mismatch, {hidden_states.shape=} vs {gating_output.shape=}"
     M, _ = hidden_states.shape
-    topk_weights = torch.empty(
-        M, topk, dtype=torch.float32, device=hidden_states.device
-    )
-    topk_ids = torch.empty(M, topk, dtype=torch.int32, device=hidden_states.device)
     topk_weights = F.softmax(gating_output.float(), dim=-1)
     topk_weights, topk_ids = torch.topk(topk_weights, topk, dim=-1)
     if renormalize:
@@ -69,17 +65,13 @@ def fused_topk(
         M, topk, dtype=torch.float32, device=hidden_states.device
     )
     topk_ids = torch.empty(M, topk, dtype=torch.int32, device=hidden_states.device)
-    token_expert_indicies = torch.empty(
-        M, topk, dtype=torch.int32, device=hidden_states.device
-    )
 
     topk_softmax(
         topk_weights,
         topk_ids,
-        token_expert_indicies,
+        topk_ids,  # Reuse topk_ids buffer to avoid extra allocation
         gating_output.float(),
     )
-    del token_expert_indicies
 
     if renormalize:
         topk_weights = topk_weights / topk_weights.sum(dim=-1, keepdim=True)
