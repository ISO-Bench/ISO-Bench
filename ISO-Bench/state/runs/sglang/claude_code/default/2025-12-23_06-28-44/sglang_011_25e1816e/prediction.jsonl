{"instance_id": "/home/ubuntu/OmniPerf-Bench/sglan__repo-a53fe42", "model_patch": "diff --git a/sgl-kernel/csrc/allreduce/trt_reduce_internal.cu b/sgl-kernel/csrc/allreduce/trt_reduce_internal.cu\nindex f1ee5d40e..a61d8937d 100644\n--- a/sgl-kernel/csrc/allreduce/trt_reduce_internal.cu\n+++ b/sgl-kernel/csrc/allreduce/trt_reduce_internal.cu\n@@ -109,13 +109,13 @@ struct PackedOn16Bytes<__nv_bfloat16> {\n \n // add two 128b data\n template <typename T>\n-inline __device__ int4 add128b(T& a, T& b) {\n-  T c;\n-  c.unpacked[0] = a.unpacked[0] + b.unpacked[0];\n-  c.unpacked[1] = a.unpacked[1] + b.unpacked[1];\n-  c.unpacked[2] = a.unpacked[2] + b.unpacked[2];\n-  c.unpacked[3] = a.unpacked[3] + b.unpacked[3];\n-  return c.packed;\n+__forceinline__ __device__ int4 add128b(T& a, T& b) {\n+  // Optimize: reuse a's storage instead of allocating new struct\n+  a.unpacked[0] = a.unpacked[0] + b.unpacked[0];\n+  a.unpacked[1] = a.unpacked[1] + b.unpacked[1];\n+  a.unpacked[2] = a.unpacked[2] + b.unpacked[2];\n+  a.unpacked[3] = a.unpacked[3] + b.unpacked[3];\n+  return a.packed;\n }\n \n __inline__ __device__ void multi_gpu_barrier(\n@@ -140,7 +140,9 @@ __inline__ __device__ void multi_gpu_barrier(\n     // All blocks check that corresponding block 0 on other GPUs have set the flag\n     // No deadlock because block #0 is always the first block started\n     uint32_t* peer_barrier_d = signals[local_rank] + offset + tidx;\n+    // Optimize: add brief pause in spin loop to reduce memory traffic\n     while (ld_flag_acquire(peer_barrier_d) != flag) {\n+      __nanosleep(20);\n     }\n   }\n \n@@ -172,13 +174,16 @@ __inline__ __device__ void block_barrier(\n \n     uint32_t* peer_barrier_d = signals[local_rank] + flag_block_offset + tidx;\n     // Blocks check that corresponding blocks on other GPUs have also set the flag\n+    // Optimize: add brief pause in spin loops to reduce memory traffic\n     if constexpr (need_fence) {\n       st_flag_release(flag, signals[tidx] + flag_block_offset + local_rank);\n       while (ld_flag_acquire(peer_barrier_d) != flag) {\n+        __nanosleep(20);\n       }\n     } else {\n       st_flag_volatile(flag, signals[tidx] + flag_block_offset + local_rank);\n       while (ld_flag_volatile(peer_barrier_d) != flag) {\n+        __nanosleep(20);\n       }\n     }\n   }\n@@ -230,8 +235,9 @@ static __global__ void __launch_bounds__(512, 1) oneShotAllReduceKernel(AllReduc\n   size_t chunk_end = std::min((bidx + 1) * params.elts_per_block, params.elts_per_rank);\n \n   if constexpr (COPY_INPUT) {\n-    T const* local_input_buffer = reinterpret_cast<T const*>(params.local_input_buffer_ptr);\n+    T const* __restrict__ local_input_buffer = reinterpret_cast<T const*>(params.local_input_buffer_ptr);\n     // Copy from local buffer to shareable buffer\n+    // Optimize: use restrict keyword for better compiler optimization\n     for (size_t iter_offset = chunk_start; iter_offset < chunk_end; iter_offset += blockDim.x * NUM_ELTS) {\n       *reinterpret_cast<int4*>(&local_shared_buffer[iter_offset]) =\n           *reinterpret_cast<int4 const*>(&local_input_buffer[iter_offset]);\n@@ -245,16 +251,19 @@ static __global__ void __launch_bounds__(512, 1) oneShotAllReduceKernel(AllReduc\n   for (size_t iter_offset = chunk_start; iter_offset < chunk_end; iter_offset += blockDim.x * NUM_ELTS) {\n     // Iterate over the different ranks/devices on the node to load the values.\n     PackedStruct vals[RANKS_PER_NODE];\n+    // Optimize: use efficient memory loads\n #pragma unroll\n     for (int ii = 0; ii < RANKS_PER_NODE; ++ii) {\n-      vals[ii].packed = *reinterpret_cast<int4 const*>(&((T*)peer_comm_buffer_ptrs[ii])[iter_offset]);\n+      T const* src_ptr = &((T const*)peer_comm_buffer_ptrs[ii])[iter_offset];\n+      vals[ii].packed = *reinterpret_cast<int4 const*>(src_ptr);\n     }\n \n     // Sum the values from the different ranks.\n     PackedStruct sums;\n-    sums.packed = {0, 0, 0, 0};\n+    // Optimize: directly use first value instead of initializing to zero then adding\n+    sums.packed = vals[0].packed;\n #pragma unroll\n-    for (int rank = 0; rank < RANKS_PER_NODE; ++rank) {\n+    for (int rank = 1; rank < RANKS_PER_NODE; ++rank) {\n       // Always reduce from rank 0 to ensure stable reduce order.\n       sums.packed = add128b(sums, vals[rank]);\n     }\n@@ -312,10 +321,11 @@ static __global__ void __launch_bounds__(512, 1) twoShotAllReduceKernel(AllReduc\n   static constexpr int PACKED_ELTS = 16 / sizeof(T);\n   using PackedType = typename PackedOn16Bytes<T>::Type;\n \n-  T const* local_input_buffer = reinterpret_cast<T const*>(params.local_input_buffer_ptr);\n+  // Optimize: use restrict keyword for better compiler optimization\n+  T const* __restrict__ local_input_buffer = reinterpret_cast<T const*>(params.local_input_buffer_ptr);\n   auto peer_comm_buffer_ptrs = params.peer_comm_buffer_ptrs->ptrs;\n-  T* local_shared_buffer = reinterpret_cast<T*>(peer_comm_buffer_ptrs[params.local_rank]);\n-  T* local_output_buffer = reinterpret_cast<T*>(params.local_output_buffer_ptr);\n+  T* __restrict__ local_shared_buffer = reinterpret_cast<T*>(peer_comm_buffer_ptrs[params.local_rank]);\n+  T* __restrict__ local_output_buffer = reinterpret_cast<T*>(params.local_output_buffer_ptr);\n \n   size_t const chunk_start = bidx * params.elts_per_block + tidx * PACKED_ELTS;\n   size_t const chunk_end = min(chunk_start + params.elts_per_block, params.elts_per_rank);\n@@ -361,16 +371,19 @@ static __global__ void __launch_bounds__(512, 1) twoShotAllReduceKernel(AllReduc\n \n     // Iterate over the different ranks/devices on the node to load the values.\n     PackedType vals[RANKS_PER_NODE];\n+    // Optimize: use efficient memory loads\n #pragma unroll\n     for (int ii = 0; ii < RANKS_PER_NODE; ++ii) {\n-      vals[ii].packed = *reinterpret_cast<int4 const*>(&buffers_unorder[ii][responsible_block_offset]);\n+      T const* src_ptr = &buffers_unorder[ii][responsible_block_offset];\n+      vals[ii].packed = *reinterpret_cast<int4 const*>(src_ptr);\n     }\n \n     // Sum the values from the different ranks.\n     PackedType sums;\n-    sums.packed = {0, 0, 0, 0};\n+    // Optimize: directly use first value instead of initializing to zero then adding\n+    sums.packed = vals[0].packed;\n #pragma unroll\n-    for (int rank = 0; rank < RANKS_PER_NODE; ++rank) {\n+    for (int rank = 1; rank < RANKS_PER_NODE; ++rank) {\n       // Always reduce from rank 0 to ensure stable reduce order.\n       sums.packed = add128b(sums, vals[rank]);\n     }\n@@ -413,11 +426,11 @@ static __global__ void __launch_bounds__(512, 1) twoShotAllReduceKernel(AllReduc\n \n ////////////////////////////////////////////////////////////////////////////////////////////////////\n \n-inline int divUp(int a, int b) {\n+__forceinline__ int divUp(int a, int b) {\n   return (a + b - 1) / b;\n }\n \n-inline int roundUp(int a, int n) {\n+__forceinline__ int roundUp(int a, int n) {\n   return divUp(a, n) * n;\n }\n \n", "model_name_or_path": "gpt-5-2025-08-07"}
