{"instance_id": "/home/ubuntu/OmniPerf-Bench/sglan__repo-6252ade", "model_patch": "diff --git a/python/sglang/srt/layers/attention/triton_ops/extend_attention.py b/python/sglang/srt/layers/attention/triton_ops/extend_attention.py\nindex 608f9ba..7da696f 100644\n--- a/python/sglang/srt/layers/attention/triton_ops/extend_attention.py\n+++ b/python/sglang/srt/layers/attention/triton_ops/extend_attention.py\n@@ -123,7 +123,7 @@ def _fwd_kernel(\n \n     acc = tl.zeros([BLOCK_M, BLOCK_DV], dtype=tl.float32)\n     deno = tl.zeros([BLOCK_M], dtype=tl.float32)\n-    e_max = tl.zeros([BLOCK_M], dtype=tl.float32) - float(\"inf\")\n+    e_max = tl.full([BLOCK_M], float(\"-inf\"), dtype=tl.float32)\n \n     for start_n in range(0, cur_seq_len_prefix, BLOCK_N):\n         start_n = tl.multiple_of(start_n, BLOCK_N)\n@@ -142,7 +142,7 @@ def _fwd_kernel(\n             K_Buffer + offs_buf_k, mask=(mask_n[None, :]) & (mask_d[:, None]), other=0.0\n         )\n \n-        qk = tl.dot(q.to(k.dtype), k)\n+        qk = tl.dot(q, k, out_dtype=tl.float32)\n         if BLOCK_DPE > 0:\n             offs_kpe = (\n                 offs_kv_loc[None, :] * stride_buf_kbs\n@@ -154,7 +154,7 @@ def _fwd_kernel(\n                 mask=mask_n[None, :],\n                 other=0.0,\n             )\n-            qk += tl.dot(qpe.to(kpe.dtype), kpe)\n+            qk += tl.dot(qpe, kpe, out_dtype=tl.float32)\n         qk *= sm_scale\n \n         if logit_cap > 0:\n@@ -188,8 +188,7 @@ def _fwd_kernel(\n         v = tl.load(\n             V_Buffer + offs_buf_v, mask=mask_n[:, None] & mask_dv[None, :], other=0.0\n         )\n-        p = p.to(v.dtype)\n-        acc = acc * re_scale[:, None] + tl.dot(p, v)\n+        acc = acc * re_scale[:, None] + tl.dot(p, v, out_dtype=tl.float32)\n \n         e_max = n_e_max\n \n@@ -222,7 +221,7 @@ def _fwd_kernel(\n                 mask=mask_n[None, :],\n                 other=0.0,\n             )\n-            qk += tl.dot(qpe, kpe)\n+            qk += tl.dot(qpe, kpe, out_dtype=tl.float32)\n \n         qk *= sm_scale\n \n@@ -262,8 +261,7 @@ def _fwd_kernel(\n         v = tl.load(\n             V_Extend + offs_v, mask=mask_n[:, None] & mask_dv[None, :], other=0.0\n         )\n-        p = p.to(v.dtype)\n-        acc = acc * re_scale[:, None] + tl.dot(p, v)\n+        acc = acc * re_scale[:, None] + tl.dot(p, v, out_dtype=tl.float32)\n \n         e_max = n_e_max\n \n", "model_name_or_path": "gpt-5-2025-08-07"}
