diff --git a/python/sglang/srt/managers/schedule_batch.py b/python/sglang/srt/managers/schedule_batch.py
index fcd06d8..6f31b3d 100644
--- a/python/sglang/srt/managers/schedule_batch.py
+++ b/python/sglang/srt/managers/schedule_batch.py
@@ -548,8 +548,8 @@ class ScheduleBatch:
                     or len(req.prefix_indices) >= im.num_image_tokens
                 )
 
-        self.encoder_lens = torch.tensor(self.encoder_lens_cpu, dtype=torch.int32).to(
-            self.device, non_blocking=True
+        self.encoder_lens = torch.tensor(
+            self.encoder_lens_cpu, dtype=torch.int32, device=self.device
         )
 
         # Strip encoder infos
@@ -579,23 +579,19 @@ class ScheduleBatch:
             pt += req.extend_input_len
 
         # Reassign
-        self.input_ids = torch.tensor(sum(input_ids, []), dtype=torch.int32).to(
-            self.device, non_blocking=True
-        )
-        self.seq_lens = torch.tensor(seq_lens, dtype=torch.int32).to(
-            self.device, non_blocking=True
+        self.input_ids = torch.tensor(
+            sum(input_ids, []), dtype=torch.int32, device=self.device
         )
+        self.seq_lens = torch.tensor(seq_lens, dtype=torch.int32, device=self.device)
 
         if not decoder_out_cache_loc:
-            self.out_cache_loc = torch.empty(0, dtype=torch.int32).to(
-                self.device, non_blocking=True
-            )
+            self.out_cache_loc = torch.empty(0, dtype=torch.int32, device=self.device)
         else:
             self.out_cache_loc = torch.cat(decoder_out_cache_loc)
 
         if not encoder_out_cache_loc:
-            self.encoder_out_cache_loc = torch.empty(0, dtype=torch.int32).to(
-                self.device, non_blocking=True
+            self.encoder_out_cache_loc = torch.empty(
+                0, dtype=torch.int32, device=self.device
             )
         else:
             self.encoder_out_cache_loc = torch.cat(encoder_out_cache_loc)
@@ -650,15 +646,13 @@ class ScheduleBatch:
             pt += req.extend_input_len
 
         # Set fields
-        self.input_ids = torch.tensor(sum(input_ids, []), dtype=torch.int32).to(
-            self.device, non_blocking=True
-        )
-        self.req_pool_indices = torch.tensor(req_pool_indices, dtype=torch.int32).to(
-            self.device, non_blocking=True
+        self.input_ids = torch.tensor(
+            sum(input_ids, []), dtype=torch.int32, device=self.device
         )
-        self.seq_lens = torch.tensor(seq_lens, dtype=torch.int32).to(
-            self.device, non_blocking=True
+        self.req_pool_indices = torch.tensor(
+            req_pool_indices, dtype=torch.int32, device=self.device
         )
+        self.seq_lens = torch.tensor(seq_lens, dtype=torch.int32, device=self.device)
 
         self.out_cache_loc = out_cache_loc
 
@@ -931,8 +925,8 @@ class ScheduleBatch:
             self.encoder_lens_cpu = [self.encoder_lens_cpu[i] for i in keep_indices]
 
         self.reqs = [self.reqs[i] for i in keep_indices]
-        new_indices = torch.tensor(keep_indices, dtype=torch.int32).to(
-            self.device, non_blocking=True
+        new_indices = torch.tensor(
+            keep_indices, dtype=torch.int32, device=self.device
         )
         self.req_pool_indices = self.req_pool_indices[new_indices]
         self.seq_lens = self.seq_lens[new_indices]
diff --git a/python/sglang/srt/managers/scheduler.py b/python/sglang/srt/managers/scheduler.py
index 55b05f8..d4ac818 100644
--- a/python/sglang/srt/managers/scheduler.py
+++ b/python/sglang/srt/managers/scheduler.py
@@ -736,10 +736,14 @@ class Scheduler:
                 logits_output = None
                 if self.tokenizer is not None:
                     next_token_ids = torch.full(
-                        (batch.batch_size(),), self.tokenizer.eos_token_id
+                        (batch.batch_size(),),
+                        self.tokenizer.eos_token_id,
+                        device=self.device,
                     )
                 else:
-                    next_token_ids = torch.full((batch.batch_size(),), 0)
+                    next_token_ids = torch.full(
+                        (batch.batch_size(),), 0, device=self.device
+                    )
             batch.output_ids = next_token_ids
             ret = logits_output, next_token_ids, model_worker_batch.bid
         else:  # embedding or reward model
diff --git a/python/sglang/srt/sampling/sampling_batch_info.py b/python/sglang/srt/sampling/sampling_batch_info.py
index 27a2d07..87bd82a 100644
--- a/python/sglang/srt/sampling/sampling_batch_info.py
+++ b/python/sglang/srt/sampling/sampling_batch_info.py
@@ -52,23 +52,20 @@ class SamplingBatchInfo:
     ):
         reqs = batch.reqs
         device = batch.device
-        temperatures = (
-            torch.tensor(
-                [r.sampling_params.temperature for r in reqs],
-                dtype=torch.float,
-            )
-            .view(-1, 1)
-            .to(device, non_blocking=True)
-        )
+        temperatures = torch.tensor(
+            [r.sampling_params.temperature for r in reqs],
+            dtype=torch.float,
+            device=device,
+        ).view(-1, 1)
         top_ps = torch.tensor(
-            [r.sampling_params.top_p for r in reqs], dtype=torch.float
-        ).to(device, non_blocking=True)
+            [r.sampling_params.top_p for r in reqs], dtype=torch.float, device=device
+        )
         top_ks = torch.tensor(
-            [r.sampling_params.top_k for r in reqs], dtype=torch.int32
-        ).to(device, non_blocking=True)
+            [r.sampling_params.top_k for r in reqs], dtype=torch.int32, device=device
+        )
         min_ps = torch.tensor(
-            [r.sampling_params.min_p for r in reqs], dtype=torch.float
-        ).to(device, non_blocking=True)
+            [r.sampling_params.min_p for r in reqs], dtype=torch.float, device=device
+        )
 
         ret = cls(
             temperatures=temperatures,
@@ -141,7 +138,7 @@ class SamplingBatchInfo:
             self.vocab_mask = None
             return
 
-        self.vocab_mask = torch.zeros(
+        self.vocab_mask = torch.empty(
             len(self.temperatures),
             self.vocab_size,
             dtype=torch.bool,
@@ -149,10 +146,10 @@ class SamplingBatchInfo:
         )
         for i, regex_fsm in enumerate(self.regex_fsms):
             if regex_fsm is not None:
-                self.vocab_mask[i].fill_(1)
+                self.vocab_mask[i].fill_(True)
                 self.vocab_mask[i][
                     regex_fsm.get_next_instruction(self.regex_fsm_states[i]).tokens
-                ] = 0
+                ] = False
 
     def filter_batch(self, unfinished_indices: List[int], new_indices: torch.Tensor):
         if self.penalizer_orchestrator:
@@ -185,11 +182,16 @@ class SamplingBatchInfo:
                 shape, dtype = lhs.shape[1:], lhs.dtype
             else:
                 shape, dtype = rhs.shape[1:], rhs.dtype
-            with torch.dtype(dtype):
-                if lhs is None:
-                    lhs = torch.empty((bs1, *shape), device=device).fill_(default)
-                if rhs is None:
-                    rhs = torch.empty((bs2, *shape), device=device).fill_(default)
+            if lhs is None:
+                if default == 0:
+                    lhs = torch.zeros((bs1, *shape), dtype=dtype, device=device)
+                else:
+                    lhs = torch.full((bs1, *shape), default, dtype=dtype, device=device)
+            if rhs is None:
+                if default == 0:
+                    rhs = torch.zeros((bs2, *shape), dtype=dtype, device=device)
+                else:
+                    rhs = torch.full((bs2, *shape), default, dtype=dtype, device=device)
             return torch.cat([lhs, rhs])
 
         return None
