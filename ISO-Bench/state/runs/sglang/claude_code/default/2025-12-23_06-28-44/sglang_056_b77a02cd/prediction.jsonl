{"instance_id": "/home/ubuntu/OmniPerf-Bench/sglan__repo-30643fe", "model_patch": "diff --git a/python/sglang/srt/managers/schedule_batch.py b/python/sglang/srt/managers/schedule_batch.py\nindex fcd06d8..6f31b3d 100644\n--- a/python/sglang/srt/managers/schedule_batch.py\n+++ b/python/sglang/srt/managers/schedule_batch.py\n@@ -548,8 +548,8 @@ class ScheduleBatch:\n                     or len(req.prefix_indices) >= im.num_image_tokens\n                 )\n \n-        self.encoder_lens = torch.tensor(self.encoder_lens_cpu, dtype=torch.int32).to(\n-            self.device, non_blocking=True\n+        self.encoder_lens = torch.tensor(\n+            self.encoder_lens_cpu, dtype=torch.int32, device=self.device\n         )\n \n         # Strip encoder infos\n@@ -579,23 +579,19 @@ class ScheduleBatch:\n             pt += req.extend_input_len\n \n         # Reassign\n-        self.input_ids = torch.tensor(sum(input_ids, []), dtype=torch.int32).to(\n-            self.device, non_blocking=True\n-        )\n-        self.seq_lens = torch.tensor(seq_lens, dtype=torch.int32).to(\n-            self.device, non_blocking=True\n+        self.input_ids = torch.tensor(\n+            sum(input_ids, []), dtype=torch.int32, device=self.device\n         )\n+        self.seq_lens = torch.tensor(seq_lens, dtype=torch.int32, device=self.device)\n \n         if not decoder_out_cache_loc:\n-            self.out_cache_loc = torch.empty(0, dtype=torch.int32).to(\n-                self.device, non_blocking=True\n-            )\n+            self.out_cache_loc = torch.empty(0, dtype=torch.int32, device=self.device)\n         else:\n             self.out_cache_loc = torch.cat(decoder_out_cache_loc)\n \n         if not encoder_out_cache_loc:\n-            self.encoder_out_cache_loc = torch.empty(0, dtype=torch.int32).to(\n-                self.device, non_blocking=True\n+            self.encoder_out_cache_loc = torch.empty(\n+                0, dtype=torch.int32, device=self.device\n             )\n         else:\n             self.encoder_out_cache_loc = torch.cat(encoder_out_cache_loc)\n@@ -650,15 +646,13 @@ class ScheduleBatch:\n             pt += req.extend_input_len\n \n         # Set fields\n-        self.input_ids = torch.tensor(sum(input_ids, []), dtype=torch.int32).to(\n-            self.device, non_blocking=True\n-        )\n-        self.req_pool_indices = torch.tensor(req_pool_indices, dtype=torch.int32).to(\n-            self.device, non_blocking=True\n+        self.input_ids = torch.tensor(\n+            sum(input_ids, []), dtype=torch.int32, device=self.device\n         )\n-        self.seq_lens = torch.tensor(seq_lens, dtype=torch.int32).to(\n-            self.device, non_blocking=True\n+        self.req_pool_indices = torch.tensor(\n+            req_pool_indices, dtype=torch.int32, device=self.device\n         )\n+        self.seq_lens = torch.tensor(seq_lens, dtype=torch.int32, device=self.device)\n \n         self.out_cache_loc = out_cache_loc\n \n@@ -931,8 +925,8 @@ class ScheduleBatch:\n             self.encoder_lens_cpu = [self.encoder_lens_cpu[i] for i in keep_indices]\n \n         self.reqs = [self.reqs[i] for i in keep_indices]\n-        new_indices = torch.tensor(keep_indices, dtype=torch.int32).to(\n-            self.device, non_blocking=True\n+        new_indices = torch.tensor(\n+            keep_indices, dtype=torch.int32, device=self.device\n         )\n         self.req_pool_indices = self.req_pool_indices[new_indices]\n         self.seq_lens = self.seq_lens[new_indices]\ndiff --git a/python/sglang/srt/managers/scheduler.py b/python/sglang/srt/managers/scheduler.py\nindex 55b05f8..d4ac818 100644\n--- a/python/sglang/srt/managers/scheduler.py\n+++ b/python/sglang/srt/managers/scheduler.py\n@@ -736,10 +736,14 @@ class Scheduler:\n                 logits_output = None\n                 if self.tokenizer is not None:\n                     next_token_ids = torch.full(\n-                        (batch.batch_size(),), self.tokenizer.eos_token_id\n+                        (batch.batch_size(),),\n+                        self.tokenizer.eos_token_id,\n+                        device=self.device,\n                     )\n                 else:\n-                    next_token_ids = torch.full((batch.batch_size(),), 0)\n+                    next_token_ids = torch.full(\n+                        (batch.batch_size(),), 0, device=self.device\n+                    )\n             batch.output_ids = next_token_ids\n             ret = logits_output, next_token_ids, model_worker_batch.bid\n         else:  # embedding or reward model\ndiff --git a/python/sglang/srt/sampling/sampling_batch_info.py b/python/sglang/srt/sampling/sampling_batch_info.py\nindex 27a2d07..87bd82a 100644\n--- a/python/sglang/srt/sampling/sampling_batch_info.py\n+++ b/python/sglang/srt/sampling/sampling_batch_info.py\n@@ -52,23 +52,20 @@ class SamplingBatchInfo:\n     ):\n         reqs = batch.reqs\n         device = batch.device\n-        temperatures = (\n-            torch.tensor(\n-                [r.sampling_params.temperature for r in reqs],\n-                dtype=torch.float,\n-            )\n-            .view(-1, 1)\n-            .to(device, non_blocking=True)\n-        )\n+        temperatures = torch.tensor(\n+            [r.sampling_params.temperature for r in reqs],\n+            dtype=torch.float,\n+            device=device,\n+        ).view(-1, 1)\n         top_ps = torch.tensor(\n-            [r.sampling_params.top_p for r in reqs], dtype=torch.float\n-        ).to(device, non_blocking=True)\n+            [r.sampling_params.top_p for r in reqs], dtype=torch.float, device=device\n+        )\n         top_ks = torch.tensor(\n-            [r.sampling_params.top_k for r in reqs], dtype=torch.int32\n-        ).to(device, non_blocking=True)\n+            [r.sampling_params.top_k for r in reqs], dtype=torch.int32, device=device\n+        )\n         min_ps = torch.tensor(\n-            [r.sampling_params.min_p for r in reqs], dtype=torch.float\n-        ).to(device, non_blocking=True)\n+            [r.sampling_params.min_p for r in reqs], dtype=torch.float, device=device\n+        )\n \n         ret = cls(\n             temperatures=temperatures,\n@@ -141,7 +138,7 @@ class SamplingBatchInfo:\n             self.vocab_mask = None\n             return\n \n-        self.vocab_mask = torch.zeros(\n+        self.vocab_mask = torch.empty(\n             len(self.temperatures),\n             self.vocab_size,\n             dtype=torch.bool,\n@@ -149,10 +146,10 @@ class SamplingBatchInfo:\n         )\n         for i, regex_fsm in enumerate(self.regex_fsms):\n             if regex_fsm is not None:\n-                self.vocab_mask[i].fill_(1)\n+                self.vocab_mask[i].fill_(True)\n                 self.vocab_mask[i][\n                     regex_fsm.get_next_instruction(self.regex_fsm_states[i]).tokens\n-                ] = 0\n+                ] = False\n \n     def filter_batch(self, unfinished_indices: List[int], new_indices: torch.Tensor):\n         if self.penalizer_orchestrator:\n@@ -185,11 +182,16 @@ class SamplingBatchInfo:\n                 shape, dtype = lhs.shape[1:], lhs.dtype\n             else:\n                 shape, dtype = rhs.shape[1:], rhs.dtype\n-            with torch.dtype(dtype):\n-                if lhs is None:\n-                    lhs = torch.empty((bs1, *shape), device=device).fill_(default)\n-                if rhs is None:\n-                    rhs = torch.empty((bs2, *shape), device=device).fill_(default)\n+            if lhs is None:\n+                if default == 0:\n+                    lhs = torch.zeros((bs1, *shape), dtype=dtype, device=device)\n+                else:\n+                    lhs = torch.full((bs1, *shape), default, dtype=dtype, device=device)\n+            if rhs is None:\n+                if default == 0:\n+                    rhs = torch.zeros((bs2, *shape), dtype=dtype, device=device)\n+                else:\n+                    rhs = torch.full((bs2, *shape), default, dtype=dtype, device=device)\n             return torch.cat([lhs, rhs])\n \n         return None\n", "model_name_or_path": "gpt-5-2025-08-07"}
