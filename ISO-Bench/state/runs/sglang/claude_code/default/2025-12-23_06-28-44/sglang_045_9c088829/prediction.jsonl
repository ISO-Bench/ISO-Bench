{"instance_id": "/home/ubuntu/OmniPerf-Bench/sglan__repo-005aad3", "model_patch": "diff --git a/python/sglang/srt/distributed/parallel_state.py b/python/sglang/srt/distributed/parallel_state.py\nindex fdde7dd..4de85c8 100644\n--- a/python/sglang/srt/distributed/parallel_state.py\n+++ b/python/sglang/srt/distributed/parallel_state.py\n@@ -581,15 +581,16 @@ class GroupCoordinator:\n         if self.mq_broadcaster is not None:\n             assert src == 0, \"Message queue broadcaster only supports src=0\"\n             return self.mq_broadcaster.broadcast_object(obj)\n+        src_rank = self.ranks[src]  # Cache the src rank lookup\n         if self.rank_in_group == src:\n             torch.distributed.broadcast_object_list(\n-                [obj], src=self.ranks[src], group=self.cpu_group\n+                [obj], src=src_rank, group=self.cpu_group\n             )\n             return obj\n         else:\n             recv = [None]\n             torch.distributed.broadcast_object_list(\n-                recv, src=self.ranks[src], group=self.cpu_group\n+                recv, src=src_rank, group=self.cpu_group\n             )\n             return recv[0]\n \n@@ -624,16 +625,15 @@ class GroupCoordinator:\n         # Serialize object to tensor and get the size as well\n         object_tensor = torch.frombuffer(pickle.dumps(obj), dtype=torch.uint8)\n \n-        size_tensor = torch.tensor(\n-            [object_tensor.numel()], dtype=torch.long, device=\"cpu\"\n-        )\n+        size_tensor = torch.empty(1, dtype=torch.long, device=\"cpu\")\n+        size_tensor[0] = object_tensor.numel()\n \n         # Send object size\n-\n-        torch.distributed.send(size_tensor, dst=self.ranks[dst], group=self.cpu_group)\n+        dst_rank = self.ranks[dst]  # Cache the dst rank lookup\n+        torch.distributed.send(size_tensor, dst=dst_rank, group=self.cpu_group)\n \n         # Send object\n-        torch.distributed.send(object_tensor, dst=self.ranks[dst], group=self.cpu_group)\n+        torch.distributed.send(object_tensor, dst=dst_rank, group=self.cpu_group)\n \n         return None\n \n@@ -650,8 +650,9 @@ class GroupCoordinator:\n         size_tensor = torch.empty(1, dtype=torch.long, device=\"cpu\")\n \n         # Receive object size\n+        src_rank = self.ranks[src]  # Cache the src rank lookup\n         rank_size = torch.distributed.recv(\n-            size_tensor, src=self.ranks[src], group=self.cpu_group\n+            size_tensor, src=src_rank, group=self.cpu_group\n         )\n \n         # Tensor to receive serialized objects into.\n@@ -662,7 +663,7 @@ class GroupCoordinator:\n         )\n \n         rank_object = torch.distributed.recv(\n-            object_tensor, src=self.ranks[src], group=self.cpu_group\n+            object_tensor, src=src_rank, group=self.cpu_group\n         )\n \n         assert (\n@@ -692,6 +693,7 @@ class GroupCoordinator:\n         assert src < self.world_size, f\"Invalid src rank ({src})\"\n \n         rank_in_group = self.rank_in_group\n+        src_rank = self.ranks[src]  # Cache the src rank lookup\n         if rank_in_group == src:\n             metadata_list: List[Tuple[Any, Any]] = []\n             assert isinstance(\n@@ -710,12 +712,12 @@ class GroupCoordinator:\n                 if tensor.is_cpu:\n                     # use metadata_group for CPU tensors\n                     handle = torch.distributed.broadcast(\n-                        tensor, src=self.ranks[src], group=metadata_group, async_op=True\n+                        tensor, src=src_rank, group=metadata_group, async_op=True\n                     )\n                 else:\n                     # use group for GPU tensors\n                     handle = torch.distributed.broadcast(\n-                        tensor, src=self.ranks[src], group=group, async_op=True\n+                        tensor, src=src_rank, group=group, async_op=True\n                     )\n                 async_handles.append(handle)\n             for async_handle in async_handles:\n@@ -738,14 +740,14 @@ class GroupCoordinator:\n                         # use metadata_group for CPU tensors\n                         handle = torch.distributed.broadcast(\n                             tensor,\n-                            src=self.ranks[src],\n+                            src=src_rank,\n                             group=metadata_group,\n                             async_op=True,\n                         )\n                     else:\n                         # use group for GPU tensors\n                         handle = torch.distributed.broadcast(\n-                            tensor, src=self.ranks[src], group=group, async_op=True\n+                            tensor, src=src_rank, group=group, async_op=True\n                         )\n                     async_handles.append(handle)\n                     tensor_dict[key] = tensor\n@@ -789,6 +791,7 @@ class GroupCoordinator:\n         # `send_object_list` has serialization & deserialization,\n         # all happening on CPU. Therefore, we can use the CPU group.\n         self.send_object(metadata_list, dst=dst)\n+        dst_rank = self.ranks[dst]  # Cache the dst rank lookup\n         for tensor in tensor_list:\n             if tensor.numel() == 0:\n                 # Skip sending empty tensors.\n@@ -801,11 +804,11 @@ class GroupCoordinator:\n             if tensor.is_cpu:\n                 # use metadata_group for CPU tensors\n                 torch.distributed.send(\n-                    tensor, dst=self.ranks[dst], group=metadata_group\n+                    tensor, dst=dst_rank, group=metadata_group\n                 )\n             else:\n                 # use group for GPU tensors\n-                torch.distributed.send(tensor, dst=self.ranks[dst], group=group)\n+                torch.distributed.send(tensor, dst=dst_rank, group=group)\n         return None\n \n     def recv_tensor_dict(\n@@ -834,6 +837,7 @@ class GroupCoordinator:\n \n         recv_metadata_list = self.recv_object(src=src)\n         tensor_dict: Dict[str, Any] = {}\n+        src_rank = self.ranks[src]  # Cache the src rank lookup\n         for key, value in recv_metadata_list:\n             if isinstance(value, TensorMetadata):\n                 tensor = torch.empty(value.size, dtype=value.dtype, device=value.device)\n@@ -855,11 +859,11 @@ class GroupCoordinator:\n                 if tensor.is_cpu:\n                     # use metadata_group for CPU tensors\n                     torch.distributed.recv(\n-                        tensor, src=self.ranks[src], group=metadata_group\n+                        tensor, src=src_rank, group=metadata_group\n                     )\n                 else:\n                     # use group for GPU tensors\n-                    torch.distributed.recv(tensor, src=self.ranks[src], group=group)\n+                    torch.distributed.recv(tensor, src=src_rank, group=group)\n                 if use_all_gather:\n                     # do the allgather\n                     tensor = all_gather_group.all_gather(tensor, dim=0)  # type: ignore\n@@ -889,7 +893,8 @@ class GroupCoordinator:\n         if pynccl_comm is not None and not pynccl_comm.disabled:\n             pynccl_comm.send(tensor, dst)\n         else:\n-            torch.distributed.send(tensor, self.ranks[dst], self.device_group)\n+            dst_rank = self.ranks[dst]\n+            torch.distributed.send(tensor, dst_rank, self.device_group)\n \n     def recv(\n         self, size: torch.Size, dtype: torch.dtype, src: Optional[int] = None\n@@ -904,7 +909,8 @@ class GroupCoordinator:\n         if pynccl_comm is not None and not pynccl_comm.disabled:\n             pynccl_comm.recv(tensor, src)\n         else:\n-            torch.distributed.recv(tensor, self.ranks[src], self.device_group)\n+            src_rank = self.ranks[src]\n+            torch.distributed.recv(tensor, src_rank, self.device_group)\n         return tensor\n \n     def destroy(self):\n@@ -1290,7 +1296,7 @@ def in_the_same_node_as(pg: ProcessGroup, source_rank: int = 0) -> List[bool]:\n     world_size = torch.distributed.get_world_size(group=pg)\n \n     # local tensor in each process to store the result\n-    is_in_the_same_node = torch.tensor([0] * world_size, dtype=torch.int32)\n+    is_in_the_same_node = torch.zeros(world_size, dtype=torch.int32)\n \n     # global ranks of the processes in the group\n     ranks = torch.distributed.get_process_group_ranks(pg)\n", "model_name_or_path": "gpt-5-2025-08-07"}
