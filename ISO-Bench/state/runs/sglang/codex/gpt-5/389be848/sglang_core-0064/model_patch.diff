diff --git a/python/sglang/srt/models/llama4.py b/python/sglang/srt/models/llama4.py
index d933f27ae..789985c9d 100644
--- a/python/sglang/srt/models/llama4.py
+++ b/python/sglang/srt/models/llama4.py
@@ -243,6 +243,16 @@ class Llama4Attention(nn.Module):
 
         return attn_scale.unsqueeze(-1)
 
+    @torch.compile(dynamic=True, backend=get_compiler_backend())
+    def _mul_attn_scale(self, positions: torch.Tensor, q: torch.Tensor) -> torch.Tensor:
+        """Fuse attention-scale computation and multiplication.
+
+        This reduces intermediate allocations and lets the compiler
+        generate a fused kernel for the temperature tuning path.
+        """
+        attn_scale = self._get_attn_scale(positions)
+        return (q * attn_scale).to(q.dtype)
+
     def forward(
         self,
         positions: torch.Tensor,
@@ -250,10 +260,16 @@ class Llama4Attention(nn.Module):
         forward_batch: ForwardBatch,
     ) -> torch.Tensor:
         qkv, _ = self.qkv_proj(hidden_states)
-        q, k, v = qkv.split([self.q_size, self.kv_size, self.kv_size], dim=-1)
+        # Split once to get (q+k) and v to reduce overhead from 3-way split.
+        qk, v = qkv.split([self.q_size + self.kv_size, self.kv_size], dim=-1)
 
         if self.rotary_emb is not None:
-            q, k = self.rotary_emb(positions, q, k)
+            # Defer splitting q/k until needed to minimize ops and let
+            # rotary embedding operate on views.
+            q_view, k_view = qk.split([self.q_size, self.kv_size], dim=-1)
+            q, k = self.rotary_emb(positions, q_view, k_view)
+        else:
+            q, k = qk.split([self.q_size, self.kv_size], dim=-1)
 
         if self.qk_norm is not None:
             # TODO: support float
@@ -269,8 +285,8 @@ class Llama4Attention(nn.Module):
         # while working at very long context
         # https://arxiv.org/abs/2501.19399
         if self.attn_temperature_tuning and not self.use_rope:
-            attn_scale = self._get_attn_scale(positions)
-            q = (q * attn_scale).to(q.dtype)
+            # Use compiled fused path to compute and apply the scale.
+            q = self._mul_attn_scale(positions, q)
 
         attn_output = self.attn(q, k, v, forward_batch)
         output, _ = self.o_proj(attn_output)
