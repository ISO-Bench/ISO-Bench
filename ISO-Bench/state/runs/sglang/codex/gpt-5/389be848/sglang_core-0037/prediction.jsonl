{"instance_id": "/home/ubuntu/OmniPerf-Bench/sglan__repo-9282306", "model_patch": "diff --git a/python/sglang/srt/models/llama4.py b/python/sglang/srt/models/llama4.py\nindex 4e4ba9a1e..8015c18a0 100644\n--- a/python/sglang/srt/models/llama4.py\n+++ b/python/sglang/srt/models/llama4.py\n@@ -48,7 +48,7 @@ from sglang.srt.layers.vocab_parallel_embedding import VocabParallelEmbedding\n from sglang.srt.managers.schedule_batch import global_server_args_dict\n from sglang.srt.model_executor.forward_batch_info import ForwardBatch\n from sglang.srt.models.llama import LlamaForCausalLM, LlamaMLP\n-from sglang.srt.utils import add_prefix, get_compiler_backend, make_layers\n+from sglang.srt.utils import add_prefix, fast_topk, get_compiler_backend, make_layers\n \n logger = logging.getLogger(__name__)\n \n@@ -63,7 +63,7 @@ class Llama4MoE(nn.Module):\n         topk: int,\n         renormalize: bool,\n     ) -> Tuple[torch.Tensor, torch.Tensor]:\n-        router_scores_aK, router_indices_aK = torch.topk(gating_output, topk, dim=-1)\n+        router_scores_aK, router_indices_aK = fast_topk(gating_output, topk, dim=-1)\n         router_scores_aK = torch.sigmoid(router_scores_aK.float()).to(\n             hidden_states.dtype\n         )\ndiff --git a/python/sglang/srt/speculative/eagle_utils.py b/python/sglang/srt/speculative/eagle_utils.py\nindex 19fa1807c..cd4de0495 100644\n--- a/python/sglang/srt/speculative/eagle_utils.py\n+++ b/python/sglang/srt/speculative/eagle_utils.py\n@@ -19,7 +19,7 @@ from sglang.srt.managers.schedule_batch import (\n from sglang.srt.mem_cache.memory_pool import TokenToKVPoolAllocator\n from sglang.srt.model_executor.forward_batch_info import CaptureHiddenMode\n from sglang.srt.speculative.build_eagle_tree import build_tree_kernel_efficient\n-from sglang.srt.utils import is_cuda_available, is_hip, next_power_of_2\n+from sglang.srt.utils import fast_topk, is_cuda_available, is_hip, next_power_of_2\n \n if is_cuda_available():\n     from sgl_kernel import (\n@@ -117,10 +117,13 @@ class EagleDraftInput:\n     ):\n         bs = self.accept_length.numel()\n \n-        qo_indptr = torch.zeros((bs + 1,), dtype=torch.int32, device=\"cuda\")\n+        # Avoid zero-initializing entire buffers; only set required entries.\n+        qo_indptr = torch.empty((bs + 1,), dtype=torch.int32, device=\"cuda\")\n+        qo_indptr[0] = 0\n         qo_indptr[1:] = torch.cumsum(self.accept_length, dim=0)\n \n-        cum_kv_seq_len = torch.zeros((bs + 1,), dtype=torch.int32, device=\"cuda\")\n+        cum_kv_seq_len = torch.empty((bs + 1,), dtype=torch.int32, device=\"cuda\")\n+        cum_kv_seq_len[0] = 0\n         cum_kv_seq_len[1:] = torch.cumsum(paged_kernel_lens, dim=0)\n \n         # TODO: replace cum_kv_seq_len[-1] with paged_kernel_lens_sum to avoid the device sync.\n@@ -278,11 +281,10 @@ class EagleVerifyInput:\n             dtype=torch.int32,\n             device=\"cuda\",\n         )\n-        cum_kv_seq_len = torch.zeros(\n-            (batch_size + 1,), dtype=torch.int32, device=\"cuda\"\n-        )\n+        cum_kv_seq_len = torch.empty((batch_size + 1,), dtype=torch.int32, device=\"cuda\")\n \n         paged_kernel_lens = paged_kernel_lens + self.draft_token_num\n+        cum_kv_seq_len[0] = 0\n         cum_kv_seq_len[1:] = torch.cumsum(paged_kernel_lens, dim=0)\n \n         kv_indices = torch.empty(\n@@ -770,16 +772,7 @@ def select_top_k_tokens(\n         )\n \n     return input_ids, hidden_states, scores, tree_info\n-\n-\n-def fast_topk(values, topk, dim):\n-    if topk == 1:\n-        # Use max along the specified dimension to get both value and index\n-        max_value, max_index = torch.max(values, dim=dim)\n-        return max_value.unsqueeze(1), max_index.unsqueeze(1)\n-    else:\n-        # Use topk for efficiency with larger k values\n-        return torch.topk(values, topk, dim=dim)\n+ \n \n \n def _generate_simulated_accept_index(\ndiff --git a/python/sglang/srt/speculative/eagle_worker.py b/python/sglang/srt/speculative/eagle_worker.py\nindex 514603424..9967cf6ac 100644\n--- a/python/sglang/srt/speculative/eagle_worker.py\n+++ b/python/sglang/srt/speculative/eagle_worker.py\n@@ -31,11 +31,15 @@ from sglang.srt.speculative.eagle_utils import (\n     EagleVerifyInput,\n     EagleVerifyOutput,\n     assign_draft_cache_locs,\n-    fast_topk,\n     select_top_k_tokens,\n )\n from sglang.srt.speculative.spec_info import SpeculativeAlgorithm\n-from sglang.srt.utils import empty_context, get_available_gpu_memory, is_cuda_available\n+from sglang.srt.utils import (\n+    empty_context,\n+    fast_topk,\n+    get_available_gpu_memory,\n+    is_cuda_available,\n+)\n \n if is_cuda_available():\n     from sgl_kernel import segment_packbits\ndiff --git a/python/sglang/srt/utils.py b/python/sglang/srt/utils.py\nindex b43fe4273..6f29acee1 100644\n--- a/python/sglang/srt/utils.py\n+++ b/python/sglang/srt/utils.py\n@@ -257,6 +257,21 @@ def calculate_time(show=False, min_cost_ms=0.0):\n     return wrapper\n \n \n+def fast_topk(values: torch.Tensor, topk: int, dim: int = -1):\n+    \"\"\"Fast top-k utility.\n+\n+    Optimized for the common case topk == 1 by using torch.max with keepdim=True\n+    to avoid allocating/sorting unnecessary buffers. Falls back to torch.topk\n+    otherwise.\n+\n+    Returns a (values, indices) tuple consistent with torch.topk.\n+    \"\"\"\n+    if topk == 1:\n+        max_value, max_index = torch.max(values, dim=dim, keepdim=True)\n+        return max_value, max_index\n+    return torch.topk(values, topk, dim=dim)\n+\n+\n def get_available_gpu_memory(device, gpu_id, distributed=False, empty_cache=True):\n     \"\"\"\n     Get available memory for cuda:gpu_id device.\n", "model_name_or_path": "gpt-5-2025-08-07"}
