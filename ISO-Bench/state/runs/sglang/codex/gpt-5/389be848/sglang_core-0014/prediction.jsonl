{"instance_id": "/home/ubuntu/OmniPerf-Bench/sglan__repo-42a2d82", "model_patch": "diff --git a/python/sglang/bench_latency.py b/python/sglang/bench_latency.py\nindex 7a03e162f..ac6b1fb6f 100644\n--- a/python/sglang/bench_latency.py\n+++ b/python/sglang/bench_latency.py\n@@ -260,7 +260,7 @@ def correctness_test(\n \n     # Decode\n     output_ids = [input_ids[i] + [next_token_ids[i]] for i in range(len(input_ids))]\n-    for _ in range(bench_args.output_len[0]):\n+    for _ in range(bench_args.output_len[0] - 1):\n         next_token_ids, _ = decode(next_token_ids, batch, model_runner)\n         for i in range(len(reqs)):\n             output_ids[i].append(next_token_ids[i])\n@@ -311,7 +311,7 @@ def latency_test_run_once(\n \n     # Decode\n     decode_latencies = []\n-    for i in range(output_len):\n+    for i in range(output_len - 1):\n         torch.cuda.synchronize()\n         tic = time.time()\n         next_token_ids, _ = decode(next_token_ids, batch, model_runner)\ndiff --git a/python/sglang/srt/managers/schedule_batch.py b/python/sglang/srt/managers/schedule_batch.py\nindex 2ab041726..de95808c0 100644\n--- a/python/sglang/srt/managers/schedule_batch.py\n+++ b/python/sglang/srt/managers/schedule_batch.py\n@@ -471,7 +471,8 @@ class ScheduleBatch:\n             self.input_ids = torch.tensor(sum(input_ids, []), dtype=torch.int32)\n             self.req_pool_indices = torch.tensor(req_pool_indices_cpu)\n             self.seq_lens = torch.tensor(seq_lens, dtype=torch.int32)\n-            self.position_ids_offsets = torch.zeros((bs,), dtype=torch.int64)\n+            # Initialize on-device offsets without extra kernel work\n+            self.position_ids_offsets = torch.zeros_like(self.seq_lens, dtype=torch.int64)\n \n         self.extend_num_tokens = extend_num_tokens\n         self.out_cache_loc = out_cache_loc\n@@ -681,7 +682,12 @@ class ScheduleBatch:\n                 for r in self.reqs\n             ]\n         else:\n-            self.sampling_info.penalizer_orchestrator.cumulate_input_tokens(input_ids)\n+            # Avoid unnecessary overhead if no penalizer is active\n+            orchestrator = self.sampling_info.penalizer_orchestrator\n+            if orchestrator is not None:\n+                penalizers = getattr(orchestrator, \"penalizers\", {})\n+                if any(p.is_prepared() for p in penalizers.values()):\n+                    orchestrator.cumulate_input_tokens(input_ids)\n \n         self.input_ids = torch.tensor(input_ids, dtype=torch.int32, device=\"cuda\")\n         self.seq_lens.add_(1)\ndiff --git a/python/sglang/srt/model_executor/forward_batch_info.py b/python/sglang/srt/model_executor/forward_batch_info.py\nindex 4815fbc56..c429ad4af 100644\n--- a/python/sglang/srt/model_executor/forward_batch_info.py\n+++ b/python/sglang/srt/model_executor/forward_batch_info.py\n@@ -100,39 +100,20 @@ class InputMetadata:\n         position_ids_offsets = batch.position_ids_offsets\n \n         if self.forward_mode.is_decode():\n-            if True:\n-                self.positions = self.seq_lens - 1\n-            else:\n-                # Deprecated\n-                self.positions = (self.seq_lens - 1) + position_ids_offsets\n+            # Decode positions are simply seq_lens - 1\n+            self.positions = self.seq_lens - 1\n         else:\n-            if True:\n-                self.positions = torch.tensor(\n-                    np.concatenate(\n-                        [\n-                            np.arange(batch.prefix_lens_cpu[i], len(req.fill_ids))\n-                            for i, req in enumerate(batch.reqs)\n-                        ],\n-                        axis=0,\n-                    ),\n-                    device=\"cuda\",\n-                )\n-            else:\n-                # Deprecated\n-                position_ids_offsets_cpu = position_ids_offsets.cpu().numpy()\n-                self.positions = torch.tensor(\n-                    np.concatenate(\n-                        [\n-                            np.arange(\n-                                batch.prefix_lens_cpu[i] + position_ids_offsets_cpu[i],\n-                                len(req.fill_ids) + position_ids_offsets_cpu[i],\n-                            )\n-                            for i, req in enumerate(batch.reqs)\n-                        ],\n-                        axis=0,\n-                    ),\n+            # Build positions directly on GPU to avoid NumPy host work\n+            ranges = [\n+                torch.arange(\n+                    start=batch.prefix_lens_cpu[i],\n+                    end=len(req.fill_ids),\n                     device=\"cuda\",\n+                    dtype=torch.int64,\n                 )\n+                for i, req in enumerate(batch.reqs)\n+            ]\n+            self.positions = torch.cat(ranges, dim=0) if ranges else torch.empty(0, dtype=torch.int64, device=\"cuda\")\n \n         # Positions should be in long type\n         self.positions = self.positions.to(torch.int64)\ndiff --git a/python/sglang/srt/model_executor/model_runner.py b/python/sglang/srt/model_executor/model_runner.py\nindex 049a43840..8900b073a 100644\n--- a/python/sglang/srt/model_executor/model_runner.py\n+++ b/python/sglang/srt/model_executor/model_runner.py\n@@ -552,15 +552,26 @@ class ModelRunner:\n     def sample(\n         self, logits_output: LogitsProcessorOutput, batch: ScheduleBatch\n     ) -> torch.Tensor:\n-        # Put CPU-heavy tasks here. They will be overlapped with the forward pass.\n-        batch.sampling_info.update_regex_vocab_mask(batch)\n-        batch.sampling_info.update_penalties()\n-        logits = self._apply_logits_bias(\n-            logits_output.next_token_logits, batch.sampling_info\n-        )\n+        # Fast-path: skip penalty and regex work when not used\n+        sampling_info = batch.sampling_info\n+        has_regex = any(req.regex_fsm is not None for req in batch.reqs)\n+        orchestrator = getattr(sampling_info, \"penalizer_orchestrator\", None)\n+        penalizers_prepared = False\n+        if orchestrator is not None:\n+            penalizers_prepared = any(\n+                p.is_prepared() for p in orchestrator.penalizers.values()\n+            )\n+\n+        if not has_regex and not penalizers_prepared and sampling_info.logit_bias is None:\n+            logits = logits_output.next_token_logits\n+        else:\n+            # Put CPU-heavy tasks here. They will be overlapped with the forward pass.\n+            sampling_info.update_regex_vocab_mask(batch)\n+            sampling_info.update_penalties()\n+            logits = self._apply_logits_bias(logits_output.next_token_logits, sampling_info)\n \n         # Sample the next tokens.\n-        next_token_ids = self.sampler(logits, batch.sampling_info)\n+        next_token_ids = self.sampler(logits, sampling_info)\n         return next_token_ids\n \n \ndiff --git a/scripts/playground/reference_hf.py b/scripts/playground/reference_hf.py\nindex 1eb7b0dd2..ff582bc2c 100644\n--- a/scripts/playground/reference_hf.py\n+++ b/scripts/playground/reference_hf.py\n@@ -81,17 +81,16 @@ def synthetic_tokens(args):\n \n     for p in prompts:\n         input_ids = p\n-        for i in range(output_len + 1):\n-            prefill_logits = m.forward(torch.tensor([input_ids], device=\"cuda\")).logits[\n-                0\n-            ][-1]\n-\n-            if i == 0:\n-                print(\"prefill logits\", prefill_logits)\n-            else:\n-                print(\"decode\", i - 1, prefill_logits)\n-\n-            input_ids.append(torch.argmax(prefill_logits).item())\n+        # Prefill step\n+        prefill_logits = m.forward(torch.tensor([input_ids], device=\"cuda\")).logits[0][-1]\n+        print(\"prefill logits\", prefill_logits)\n+        input_ids.append(torch.argmax(prefill_logits).item())\n+\n+        # Decode steps\n+        for i in range(output_len - 1):\n+            decode_logits = m.forward(torch.tensor([input_ids], device=\"cuda\")).logits[0][-1]\n+            print(\"decode\", i, decode_logits)\n+            input_ids.append(torch.argmax(decode_logits).item())\n \n \n if __name__ == \"__main__\":\n", "model_name_or_path": "gpt-5-2025-08-07"}
