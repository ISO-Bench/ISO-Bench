{"instance_id": "/home/ubuntu/OmniPerf-Bench/sglan__repo-f4a8987", "model_patch": "diff --git a/benchmark/kernels/fused_moe_triton/benchmark_ep_pre_reorder_triton.py b/benchmark/kernels/fused_moe_triton/benchmark_ep_pre_reorder_triton.py\nnew file mode 100644\nindex 000000000..75e09e3c2\n--- /dev/null\n+++ b/benchmark/kernels/fused_moe_triton/benchmark_ep_pre_reorder_triton.py\n@@ -0,0 +1,82 @@\n+import argparse\n+import time\n+\n+import torch\n+import triton\n+\n+from sglang.srt.layers.moe.ep_moe.kernels import pre_reorder_triton_kernel\n+\n+\n+def benchmark_pre_reorder(batch_size: int, topk: int, hidden_size: int, block_size: int):\n+    device = \"cuda\"\n+\n+    # Inputs\n+    input_ptr = torch.randn(batch_size, hidden_size, dtype=torch.float16, device=device)\n+\n+    # Use empty for outputs and index buffers to avoid unnecessary memset\n+    gateup_input_ptr = torch.empty(\n+        batch_size * topk, hidden_size, dtype=torch.float16, device=device\n+    )\n+    src2dst_ptr = torch.randint(\n+        0, batch_size * topk, (batch_size, topk), dtype=torch.int32, device=device\n+    )\n+    topk_ids_ptr = torch.randint(0, 1024, (batch_size, topk), dtype=torch.int32, device=device)\n+\n+    # No scaling by default\n+    a1_scales_ptr = None\n+    start_expert_id = 0\n+    end_expert_id = 1023\n+\n+    grid = (batch_size,)\n+\n+    # Warmup\n+    for _ in range(5):\n+        pre_reorder_triton_kernel[grid](\n+            input_ptr,\n+            gateup_input_ptr,\n+            src2dst_ptr,\n+            topk_ids_ptr,\n+            a1_scales_ptr,\n+            start_expert_id,\n+            end_expert_id,\n+            topk,\n+            hidden_size,\n+            BLOCK_SIZE=block_size,\n+        )\n+\n+    torch.cuda.synchronize()\n+    t0 = time.time()\n+    for _ in range(20):\n+        pre_reorder_triton_kernel[grid](\n+            input_ptr,\n+            gateup_input_ptr,\n+            src2dst_ptr,\n+            topk_ids_ptr,\n+            a1_scales_ptr,\n+            start_expert_id,\n+            end_expert_id,\n+            topk,\n+            hidden_size,\n+            BLOCK_SIZE=block_size,\n+        )\n+    torch.cuda.synchronize()\n+    return (time.time() - t0) / 20\n+\n+\n+def main():\n+    parser = argparse.ArgumentParser()\n+    parser.add_argument(\"--batch-size\", type=int, default=4096)\n+    parser.add_argument(\"--topk\", type=int, default=2)\n+    parser.add_argument(\"--hidden-size\", type=int, default=4096)\n+    parser.add_argument(\"--block-size\", type=int, default=128)\n+    args = parser.parse_args()\n+\n+    dur = benchmark_pre_reorder(\n+        args.batch_size, args.topk, args.hidden_size, args.block_size\n+    )\n+    print(f\"pre_reorder_triton_kernel avg time: {dur*1e3:.3f} ms\")\n+\n+\n+if __name__ == \"__main__\":\n+    main()\n+\ndiff --git a/python/sglang/srt/layers/moe/ep_moe/kernels.py b/python/sglang/srt/layers/moe/ep_moe/kernels.py\nindex 8c005527a..15a07d85e 100644\n--- a/python/sglang/srt/layers/moe/ep_moe/kernels.py\n+++ b/python/sglang/srt/layers/moe/ep_moe/kernels.py\n@@ -151,7 +151,9 @@ def compute_seg_indptr_triton_kernel(reorder_topk_ids, seg_indptr, num_toks):\n \n def run_moe_ep_preproess(topk_ids: torch.Tensor, num_experts: int):\n     reorder_topk_ids, reorder_ids = torch.sort(topk_ids.view(-1), stable=True)\n-    seg_indptr = torch.zeros(num_experts + 1, device=topk_ids.device, dtype=torch.int64)\n+    # Only seg_indptr[0] needs to be zero-initialized; others are written by the kernel.\n+    seg_indptr = torch.empty(num_experts + 1, device=topk_ids.device, dtype=torch.int64)\n+    seg_indptr[0] = 0\n     src2dst = torch.empty(topk_ids.numel(), device=topk_ids.device, dtype=torch.int32)\n \n     compute_seg_indptr_triton_kernel[(num_experts,)](\n@@ -186,20 +188,25 @@ def pre_reorder_triton_kernel(\n     topk_ids_ptr = topk_ids_ptr + src_idx * topk\n \n     src_ptr = input_ptr + src_idx * hidden_size\n-    for idx in range(topk):\n-        expert_id = tl.load(topk_ids_ptr + idx)\n-        if expert_id >= start_expert_id and expert_id <= end_expert_id:\n-            if a1_scales_ptr is not None:\n-                scale = 1.0 / tl.load(a1_scales_ptr + expert_id - start_expert_id)\n-            else:\n-                scale = 1.0\n \n-            dst_idx = tl.load(src2dst_ptr + idx)\n-            dst_ptr = gateup_input_ptr + dst_idx * hidden_size\n-            for start_offset in tl.range(0, hidden_size, BLOCK_SIZE):\n-                offset = start_offset + tl.arange(0, BLOCK_SIZE)\n-                mask = offset < hidden_size\n-                in_data = tl.load(src_ptr + offset, mask=mask).to(tl.float32)\n+    # Iterate over the hidden dimension first to reuse the loaded input\n+    # across all top-k destinations. This reduces redundant global reads\n+    # of the same source vector by a factor of topk.\n+    for start_offset in tl.range(0, hidden_size, BLOCK_SIZE):\n+        offset = start_offset + tl.arange(0, BLOCK_SIZE)\n+        mask = offset < hidden_size\n+        in_data = tl.load(src_ptr + offset, mask=mask).to(tl.float32)\n+\n+        for idx in range(topk):\n+            expert_id = tl.load(topk_ids_ptr + idx)\n+            if expert_id >= start_expert_id and expert_id <= end_expert_id:\n+                if a1_scales_ptr is not None:\n+                    scale = 1.0 / tl.load(a1_scales_ptr + expert_id - start_expert_id)\n+                else:\n+                    scale = 1.0\n+\n+                dst_idx = tl.load(src2dst_ptr + idx)\n+                dst_ptr = gateup_input_ptr + dst_idx * hidden_size\n                 out_data = (in_data * scale).to(OutDtype)\n                 tl.store(dst_ptr + offset, out_data, mask=mask)\n \n@@ -679,7 +686,10 @@ def grouped_gemm_triton(\n         \"BLOCK_SIZE_K\": 128,\n     }\n \n-    m_num_tiles_indptr = torch.zeros(batch_size + 1, device=a.device, dtype=torch.int64)\n+    # Only the first element is required to be zero; subsequent elements\n+    # are overwritten by the kernel. Avoid clearing the whole buffer.\n+    m_num_tiles_indptr = torch.empty(batch_size + 1, device=a.device, dtype=torch.int64)\n+    m_num_tiles_indptr[0] = 0\n     compute_m_num_tiles_indptr[(1,)](\n         m_num_tiles_indptr, seg_indptr, batch_size, config[\"BLOCK_SIZE_M\"]\n     )\n", "model_name_or_path": "gpt-5-2025-08-07"}
