{"instance_id": "/home/ubuntu/OmniPerf-Bench/sglan__repo-2cea614", "model_patch": "diff --git a/examples/usage/json_logprobs.py b/examples/usage/json_logprobs.py\nnew file mode 100644\nindex 000000000..5312fa4ee\n--- /dev/null\n+++ b/examples/usage/json_logprobs.py\n@@ -0,0 +1,59 @@\n+\"\"\"\n+Example: Requesting top-k logprobs for regex-constrained JSON output via HTTP.\n+\n+Usage:\n+  python -m sglang.launch_server --model-path meta-llama/Llama-2-7b-chat-hf --port 30000\n+  python examples/usage/json_logprobs.py\n+\"\"\"\n+import json\n+from concurrent.futures import ThreadPoolExecutor\n+\n+from sglang.utils import http_request\n+\n+# Reuse the regex from the JSON decode example\n+try:\n+    from examples.usage.json_decode import character_regex  # type: ignore\n+except Exception:\n+    # Fallback minimal regex\n+    character_regex = r\"\\{\\n\\s*\\\"name\\\": \\\"[\\\\w\\\\d\\\\s]{1,16}\\\"\\n\\}\"\n+\n+\n+character_names = [\"Hermione Granger\", \"Ron Weasley\", \"Harry Potter\"]\n+\n+base_url = \"http://localhost:30000\"\n+\n+prompt = (\n+    \"is a character in Harry Potter. Please fill in the following information about this character.\\n\"\n+)\n+\n+\n+def api_request(name):\n+    data = {\n+        \"model\": \"\",\n+        \"prompt\": name + \" \" + prompt,\n+        \"temperature\": 0,\n+        \"max_tokens\": 128,\n+        \"regex\": character_regex,\n+        \"logprobs\": 3,\n+    }\n+    r = http_request(base_url + \"/generate/\", json=data)\n+    assert r.status_code == 200, f\"HTTP {r.status_code}\"\n+    out = r.json()\n+    print(f\"\\n=== {name} ===\\n{text_with_logprobs(out)}\\n\")\n+\n+\n+def text_with_logprobs(resp):\n+    # Pretty print the text and a few logprobs entries\n+    text = resp.get(\"text\", \"\")\n+    lps = resp.get(\"logprobs\", {})\n+    tops = lps.get(\"top_logprobs\", [])\n+    san = []\n+    for i, step in enumerate(tops[:5]):\n+        san.append(f\"step[{i}]: \" + \", \".join([f\"{tok}:{p:.2f}\" for tok, p in step]))\n+    return text + \"\\n\" + \"\\n\".join(san)\n+\n+\n+if __name__ == \"__main__\":\n+    with ThreadPoolExecutor(len(character_names)) as ex:\n+        list(ex.map(api_request, character_names))\n+\ndiff --git a/python/sglang/srt/layers/logits_processor.py b/python/sglang/srt/layers/logits_processor.py\nindex 53d6620e9..4ca64a077 100644\n--- a/python/sglang/srt/layers/logits_processor.py\n+++ b/python/sglang/srt/layers/logits_processor.py\n@@ -38,13 +38,23 @@ class LogitsProcessor(nn.Module):\n \n     def _get_top_logprobs(self, all_logprobs, input_metadata: InputMetadata):\n         if input_metadata.forward_mode == ForwardMode.DECODE:\n+            # Batched top-k to reduce per-row kernel launches.\n+            top_ks = input_metadata.top_logprobs_nums\n+            max_k = max(top_ks) if len(top_ks) > 0 else 0\n             decode_top_logprobs = []\n-            for i in range(all_logprobs.shape[0]):\n-                k = input_metadata.top_logprobs_nums[i]\n-                t = all_logprobs[i].topk(k)\n-                v_cpu = t.values.tolist()\n-                p_cpu = t.indices.tolist()\n-                decode_top_logprobs.append(list(zip(v_cpu, p_cpu)))\n+            if max_k == 0:\n+                decode_top_logprobs = [[] for _ in range(all_logprobs.shape[0])]\n+            else:\n+                t = all_logprobs.topk(max_k, dim=-1)\n+                vs = t.values\n+                ps = t.indices\n+                for i, k in enumerate(top_ks):\n+                    if k == 0:\n+                        decode_top_logprobs.append([])\n+                        continue\n+                    v_cpu = vs[i, :k].tolist()\n+                    p_cpu = ps[i, :k].tolist()\n+                    decode_top_logprobs.append(list(zip(v_cpu, p_cpu)))\n             return None, decode_top_logprobs\n         else:\n             prefill_top_logprobs, decode_top_logprobs = [], []\ndiff --git a/python/sglang/srt/managers/router/infer_batch.py b/python/sglang/srt/managers/router/infer_batch.py\nindex dbe94371b..680df6934 100644\n--- a/python/sglang/srt/managers/router/infer_batch.py\n+++ b/python/sglang/srt/managers/router/infer_batch.py\n@@ -304,6 +304,8 @@ class Batch:\n             dtype=torch.float,\n             device=device,\n         ).view(-1, 1)\n+        # Precompute inverse temperatures to replace expensive division\n+        self.inv_temperatures = self.temperatures.reciprocal()\n         self.top_ps = torch.tensor(\n             [r.sampling_params.top_p for r in reqs], dtype=torch.float, device=device\n         ).view(-1, 1)\n@@ -507,13 +509,19 @@ class Batch:\n     def sample(self, logits: torch.Tensor):\n         # Post process logits\n         logits = logits.contiguous()\n-        logits.div_(self.temperatures)\n+        # Use multiplication with reciprocal instead of division for speed\n+        logits.mul_(self.inv_temperatures)\n         if self.logit_bias is not None:\n             logits.add_(self.logit_bias)\n \n         has_regex = any(req.regex_fsm is not None for req in self.reqs)\n         if has_regex:\n-            allowed_mask = torch.empty_like(logits[0], dtype=torch.bool)\n+            # Reuse a persistent mask buffer to avoid reallocations\n+            buf = getattr(self, \"_allowed_mask_buf\", None)\n+            if buf is None or buf.shape != logits[0].shape or buf.device != logits.device:\n+                buf = torch.empty_like(logits[0], dtype=torch.bool)\n+                self._allowed_mask_buf = buf\n+            allowed_mask = buf\n             for i, req in enumerate(self.reqs):\n                 if req.regex_fsm is not None:\n                     allowed_mask.zero_()\ndiff --git a/python/sglang/srt/managers/router/model_rpc.py b/python/sglang/srt/managers/router/model_rpc.py\nindex 6abb20b25..1a1a3f95f 100644\n--- a/python/sglang/srt/managers/router/model_rpc.py\n+++ b/python/sglang/srt/managers/router/model_rpc.py\n@@ -100,8 +100,11 @@ class ModelRpcServer:\n                 else server_args.max_prefill_num_token\n             ),\n         )\n+        # Keep integer-token logit bias on GPU to avoid repeated host->device copies\n         self.int_token_logit_bias = torch.tensor(\n-            get_int_token_logit_bias(self.tokenizer, self.model_config.vocab_size)\n+            get_int_token_logit_bias(self.tokenizer, self.model_config.vocab_size),\n+            dtype=torch.float32,\n+            device=\"cuda\",\n         )\n         set_random_seed(server_args.random_seed)\n \n", "model_name_or_path": "gpt-5-2025-08-07"}
