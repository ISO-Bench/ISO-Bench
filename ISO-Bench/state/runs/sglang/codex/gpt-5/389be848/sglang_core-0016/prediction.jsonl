{"instance_id": "/home/ubuntu/OmniPerf-Bench/sglan__repo-96c503e", "model_patch": "diff --git a/docs/test_process.md b/docs/test_process.md\nindex e7aff5b5a..60db2b9c6 100644\n--- a/docs/test_process.md\n+++ b/docs/test_process.md\n@@ -2,7 +2,16 @@\n \n ### Latency Alignment\n ```\n+# single gpu\n python -m sglang.bench_latency --model-path meta-llama/Llama-2-7b-chat-hf --mem-fraction-static 0.8 --batch 32 --input-len 512 --output-len 256\n+python -m sglang.bench_latency --model-path meta-llama/Llama-2-7b-chat-hf --mem-fraction-static 0.8 --batch 1  --input-len 512 --output-len 256\n+\n+# multiple gpu\n+python -m sglang.bench_latency --model-path meta-llama/Meta-Llama-3-70B --tp 8 --mem-fraction-static 0.6 --batch 32 --input-len 8192 --output-len 1\n+python -m sglang.bench_latency --model-path meta-llama/Meta-Llama-3-70B --tp 8 --mem-fraction-static 0.6 --batch 1  --input-len 8100 --output-len 32\n+\n+# moe model\n+python -m sglang.bench_latency --model-path databricks/dbrx-base --tp 8 --mem-fraction-static 0.6 --batch 4 --input-len 1024 --output-len 32\n ```\n \n ### High-level API\n@@ -81,4 +90,4 @@ python3 run_all.py\n ```\n cd test/srt\n python test_openai_server.py\n-```\n\\ No newline at end of file\n+```\ndiff --git a/python/sglang/bench_latency.py b/python/sglang/bench_latency.py\nindex a163cbd30..b6ad6a1b9 100644\n--- a/python/sglang/bench_latency.py\n+++ b/python/sglang/bench_latency.py\n@@ -223,12 +223,12 @@ def latency_test(\n     def run_once(output_len):\n         # Prefill\n         torch.cuda.synchronize()\n-        tot_latency = 0\n         tic = time.time()\n         next_token_ids, _, batch = extend(reqs, model_runner)\n         torch.cuda.synchronize()\n         prefill_latency = time.time() - tic\n-        tot_latency += prefill_latency\n+        # Initialize total latency directly to avoid extra ops\n+        tot_latency = prefill_latency\n         throughput = bench_args.input_len * bench_args.batch_size / prefill_latency\n         rank_print(f\"Prefill. latency: {prefill_latency:6.5f} ms, throughput: {throughput:9.2f} token/s\")\n \n@@ -296,4 +296,4 @@ if __name__ == \"__main__\":\n         format=\"%(message)s\",\n     )\n \n-    main(server_args, bench_args)\n\\ No newline at end of file\n+    main(server_args, bench_args)\ndiff --git a/python/sglang/global_config.py b/python/sglang/global_config.py\nindex 0cc0f747f..9de68afbb 100644\n--- a/python/sglang/global_config.py\n+++ b/python/sglang/global_config.py\n@@ -26,8 +26,9 @@ class GlobalConfig:\n         self.concate_and_append_mode = \"no_adjust\"\n \n         # Request dependency time due to network delay\n-        self.request_dependency_delay = 0.02\n-        self.wait_for_new_request_delay = 0.0006\n+        # Slightly tighten delays to reduce idle wait without changing semantics\n+        self.request_dependency_delay = 0.01\n+        self.wait_for_new_request_delay = 0.0004\n \n         # New generation token ratio estimation\n         self.base_new_token_ratio = 0.4\n@@ -35,5 +36,8 @@ class GlobalConfig:\n         self.new_token_ratio_decay = 0.0001\n         self.new_token_ratio_recovery = 0.05\n \n+        # Optional performance knobs (for future use)\n+        self.enable_kv_cache_buffer_reuse = True\n+\n \n global_config = GlobalConfig()\ndiff --git a/python/sglang/srt/managers/controller/model_runner.py b/python/sglang/srt/managers/controller/model_runner.py\nindex e41514706..92da20dd9 100644\n--- a/python/sglang/srt/managers/controller/model_runner.py\n+++ b/python/sglang/srt/managers/controller/model_runner.py\n@@ -69,9 +69,11 @@ class InputMetadata:\n     flashinfer_decode_wrapper: \"BatchDecodeWithPagedKVCacheWrapper\" = None\n \n     def init_flashinfer_args(self, num_qo_heads, num_kv_heads, head_dim):\n-        self.kv_indptr = torch.zeros(\n+        # Use uninitialized memory where safe to avoid extra memset overhead\n+        self.kv_indptr = torch.empty(\n             (self.batch_size + 1,), dtype=torch.int32, device=\"cuda\"\n         )\n+        self.kv_indptr[0] = 0\n         self.kv_indptr[1:] = torch.cumsum(self.seq_lens, dim=0)\n         self.kv_last_page_len = torch.ones(\n             (self.batch_size,), dtype=torch.int32, device=\"cuda\"\n@@ -92,9 +94,10 @@ class InputMetadata:\n             self.forward_mode == ForwardMode.PREFILL\n             or self.forward_mode == ForwardMode.EXTEND\n         ):\n-            self.qo_indptr = torch.zeros(\n+            self.qo_indptr = torch.empty(\n                 (self.batch_size + 1,), dtype=torch.int32, device=\"cuda\"\n             )\n+            self.qo_indptr[0] = 0\n             self.qo_indptr[1:] = torch.cumsum(self.extend_seq_lens, dim=0)\n \n             self.flashinfer_prefill_wrapper.end_forward()\n@@ -124,7 +127,9 @@ class InputMetadata:\n \n     def init_extend_args(self):\n         self.extend_seq_lens = self.seq_lens - self.prefix_lens\n-        self.extend_start_loc = torch.zeros_like(self.seq_lens)\n+        # Avoid zero-initialization; only first element must be zero\n+        self.extend_start_loc = torch.empty_like(self.seq_lens)\n+        self.extend_start_loc[0] = 0\n         self.extend_start_loc[1:] = torch.cumsum(self.extend_seq_lens[:-1], dim=0)\n         self.max_extend_len = int(torch.max(self.extend_seq_lens))\n \n@@ -147,7 +152,8 @@ class InputMetadata:\n         flashinfer_decode_wrapper=None,\n     ):\n         batch_size = len(req_pool_indices)\n-        start_loc = torch.zeros((batch_size,), dtype=torch.int32, device=\"cuda\")\n+        start_loc = torch.empty((batch_size,), dtype=torch.int32, device=\"cuda\")\n+        start_loc[0] = 0\n         start_loc[1:] = torch.cumsum(seq_lens[:-1], dim=0)\n         total_num_tokens = int(torch.sum(seq_lens))\n         max_seq_len = int(torch.max(seq_lens))\n@@ -353,8 +359,9 @@ class ModelRunner:\n         \"\"\"We need to run a small matmul to init cublas. Otherwise, it will raise some errors later.\"\"\"\n         dtype = torch.float16\n         device = \"cuda\"\n-        a = torch.ones((16, 16), dtype=dtype, device=device)\n-        b = torch.ones((16, 16), dtype=dtype, device=device)\n+        # Avoid zero-filling overhead; values are not used\n+        a = torch.empty((16, 16), dtype=dtype, device=device)\n+        b = torch.empty((16, 16), dtype=dtype, device=device)\n         c = a @ b\n         return c\n \ndiff --git a/python/sglang/srt/server.py b/python/sglang/srt/server.py\nindex e28530889..49d2ff2a7 100644\n--- a/python/sglang/srt/server.py\n+++ b/python/sglang/srt/server.py\n@@ -293,7 +293,7 @@ def launch_server(server_args: ServerArgs, pipe_finish_writer, model_overide_arg\n             app,\n             host=server_args.host,\n             port=server_args.port,\n-            log_level=server_args.log_level_http or server_args.log_level_http,\n+            log_level=server_args.log_level_http,\n             timeout_keep_alive=5,\n             loop=\"uvloop\",\n         )\n", "model_name_or_path": "gpt-5-2025-08-07"}
