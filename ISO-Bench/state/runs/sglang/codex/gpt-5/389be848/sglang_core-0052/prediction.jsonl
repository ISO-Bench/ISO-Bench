{"instance_id": "/home/ubuntu/OmniPerf-Bench/sglan__repo-62f15ee", "model_patch": "diff --git a/model_patch.diff b/model_patch.diff\nnew file mode 100644\nindex 000000000..e69de29bb\ndiff --git a/python/sglang/srt/managers/policy_scheduler.py b/python/sglang/srt/managers/policy_scheduler.py\nindex 04169e808..5a9ef2ee2 100644\n--- a/python/sglang/srt/managers/policy_scheduler.py\n+++ b/python/sglang/srt/managers/policy_scheduler.py\n@@ -139,15 +139,16 @@ class PrefillAdder:\n     def remove_running_tokens(\n         self, running_batch: ScheduleBatch, new_token_ratio: float\n     ):\n+        # Avoid building an intermediate list during summation\n         self.rem_total_tokens -= sum(\n-            [\n+            (\n                 min(\n-                    (r.sampling_params.max_new_tokens - len(r.output_ids)),\n+                    r.sampling_params.max_new_tokens - len(r.output_ids),\n                     CLIP_MAX_NEW_TOKENS,\n                 )\n                 * new_token_ratio\n                 for r in running_batch.reqs\n-            ]\n+            )\n         )\n \n     def _prefill_one_req(\ndiff --git a/python/sglang/srt/managers/tp_worker.py b/python/sglang/srt/managers/tp_worker.py\nindex 8fc03b859..d73d2fbf3 100644\n--- a/python/sglang/srt/managers/tp_worker.py\n+++ b/python/sglang/srt/managers/tp_worker.py\n@@ -26,6 +26,7 @@ from typing import Any, List, Optional, Union\n import torch\n import torch.distributed\n import torch.distributed as dist\n+import numpy as np\n \n from sglang.global_config import global_config\n from sglang.srt.constrained.fsm_cache import FSMCache\n@@ -399,6 +400,9 @@ class ModelTpServer:\n         running_bs = (\n             len(self.running_batch.reqs) if self.running_batch is not None else 0\n         )\n+        # Fast path: nothing to schedule\n+        if not self.waiting_queue and self.current_inflight_req is None:\n+            return None\n         if running_bs >= self.max_running_requests:\n             return None\n \n@@ -407,9 +411,12 @@ class ModelTpServer:\n \n         num_mixed_running = running_bs if self.is_mixed_chunk else 0\n \n+        available_tokens = (\n+            self.token_to_kv_pool.available_size() + self.tree_cache.evictable_size()\n+        )\n         adder = PrefillAdder(\n             self.tree_cache,\n-            self.token_to_kv_pool.available_size() + self.tree_cache.evictable_size(),\n+            available_tokens,\n             self.max_prefill_tokens,\n             self.chunked_prefill_size,\n             num_mixed_running,\n@@ -931,7 +938,9 @@ def broadcast_recv_input(\n         else:\n             serialized_data = pickle.dumps(data)\n             size = len(serialized_data)\n-            tensor_data = torch.ByteTensor(list(serialized_data))\n+            # Use NumPy view to avoid Python list materialization\n+            arr = np.frombuffer(serialized_data, dtype=np.uint8)\n+            tensor_data = torch.from_numpy(arr)\n             tensor_size = torch.tensor([size], dtype=torch.long)\n \n             dist.broadcast(tensor_size, src=0, group=dist_group)\n@@ -948,6 +957,7 @@ def broadcast_recv_input(\n         tensor_data = torch.empty(size, dtype=torch.uint8)\n         dist.broadcast(tensor_data, src=0, group=dist_group)\n \n-        serialized_data = bytes(tensor_data.tolist())\n+        # Convert to bytes efficiently without Python list materialization\n+        serialized_data = tensor_data.numpy().tobytes()\n         data = pickle.loads(serialized_data)\n         return data\n", "model_name_or_path": "gpt-5-2025-08-07"}
