diff --git a/model_patch.diff b/model_patch.diff
new file mode 100644
index 000000000..e69de29bb
diff --git a/python/sglang/srt/managers/policy_scheduler.py b/python/sglang/srt/managers/policy_scheduler.py
index 04169e808..5a9ef2ee2 100644
--- a/python/sglang/srt/managers/policy_scheduler.py
+++ b/python/sglang/srt/managers/policy_scheduler.py
@@ -139,15 +139,16 @@ class PrefillAdder:
     def remove_running_tokens(
         self, running_batch: ScheduleBatch, new_token_ratio: float
     ):
+        # Avoid building an intermediate list during summation
         self.rem_total_tokens -= sum(
-            [
+            (
                 min(
-                    (r.sampling_params.max_new_tokens - len(r.output_ids)),
+                    r.sampling_params.max_new_tokens - len(r.output_ids),
                     CLIP_MAX_NEW_TOKENS,
                 )
                 * new_token_ratio
                 for r in running_batch.reqs
-            ]
+            )
         )
 
     def _prefill_one_req(
diff --git a/python/sglang/srt/managers/tp_worker.py b/python/sglang/srt/managers/tp_worker.py
index 8fc03b859..d73d2fbf3 100644
--- a/python/sglang/srt/managers/tp_worker.py
+++ b/python/sglang/srt/managers/tp_worker.py
@@ -26,6 +26,7 @@ from typing import Any, List, Optional, Union
 import torch
 import torch.distributed
 import torch.distributed as dist
+import numpy as np
 
 from sglang.global_config import global_config
 from sglang.srt.constrained.fsm_cache import FSMCache
@@ -399,6 +400,9 @@ class ModelTpServer:
         running_bs = (
             len(self.running_batch.reqs) if self.running_batch is not None else 0
         )
+        # Fast path: nothing to schedule
+        if not self.waiting_queue and self.current_inflight_req is None:
+            return None
         if running_bs >= self.max_running_requests:
             return None
 
@@ -407,9 +411,12 @@ class ModelTpServer:
 
         num_mixed_running = running_bs if self.is_mixed_chunk else 0
 
+        available_tokens = (
+            self.token_to_kv_pool.available_size() + self.tree_cache.evictable_size()
+        )
         adder = PrefillAdder(
             self.tree_cache,
-            self.token_to_kv_pool.available_size() + self.tree_cache.evictable_size(),
+            available_tokens,
             self.max_prefill_tokens,
             self.chunked_prefill_size,
             num_mixed_running,
@@ -931,7 +938,9 @@ def broadcast_recv_input(
         else:
             serialized_data = pickle.dumps(data)
             size = len(serialized_data)
-            tensor_data = torch.ByteTensor(list(serialized_data))
+            # Use NumPy view to avoid Python list materialization
+            arr = np.frombuffer(serialized_data, dtype=np.uint8)
+            tensor_data = torch.from_numpy(arr)
             tensor_size = torch.tensor([size], dtype=torch.long)
 
             dist.broadcast(tensor_size, src=0, group=dist_group)
@@ -948,6 +957,7 @@ def broadcast_recv_input(
         tensor_data = torch.empty(size, dtype=torch.uint8)
         dist.broadcast(tensor_data, src=0, group=dist_group)
 
-        serialized_data = bytes(tensor_data.tolist())
+        # Convert to bytes efficiently without Python list materialization
+        serialized_data = tensor_data.numpy().tobytes()
         data = pickle.loads(serialized_data)
         return data
