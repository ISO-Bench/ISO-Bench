{"instance_id": "/home/ubuntu/OmniPerf-Bench/sglan__repo-777688b", "model_patch": "diff --git a/python/sglang/srt/lora/lora_manager.py b/python/sglang/srt/lora/lora_manager.py\nindex 45050df53..90c38b8d4 100644\n--- a/python/sglang/srt/lora/lora_manager.py\n+++ b/python/sglang/srt/lora/lora_manager.py\n@@ -77,18 +77,33 @@ class LoRAManager:\n         with torch.device(\"cuda\"):\n             self.cuda_graph_batch_info = LoRABatchInfo(\n                 bs=self.max_bs_in_cuda_graph,\n-                seg_lens=torch.zeros(self.max_bs_in_cuda_graph, dtype=torch.int32),\n-                seg_indptr=torch.zeros(\n+                seg_lens=torch.empty(self.max_bs_in_cuda_graph, dtype=torch.int32),\n+                seg_indptr=torch.empty(\n                     self.max_bs_in_cuda_graph + 1, dtype=torch.int32\n                 ),\n-                max_len=0,\n-                weight_indices=torch.zeros(\n+                max_len=1,\n+                weight_indices=torch.empty(\n                     self.max_bs_in_cuda_graph, dtype=torch.int32\n                 ),\n+                # Initialize ranks and scalings once; they will be\n+                # selectively updated for active adapters per batch.\n                 lora_ranks=torch.zeros(self.max_loras_per_batch, dtype=torch.int32),\n                 scalings=torch.zeros(self.max_loras_per_batch, dtype=torch.float),\n             )\n \n+            # Initialize seg_lens and seg_indptr for CUDA graph once as they\n+            # remain constant (all ones with cumulative indices). This avoids\n+            # per-batch fill and cumsum overhead when using CUDA graphs.\n+            self.cuda_graph_batch_info.seg_lens[: self.max_bs_in_cuda_graph].fill_(1)\n+            torch.cumsum(\n+                self.cuda_graph_batch_info.seg_lens[: self.max_bs_in_cuda_graph],\n+                dim=0,\n+                out=self.cuda_graph_batch_info.seg_indptr[\n+                    1 : self.max_bs_in_cuda_graph + 1\n+                ],\n+            )\n+            self.cuda_graph_batch_info.seg_indptr[0] = 0\n+\n     def init_loras(self):\n         # Config of each LoRA adapter\n         self.configs: Dict[str, LoRAConfig] = {}\n@@ -167,12 +182,8 @@ class LoRAManager:\n             # Do in-place updates when CUDA graph is enabled and the batch forward mode\n             # could use CUDA graph.\n             self.cuda_graph_batch_info.bs = bs\n-            self.cuda_graph_batch_info.seg_lens[:bs].fill_(1)\n-            torch.cumsum(\n-                self.cuda_graph_batch_info.seg_lens[:bs],\n-                dim=0,\n-                out=self.cuda_graph_batch_info.seg_indptr[1 : bs + 1],\n-            )\n+            # seg_lens and seg_indptr are preinitialized for the CUDA graph.\n+            # Only bs and adapter mappings need updates per batch.\n             self.cuda_graph_batch_info.max_len = 1\n \n             for i, lora_path in enumerate(forward_batch.lora_paths):\n@@ -194,13 +205,18 @@ class LoRAManager:\n                 if forward_batch.forward_mode.is_extend()\n                 else torch.ones(bs, device=self.device)\n             )\n-            seg_indptr = torch.zeros((bs + 1,), dtype=torch.int32, device=self.device)\n+            # Avoid unnecessary zero-initialization for seg_indptr; only the\n+            # first element needs to be zero, and the rest are written by cumsum.\n+            seg_indptr = torch.empty((bs + 1,), dtype=torch.int32, device=self.device)\n+            seg_indptr[0] = 0\n             seg_indptr[1:] = torch.cumsum(seg_lens, dim=0)\n             max_len = int(torch.max(seg_lens))\n             weight_indices = torch.empty((bs,), dtype=torch.int64, device=self.device)\n \n+            # Use narrower dtype for ranks to reduce memory traffic and align\n+            # with CUDA-graph path (int32).\n             lora_ranks = torch.zeros(\n-                (self.max_loras_per_batch,), dtype=torch.int64, device=\"cuda\"\n+                (self.max_loras_per_batch,), dtype=torch.int32, device=\"cuda\"\n             )\n             scalings = torch.zeros(\n                 (self.max_loras_per_batch,), dtype=torch.float, device=\"cuda\"\ndiff --git a/python/sglang/srt/lora/mem_pool.py b/python/sglang/srt/lora/mem_pool.py\nindex 8b8d21332..7d481f821 100644\n--- a/python/sglang/srt/lora/mem_pool.py\n+++ b/python/sglang/srt/lora/mem_pool.py\n@@ -129,15 +129,17 @@ class LoRAMemoryPool:\n     ):\n \n         def get_available_buffer_slot():\n-            for buffer_id in range(self.max_loras_per_batch):\n-                # Prioritize empty slots\n-                if self.buffer_id_to_uid[buffer_id] == \"\":\n-                    return buffer_id, \"\"\n-\n-            for buffer_id in range(self.max_loras_per_batch):\n-                # Evict unneeded lora\n-                if self.buffer_id_to_uid[buffer_id] not in cur_uids:\n-                    return buffer_id, self.buffer_id_to_uid[buffer_id]\n+            # Fast path: try to reuse an empty slot if available.\n+            try:\n+                empty_idx = self.buffer_id_to_uid.index(\"\")\n+                return empty_idx, \"\"\n+            except ValueError:\n+                pass\n+\n+            # Otherwise, evict the first slot not used by current batch.\n+            for buffer_id, uid in enumerate(self.buffer_id_to_uid):\n+                if uid not in cur_uids:\n+                    return buffer_id, uid\n \n             raise ValueError(\n                 \"No available buffer slots found. Please ensure the number of active loras is less than max_loras_per_batch.\"\n", "model_name_or_path": "gpt-5-2025-08-07"}
