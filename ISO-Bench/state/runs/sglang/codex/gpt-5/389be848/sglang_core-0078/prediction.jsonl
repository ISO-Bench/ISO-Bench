{"instance_id": "/home/ubuntu/OmniPerf-Bench/sglan__repo-2b3bdc9", "model_patch": "diff --git a/benchmark/benchmark_batch/benchmark_batch.py b/benchmark/benchmark_batch/benchmark_batch.py\nnew file mode 100644\nindex 000000000..2a748ceca\n--- /dev/null\n+++ b/benchmark/benchmark_batch/benchmark_batch.py\n@@ -0,0 +1,59 @@\n+import concurrent.futures\n+import os\n+import random\n+import time\n+from concurrent.futures import ThreadPoolExecutor\n+from statistics import mean\n+\n+import requests\n+\n+# Simple HTTP benchmark that sends batched prompts to a running server.\n+# This does NOT run as part of unit tests, but is helpful to quickly\n+# validate end-to-end throughput improvements when batch tokenization\n+# is enabled in the server.\n+\n+\n+ENDPOINT_URL = os.environ.get(\"SGLANG_ENDPOINT\", \"http://127.0.0.1:30000\")\n+\n+\n+def _mk_prompt(n_tokens: int) -> str:\n+    word = \"lorem\"\n+    return (word + \" \") * n_tokens\n+\n+\n+def _post_batch(batch):\n+    payload = {\n+        \"text\": batch,\n+        \"sampling_params\": {\"max_new_tokens\": 0},\n+        \"stream\": False,\n+    }\n+    r = requests.post(f\"{ENDPOINT_URL}/v1/generate\", json=payload, timeout=60)\n+    r.raise_for_status()\n+    return r.json()\n+\n+\n+def bench(num_requests=10, batch_size=8, num_tokens=512):\n+    prompts = [[_mk_prompt(num_tokens) for _ in range(batch_size)] for _ in range(num_requests)]\n+\n+    latencies = []\n+    start = time.time()\n+    with ThreadPoolExecutor(max_workers=min(8, num_requests)) as ex:\n+        futs = [ex.submit(_post_batch, b) for b in prompts]\n+        for f in futs:\n+            t0 = time.time()\n+            f.result()\n+            latencies.append(time.time() - t0)\n+    elapsed = time.time() - start\n+\n+    print(\n+        f\"Sent {num_requests} batches x {batch_size} prompts; total {num_requests*batch_size} requests\"\n+    )\n+    print(f\"Total time: {elapsed:.3f}s, avg per request: {mean(latencies):.4f}s\")\n+\n+\n+if __name__ == \"__main__\":\n+    nr = int(os.environ.get(\"NUM_REQUESTS\", 5))\n+    bs = int(os.environ.get(\"BATCH_SIZE\", 4))\n+    nt = int(os.environ.get(\"NUM_TOKENS\", 256))\n+    bench(nr, bs, nt)\n+\ndiff --git a/benchmark/benchmark_batch/benchmark_tokenizer.py b/benchmark/benchmark_batch/benchmark_tokenizer.py\nnew file mode 100644\nindex 000000000..b860845e4\n--- /dev/null\n+++ b/benchmark/benchmark_batch/benchmark_tokenizer.py\n@@ -0,0 +1,32 @@\n+import os\n+import time\n+from statistics import mean\n+\n+from transformers import AutoTokenizer\n+\n+\n+def bench_tokenizer(tokenizer_id: str, num_items=5000):\n+    tok = AutoTokenizer.from_pretrained(tokenizer_id)\n+    texts = [\"Hello world! This is a quick test.\" for _ in range(num_items)]\n+\n+    t0 = time.time()\n+    _ = [tok.encode(t) for t in texts]\n+    t_item = time.time() - t0\n+\n+    t0 = time.time()\n+    enc = tok(texts, add_special_tokens=False, padding=False)\n+    _ = enc[\"input_ids\"]\n+    t_batch = time.time() - t0\n+\n+    print(f\"Tokenizer: {tokenizer_id}\")\n+    print(f\"Per-item: {t_item:.4f}s for {num_items}\")\n+    print(f\"Batch:    {t_batch:.4f}s for {num_items}\")\n+    if t_batch > 0:\n+        print(\"Speedup x\", round(t_item / t_batch, 2))\n+\n+\n+if __name__ == \"__main__\":\n+    model = os.environ.get(\"TOKENIZER_ID\", \"gpt2\")\n+    n = int(os.environ.get(\"NUM_ITEMS\", 5000))\n+    bench_tokenizer(model, n)\n+\ndiff --git a/python/sglang/srt/managers/tokenizer_manager.py b/python/sglang/srt/managers/tokenizer_manager.py\nindex a391dd719..3d4f7b7e6 100644\n--- a/python/sglang/srt/managers/tokenizer_manager.py\n+++ b/python/sglang/srt/managers/tokenizer_manager.py\n@@ -212,6 +212,8 @@ class TokenizerManager:\n                     trust_remote_code=server_args.trust_remote_code,\n                     revision=server_args.revision,\n                 )\n+                # Reduce contention from HF tokenizers' internal thread pools\n+                os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n \n         # Store states\n         self.no_create_loop = False\n@@ -483,6 +485,144 @@ class TokenizerManager:\n \n         return tokenized_obj\n \n+    async def _batch_tokenize_requests(\n+        self, objs: List[Union[GenerateReqInput, EmbeddingReqInput]]\n+    ) -> List[Union[TokenizedGenerateReqInput, TokenizedEmbeddingReqInput]]:\n+        \"\"\"Batch tokenize a list of requests.\n+\n+        Uses fast tokenizer batch APIs to reduce per-request overhead.\n+        Falls back to pre-provided input_ids or input_embeds when present.\n+        \"\"\"\n+        # Preprocess multimodal data concurrently (may inject input ids)\n+        mm_tasks = [\n+            self.mm_processor.process_mm_data_async(\n+                obj.image_data,\n+                obj.text if obj.text is not None else obj.input_ids,\n+                obj,\n+                self.max_req_input_len,\n+            )\n+            for obj in objs\n+        ]\n+        image_inputs_list: List[Dict] = await asyncio.gather(*mm_tasks)\n+\n+        # Collect plain-text requests that need tokenization\n+        idxs_to_tokenize: List[int] = []\n+        texts: List[str] = []\n+        for i, (obj, mm_inp) in enumerate(zip(objs, image_inputs_list)):\n+            if obj.input_embeds is not None:\n+                continue\n+            if obj.input_ids is not None:\n+                continue\n+            if mm_inp and \"input_ids\" in mm_inp:\n+                continue\n+            if self.tokenizer is None:\n+                raise ValueError(\n+                    \"The engine initialized with skip_tokenizer_init=True cannot accept text prompts.\"\n+                )\n+            texts.append(obj.text)\n+            idxs_to_tokenize.append(i)\n+\n+        # Execute a single batch tokenization for all pending texts\n+        if texts:\n+            enc = self.tokenizer(texts, add_special_tokens=False, padding=False)\n+            tok_ids = enc[\"input_ids\"]\n+            for j, i in enumerate(idxs_to_tokenize):\n+                objs[i].input_ids = tok_ids[j]\n+\n+        # Build tokenized objects and validate\n+        out: List[Union[TokenizedGenerateReqInput, TokenizedEmbeddingReqInput]] = []\n+        for obj, mm_inp in zip(objs, image_inputs_list):\n+            input_text = obj.text\n+            input_embeds = None\n+            input_ids = None\n+\n+            if obj.input_embeds is not None:\n+                if self.is_generation:\n+                    raise ValueError(\n+                        \"The generation server does not work with input_embeds. Please use the embedding server.\"\n+                    )\n+                input_embeds = obj.input_embeds\n+                input_ids = obj.input_ids\n+            elif obj.input_ids is not None:\n+                input_ids = obj.input_ids\n+            else:\n+                # If multimodal processor provided ids, use them\n+                if mm_inp and \"input_ids\" in mm_inp:\n+                    input_ids = mm_inp[\"input_ids\"]\n+                else:\n+                    input_ids = obj.input_ids\n+\n+            # Merge multimodal inputs\n+            image_inputs: Dict = mm_inp\n+            if image_inputs and \"input_ids\" in image_inputs:\n+                input_ids = image_inputs[\"input_ids\"]\n+\n+            # Generation-specific\n+            if self.is_generation:\n+                return_logprob = obj.return_logprob\n+                logprob_start_len = obj.logprob_start_len\n+                top_logprobs_num = obj.top_logprobs_num\n+                token_ids_logprob = obj.token_ids_logprob\n+                session_params = (\n+                    SessionParams(**obj.session_params)\n+                    if obj.session_params\n+                    else None\n+                )\n+\n+            # Context length check\n+            input_token_num = len(input_ids) if input_ids is not None else 0\n+            if input_token_num >= self.context_len:\n+                raise ValueError(\n+                    f\"The input ({input_token_num} tokens) is longer than the model's context length ({self.context_len} tokens).\"\n+                )\n+\n+            if (\n+                obj.sampling_params.get(\"max_new_tokens\") is not None\n+                and obj.sampling_params.get(\"max_new_tokens\") + input_token_num\n+                >= self.context_len\n+            ):\n+                raise ValueError(\n+                    \"Requested token count exceeds the model's maximum context length\"\n+                )\n+\n+            # Sampling params\n+            sampling_params = SamplingParams(**obj.sampling_params)\n+            sampling_params.normalize(self.tokenizer)\n+            sampling_params.verify()\n+\n+            if isinstance(obj, GenerateReqInput):\n+                tokenized_obj = TokenizedGenerateReqInput(\n+                    obj.rid,\n+                    input_text,\n+                    input_ids,\n+                    image_inputs,\n+                    sampling_params,\n+                    return_logprob,\n+                    logprob_start_len,\n+                    top_logprobs_num,\n+                    token_ids_logprob,\n+                    obj.stream,\n+                    bootstrap_host=obj.bootstrap_host,\n+                    bootstrap_room=obj.bootstrap_room,\n+                    lora_path=obj.lora_path,\n+                    input_embeds=input_embeds,\n+                    session_params=session_params,\n+                    custom_logit_processor=obj.custom_logit_processor,\n+                    return_hidden_states=obj.return_hidden_states,\n+                )\n+            else:\n+                tokenized_obj = TokenizedEmbeddingReqInput(\n+                    obj.rid,\n+                    input_text,\n+                    input_ids,\n+                    image_inputs,\n+                    sampling_params,\n+                )\n+\n+            out.append(tokenized_obj)\n+\n+        return out\n+\n     def _send_one_request(\n         self,\n         obj: Union[GenerateReqInput, EmbeddingReqInput],\n@@ -561,13 +701,22 @@ class TokenizerManager:\n         generators = []\n         rids = []\n         if getattr(obj, \"parallel_sample_num\", 1) == 1:\n-            # Send all requests\n-            for i in range(batch_size):\n-                tmp_obj = obj[i]\n-                tokenized_obj = await self._tokenize_one_request(tmp_obj)\n-                self._send_one_request(tmp_obj, tokenized_obj, created_time)\n-                generators.append(self._wait_one_response(tmp_obj, request))\n-                rids.append(tmp_obj.rid)\n+            if getattr(self.server_args, \"disable_batch_tokenization\", False):\n+                # Send all requests one by one (legacy behavior)\n+                for i in range(batch_size):\n+                    tmp_obj = obj[i]\n+                    tokenized_obj = await self._tokenize_one_request(tmp_obj)\n+                    self._send_one_request(tmp_obj, tokenized_obj, created_time)\n+                    generators.append(self._wait_one_response(tmp_obj, request))\n+                    rids.append(tmp_obj.rid)\n+            else:\n+                # Fast path: batch tokenization to reduce overhead per request\n+                tmp_objs = [obj[i] for i in range(batch_size)]\n+                tokenized_list = await self._batch_tokenize_requests(tmp_objs)\n+                for tmp_obj, tokenized_obj in zip(tmp_objs, tokenized_list):\n+                    self._send_one_request(tmp_obj, tokenized_obj, created_time)\n+                    generators.append(self._wait_one_response(tmp_obj, request))\n+                    rids.append(tmp_obj.rid)\n         else:\n             # FIXME: When using batch and parallel_sample_num together, the perf is not optimal.\n             if batch_size > 128:\n@@ -577,10 +726,14 @@ class TokenizerManager:\n                     \"many threads to send them one by one with parallel sampling (n > 1).\"\n                 )\n \n-            # Tokenize all requests\n+            # Tokenize all requests (use batched path when possible)\n             objs = [obj[i] for i in range(batch_size)]\n-            tokenized_objs = await asyncio.gather(\n-                *(self._tokenize_one_request(obj) for obj in objs)\n+            tokenized_objs = (\n+                await self._batch_tokenize_requests(objs)\n+                if not getattr(self.server_args, \"disable_batch_tokenization\", False)\n+                else await asyncio.gather(\n+                    *(self._tokenize_one_request(o) for o in objs)\n+                )\n             )\n \n             # Cache the common prefix for parallel sampling\ndiff --git a/python/sglang/srt/server_args.py b/python/sglang/srt/server_args.py\nindex e1768b52e..f2f6f0947 100644\n--- a/python/sglang/srt/server_args.py\n+++ b/python/sglang/srt/server_args.py\n@@ -185,6 +185,8 @@ class ServerArgs:\n     n_share_experts_fusion: int = 0\n     disable_chunked_prefix_cache: bool = False\n     disable_fast_image_processor: bool = False\n+    # Tokenizer optimizations\n+    disable_batch_tokenization: bool = False\n \n     # Debug tensor dumps\n     debug_tensor_dump_output_folder: Optional[str] = None\n@@ -1139,6 +1141,11 @@ class ServerArgs:\n             action=\"store_true\",\n             help=\"Adopt base image processor instead of fast image processor.\",\n         )\n+        parser.add_argument(\n+            \"--disable-batch-tokenization\",\n+            action=\"store_true\",\n+            help=\"Disable batched tokenization in the tokenizer manager.\",\n+        )\n \n         # Server warmups\n         parser.add_argument(\n", "model_name_or_path": "gpt-5-2025-08-07"}
