{"instance_id": "/home/ubuntu/OmniPerf-Bench/sglan__repo-ef21729", "model_patch": "diff --git a/python/sglang/srt/managers/expert_location.py b/python/sglang/srt/managers/expert_location.py\nindex 615e0a440..51f29a3e0 100644\n--- a/python/sglang/srt/managers/expert_location.py\n+++ b/python/sglang/srt/managers/expert_location.py\n@@ -26,6 +26,7 @@ from sglang.srt.configs.model_config import ModelConfig\n from sglang.srt.managers import eplb_algorithms\n from sglang.srt.model_loader import get_model_architecture\n from sglang.srt.server_args import ServerArgs\n+from sglang.srt.managers.schedule_batch import global_server_args_dict\n \n logger = logging.getLogger(__name__)\n \n@@ -35,7 +36,8 @@ class ExpertLocationMetadata:\n     physical_to_logical_map: torch.Tensor  # (layers, num_physical_experts)\n     logical_to_all_physical_map: torch.Tensor  # (layers, num_logical_experts, X)\n     logical_to_all_physical_map_num_valid: torch.Tensor  # (layers, num_logical_experts)\n-    logical_to_rank_dispatch_physical_map: torch.Tensor  # (layers, num_logical_experts)\n+    # (layers, num_logical_experts). Optional when dispatch is non-static\n+    logical_to_rank_dispatch_physical_map: Optional[torch.Tensor]\n \n     # -------------------------------- properties ------------------------------------\n \n@@ -70,11 +72,8 @@ class ExpertLocationMetadata:\n         num_layers_2, num_logical_experts_1 = (\n             self.logical_to_all_physical_map_num_valid.shape\n         )\n-        num_layers_3, num_logical_experts_2 = (\n-            self.logical_to_rank_dispatch_physical_map.shape\n-        )\n-        assert num_layers_0 == num_layers_1 == num_layers_2 == num_layers_3\n-        assert num_logical_experts_0 == num_logical_experts_1 == num_logical_experts_2\n+        assert num_layers_0 == num_layers_1 == num_layers_2\n+        assert num_logical_experts_0 == num_logical_experts_1\n         assert num_physical_experts_0 == num_physical_experts_1\n \n     # -------------------------------- construction ------------------------------------\n@@ -200,17 +199,23 @@ class ExpertLocationMetadata:\n             logical_to_all_physical_map != -1, dim=-1\n         )\n \n-        return ExpertLocationMetadata(\n-            physical_to_logical_map=physical_to_logical_map,\n-            logical_to_all_physical_map=logical_to_all_physical_map_padded,\n-            logical_to_all_physical_map_num_valid=logical_to_all_physical_map_num_valid,\n-            logical_to_rank_dispatch_physical_map=compute_logical_to_rank_dispatch_physical_map(\n+        # Only precompute the rank dispatch map for static algorithm\n+        ep_dispatch_algorithm = global_server_args_dict.get(\"ep_dispatch_algorithm\")\n+        logical_to_rank_dispatch_physical_map = None\n+        if ep_dispatch_algorithm == \"static\":\n+            logical_to_rank_dispatch_physical_map = compute_logical_to_rank_dispatch_physical_map(\n                 logical_to_all_physical_map=logical_to_all_physical_map,\n                 num_gpus=ep_size,\n                 num_physical_experts=num_physical_experts,\n                 # TODO improve when we have real EP rank\n                 ep_rank=torch.distributed.get_rank() % ep_size,\n-            ),\n+            )\n+\n+        return ExpertLocationMetadata(\n+            physical_to_logical_map=physical_to_logical_map,\n+            logical_to_all_physical_map=logical_to_all_physical_map_padded,\n+            logical_to_all_physical_map_num_valid=logical_to_all_physical_map_num_valid,\n+            logical_to_rank_dispatch_physical_map=logical_to_rank_dispatch_physical_map,\n         )\n \n     # -------------------------------- mutation ------------------------------------\n@@ -230,8 +235,15 @@ class ExpertLocationMetadata:\n             \"logical_to_all_physical_map_num_valid\",\n             \"logical_to_rank_dispatch_physical_map\",\n         ]:\n+            src = getattr(other, field)\n             dst = getattr(self, field)\n-            dst[...] = getattr(other, field)\n+            if (dst is None) or (src is None):\n+                # Either both are None or src is intentionally omitted\n+                assert (dst is None) or (src is None)\n+                # Keep existing dst if src is None; not used in non-static modes\n+                continue\n+            # In-place copy to preserve storage\n+            dst[...] = src\n \n     # -------------------------------- usage ------------------------------------\n \n@@ -305,49 +317,56 @@ def compute_logical_to_rank_dispatch_physical_map(\n     ep_rank: int,\n     seed: int = 42,\n ):\n+    # Optimized to compute only the slice needed for ep_rank\n     r = random.Random(seed)\n \n     num_local_physical_experts = num_physical_experts // num_gpus\n     num_layers, num_logical_experts, _ = logical_to_all_physical_map.shape\n     dtype = logical_to_all_physical_map.dtype\n \n-    logical_to_rank_dispatch_physical_map = torch.full(\n-        size=(num_gpus, num_layers, num_logical_experts),\n-        fill_value=-1,\n-        dtype=dtype,\n-    )\n-\n+    # Collect results in Python list to reduce tensor indexing overhead\n+    result = []\n     for layer_id in range(num_layers):\n+        row = []\n         for logical_expert_id in range(num_logical_experts):\n             candidate_physical_expert_ids = _logical_to_all_physical_raw(\n                 logical_to_all_physical_map, layer_id, logical_expert_id\n             )\n-            output_partial = logical_to_rank_dispatch_physical_map[\n-                :, layer_id, logical_expert_id\n-            ]\n-\n-            for gpu_id in range(num_gpus):\n-                same_gpu_physical_expert_ids = [\n-                    physical_expert_id\n-                    for physical_expert_id in candidate_physical_expert_ids\n-                    if _compute_gpu_id_of_physical_expert(\n-                        physical_expert_id, num_local_physical_experts\n-                    )\n-                    == gpu_id\n-                ]\n-                if len(same_gpu_physical_expert_ids) > 0:\n-                    output_partial[gpu_id] = same_gpu_physical_expert_ids[0]\n-\n-            num_remain = torch.sum(output_partial == -1).item()\n-            output_partial[output_partial == -1] = torch.tensor(\n-                _fair_choices(candidate_physical_expert_ids, k=num_remain, r=r),\n-                dtype=dtype,\n+\n+            # Determine GPUs that already have local candidates\n+            gpus_with_local = set(\n+                _compute_gpu_id_of_physical_expert(x, num_local_physical_experts)\n+                for x in candidate_physical_expert_ids\n             )\n \n-    assert torch.all(logical_to_rank_dispatch_physical_map != -1)\n+            if ep_rank in gpus_with_local:\n+                # Pick the first candidate on the same GPU\n+                for pid in candidate_physical_expert_ids:\n+                    if (\n+                        _compute_gpu_id_of_physical_expert(\n+                            pid, num_local_physical_experts\n+                        )\n+                        == ep_rank\n+                    ):\n+                        row.append(pid)\n+                        break\n+                # Advance RNG state to keep deterministic behavior across layers\n+                num_remain = num_gpus - len(gpus_with_local)\n+                if num_remain > 0:\n+                    _fair_choices(candidate_physical_expert_ids, k=num_remain, r=r)\n+            else:\n+                # Compute fair choices for remaining GPUs and select the element for ep_rank\n+                num_remain = num_gpus - len(gpus_with_local)\n+                ans = _fair_choices(\n+                    candidate_physical_expert_ids, k=num_remain, r=r\n+                )\n+                # Position of ep_rank among remaining GPUs in ascending order\n+                pos = sum((g not in gpus_with_local) for g in range(ep_rank))\n+                row.append(ans[pos])\n+        result.append(row)\n \n     device = logical_to_all_physical_map.device\n-    return logical_to_rank_dispatch_physical_map[ep_rank, :, :].to(device)\n+    return torch.tensor(result, dtype=dtype, device=device)\n \n \n def _logical_to_all_physical_raw(\ndiff --git a/python/sglang/srt/managers/expert_location_dispatch.py b/python/sglang/srt/managers/expert_location_dispatch.py\nindex 6880b01a2..d9b9e3dde 100644\n--- a/python/sglang/srt/managers/expert_location_dispatch.py\n+++ b/python/sglang/srt/managers/expert_location_dispatch.py\n@@ -23,9 +23,9 @@ from sglang.srt.managers.schedule_batch import global_server_args_dict\n \n @dataclass\n class ExpertLocationDispatchInfo:\n-    ep_dispatch_algorithm: Literal[\"static\", \"random\"]\n+    ep_dispatch_algorithm: Literal[\"static\", \"dynamic\", \"fake\"]\n     # (num_logical_experts,)\n-    partial_logical_to_rank_dispatch_physical_map: torch.Tensor\n+    partial_logical_to_rank_dispatch_physical_map: Optional[torch.Tensor]\n     # (num_logical_experts, X)\n     partial_logical_to_all_physical_map: torch.Tensor\n     # (num_logical_experts,)\n@@ -42,9 +42,13 @@ class ExpertLocationDispatchInfo:\n \n         return cls(\n             ep_dispatch_algorithm=ep_dispatch_algorithm,\n-            partial_logical_to_rank_dispatch_physical_map=expert_location_metadata.logical_to_rank_dispatch_physical_map[\n-                layer_id, :\n-            ],\n+            partial_logical_to_rank_dispatch_physical_map=(\n+                expert_location_metadata.logical_to_rank_dispatch_physical_map[\n+                    layer_id, :\n+                ]\n+                if ep_dispatch_algorithm == \"static\"\n+                else None\n+            ),\n             partial_logical_to_all_physical_map=expert_location_metadata.logical_to_all_physical_map[\n                 layer_id, :\n             ],\n@@ -63,7 +67,8 @@ def transform_select_experts_inputs(\n     if (info is not None) and (info.ep_dispatch_algorithm == \"fake\"):\n         router_logits = torch.randn_like(router_logits)\n         if correction_bias is not None:\n-            correction_bias = torch.zeros_like(correction_bias)\n+            # Zero in-place to avoid extra allocation\n+            correction_bias.zero_()\n     return router_logits, correction_bias\n \n \n@@ -83,6 +88,7 @@ def topk_ids_logical_to_physical(\n def _topk_ids_logical_to_physical_static(\n     topk_ids: torch.Tensor, info: Optional[ExpertLocationDispatchInfo]\n ) -> torch.Tensor:\n+    assert info is not None and info.partial_logical_to_rank_dispatch_physical_map is not None\n     return info.partial_logical_to_rank_dispatch_physical_map[topk_ids]\n \n \n", "model_name_or_path": "gpt-5-2025-08-07"}
