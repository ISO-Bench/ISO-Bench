{"instance_id": "/home/ubuntu/OmniPerf-Bench/sglan__repo-148254d", "model_patch": "diff --git a/python/sglang/srt/managers/io_struct.py b/python/sglang/srt/managers/io_struct.py\nindex 13eb233bd..1aae28b00 100644\n--- a/python/sglang/srt/managers/io_struct.py\n+++ b/python/sglang/srt/managers/io_struct.py\n@@ -426,8 +426,7 @@ class UpdateWeightsFromDistributedReqOutput:\n \n @dataclass\n class UpdateWeightsFromTensorReqInput:\n-    name: str\n-    tensor: torch.Tensor\n+    serialized_named_tensors: bytes  # indeed Dict[str, torch.Tensor]\n \n \n @dataclass\ndiff --git a/python/sglang/srt/managers/tp_worker.py b/python/sglang/srt/managers/tp_worker.py\nindex c8e14a746..c7500f1d2 100644\n--- a/python/sglang/srt/managers/tp_worker.py\n+++ b/python/sglang/srt/managers/tp_worker.py\n@@ -30,7 +30,7 @@ from sglang.srt.managers.schedule_batch import ModelWorkerBatch, global_server_a\n from sglang.srt.model_executor.forward_batch_info import ForwardBatch\n from sglang.srt.model_executor.model_runner import ModelRunner\n from sglang.srt.server_args import ServerArgs\n-from sglang.srt.utils import broadcast_pyobj, set_random_seed\n+from sglang.srt.utils import MultiprocessingSerializer, broadcast_pyobj, set_random_seed\n \n logger = logging.getLogger(__name__)\n \n@@ -196,10 +196,19 @@ class TpModelWorker:\n         return success, message\n \n     def update_weights_from_tensor(self, recv_req: UpdateWeightsFromTensorReqInput):\n-        success, message = self.model_runner.update_weights_from_tensor(\n-            recv_req.name, recv_req.tensor\n+        named_tensors = MultiprocessingSerializer.loads_named_tensors(\n+            recv_req.serialized_named_tensors\n         )\n-        return success, message\n+        # Support updating one or multiple tensors in a single request\n+        last_success, last_message = True, \"Success\"\n+        for name, tensor in named_tensors.items():\n+            success, message = self.model_runner.update_weights_from_tensor(\n+                name, tensor\n+            )\n+            if not success:\n+                return success, message\n+            last_success, last_message = success, message\n+        return last_success, last_message\n \n     def get_weights_by_name(self, recv_req: GetWeightsByNameReqInput):\n         parameter = self.model_runner.get_weights_by_name(\ndiff --git a/python/sglang/srt/model_executor/model_runner.py b/python/sglang/srt/model_executor/model_runner.py\nindex 786f654de..453a64ee1 100644\n--- a/python/sglang/srt/model_executor/model_runner.py\n+++ b/python/sglang/srt/model_executor/model_runner.py\n@@ -404,16 +404,14 @@ class ModelRunner:\n             dtype: the data type of the parameter to be updated.\n             shape: the shape of the parameter to be updated.\n         \"\"\"\n-        target_dtype = (\n-            dtype if isinstance(dtype, torch.dtype) else getattr(torch, dtype)\n-        )\n-        current_dtype = self.dtype if isinstance(self.dtype, str) else self.dtype\n+        target_dtype = dtype if isinstance(dtype, torch.dtype) else getattr(torch, dtype)\n \n         assert (\n             self._model_update_group is not None\n         ), \"model update group must be initialized\"\n \n         try:\n+            # Allocate directly on target device to avoid an extra copy\n             weights = torch.empty(shape, dtype=target_dtype, device=self.device)\n             torch.distributed.broadcast(weights, src=0, group=self._model_update_group)\n             self.model.load_weights([(name, weights)])\ndiff --git a/python/sglang/srt/server.py b/python/sglang/srt/server.py\nindex d95ce5931..a98b1f494 100644\n--- a/python/sglang/srt/server.py\n+++ b/python/sglang/srt/server.py\n@@ -875,8 +875,10 @@ class Engine:\n         )\n \n     def update_weights_from_tensor(self, name, tensor):\n-        \"\"\"Update weights from distributed source.\"\"\"\n-        obj = UpdateWeightsFromTensorReqInput(name=name, tensor=tensor)\n+        \"\"\"Update weights from tensor in current process via fast serialization.\"\"\"\n+        from sglang.srt.utils import MultiprocessingSerializer\n+        payload = MultiprocessingSerializer.dumps_named_tensors({name: tensor})\n+        obj = UpdateWeightsFromTensorReqInput(serialized_named_tensors=payload)\n         loop = asyncio.get_event_loop()\n         return loop.run_until_complete(\n             tokenizer_manager.update_weights_from_tensor(obj, None)\ndiff --git a/python/sglang/srt/utils.py b/python/sglang/srt/utils.py\nindex 7c3efa9a2..be016b601 100644\n--- a/python/sglang/srt/utils.py\n+++ b/python/sglang/srt/utils.py\n@@ -718,9 +718,8 @@ def broadcast_pyobj(\n         else:\n             serialized_data = pickle.dumps(data)\n             size = len(serialized_data)\n-            tensor_data = torch.ByteTensor(\n-                np.frombuffer(serialized_data, dtype=np.uint8)\n-            )\n+            # Avoid intermediate NumPy copy by leveraging buffer protocol\n+            tensor_data = torch.frombuffer(memoryview(serialized_data), dtype=torch.uint8).clone()\n             tensor_size = torch.tensor([size], dtype=torch.long)\n \n             dist.broadcast(tensor_size, src=0, group=dist_group)\n@@ -737,10 +736,41 @@ def broadcast_pyobj(\n         tensor_data = torch.empty(size, dtype=torch.uint8)\n         dist.broadcast(tensor_data, src=0, group=dist_group)\n \n-        serialized_data = bytes(tensor_data.cpu().numpy())\n+        # Convert tensor to bytes without routing through NumPy\n+        serialized_data = memoryview(tensor_data.cpu()).tobytes()\n         data = pickle.loads(serialized_data)\n         return data\n \n+class MultiprocessingSerializer:\n+    \"\"\"Fast serializer for cross-process transfer of named tensors.\n+\n+    This avoids pickling Torch tensors directly across processes and instead\n+    serializes a Dict[str, torch.Tensor] to bytes via torch.save/torch.load.\n+    Tensors are moved to CPU and made contiguous before serialization to\n+    reduce device sync and copy overhead.\n+    \"\"\"\n+\n+    @staticmethod\n+    def dumps_named_tensors(named_tensors: Dict[str, torch.Tensor]) -> bytes:\n+        cpu_named: Dict[str, torch.Tensor] = {}\n+        for k, v in named_tensors.items():\n+            t = v.detach()\n+            if t.device.type != \"cpu\":\n+                t = t.to(\"cpu\")\n+            if not t.is_contiguous():\n+                t = t.contiguous()\n+            cpu_named[k] = t\n+        buf = BytesIO()\n+        torch.save(cpu_named, buf)\n+        return buf.getvalue()\n+\n+    @staticmethod\n+    def loads_named_tensors(data: bytes) -> Dict[str, torch.Tensor]:\n+        buf = BytesIO(data)\n+        named = torch.load(buf)\n+        assert isinstance(named, dict), \"Deserialized object must be a dict[str, Tensor]\"\n+        return named\n+\n \n step_counter = 0\n \n", "model_name_or_path": "gpt-5-2025-08-07"}
