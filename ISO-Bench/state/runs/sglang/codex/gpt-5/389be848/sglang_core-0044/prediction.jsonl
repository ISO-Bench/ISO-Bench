{"instance_id": "/home/ubuntu/OmniPerf-Bench/sglan__repo-db45276", "model_patch": "diff --git a/python/sglang/srt/layers/attention/flashattention_backend.py b/python/sglang/srt/layers/attention/flashattention_backend.py\nindex 62604fe56..d405b6dea 100644\n--- a/python/sglang/srt/layers/attention/flashattention_backend.py\n+++ b/python/sglang/srt/layers/attention/flashattention_backend.py\n@@ -1,9 +1,5 @@\n from __future__ import annotations\n \n-import numpy as np\n-\n-from sglang.srt.speculative.eagle_utils import EagleDraftInput, EagleVerifyInput\n-\n \"\"\"\n Support different attention backends.\n Now there are three backends: FlashInfer, Triton and FlashAttention.\n@@ -13,12 +9,14 @@ Each backend supports two operators: extend (i.e. prefill with cached prefix) an\n from dataclasses import dataclass\n from typing import TYPE_CHECKING, Optional, Union\n \n+import numpy as np\n import torch\n \n from sglang.srt.configs.model_config import AttentionArch\n from sglang.srt.layers.attention.base_attn_backend import AttentionBackend\n from sglang.srt.managers.schedule_batch import global_server_args_dict\n from sglang.srt.model_executor.forward_batch_info import ForwardBatch, ForwardMode\n+from sglang.srt.speculative.eagle_utils import EagleDraftInput, EagleVerifyInput\n \n if TYPE_CHECKING:\n     from sglang.srt.layers.radix_attention import RadixAttention\n@@ -720,13 +718,13 @@ class FlashAttentionBackend(AttentionBackend):\n         \"\"\"\n         self.decode_cuda_graph_metadata = {\n             # Page table for token mapping (batch_size, max_context_len)\n-            \"page_table\": torch.zeros(\n+            \"page_table\": torch.empty(\n                 max_bs,\n                 (self.max_context_len + self.page_size - 1) // self.page_size,\n                 dtype=torch.int32,\n                 device=self.device,\n             ),\n-            \"page_table_draft_decode\": torch.zeros(\n+            \"page_table_draft_decode\": torch.empty(\n                 max_bs,\n                 (self.max_context_len + self.page_size - 1) // self.page_size,\n                 dtype=torch.int32,\n@@ -735,27 +733,28 @@ class FlashAttentionBackend(AttentionBackend):\n             \"strided_indices\": torch.arange(\n                 0, self.max_context_len, self.page_size, device=self.device\n             ),\n-            \"cache_seqlens\": torch.zeros(max_bs, dtype=torch.int32, device=self.device),\n+            \"cache_seqlens\": torch.empty(max_bs, dtype=torch.int32, device=self.device),\n             \"cu_seqlens_q\": torch.arange(\n                 0, max_bs + 128, dtype=torch.int32, device=self.device\n             ),\n-            \"cu_seqlens_k\": torch.zeros(\n+            \"cu_seqlens_k\": torch.empty(\n                 max_bs + 128, dtype=torch.int32, device=self.device\n             ),\n         }\n \n         self.target_verify_metadata = {\n-            \"page_table\": torch.zeros(\n+            \"page_table\": torch.empty(\n                 max_bs,\n                 (self.max_context_len + self.page_size - 1) // self.page_size,\n                 dtype=torch.int32,\n                 device=self.device,\n             ),\n-            \"cache_seqlens\": torch.zeros(max_bs, dtype=torch.int32, device=self.device),\n-            \"cu_seqlens_q\": torch.zeros(\n-                max_bs + 128, dtype=torch.int32, device=self.device\n+            \"cache_seqlens\": torch.empty(max_bs, dtype=torch.int32, device=self.device),\n+            # Use arange so capture-time slices are valid and deterministic\n+            \"cu_seqlens_q\": torch.arange(\n+                0, max_bs + 128, dtype=torch.int32, device=self.device\n             ),\n-            \"cu_seqlens_k\": torch.zeros(\n+            \"cu_seqlens_k\": torch.empty(\n                 max_bs + 128, dtype=torch.int32, device=self.device\n             ),\n             \"max_seqlen_q\": 0,\n@@ -783,6 +782,7 @@ class FlashAttentionBackend(AttentionBackend):\n                 metadata.cu_seqlens_q = torch.arange(\n                     0, bs + 1, dtype=torch.int32, device=device\n                 )\n+                # Reuse preallocated buffers to avoid allocations during capture\n                 metadata.cache_seqlens_int32 = self.decode_cuda_graph_metadata[\n                     \"cache_seqlens\"\n                 ][:bs]\n@@ -790,26 +790,26 @@ class FlashAttentionBackend(AttentionBackend):\n                 metadata.cu_seqlens_q = self.decode_cuda_graph_metadata[\"cu_seqlens_q\"][\n                     : bs + 1\n                 ]\n-\n-                metadata.cu_seqlens_k = torch.nn.functional.pad(\n-                    torch.cumsum(\n-                        metadata.cache_seqlens_int32, dim=0, dtype=torch.int32\n-                    ),\n-                    (1, 0),\n-                )\n+                # Point to a reusable buffer for cu_seqlens_k; contents will be\n+                # populated during replay via copy_.\n+                metadata.cu_seqlens_k = self.decode_cuda_graph_metadata[\n+                    \"cu_seqlens_k\"\n+                ][: (bs + 1)]\n                 metadata.max_seq_len_k = seq_lens.max().item() + (self.step_id + 1)\n                 metadata.page_table = self.decode_cuda_graph_metadata[\n                     \"page_table_draft_decode\"\n                 ][req_pool_indices, :]\n             else:\n                 # Normal Decode\n-                # Get sequence information\n-                metadata.cache_seqlens_int32 = seq_lens.to(torch.int32)\n+                # Get sequence information; reuse preallocated buffers\n+                metadata.cache_seqlens_int32 = self.decode_cuda_graph_metadata[\n+                    \"cache_seqlens\"\n+                ][:bs]\n                 batch_size = len(seq_lens)\n                 device = seq_lens.device\n-                metadata.cu_seqlens_k = torch.nn.functional.pad(\n-                    torch.cumsum(seq_lens, dim=0, dtype=torch.int32), (1, 0)\n-                )\n+                metadata.cu_seqlens_k = self.decode_cuda_graph_metadata[\n+                    \"cu_seqlens_k\"\n+                ][: (bs + 1)]\n                 # Precompute maximum sequence length\n                 metadata.max_seq_len_k = seq_lens.max().item()\n                 # Precompute page table\n@@ -906,14 +906,20 @@ class FlashAttentionBackend(AttentionBackend):\n                 ]\n \n                 metadata.page_table[:, : metadata.max_seq_len_k].copy_(page_table)\n+                # Ensure the unused region remains zero-initialized\n+                metadata.page_table[:, metadata.max_seq_len_k :].fill_(0)\n             else:\n                 # Normal Decode\n                 max_len = seq_lens_cpu.max().item()\n                 metadata.max_seq_len_k = max_len\n \n-                metadata.cache_seqlens_int32 = seq_lens.to(torch.int32)\n-                metadata.cu_seqlens_k = torch.nn.functional.pad(\n-                    torch.cumsum(seq_lens, dim=0, dtype=torch.int32), (1, 0)\n+                # Reuse preallocated buffers to avoid rebinds/allocations\n+                metadata.cache_seqlens_int32.copy_(seq_lens.to(torch.int32))\n+                metadata.cu_seqlens_k.copy_(\n+                    torch.nn.functional.pad(\n+                        torch.cumsum(metadata.cache_seqlens_int32, dim=0, dtype=torch.int32),\n+                        (1, 0),\n+                    )\n                 )\n \n                 max_seq_pages = (\n@@ -955,6 +961,8 @@ class FlashAttentionBackend(AttentionBackend):\n             )\n             page_table = self.req_to_token[req_pool_indices, : metadata.max_seq_len_k]\n             metadata.page_table[:, : metadata.max_seq_len_k].copy_(page_table)\n+            # Ensure the unused region is zero for safety\n+            metadata.page_table[:, metadata.max_seq_len_k :].fill_(0)\n \n         self.forward_metadata = metadata\n \n", "model_name_or_path": "gpt-5-2025-08-07"}
