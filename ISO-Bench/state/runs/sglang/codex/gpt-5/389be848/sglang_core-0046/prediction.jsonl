{"instance_id": "/home/ubuntu/OmniPerf-Bench/sglan__repo-005aad3", "model_patch": "diff --git a/python/sglang/srt/distributed/parallel_state.py b/python/sglang/srt/distributed/parallel_state.py\nindex fdde7dde8..da4c14fd0 100644\n--- a/python/sglang/srt/distributed/parallel_state.py\n+++ b/python/sglang/srt/distributed/parallel_state.py\n@@ -246,25 +246,34 @@ class GroupCoordinator:\n         self.use_xpu_communicator = use_xpu_communicator\n         self.use_message_queue_broadcaster = use_message_queue_broadcaster\n \n-        # lazy import to avoid documentation build error\n-        from sglang.srt.distributed.device_communicators.custom_all_reduce import (\n-            CustomAllreduce,\n-        )\n-        from sglang.srt.distributed.device_communicators.pynccl import (\n-            PyNcclCommunicator,\n-        )\n+        # Reusable small CPU buffer to avoid frequent tiny allocations\n+        # during object size send/recv in point-to-point ops.\n+        self._size_tensor_cpu = torch.empty(1, dtype=torch.long, device=\"cpu\")\n+\n+        # Lazy imports only when actually needed to avoid importing\n+        # heavy optional backends on CPU-only or single-process setups.\n \n-        self.pynccl_comm: Optional[PyNcclCommunicator] = None\n+        # PyNccl communicator\n+        self.pynccl_comm: Optional[Any] = None\n         if use_pynccl and self.world_size > 1:\n+            from sglang.srt.distributed.device_communicators.pynccl import (\n+                PyNcclCommunicator,\n+            )\n+\n             self.pynccl_comm = PyNcclCommunicator(\n                 group=self.cpu_group,\n                 device=self.device,\n             )\n \n-        self.ca_comm: Optional[CustomAllreduce] = None\n+        # Custom AllReduce\n+        self.ca_comm: Optional[Any] = None\n         if use_custom_allreduce and self.world_size > 1:\n             # Initialize a custom fast all-reduce implementation.\n             try:\n+                from sglang.srt.distributed.device_communicators.custom_all_reduce import (\n+                    CustomAllreduce,\n+                )\n+\n                 self.ca_comm = CustomAllreduce(\n                     group=self.cpu_group,\n                     device=self.device,\n@@ -275,28 +284,31 @@ class GroupCoordinator:\n                     \"warning, specify --disable-custom-all-reduce explicitly.\"\n                 )\n \n-        from sglang.srt.distributed.device_communicators.hpu_communicator import (\n-            HpuCommunicator,\n-        )\n-\n-        self.hpu_communicator: Optional[HpuCommunicator] = None\n+        # HPU communicator\n+        self.hpu_communicator: Optional[Any] = None\n         if use_hpu_communicator and self.world_size > 1:\n-            self.hpu_communicator = HpuCommunicator(group=self.device_group)\n+            from sglang.srt.distributed.device_communicators.hpu_communicator import (\n+                HpuCommunicator,\n+            )\n \n-        from sglang.srt.distributed.device_communicators.xpu_communicator import (\n-            XpuCommunicator,\n-        )\n+            self.hpu_communicator = HpuCommunicator(group=self.device_group)\n \n-        self.xpu_communicator: Optional[XpuCommunicator] = None\n+        # XPU communicator\n+        self.xpu_communicator: Optional[Any] = None\n         if use_xpu_communicator and self.world_size > 1:\n-            self.xpu_communicator = XpuCommunicator(group=self.device_group)\n+            from sglang.srt.distributed.device_communicators.xpu_communicator import (\n+                XpuCommunicator,\n+            )\n \n-        from sglang.srt.distributed.device_communicators.shm_broadcast import (\n-            MessageQueue,\n-        )\n+            self.xpu_communicator = XpuCommunicator(group=self.device_group)\n \n-        self.mq_broadcaster: Optional[MessageQueue] = None\n+        # Shared-memory broadcaster\n+        self.mq_broadcaster: Optional[Any] = None\n         if use_message_queue_broadcaster and self.world_size > 1:\n+            from sglang.srt.distributed.device_communicators.shm_broadcast import (\n+                MessageQueue,\n+            )\n+\n             self.mq_broadcaster = MessageQueue.create_from_process_group(\n                 self.cpu_group, 1 << 22, 6\n             )\n@@ -624,9 +636,8 @@ class GroupCoordinator:\n         # Serialize object to tensor and get the size as well\n         object_tensor = torch.frombuffer(pickle.dumps(obj), dtype=torch.uint8)\n \n-        size_tensor = torch.tensor(\n-            [object_tensor.numel()], dtype=torch.long, device=\"cpu\"\n-        )\n+        size_tensor = self._size_tensor_cpu\n+        size_tensor[0] = object_tensor.numel()\n \n         # Send object size\n \n@@ -647,7 +658,7 @@ class GroupCoordinator:\n             src != self.rank_in_group\n         ), \"Invalid source rank. Source rank is the same as the current rank.\"\n \n-        size_tensor = torch.empty(1, dtype=torch.long, device=\"cpu\")\n+        size_tensor = self._size_tensor_cpu\n \n         # Receive object size\n         rank_size = torch.distributed.recv(\n@@ -1055,11 +1066,6 @@ def init_distributed_environment(\n             world_size=world_size,\n             rank=rank,\n             timeout=timeout,\n-            device_id=torch.device(\n-                f\"cuda:{torch.cuda.current_device()}\"\n-                if hasattr(torch, \"cuda\") and torch.cuda.is_available()\n-                else None\n-            ),  # Allow NCCL to eagerly init communicator\n         )\n \n     # set the local rank\n@@ -1289,8 +1295,8 @@ def in_the_same_node_as(pg: ProcessGroup, source_rank: int = 0) -> List[bool]:\n     rank = torch.distributed.get_rank(group=pg)\n     world_size = torch.distributed.get_world_size(group=pg)\n \n-    # local tensor in each process to store the result\n-    is_in_the_same_node = torch.tensor([0] * world_size, dtype=torch.int32)\n+    # local tensor in each process to store the result (avoid Python list init)\n+    is_in_the_same_node = torch.zeros(world_size, dtype=torch.int32)\n \n     # global ranks of the processes in the group\n     ranks = torch.distributed.get_process_group_ranks(pg)\n", "model_name_or_path": "gpt-5-2025-08-07"}
