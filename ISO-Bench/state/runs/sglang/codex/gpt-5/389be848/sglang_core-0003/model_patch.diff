diff --git a/python/sglang/srt/managers/scheduler.py b/python/sglang/srt/managers/scheduler.py
index 1d1cf3688..f404312f3 100644
--- a/python/sglang/srt/managers/scheduler.py
+++ b/python/sglang/srt/managers/scheduler.py
@@ -72,6 +72,7 @@ from sglang.srt.utils import (
     configure_logger,
     crash_on_warnings,
     get_zmq_socket,
+    gpu_proc_affinity,
     kill_parent_process,
     set_random_seed,
     suppress_other_loggers,
@@ -1393,6 +1394,13 @@ def run_scheduler_process(
     dp_rank: Optional[int],
     pipe_writer,
 ):
+    # Pin this scheduler process to a subset of CPU cores based on GPU id.
+    # This reduces cross-socket memory traffic and improves locality.
+    try:
+        gpu_proc_affinity(server_args.tp_size, server_args.nnodes, gpu_id)
+    except Exception:
+        # Affinity setup is best-effort; ignore failures on unsupported platforms.
+        pass
     # [For Router] if env var "DP_RANK" exist, set dp_rank to the value of the env var
     if dp_rank is None and "DP_RANK" in os.environ:
         dp_rank = int(os.environ["DP_RANK"])
diff --git a/python/sglang/srt/utils.py b/python/sglang/srt/utils.py
index e947d1a92..8a3380df0 100644
--- a/python/sglang/srt/utils.py
+++ b/python/sglang/srt/utils.py
@@ -35,6 +35,7 @@ from typing import Any, Callable, Dict, List, Optional, Protocol, Tuple, Union
 
 import numpy as np
 import psutil
+import functools
 import requests
 import torch
 import torch.distributed as dist
@@ -692,9 +693,7 @@ def broadcast_pyobj(
         else:
             serialized_data = pickle.dumps(data)
             size = len(serialized_data)
-            tensor_data = torch.ByteTensor(
-                np.frombuffer(serialized_data, dtype=np.uint8)
-            )
+            tensor_data = _bytes_to_uint8_tensor(serialized_data)
             tensor_size = torch.tensor([size], dtype=torch.long)
 
             dist.broadcast(tensor_size, src=0, group=dist_group)
@@ -716,6 +715,22 @@ def broadcast_pyobj(
         return data
 
 
+def _bytes_to_uint8_tensor(b: bytes) -> torch.Tensor:
+    """Convert bytes to a CPU uint8 tensor with minimal copies.
+
+    Uses torch.frombuffer when available; otherwise falls back to numpy.
+    """
+    try:
+        mv = memoryview(b)
+        t = torch.frombuffer(mv, dtype=torch.uint8)  # type: ignore[arg-type]
+        # keep a reference to avoid GC of the underlying buffer
+        t._mv_ref = mv  # type: ignore[attr-defined]
+        return t
+    except Exception:
+        arr = np.frombuffer(b, dtype=np.uint8)
+        return torch.from_numpy(arr)
+
+
 step_counter = 0
 
 
@@ -755,13 +770,8 @@ def first_rank_print(*args, **kwargs):
 
 
 def get_zmq_socket(context: zmq.Context, socket_type: zmq.SocketType, endpoint: str):
-    mem = psutil.virtual_memory()
-    total_mem = mem.total / 1024**3
-    available_mem = mem.available / 1024**3
-    if total_mem > 32 and available_mem > 16:
-        buf_size = int(0.5 * 1024**3)
-    else:
-        buf_size = -1
+    # Use cached buffer size heuristic to avoid repeated psutil calls.
+    buf_size = _zmq_default_buf_size()
 
     socket = context.socket(socket_type)
     if socket_type == zmq.PUSH:
@@ -778,6 +788,20 @@ def get_zmq_socket(context: zmq.Context, socket_type: zmq.SocketType, endpoint:
     return socket
 
 
+@functools.lru_cache(maxsize=1)
+def _zmq_default_buf_size() -> int:
+    """Heuristic for ZMQ socket buffer size; cached to avoid psutil overhead."""
+    try:
+        mem = psutil.virtual_memory()
+        total_mem_gb = mem.total / (1024**3)
+        available_mem_gb = mem.available / (1024**3)
+        if total_mem_gb > 32 and available_mem_gb > 16:
+            return int(0.5 * (1024**3))
+    except Exception:
+        pass
+    return -1
+
+
 def dump_to_file(dirpath, name, value):
     from vllm.distributed import get_tensor_model_parallel_rank
 
@@ -947,6 +971,104 @@ def get_device_name(device_id: int = 0) -> str:
         return torch.hpu.get_device_name(device_id)
 
 
+def _parse_cpulist(cpulist: str) -> List[int]:
+    """Parse Linux cpulist format like '0-3,8,10-12' -> [0,1,2,3,8,10,11,12]."""
+    res: List[int] = []
+    for part in filter(None, cpulist.split(",")):
+        if "-" in part:
+            a, b = part.split("-", 1)
+            start, end = int(a), int(b)
+            res.extend(range(start, end + 1))
+        else:
+            res.append(int(part))
+    return res
+
+
+def _read_numa_cpu_lists() -> List[List[int]]:
+    """Read Linux NUMA node CPU lists, return list of CPU id lists per node.
+
+    Falls back to a single group with all available CPUs when not available.
+    """
+    nodes: List[List[int]] = []
+    sysfs_nodes = "/sys/devices/system/node"
+    try:
+        if os.path.isdir(sysfs_nodes):
+            entries = sorted([d for d in os.listdir(sysfs_nodes) if d.startswith("node")])
+            for d in entries:
+                path = os.path.join(sysfs_nodes, d, "cpulist")
+                if not os.path.exists(path):
+                    continue
+                with open(path, "r") as f:
+                    cpulist = f.read().strip()
+                cpus = _parse_cpulist(cpulist)
+                if cpus:
+                    nodes.append(cpus)
+    except Exception:
+        nodes = []
+    if not nodes:
+        # Fallback to all allowed CPUs
+        try:
+            allowed = sorted(list(os.sched_getaffinity(0)))  # type: ignore[attr-defined]
+        except Exception:
+            allowed = list(range(psutil.cpu_count(logical=True) or 1))
+        nodes = [allowed]
+    return nodes
+
+
+def gpu_proc_affinity(tp_size: int, nnodes: Optional[int], gpu_id: int) -> None:
+    """Pin current process to a subset of CPUs for the given GPU.
+
+    - Splits available CPUs per NUMA node when possible for locality.
+    - Evenly partitions CPUs across the number of local GPUs.
+    - Best-effort; silently returns on unsupported platforms.
+    """
+    if os.getenv("SGLANG_DISABLE_CPU_AFFINITY", "").lower() in {"1", "true", "yes"}:
+        return
+
+    try:
+        local_gpus = torch.cuda.device_count() if torch.cuda.is_available() else 1
+        if local_gpus <= 0:
+            return
+
+        # Derive local GPU index (best-effort)
+        local_gpu_idx = gpu_id % max(1, local_gpus)
+
+        numa_nodes = _read_numa_cpu_lists()  # list of cpu-id lists per NUMA node
+        # Flatten NUMA nodes into a single ordered list while keeping node groups
+        per_node_cpus = [list(cpus) for cpus in numa_nodes if cpus]
+        all_cpus: List[int] = [cpu for group in per_node_cpus for cpu in group]
+        if not all_cpus:
+            return
+
+        # Partition CPUs across local GPUs. Prefer node-granularity if possible.
+        if len(per_node_cpus) >= local_gpus:
+            # Assign whole nodes to GPUs in order
+            groups: List[List[int]] = [[] for _ in range(local_gpus)]
+            for i, node in enumerate(per_node_cpus):
+                groups[i % local_gpus].extend(node)
+            target_cpus = sorted(set(groups[local_gpu_idx]))
+        else:
+            # Evenly slice all CPUs
+            chunk = max(1, len(all_cpus) // local_gpus)
+            start = local_gpu_idx * chunk
+            end = len(all_cpus) if local_gpu_idx == local_gpus - 1 else start + chunk
+            target_cpus = sorted(set(all_cpus[start:end]))
+
+        if not target_cpus:
+            return
+
+        p = psutil.Process(os.getpid())
+        p.cpu_affinity(target_cpus)
+        try:
+            # Limit torch intraop threads to stay within the CPU set
+            torch.set_num_threads(min(len(target_cpus), os.cpu_count() or 1))
+        except Exception:
+            pass
+    except Exception:
+        # Best effort only; ignore failures on unsupported systems
+        return
+
+
 sglang_lib = Library("sglang", "FRAGMENT")  # noqa
 
 
