{"instance_id": "/home/ubuntu/OmniPerf-Bench/sglan__repo-5d264a9", "model_patch": "diff --git a/benchmark/latency_throughput/bench_one.py b/benchmark/latency_throughput/bench_one.py\nindex b912d3a02..7bc1eca5d 100644\n--- a/benchmark/latency_throughput/bench_one.py\n+++ b/benchmark/latency_throughput/bench_one.py\n@@ -17,7 +17,13 @@ def run_one_batch_size(bs):\n \n     if args.input_len:\n         input_ids = [\n-            [int(x) for x in np.random.randint(0, high=16384, size=(args.input_len,))] for _ in range(bs)\n+            [\n+                int(x)\n+                for x in np.random.randint(\n+                    0, high=16384, size=(args.input_len,)\n+                )\n+            ]\n+            for _ in range(bs)\n         ]\n     else:\n         text = [f\"{i, }\" for i in range(bs)]\n@@ -116,9 +122,11 @@ if __name__ == \"__main__\":\n     parser.add_argument(\"--port\", type=int, default=None)\n     parser.add_argument(\"--backend\", type=str, default=\"srt\")\n     parser.add_argument(\"--input-len\", type=int, default=None)\n-    parser.add_argument(\"--batch-size\", type=int, nargs='*', default=[1])\n+    parser.add_argument(\"--batch-size\", type=int, nargs=\"*\", default=[1])\n     parser.add_argument(\"--max-tokens\", type=int, default=256)\n-    parser.add_argument(\"--vllm-model-name\", type=str, default=\"meta-llama/Meta-Llama-3-70B\")\n+    parser.add_argument(\n+        \"--vllm-model-name\", type=str, default=\"meta-llama/Meta-Llama-3-70B\"\n+    )\n     args = parser.parse_args()\n \n     if args.port is None:\ndiff --git a/python/sglang/srt/managers/controller/cuda_graph_runner.py b/python/sglang/srt/managers/controller/cuda_graph_runner.py\nindex 2e37e55b5..648462d69 100644\n--- a/python/sglang/srt/managers/controller/cuda_graph_runner.py\n+++ b/python/sglang/srt/managers/controller/cuda_graph_runner.py\n@@ -23,6 +23,7 @@ class CudaGraphRunner:\n \n         # Common inputs\n         self.max_bs = max_batch_size_to_capture\n+        # Use valid default values for warmup during graph capture\n         self.input_ids = torch.zeros((self.max_bs,), dtype=torch.int32, device=\"cuda\")\n         self.req_pool_indices = torch.zeros((self.max_bs,), dtype=torch.int32, device=\"cuda\")\n         self.seq_lens = torch.ones((self.max_bs,), dtype=torch.int32, device=\"cuda\")\n@@ -31,6 +32,7 @@ class CudaGraphRunner:\n \n         # FlashInfer inputs\n         self.flashinfer_workspace_buffer = self.model_runner.flashinfer_workspace_buffers[0]\n+        # Keep these zero-initialized to ensure safe values during capture warmup\n         self.flashinfer_kv_indptr = torch.zeros(\n             (self.max_bs + 1,), dtype=torch.int32, device=\"cuda\"\n         )\n@@ -132,9 +134,10 @@ class CudaGraphRunner:\n         index = bisect.bisect_left(self.batch_size_list, raw_bs)\n         bs = self.batch_size_list[index]\n         if bs != raw_bs:\n-            self.seq_lens.zero_()\n-            self.position_ids_offsets.fill_(1)\n-            self.out_cache_loc.zero_()\n+            # Only initialize the padded region to reduce unnecessary writes\n+            self.seq_lens[raw_bs:bs].zero_()\n+            self.position_ids_offsets[raw_bs:bs].fill_(1)\n+            self.out_cache_loc[raw_bs:bs].zero_()\n \n         # Common inputs\n         self.input_ids[:raw_bs] = batch.input_ids\ndiff --git a/python/sglang/srt/managers/controller/infer_batch.py b/python/sglang/srt/managers/controller/infer_batch.py\nindex d89e9786e..ade0e7b05 100644\n--- a/python/sglang/srt/managers/controller/infer_batch.py\n+++ b/python/sglang/srt/managers/controller/infer_batch.py\n@@ -781,8 +781,11 @@ class InputMetadata:\n                 device=\"cuda\",\n             )\n             extend_seq_lens = seq_lens - prefix_lens\n-            extend_start_loc = torch.zeros_like(seq_lens)\n-            extend_start_loc[1:] = torch.cumsum(extend_seq_lens[:-1], dim=0)\n+            extend_start_loc = torch.empty_like(seq_lens)\n+            if len(seq_lens) > 0:\n+                extend_start_loc[0] = 0\n+            if len(seq_lens) > 1:\n+                extend_start_loc[1:] = torch.cumsum(extend_seq_lens[:-1], dim=0)\n             extend_no_prefix = torch.all(prefix_lens == 0)\n             total_num_tokens = int(torch.sum(seq_lens))\n \n@@ -827,9 +830,9 @@ def init_flashinfer_args(forward_mode, model_runner, req_pool_indices, seq_lens,\n     else:\n         paged_kernel_lens = prefix_lens\n \n-    kv_indptr = torch.zeros(\n-        (batch_size + 1,), dtype=torch.int32, device=\"cuda\"\n-    )\n+    # Avoid zero-initialization; set head explicitly then cumsum\n+    kv_indptr = torch.empty((batch_size + 1,), dtype=torch.int32, device=\"cuda\")\n+    kv_indptr[0] = 0\n     kv_indptr[1:] = torch.cumsum(paged_kernel_lens, dim=0)\n     req_pool_indices_cpu = req_pool_indices.cpu().numpy()\n     paged_kernel_lens_cpu = paged_kernel_lens.cpu().numpy()\n@@ -859,9 +862,8 @@ def init_flashinfer_args(forward_mode, model_runner, req_pool_indices, seq_lens,\n         )\n     else:\n         # extend part\n-        qo_indptr = torch.zeros(\n-            (batch_size + 1,), dtype=torch.int32, device=\"cuda\"\n-        )\n+        qo_indptr = torch.empty((batch_size + 1,), dtype=torch.int32, device=\"cuda\")\n+        qo_indptr[0] = 0\n         qo_indptr[1:] = torch.cumsum(seq_lens - prefix_lens, dim=0)\n \n         model_runner.flashinfer_prefill_wrapper_ragged.end_forward()\n@@ -890,8 +892,11 @@ def init_flashinfer_args(forward_mode, model_runner, req_pool_indices, seq_lens,\n def init_triton_args(forward_mode, seq_lens, prefix_lens):\n     batch_size = len(seq_lens)\n     max_seq_len = int(torch.max(seq_lens))\n-    start_loc = torch.zeros((batch_size,), dtype=torch.int32, device=\"cuda\")\n-    start_loc[1:] = torch.cumsum(seq_lens[:-1], dim=0)\n+    start_loc = torch.empty((batch_size,), dtype=torch.int32, device=\"cuda\")\n+    if batch_size > 0:\n+        start_loc[0] = 0\n+    if batch_size > 1:\n+        start_loc[1:] = torch.cumsum(seq_lens[:-1], dim=0)\n \n     if forward_mode == ForwardMode.DECODE:\n         max_extend_len = None\ndiff --git a/python/sglang/srt/utils.py b/python/sglang/srt/utils.py\nindex 03a2d60ab..13a3ab4c8 100644\n--- a/python/sglang/srt/utils.py\n+++ b/python/sglang/srt/utils.py\n@@ -610,7 +610,8 @@ def receive_addrs(model_port_args, server_args):\n     )\n \n     for src_rank in range(1, server_args.nnodes):\n-        tensor = torch.zeros(4 + num_tp_ports, dtype=torch.int)\n+        # Allocate without zero fill; recv will overwrite all elements\n+        tensor = torch.empty(4 + num_tp_ports, dtype=torch.int)\n         dist.recv(tensor, src=src_rank)\n         ip = \".\".join([str(x) for x in tensor[:4].tolist()])\n         ports = tensor[4:].tolist()\n", "model_name_or_path": "gpt-5-2025-08-07"}
