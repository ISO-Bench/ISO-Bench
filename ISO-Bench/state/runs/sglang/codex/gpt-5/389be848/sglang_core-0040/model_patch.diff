diff --git a/python/sglang/srt/layers/attention/flashattention_backend.py b/python/sglang/srt/layers/attention/flashattention_backend.py
index f200a367b..3afd1f1b1 100644
--- a/python/sglang/srt/layers/attention/flashattention_backend.py
+++ b/python/sglang/srt/layers/attention/flashattention_backend.py
@@ -1130,21 +1130,24 @@ class FlashAttentionBackend(AttentionBackend):
         to avoid memory allocations.
         """
         # This is being used by normal decode and draft decode when topk == 1
+        # Prefer torch.empty for buffers that will be fully written later.
+        cu_seqlens_k_buf = torch.empty(
+            max_bs + 1, dtype=torch.int32, device=self.device
+        )
+        cu_seqlens_k_buf[0] = 0  # cumsum pad leading zero
         self.decode_cuda_graph_metadata = {
-            "cache_seqlens": torch.zeros(max_bs, dtype=torch.int32, device=self.device),
+            "cache_seqlens": torch.empty(max_bs, dtype=torch.int32, device=self.device),
             "cu_seqlens_q": torch.arange(
                 0, max_bs + 1, dtype=torch.int32, device=self.device
             ),
-            "cu_seqlens_k": torch.zeros(
-                max_bs + 1, dtype=torch.int32, device=self.device
-            ),
-            "page_table": torch.zeros(
+            "cu_seqlens_k": cu_seqlens_k_buf,
+            "page_table": torch.empty(
                 max_bs,
                 (self.max_context_len + self.page_size - 1) // self.page_size,
                 dtype=torch.int32,
                 device=self.device,
             ),
-            "page_table_draft_decode": torch.zeros(
+            "page_table_draft_decode": torch.empty(
                 max_bs,
                 (self.max_context_len + self.page_size - 1) // self.page_size,
                 dtype=torch.int32,
@@ -1165,19 +1168,20 @@ class FlashAttentionBackend(AttentionBackend):
             max_virtual_batches = max_bs * (
                 (max_seq_len + attn_chunk_size - 1) // attn_chunk_size
             )
-            max_blocks_per_seq = (max_seq_len + attn_chunk_size - 1) // attn_chunk_size
             max_pages_per_block = (attn_chunk_size + page_size - 1) // page_size
 
             self.decode_cuda_graph_local_attn_metadata = {
-                "local_query_start_loc": torch.zeros(
+                # These buffers are populated before replay; avoid zero-fill overhead.
+                "local_query_start_loc": torch.empty(
                     max_virtual_batches + 1, dtype=torch.int32, device=self.device
                 ),
-                "local_seqused_k": torch.zeros(
+                "local_seqused_k": torch.empty(
                     max_virtual_batches, dtype=torch.int32, device=self.device
                 ),
-                "local_block_table": torch.zeros(
+                # Only pages per local block are needed per virtual batch.
+                "local_block_table": torch.empty(
                     max_virtual_batches,
-                    max_blocks_per_seq * max_pages_per_block,
+                    max_pages_per_block,
                     dtype=torch.int32,
                     device=self.device,
                 ),
@@ -1185,8 +1189,12 @@ class FlashAttentionBackend(AttentionBackend):
 
         # This is used by draft decode's first half of metadata when topk > 1
         if self.topk > 1:
+            cu_seqlens_k_topk = torch.empty(
+                max_bs + 1, dtype=torch.int32, device=self.device
+            )
+            cu_seqlens_k_topk[0] = 0
             self.draft_decode_metadata_topk_normal = {
-                "cache_seqlens": torch.zeros(
+                "cache_seqlens": torch.empty(
                     max_bs, dtype=torch.int32, device=self.device
                 ),
                 "cu_seqlens_q": torch.arange(
@@ -1196,10 +1204,8 @@ class FlashAttentionBackend(AttentionBackend):
                     dtype=torch.int32,
                     device=self.device,
                 ),
-                "cu_seqlens_k": torch.zeros(
-                    max_bs + 1, dtype=torch.int32, device=self.device
-                ),
-                "page_table": torch.zeros(
+                "cu_seqlens_k": cu_seqlens_k_topk,
+                "page_table": torch.empty(
                     max_bs,
                     self.max_context_len,
                     dtype=torch.int32,
@@ -1242,7 +1248,7 @@ class FlashAttentionBackend(AttentionBackend):
             and self.speculative_num_draft_tokens > 0
         ):
             self.target_verify_metadata = {
-                "cache_seqlens": torch.zeros(
+                "cache_seqlens": torch.empty(
                     max_bs, dtype=torch.int32, device=self.device
                 ),
                 "cu_seqlens_q": torch.arange(
@@ -1252,10 +1258,12 @@ class FlashAttentionBackend(AttentionBackend):
                     dtype=torch.int32,
                     device=self.device,
                 ),
-                "cu_seqlens_k": torch.zeros(
-                    max_bs + 1, dtype=torch.int32, device=self.device
-                ),
-                "page_table": torch.zeros(
+                "cu_seqlens_k": (lambda n=max_bs + 1: (lambda t: t)(
+                    (lambda t: (t.__setitem__(0, 0) or t))(
+                        torch.empty(n, dtype=torch.int32, device=self.device)
+                    )
+                ))(),
+                "page_table": torch.empty(
                     max_bs,
                     (self.max_context_len + self.page_size - 1) // self.page_size,
                     dtype=torch.int32,
@@ -1267,8 +1275,12 @@ class FlashAttentionBackend(AttentionBackend):
             }
 
         if self.topk > 1:
+            cu_seqlens_k_tv = torch.empty(
+                max_bs + 1, dtype=torch.int32, device=self.device
+            )
+            cu_seqlens_k_tv[0] = 0
             self.target_verify_metadata_topk_normal = {
-                "cache_seqlens": torch.zeros(
+                "cache_seqlens": torch.empty(
                     max_bs, dtype=torch.int32, device=self.device
                 ),
                 "cu_seqlens_q": torch.arange(
@@ -1278,10 +1290,8 @@ class FlashAttentionBackend(AttentionBackend):
                     dtype=torch.int32,
                     device=self.device,
                 ),
-                "cu_seqlens_k": torch.zeros(
-                    max_bs + 1, dtype=torch.int32, device=self.device
-                ),
-                "page_table": torch.zeros(
+                "cu_seqlens_k": cu_seqlens_k_tv,
+                "page_table": torch.empty(
                     max_bs,
                     self.max_context_len,
                     dtype=torch.int32,
@@ -1290,23 +1300,23 @@ class FlashAttentionBackend(AttentionBackend):
             }
 
             self.target_verify_metadata_topk_expand = {
-                "cache_seqlens": torch.zeros(
+                "cache_seqlens": torch.empty(
                     max_bs * self.speculative_num_draft_tokens,
                     dtype=torch.int32,
                     device=self.device,
                 ),
-                "cu_seqlens_k": torch.zeros(
-                    max_bs * self.speculative_num_draft_tokens + 1,
-                    dtype=torch.int32,
-                    device=self.device,
-                ),
+                "cu_seqlens_k": (lambda n=max_bs * self.speculative_num_draft_tokens + 1: (lambda t: t)(
+                    (lambda t: (t.__setitem__(0, 0) or t))(
+                        torch.empty(n, dtype=torch.int32, device=self.device)
+                    )
+                ))(),
                 "cu_seqlens_q": torch.arange(
                     0,
                     max_bs * self.speculative_num_draft_tokens + 1,
                     dtype=torch.int32,
                     device=self.device,
                 ),
-                "page_table": torch.zeros(
+                "page_table": torch.empty(
                     max_bs * self.speculative_num_draft_tokens,
                     self.speculative_num_draft_tokens,
                     dtype=torch.int32,
@@ -1315,18 +1325,20 @@ class FlashAttentionBackend(AttentionBackend):
             }
 
         self.encoder_metadata = {
-            "encoder_page_table": torch.zeros(
+            "encoder_page_table": torch.empty(
                 max_bs,
                 self.max_context_len,
                 dtype=torch.int32,
                 device=self.device,
             ),
-            "encoder_lens_int32": torch.zeros(
+            "encoder_lens_int32": torch.empty(
                 max_bs, dtype=torch.int32, device=self.device
             ),
-            "encoder_cu_seqlens_k": torch.zeros(
-                max_bs + 1, dtype=torch.int32, device=self.device
-            ),
+            "encoder_cu_seqlens_k": (lambda n=max_bs + 1: (lambda t: t)(
+                (lambda t: (t.__setitem__(0, 0) or t))(
+                    torch.empty(n, dtype=torch.int32, device=self.device)
+                )
+            ))(),
         }
 
     def init_forward_metadata_capture_cuda_graph(
@@ -1867,14 +1879,10 @@ class FlashAttentionBackend(AttentionBackend):
         k_len = seqlens_k_local.shape[0]
         b0, b1 = block_table_local.shape
 
-        # In-place updates into preallocated tensors and zero out the unused space
+        # In-place updates into preallocated tensors; only the valid ranges are read by kernels.
         local_q_buf[:q_len].copy_(cu_seqlens_q_local)
-        local_q_buf[q_len:].fill_(0)
         local_k_buf[:k_len].copy_(seqlens_k_local)
-        local_k_buf[k_len:].fill_(0)
         local_block_buf[:b0, :b1].copy_(block_table_local)
-        local_block_buf[b0:, :].fill_(0)
-        local_block_buf[:b0, b1:].fill_(0)
 
         if metadata.local_attn_metadata is not None:
             lam = metadata.local_attn_metadata
