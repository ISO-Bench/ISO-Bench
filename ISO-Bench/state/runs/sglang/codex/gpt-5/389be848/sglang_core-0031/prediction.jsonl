{"instance_id": "/home/ubuntu/OmniPerf-Bench/sglan__repo-e9a47f4", "model_patch": "diff --git a/benchmark/bench_in_batch_prefix/bench_in_batch_prefix.py b/benchmark/bench_in_batch_prefix/bench_in_batch_prefix.py\nindex 86648e5ff..282097112 100644\n--- a/benchmark/bench_in_batch_prefix/bench_in_batch_prefix.py\n+++ b/benchmark/bench_in_batch_prefix/bench_in_batch_prefix.py\n@@ -64,11 +64,11 @@ def test_batch_by_batch(all_prompts, gen_len):\n \n     tot_time = 0\n     for i in range(len(all_prompts)):\n-        tic = time.time()\n+        tic = time.perf_counter()\n         text_qa.run_batch(\n             list(zip(all_prompts[i], [gen_len] * len(all_prompts[i]))),\n         )\n-        tot_time += time.time() - tic\n+        tot_time += time.perf_counter() - tic\n \n     return tot_time\n \n@@ -78,13 +78,13 @@ def test_batch_by_batch_with_hint(all_prompts, gen_len):\n \n     tot_time = 0\n     for i in range(len(all_prompts)):\n-        tic = time.time()\n+        tic = time.perf_counter()\n         # Send a hint to cache the prefix\n         text_qa.run_batch(list(zip(all_prompts[i][:1], [gen_len])))\n         # Send the batch\n         text_qa.run_batch(list(zip(all_prompts[i], [gen_len] * len(all_prompts[i]))))\n \n-        tot_time += time.time() - tic\n+        tot_time += time.perf_counter() - tic\n \n     return tot_time\n \n@@ -94,11 +94,11 @@ def test_send_all(all_prompts, gen_len):\n \n     all_prompts = [x for prompt_list in all_prompts for x in prompt_list]\n \n-    tic = time.time()\n+    tic = time.perf_counter()\n     text_qa.run_batch(\n         list(zip(all_prompts, [gen_len] * len(all_prompts))),\n     )\n-    tot_time = time.time() - tic\n+    tot_time = time.perf_counter() - tic\n \n     return tot_time\n \ndiff --git a/benchmark/benchmark_batch/benchmark_batch.py b/benchmark/benchmark_batch/benchmark_batch.py\nindex 15ef0ab6a..c23ea4654 100644\n--- a/benchmark/benchmark_batch/benchmark_batch.py\n+++ b/benchmark/benchmark_batch/benchmark_batch.py\n@@ -81,7 +81,7 @@ def send_batch_request(endpoint, prompts, gen_tokens, request_id):\n     }\n     data = {\"text\": prompts, \"sampling_params\": sampling_params}\n \n-    start_time = time.time()\n+    start_time = time.perf_counter()\n     try:\n         response = requests.post(\n             endpoint.base_url + \"/generate\", json=data, timeout=3600\n@@ -90,7 +90,7 @@ def send_batch_request(endpoint, prompts, gen_tokens, request_id):\n             error = response.json()\n             raise RuntimeError(f\"Request {request_id} failed: {error}\")\n         result = response.json()\n-        elapsed_time = (time.time() - start_time) * 1000  # Convert to ms\n+        elapsed_time = (time.perf_counter() - start_time) * 1000  # Convert to ms\n         avg_per_prompt = elapsed_time / len(prompts) if prompts else 0\n         return request_id, elapsed_time, avg_per_prompt, True, len(prompts)\n     except Exception as e:\n@@ -104,7 +104,7 @@ def run_benchmark(endpoint, batched_prompts, batch_size, gen_tokens):\n     num_requests = len(batched_prompts)\n \n     # Record start time for total latency\n-    benchmark_start_time = time.time()\n+    benchmark_start_time = time.perf_counter()\n \n     for i, batch_prompts in enumerate(batched_prompts):\n         request_id = i + 1\n@@ -113,13 +113,13 @@ def run_benchmark(endpoint, batched_prompts, batch_size, gen_tokens):\n         ), f\"Request {request_id} should have {batch_size} prompts, got {len(batch_prompts)}\"\n \n         print(\n-            f\"[Request] Sending request {request_id}/{num_requests} with {len(batch_prompts)} prompts at {int(time.time()*1000)}\"\n+            f\"[Request] Sending request {request_id}/{num_requests} with {len(batch_prompts)} prompts at {int(time.perf_counter()*1000)}\"\n         )\n         result = send_batch_request(endpoint, batch_prompts, gen_tokens, request_id)\n         results.append(result)\n \n     # Calculate total latency\n-    total_latency = (time.time() - benchmark_start_time) * 1000  # Convert to ms\n+    total_latency = (time.perf_counter() - benchmark_start_time) * 1000  # Convert to ms\n \n     return results, total_latency\n \ndiff --git a/benchmark/benchmark_batch/benchmark_tokenizer.py b/benchmark/benchmark_batch/benchmark_tokenizer.py\nindex c00bfb84b..88a5820b6 100644\n--- a/benchmark/benchmark_batch/benchmark_tokenizer.py\n+++ b/benchmark/benchmark_batch/benchmark_tokenizer.py\n@@ -44,10 +44,10 @@ def benchmark_sequential_vs_batch(prompts, batch_size, tokenizer):\n     for run in range(NUM_RUNS):\n         batch_prompts = prompts[:batch_size]  # Use same prompts for fair comparison\n \n-        start_time = time.time()\n+        start_time = time.perf_counter()\n         for prompt in batch_prompts:\n             tokens = tokenizer.encode(prompt)\n-        sequential_time = (time.time() - start_time) * 1000\n+        sequential_time = (time.perf_counter() - start_time) * 1000\n         sequential_times.append(sequential_time)\n \n     # Batch tokenization using tokenizer()\n@@ -55,9 +55,9 @@ def benchmark_sequential_vs_batch(prompts, batch_size, tokenizer):\n     for run in range(NUM_RUNS):\n         batch_prompts = prompts[:batch_size]  # Use same prompts for fair comparison\n \n-        start_time = time.time()\n+        start_time = time.perf_counter()\n         tokens = tokenizer(batch_prompts)\n-        batch_time = (time.time() - start_time) * 1000\n+        batch_time = (time.perf_counter() - start_time) * 1000\n         batch_times.append(batch_time)\n \n     return {\ndiff --git a/benchmark/generative_agents/bench_other.py b/benchmark/generative_agents/bench_other.py\nindex 48f6ebc40..c0b3a3406 100644\n--- a/benchmark/generative_agents/bench_other.py\n+++ b/benchmark/generative_agents/bench_other.py\n@@ -39,7 +39,7 @@ def main(args):\n         answer = await call_generate(**arg, temperature=0)\n         states.append(answer)\n \n-    tic = time.time()\n+    tic = time.perf_counter()\n     # we always sequentially execute agent calls to maintain its dependency\n     if args.backend != \"lmql\":\n         for arg in tqdm(arguments):\n@@ -50,7 +50,7 @@ def main(args):\n         loop = asyncio.get_event_loop()\n         for arg in tqdm(arguments):\n             loop.run_until_complete(get_one_answer_async(arg))\n-    latency = time.time() - tic\n+    latency = time.perf_counter() - tic\n \n     print(f\"Latency: {latency:.3f}\")\n \ndiff --git a/benchmark/generative_agents/bench_sglang.py b/benchmark/generative_agents/bench_sglang.py\nindex b42a32b44..034b16591 100644\n--- a/benchmark/generative_agents/bench_sglang.py\n+++ b/benchmark/generative_agents/bench_sglang.py\n@@ -35,14 +35,14 @@ def main(args):\n \n     states = []\n     # Run requests\n-    tic = time.time()\n+    tic = time.perf_counter()\n     for a in arguments:\n         # only a single key in the dict\n         for func, arg in a.items():\n             result = func.run(**arg)\n         result.sync()\n         states.append(result)\n-    latency = time.time() - tic\n+    latency = time.perf_counter() - tic\n \n     # Compute accuracy\n     print(f\"Latency: {latency:.3f}\")\ndiff --git a/benchmark/gsm8k/bench_other.py b/benchmark/gsm8k/bench_other.py\nindex a8bbcfb5c..6dcb9ad7c 100644\n--- a/benchmark/gsm8k/bench_other.py\n+++ b/benchmark/gsm8k/bench_other.py\n@@ -75,7 +75,7 @@ def main(args):\n             )\n             states[i] = answer\n \n-        tic = time.time()\n+        tic = time.perf_counter()\n         if args.parallel == 1:\n             for i in tqdm(range(len(questions))):\n                 get_one_answer(i)\n@@ -106,9 +106,9 @@ def main(args):\n                 for j in range(len(rets)):\n                     states[i + j] = rets[j]\n \n-        tic = time.time()\n+        tic = time.perf_counter()\n         asyncio.run(batched_call(batch_size=args.parallel))\n-    latency = time.time() - tic\n+    latency = time.perf_counter() - tic\n \n     preds = []\n     for i in range(len(states)):\ndiff --git a/benchmark/gsm8k/bench_sglang.py b/benchmark/gsm8k/bench_sglang.py\nindex b6bdbef09..05ac0beb1 100644\n--- a/benchmark/gsm8k/bench_sglang.py\n+++ b/benchmark/gsm8k/bench_sglang.py\n@@ -84,14 +84,14 @@ def main(args):\n     #####################################\n \n     # Run requests\n-    tic = time.time()\n+    tic = time.perf_counter()\n     states = few_shot_gsm8k.run_batch(\n         arguments,\n         temperature=0,\n         num_threads=args.parallel,\n         progress_bar=True,\n     )\n-    latency = time.time() - tic\n+    latency = time.perf_counter() - tic\n \n     preds = []\n     for i in range(len(states)):\ndiff --git a/benchmark/hellaswag/bench_other.py b/benchmark/hellaswag/bench_other.py\nindex 04be4569a..cde0794bb 100644\n--- a/benchmark/hellaswag/bench_other.py\n+++ b/benchmark/hellaswag/bench_other.py\n@@ -57,7 +57,7 @@ def main(args):\n                 context=few_shot_examples + questions[i], choices=choices[i]\n             )\n \n-        tic = time.time()\n+        tic = time.perf_counter()\n         if args.parallel == 1:\n             for i in tqdm(range(len(questions))):\n                 get_one_answer(i)\n@@ -82,10 +82,10 @@ def main(args):\n                 for j in range(len(rets)):\n                     preds[i + j] = rets[j]\n \n-        tic = time.time()\n+        tic = time.perf_counter()\n         asyncio.run(batched_call(batch_size=args.parallel))\n \n-    latency = time.time() - tic\n+    latency = time.perf_counter() - tic\n \n     # Compute accuracy\n     acc = np.mean(np.array(preds) == np.array(labels))\ndiff --git a/benchmark/hellaswag/bench_sglang.py b/benchmark/hellaswag/bench_sglang.py\nindex 798521f97..6345a453b 100644\n--- a/benchmark/hellaswag/bench_sglang.py\n+++ b/benchmark/hellaswag/bench_sglang.py\n@@ -68,7 +68,7 @@ def main(args):\n     #####################################\n \n     # Run requests\n-    tic = time.time()\n+    tic = time.perf_counter()\n     rets = few_shot_hellaswag.run_batch(\n         arguments,\n         temperature=0,\n@@ -76,7 +76,7 @@ def main(args):\n         progress_bar=True,\n     )\n     preds = [choices[i].index(rets[i][\"answer\"]) for i in range(len(rets))]\n-    latency = time.time() - tic\n+    latency = time.perf_counter() - tic\n \n     # Compute accuracy\n     acc = np.mean(np.array(preds) == np.array(labels))\ndiff --git a/benchmark/hicache/bench_multiturn.py b/benchmark/hicache/bench_multiturn.py\nindex 6bd0bd99e..a2a88b634 100644\n--- a/benchmark/hicache/bench_multiturn.py\n+++ b/benchmark/hicache/bench_multiturn.py\n@@ -261,7 +261,7 @@ class WorkloadGenerator:\n             client_id, payload = item\n             response = await async_request_sglang_generate(payload, self.url, self.pbar)\n             if self.pbar.n == self.pbar.total:\n-                self.finished_time = time.time()\n+                self.finished_time = time.perf_counter()\n             self.response_queue.put((client_id, response))\n         except Exception as e:\n             print(f\"Request failed: {e}\")\n@@ -334,7 +334,7 @@ class WorkloadGenerator:\n         request_thread = threading.Thread(target=self.request_sender, daemon=True)\n         response_thread = threading.Thread(target=self.response_handler, daemon=True)\n \n-        self.start_time = time.time()\n+        self.start_time = time.perf_counter()\n         request_thread.start()\n         response_thread.start()\n \ndiff --git a/benchmark/json_decode_regex/bench_other.py b/benchmark/json_decode_regex/bench_other.py\nindex d80ea1de7..87051ea82 100644\n--- a/benchmark/json_decode_regex/bench_other.py\n+++ b/benchmark/json_decode_regex/bench_other.py\n@@ -53,7 +53,7 @@ def main(args):\n     def get_one_answer(i):\n         states[i] = json_decode(generate=call_generate, **arguments[i])\n \n-    tic = time.time()\n+    tic = time.perf_counter()\n     if args.parallel == 1:\n         for i in tqdm(range(len(arguments))):\n             get_one_answer(i)\n@@ -68,7 +68,7 @@ def main(args):\n             for _ in rets:\n                 pass\n \n-    latency = time.time() - tic\n+    latency = time.perf_counter() - tic\n \n     # Compute accuracy\n     print(f\"Latency: {latency:.3f}\")\ndiff --git a/benchmark/json_decode_regex/bench_sglang.py b/benchmark/json_decode_regex/bench_sglang.py\nindex 4139ebf8a..9aab11e43 100644\n--- a/benchmark/json_decode_regex/bench_sglang.py\n+++ b/benchmark/json_decode_regex/bench_sglang.py\n@@ -63,11 +63,11 @@ def main(args):\n     json_warm_up.run().sync()\n \n     # Run requests\n-    tic = time.time()\n+    tic = time.perf_counter()\n     states = json_decode.run_batch(\n         arguments, temperature=0, num_threads=args.parallel, progress_bar=True\n     )\n-    latency = time.time() - tic\n+    latency = time.perf_counter() - tic\n \n     # Compute accuracy\n     print(f\"Latency: {latency:.3f}\")\ndiff --git a/benchmark/json_jump_forward/bench_other.py b/benchmark/json_jump_forward/bench_other.py\nindex 9eb5c58b3..a64e950d7 100644\n--- a/benchmark/json_jump_forward/bench_other.py\n+++ b/benchmark/json_jump_forward/bench_other.py\n@@ -175,7 +175,7 @@ def bench_character(args):\n     else:\n         raise ValueError(f\"Invalid backend: {args.backend}\")\n \n-    tic = time.time()\n+    tic = time.perf_counter()\n \n     if args.backend != \"lmql\":\n         if args.parallel == 1:\n@@ -202,7 +202,7 @@ def bench_character(args):\n                 asyncio.gather(*[get_one_answer_async(i) for i in bt])\n             )\n \n-    latency = time.time() - tic\n+    latency = time.perf_counter() - tic\n \n     return states, latency\n \n@@ -236,7 +236,7 @@ def bench_city_doc(args):\n     else:\n         raise ValueError(f\"Invalid backend: {args.backend}\")\n \n-    tic = time.time()\n+    tic = time.perf_counter()\n     if args.parallel == 1:\n         for i in tqdm(range(len(arguments))):\n             get_one_answer(i)\n@@ -246,7 +246,7 @@ def bench_city_doc(args):\n             for _ in rets:\n                 pass\n \n-    latency = time.time() - tic\n+    latency = time.perf_counter() - tic\n \n     return states, latency\n \ndiff --git a/benchmark/json_jump_forward/bench_sglang.py b/benchmark/json_jump_forward/bench_sglang.py\nindex 10cf2699b..29f635f75 100644\n--- a/benchmark/json_jump_forward/bench_sglang.py\n+++ b/benchmark/json_jump_forward/bench_sglang.py\n@@ -67,14 +67,14 @@ def bench_city_doc(args):\n     sgl.set_default_backend(backend)\n \n     # Run requests\n-    tic = time.time()\n+    tic = time.perf_counter()\n     states = city_gen.run_batch(\n         arguments,\n         temperature=0,\n         num_threads=args.parallel,\n         progress_bar=True,\n     )\n-    latency = time.time() - tic\n+    latency = time.perf_counter() - tic\n \n     return states, latency\n \n@@ -91,14 +91,14 @@ def bench_character(args):\n     sgl.set_default_backend(backend)\n \n     # Run requests\n-    tic = time.time()\n+    tic = time.perf_counter()\n     states = character_gen.run_batch(\n         arguments,\n         temperature=0,\n         num_threads=args.parallel,\n         progress_bar=True,\n     )\n-    latency = time.time() - tic\n+    latency = time.perf_counter() - tic\n \n     return states, latency\n \ndiff --git a/benchmark/json_schema/bench_sglang.py b/benchmark/json_schema/bench_sglang.py\nindex 4693baae3..55365ff2e 100644\n--- a/benchmark/json_schema/bench_sglang.py\n+++ b/benchmark/json_schema/bench_sglang.py\n@@ -85,14 +85,14 @@ def bench_schema(args):\n     sgl.set_default_backend(backend)\n \n     # Run requests\n-    tic = time.time()\n+    tic = time.perf_counter()\n     states = schema_gen.run_batch(\n         arguments,\n         temperature=0,\n         num_threads=args.parallel,\n         progress_bar=True,\n     )\n-    latency = time.time() - tic\n+    latency = time.perf_counter() - tic\n \n     # Check if the outputs are valid\n     indexes = []\ndiff --git a/benchmark/kernels/fused_moe_triton/tuning_fused_moe_triton.py b/benchmark/kernels/fused_moe_triton/tuning_fused_moe_triton.py\nindex a3ead1eca..be349e456 100644\n--- a/benchmark/kernels/fused_moe_triton/tuning_fused_moe_triton.py\n+++ b/benchmark/kernels/fused_moe_triton/tuning_fused_moe_triton.py\n@@ -487,7 +487,7 @@ def main(args: argparse.Namespace):\n             ]\n         print(f\"Start tuning over {len(search_space)} configurations...\")\n \n-        start = time.time()\n+        start = time.perf_counter()\n         configs = _distribute(\n             \"tune\",\n             [\n@@ -522,7 +522,7 @@ def main(args: argparse.Namespace):\n             use_int8_w8a16,\n             block_shape,\n         )\n-        end = time.time()\n+        end = time.perf_counter()\n         print(f\"Tuning took {end - start:.2f} seconds\")\n     else:\n         outputs = _distribute(\ndiff --git a/benchmark/kernels/quantization/tuning_block_wise_kernel.py b/benchmark/kernels/quantization/tuning_block_wise_kernel.py\nindex 7b0dfb47a..1b51e54b7 100644\n--- a/benchmark/kernels/quantization/tuning_block_wise_kernel.py\n+++ b/benchmark/kernels/quantization/tuning_block_wise_kernel.py\n@@ -359,7 +359,7 @@ def tune_on_gpu(args_dict):\n         config for config in search_space if block_k % config[\"BLOCK_SIZE_K\"] == 0\n     ]\n \n-    start = time.time()\n+    start = time.perf_counter()\n     results = {}\n     for shape in tqdm(weight_shapes, desc=f\"GPU {gpu_id} - Shapes\"):\n         N, K = shape[0], shape[1]\n@@ -379,7 +379,7 @@ def tune_on_gpu(args_dict):\n         best_configs = {M: config for M, config in zip(batch_sizes, benchmark_results)}\n         save_configs(N, K, block_n, block_k, best_configs, save_path, input_type)\n \n-    end = time.time()\n+    end = time.perf_counter()\n     print(f\"Tuning on GPU {gpu_id} took {end - start:.2f} seconds\")\n \n \ndiff --git a/benchmark/line_retrieval/bench_sglang.py b/benchmark/line_retrieval/bench_sglang.py\nindex 922d5009d..e974e7dd3 100644\n--- a/benchmark/line_retrieval/bench_sglang.py\n+++ b/benchmark/line_retrieval/bench_sglang.py\n@@ -70,7 +70,7 @@ def eval_model(args, line_obj, num_hoops, src_indices, dst_percents):\n     # Select backend\n     backend = select_sglang_backend(args)\n \n-    tic = time.time()\n+    tic = time.perf_counter()\n     states = line_retrieval.run_batch(\n         arguments,\n         temperature=0,\n@@ -78,7 +78,7 @@ def eval_model(args, line_obj, num_hoops, src_indices, dst_percents):\n         num_threads=args.parallel,\n         progress_bar=True,\n     )\n-    latency = time.time() - tic\n+    latency = time.perf_counter() - tic\n \n     corrects = []\n     for i in range(len(arguments)):\ndiff --git a/benchmark/llava_bench/bench_sglang.py b/benchmark/llava_bench/bench_sglang.py\nindex f84c8a90f..b9e8c1405 100644\n--- a/benchmark/llava_bench/bench_sglang.py\n+++ b/benchmark/llava_bench/bench_sglang.py\n@@ -41,7 +41,7 @@ def main(args):\n     sgl.set_default_backend(backend)\n \n     # Run requests\n-    tic = time.time()\n+    tic = time.perf_counter()\n     if args.parallel == 1:\n         for i in tqdm.tqdm(range(len(lines))):\n             image_file = arguments[i][\"image_file\"]\n@@ -52,7 +52,7 @@ def main(args):\n         states = image_qa.run_batch(\n             arguments, temperature=0, num_threads=args.parallel, progress_bar=True\n         )\n-    latency = time.time() - tic\n+    latency = time.perf_counter() - tic\n \n     print(f\"Latency: {latency:.3f}\")\n \ndiff --git a/benchmark/llm_judge/bench_other.py b/benchmark/llm_judge/bench_other.py\nindex 2231bcdbb..8e6029067 100644\n--- a/benchmark/llm_judge/bench_other.py\n+++ b/benchmark/llm_judge/bench_other.py\n@@ -85,7 +85,7 @@ def main(args):\n     call_generate = partial(get_call_generate(args), temperature=0)\n \n     # Run requests\n-    tic = time.time()\n+    tic = time.perf_counter()\n \n     if args.backend != \"lmql\":\n \n@@ -120,7 +120,7 @@ def main(args):\n                 asyncio.gather(*[get_one_answer_async(i) for i in bt])\n             )\n \n-    latency = time.time() - tic\n+    latency = time.perf_counter() - tic\n \n     # Compute accuracy\n     print(f\"Latency: {latency:.3f}\")\ndiff --git a/benchmark/llm_judge/bench_sglang.py b/benchmark/llm_judge/bench_sglang.py\nindex 38c95974e..97e6c3979 100644\n--- a/benchmark/llm_judge/bench_sglang.py\n+++ b/benchmark/llm_judge/bench_sglang.py\n@@ -59,7 +59,7 @@ def main(args):\n     backend = select_sglang_backend(args)\n \n     # Run requests\n-    tic = time.time()\n+    tic = time.perf_counter()\n     states = multi_dimension_judge.run_batch(\n         arguments,\n         temperature=0,\n@@ -67,7 +67,7 @@ def main(args):\n         num_threads=args.parallel,\n         progress_bar=True,\n     )\n-    latency = time.time() - tic\n+    latency = time.perf_counter() - tic\n \n     print(f\"Latency: {latency:.3f}\")\n \ndiff --git a/benchmark/long_json_decode/bench_other.py b/benchmark/long_json_decode/bench_other.py\nindex a83c797c4..0ad38a014 100644\n--- a/benchmark/long_json_decode/bench_other.py\n+++ b/benchmark/long_json_decode/bench_other.py\n@@ -45,7 +45,7 @@ def main(args):\n     def get_one_answer(i):\n         states[i] = json_decode(generate=call_generate, **arguments[i])\n \n-    tic = time.time()\n+    tic = time.perf_counter()\n     if args.parallel == 1:\n         for i in tqdm(range(len(arguments))):\n             get_one_answer(i)\n@@ -58,7 +58,7 @@ def main(args):\n                 )\n             )\n \n-    latency = time.time() - tic\n+    latency = time.perf_counter() - tic\n \n     # Compute accuracy\n     print(f\"Latency: {latency:.3f}\")\ndiff --git a/benchmark/long_json_decode/bench_sglang.py b/benchmark/long_json_decode/bench_sglang.py\nindex 6e19a732f..8394cfc2e 100644\n--- a/benchmark/long_json_decode/bench_sglang.py\n+++ b/benchmark/long_json_decode/bench_sglang.py\n@@ -46,11 +46,11 @@ def main(args):\n     sgl.set_default_backend(backend)\n \n     # Run requests\n-    tic = time.time()\n+    tic = time.perf_counter()\n     states = json_decode.run_batch(\n         arguments, temperature=0, num_threads=args.parallel, progress_bar=True\n     )\n-    latency = time.time() - tic\n+    latency = time.perf_counter() - tic\n \n     # Compute accuracy\n     print(f\"Latency: {latency:.3f}\")\ndiff --git a/benchmark/mmlu/bench_other.py b/benchmark/mmlu/bench_other.py\nindex c5d48dac6..f1b166c2b 100644\n--- a/benchmark/mmlu/bench_other.py\n+++ b/benchmark/mmlu/bench_other.py\n@@ -76,7 +76,7 @@ def evaluate(args, subject, dev_df, test_df, call_generate):\n             pred = call_generate(prompts[i], temperature=0, max_tokens=max_tokens)\n             preds[i] = pred.strip()[0]\n \n-        tic = time.time()\n+        tic = time.perf_counter()\n         if args.parallel == 1:\n             for i in range(len(prompts)):\n                 get_one_answer(i)\n@@ -94,9 +94,9 @@ def evaluate(args, subject, dev_df, test_df, call_generate):\n                 for j in range(len(rets)):\n                     preds[i + j] = rets[j].strip()[0]\n \n-        tic = time.time()\n+        tic = time.perf_counter()\n         asyncio.run(batched_call(batch_size=args.parallel))\n-    latency = time.time() - tic\n+    latency = time.perf_counter() - tic\n \n     # Compute accuracy\n     cors = [pred == label for pred, label in zip(preds, labels)]\ndiff --git a/benchmark/mmlu/bench_sglang.py b/benchmark/mmlu/bench_sglang.py\nindex 210b6111e..0bae7b6e4 100644\n--- a/benchmark/mmlu/bench_sglang.py\n+++ b/benchmark/mmlu/bench_sglang.py\n@@ -116,7 +116,7 @@ def main(args):\n     backend = select_sglang_backend(args)\n \n     # Run\n-    tic = time.time()\n+    tic = time.perf_counter()\n     states = few_shot_mmlu.run_batch(\n         arguments,\n         temperature=0,\n@@ -128,7 +128,7 @@ def main(args):\n     preds = [\n         s[\"answer\"].strip()[0] if len(s[\"answer\"].strip()) > 0 else \"\" for s in states\n     ]\n-    latency = time.time() - tic\n+    latency = time.perf_counter() - tic\n \n     # Compute accuracy\n     cors = [pred == label for pred, label in zip(preds, labels)]\ndiff --git a/benchmark/mmmu/bench_sglang.py b/benchmark/mmmu/bench_sglang.py\nindex 58a4039ef..a177fd137 100644\n--- a/benchmark/mmmu/bench_sglang.py\n+++ b/benchmark/mmmu/bench_sglang.py\n@@ -119,7 +119,7 @@ async def eval_mmmu(args) -> None:\n         api_key=\"sk\", base_url=f\"http://127.0.0.1:{args.port}/v1\"\n     )\n     semaphore = asyncio.Semaphore(args.concurrency)\n-    start = time.time()\n+    start = time.perf_counter()\n     base_url = f\"http://127.0.0.1:{args.port}\"\n \n     if args.profile:\n@@ -147,7 +147,7 @@ async def eval_mmmu(args) -> None:\n         if profile_output.success:\n             print(\"Profiler stopped\")\n \n-    print(f\"Benchmark time: {time.time() - start}\")\n+    print(f\"Benchmark time: {time.perf_counter() - start}\")\n     args.output_path = f\"./val_sglang.json\"\n     save_json(args.output_path, out_samples)\n     eval_result(model_answer_path=args.output_path, answer_dict=answer_dict)\ndiff --git a/benchmark/mtbench/bench_other.py b/benchmark/mtbench/bench_other.py\nindex 2c321e8a1..2897480dd 100644\n--- a/benchmark/mtbench/bench_other.py\n+++ b/benchmark/mtbench/bench_other.py\n@@ -31,7 +31,7 @@ def write_answers(filename, model_id, questions, answers):\n                     \"index\": 0,\n                     \"turns\": [answers[i][0], answers[i][1]],\n                 },\n-                \"tstamp\": time.time(),\n+                \"tstamp\": time.perf_counter(),\n             }\n             fout.write(json.dumps(ans_json) + \"\\n\")\n \n@@ -66,7 +66,7 @@ def main(args):\n         answers[i] = cur_answers\n \n     # Run requests\n-    tic = time.time()\n+    tic = time.perf_counter()\n     if args.parallel == 1:\n         for i in tqdm(range(len(questions))):\n             get_answer(i)\n@@ -79,7 +79,7 @@ def main(args):\n                 )\n             )\n \n-    latency = time.time() - tic\n+    latency = time.perf_counter() - tic\n \n     print(f\"#questions: {len(questions)}, Latency: {latency:.2f}\")\n \ndiff --git a/benchmark/mtbench/bench_sglang.py b/benchmark/mtbench/bench_sglang.py\nindex b57d1647d..45b0e9be9 100644\n--- a/benchmark/mtbench/bench_sglang.py\n+++ b/benchmark/mtbench/bench_sglang.py\n@@ -31,7 +31,7 @@ def write_answers(filename, model_id, questions, answers):\n                     \"index\": 0,\n                     \"turns\": [answers[i][0], answers[i][1]],\n                 },\n-                \"tstamp\": time.time(),\n+                \"tstamp\": time.perf_counter(),\n             }\n             fout.write(json.dumps(ans_json) + \"\\n\")\n \n@@ -57,7 +57,7 @@ def main(args):\n     sgl.set_default_backend(backend)\n \n     # Run requests\n-    tic = time.time()\n+    tic = time.perf_counter()\n     rets = answer_mt_bench.run_batch(\n         arguments,\n         temperature=0,\n@@ -66,7 +66,7 @@ def main(args):\n         progress_bar=True,\n     )\n     answers = [[s[\"answer_1\"], s[\"answer_2\"]] for s in rets]\n-    latency = time.time() - tic\n+    latency = time.perf_counter() - tic\n \n     print(f\"#questions: {len(questions)}, Latency: {latency:.2f}\")\n \ndiff --git a/benchmark/mtbench/bench_sglang_eagle.py b/benchmark/mtbench/bench_sglang_eagle.py\nindex e1207afe1..266a68a98 100644\n--- a/benchmark/mtbench/bench_sglang_eagle.py\n+++ b/benchmark/mtbench/bench_sglang_eagle.py\n@@ -40,7 +40,7 @@ def write_answers(filename, model_id, questions, answers):\n                     \"index\": 0,\n                     \"turns\": [answers[i][0], answers[i][1]],\n                 },\n-                \"tstamp\": time.time(),\n+                \"tstamp\": time.perf_counter(),\n             }\n             fout.write(json.dumps(ans_json) + \"\\n\")\n \n@@ -68,7 +68,7 @@ def main(args):\n     sgl.set_default_backend(backend)\n \n     # Run requests\n-    tic = time.time()\n+    tic = time.perf_counter()\n     rets = answer_mt_bench.run_batch(\n         arguments,\n         temperature=0,\n@@ -78,7 +78,7 @@ def main(args):\n     )\n     answers = [[s[\"answer_1\"], s[\"answer_2\"]] for s in rets]\n \n-    latency = time.time() - tic\n+    latency = time.perf_counter() - tic\n     num_output_tokens = sum(\n         s.get_meta_info(\"answer_1\")[\"completion_tokens\"]\n         + s.get_meta_info(\"answer_2\")[\"completion_tokens\"]\ndiff --git a/benchmark/multi_chain_reasoning/bench_other.py b/benchmark/multi_chain_reasoning/bench_other.py\nindex e0ff2be45..f361496ad 100644\n--- a/benchmark/multi_chain_reasoning/bench_other.py\n+++ b/benchmark/multi_chain_reasoning/bench_other.py\n@@ -113,7 +113,7 @@ def main(args):\n             answer = multi_chain_gsm8k(questions[i], args.num_chains, call_generate)\n             states[i] = answer\n \n-        tic = time.time()\n+        tic = time.perf_counter()\n         if args.parallel == 1:\n             for i in tqdm(range(len(questions))):\n                 get_one_answer(i)\n@@ -134,7 +134,7 @@ def main(args):\n             )\n             states[i] = answer\n \n-        tic = time.time()\n+        tic = time.perf_counter()\n         loop = asyncio.get_event_loop()\n         batches = [\n             list(range(i, min(i + args.parallel, len(questions))))\n@@ -144,7 +144,7 @@ def main(args):\n             tasks = [get_one_answer_asyncio(k) for k in bt]\n             loop.run_until_complete(asyncio.gather(*tasks))\n \n-    latency = time.time() - tic\n+    latency = time.perf_counter() - tic\n \n     preds = []\n     for i in range(len(states)):\ndiff --git a/benchmark/multi_chain_reasoning/bench_sglang.py b/benchmark/multi_chain_reasoning/bench_sglang.py\nindex 98a6b511e..1d3129db2 100644\n--- a/benchmark/multi_chain_reasoning/bench_sglang.py\n+++ b/benchmark/multi_chain_reasoning/bench_sglang.py\n@@ -90,7 +90,7 @@ def main(args):\n     backend = select_sglang_backend(args)\n \n     # Run requests\n-    tic = time.time()\n+    tic = time.perf_counter()\n     states = multi_chain_gsm8k.run_batch(\n         arguments,\n         temperature=0,\n@@ -98,7 +98,7 @@ def main(args):\n         num_threads=args.parallel,\n         progress_bar=True,\n     )\n-    latency = time.time() - tic\n+    latency = time.perf_counter() - tic\n \n     preds = []\n     for i in range(len(states)):\ndiff --git a/benchmark/multi_document_qa/bench_other.py b/benchmark/multi_document_qa/bench_other.py\nindex 6f0addcb7..627837c5c 100644\n--- a/benchmark/multi_document_qa/bench_other.py\n+++ b/benchmark/multi_document_qa/bench_other.py\n@@ -61,7 +61,7 @@ def main(args):\n     def get_one_answer(i):\n         states[i] = multi_document_qa(generate=call_generate, **arguments[i])\n \n-    tic = time.time()\n+    tic = time.perf_counter()\n     if args.parallel == 1:\n         for i in tqdm(range(len(labels))):\n             get_one_answer(i)\n@@ -74,7 +74,7 @@ def main(args):\n                 )\n             )\n \n-    latency = time.time() - tic\n+    latency = time.perf_counter() - tic\n \n     # Compute accuracy\n     print(states)\ndiff --git a/benchmark/multi_document_qa/bench_sglang.py b/benchmark/multi_document_qa/bench_sglang.py\nindex 645520166..0b4b0dbc6 100644\n--- a/benchmark/multi_document_qa/bench_sglang.py\n+++ b/benchmark/multi_document_qa/bench_sglang.py\n@@ -49,11 +49,11 @@ def main(args):\n     sgl.set_default_backend(backend)\n \n     # Run requests\n-    tic = time.time()\n+    tic = time.perf_counter()\n     states = multi_document_qa.run_batch(\n         arguments, temperature=0, num_threads=args.parallel, progress_bar=True\n     )\n-    latency = time.time() - tic\n+    latency = time.perf_counter() - tic\n \n     # Compute accuracy\n     print([s[\"answer\"] for s in states])\ndiff --git a/benchmark/multi_turn_chat/bench_other.py b/benchmark/multi_turn_chat/bench_other.py\nindex 81d67ab7b..9189af5be 100644\n--- a/benchmark/multi_turn_chat/bench_other.py\n+++ b/benchmark/multi_turn_chat/bench_other.py\n@@ -35,7 +35,7 @@ def main(args):\n     def get_one_answer(i):\n         states[i] = multi_turns(generate=call_generate, **multi_qas[i])\n \n-    tic = time.time()\n+    tic = time.perf_counter()\n     if args.parallel == 1:\n         for i in tqdm(range(len(multi_qas))):\n             get_one_answer(i)\n@@ -50,7 +50,7 @@ def main(args):\n             for _ in rets:\n                 pass\n \n-    latency = time.time() - tic\n+    latency = time.perf_counter() - tic\n \n     # Compute accuracy\n     print(f\"Latency: {latency:.3f}\")\ndiff --git a/benchmark/multi_turn_chat/bench_sglang.py b/benchmark/multi_turn_chat/bench_sglang.py\nindex 7feaced73..1051bf19e 100644\n--- a/benchmark/multi_turn_chat/bench_sglang.py\n+++ b/benchmark/multi_turn_chat/bench_sglang.py\n@@ -27,7 +27,7 @@ def main(args):\n \n     backend = select_sglang_backend(args)\n \n-    tic = time.time()\n+    tic = time.perf_counter()\n     states = multi_turns.run_batch(\n         multi_qas,\n         temperature=0,\n@@ -35,7 +35,7 @@ def main(args):\n         num_threads=args.parallel,\n         progress_bar=True,\n     )\n-    latency = time.time() - tic\n+    latency = time.perf_counter() - tic\n \n     print(f\"Latency: {latency:.3f}\")\n \ndiff --git a/benchmark/multi_turn_chat/long_prompt_multi_turn.py b/benchmark/multi_turn_chat/long_prompt_multi_turn.py\nindex 20f6dd5e3..bda5bb9cc 100644\n--- a/benchmark/multi_turn_chat/long_prompt_multi_turn.py\n+++ b/benchmark/multi_turn_chat/long_prompt_multi_turn.py\n@@ -84,7 +84,7 @@ def main(args):\n \n     backend = select_sglang_backend(args)\n \n-    tic = time.time()\n+    tic = time.perf_counter()\n     states = multi_turns.run_batch(\n         multi_qas,\n         temperature=0,\n@@ -92,7 +92,7 @@ def main(args):\n         num_threads=\"auto\",\n         progress_bar=True,\n     )\n-    latency = time.time() - tic\n+    latency = time.perf_counter() - tic\n \n     print(f\"Latency: {latency:.3f}\")\n \ndiff --git a/benchmark/react/bench_other.py b/benchmark/react/bench_other.py\nindex 91c5546f1..08666662b 100644\n--- a/benchmark/react/bench_other.py\n+++ b/benchmark/react/bench_other.py\n@@ -146,7 +146,7 @@ def main(args):\n \n             states.append(answer)\n \n-    tic = time.time()\n+    tic = time.perf_counter()\n \n     if args.backend != \"lmql\":\n         if args.parallel == 1:\n@@ -173,7 +173,7 @@ def main(args):\n             tasks = [run_single_agent_async(arg) for arg in bt]\n             loop.run_until_complete(asyncio.gather(*tasks))\n \n-    latency = time.time() - tic\n+    latency = time.perf_counter() - tic\n \n     print(f\"Latency: {latency:.3f}\")\n \ndiff --git a/benchmark/react/bench_sglang.py b/benchmark/react/bench_sglang.py\nindex b07105e2c..331638e9f 100644\n--- a/benchmark/react/bench_sglang.py\n+++ b/benchmark/react/bench_sglang.py\n@@ -115,14 +115,14 @@ def main(args):\n     sgl.set_default_backend(backend)\n \n     states = []\n-    tic = time.time()\n+    tic = time.perf_counter()\n     states = webthink.run_batch(\n         arguments,\n         temperature=0,\n         num_threads=args.parallel,\n         progress_bar=True,\n     )\n-    latency = time.time() - tic\n+    latency = time.perf_counter() - tic\n \n     # Compute accuracy\n     print(f\"Latency: {latency:.3f}\")\ndiff --git a/benchmark/reasoning_benchmark/bench_sglang.py b/benchmark/reasoning_benchmark/bench_sglang.py\nindex c83204960..ccbff9d17 100644\n--- a/benchmark/reasoning_benchmark/bench_sglang.py\n+++ b/benchmark/reasoning_benchmark/bench_sglang.py\n@@ -51,7 +51,7 @@ def main(args):\n     )\n \n     # Run requests\n-    tic = time.time()\n+    tic = time.perf_counter()\n     states = reasoning_gen.run_batch(\n         questions,\n         num_threads=args.parallel,\n@@ -60,7 +60,7 @@ def main(args):\n         max_new_tokens=32768,\n         top_p=0.95,\n     )\n-    latency = time.time() - tic\n+    latency = time.perf_counter() - tic\n \n     # Extract results and record outcomes in a list.\n     outcomes = []\ndiff --git a/benchmark/tip_suggestion/bench_other.py b/benchmark/tip_suggestion/bench_other.py\nindex fcc4fd624..2630081bd 100644\n--- a/benchmark/tip_suggestion/bench_other.py\n+++ b/benchmark/tip_suggestion/bench_other.py\n@@ -68,7 +68,7 @@ def main(args):\n     call_generate = partial(get_call_generate(args), temperature=0)\n \n     # Run requests\n-    tic = time.time()\n+    tic = time.perf_counter()\n     if args.backend != \"lmql\":\n \n         def get_one_answer(i):\n@@ -102,7 +102,7 @@ def main(args):\n             loop.run_until_complete(\n                 asyncio.gather(*[get_one_answer_async(i) for i in batch])\n             )\n-    latency = time.time() - tic\n+    latency = time.perf_counter() - tic\n \n     # Compute accuracy\n     print(f\"Latency: {latency:.3f}\")\ndiff --git a/benchmark/tip_suggestion/bench_sglang.py b/benchmark/tip_suggestion/bench_sglang.py\nindex 6d17821bc..86c476f97 100644\n--- a/benchmark/tip_suggestion/bench_sglang.py\n+++ b/benchmark/tip_suggestion/bench_sglang.py\n@@ -65,11 +65,11 @@ def main(args):\n     sgl.set_default_backend(select_sglang_backend(args))\n \n     # Run requests\n-    tic = time.time()\n+    tic = time.perf_counter()\n     states = suggest_tips.run_batch(\n         arguments, temperature=0, num_threads=args.parallel, progress_bar=True\n     )\n-    latency = time.time() - tic\n+    latency = time.perf_counter() - tic\n \n     # Compute accuracy\n     print(f\"Latency: {latency:.3f}\")\ndiff --git a/benchmark/tree_of_thought_deep/bench_other.py b/benchmark/tree_of_thought_deep/bench_other.py\nindex 21c7df351..0ef8c6360 100644\n--- a/benchmark/tree_of_thought_deep/bench_other.py\n+++ b/benchmark/tree_of_thought_deep/bench_other.py\n@@ -138,7 +138,7 @@ def main(args):\n     # Run requests\n     states = [None] * len(questions)\n \n-    tic = time.time()\n+    tic = time.perf_counter()\n     if args.backend != \"lmql\":\n \n         def get_one_answer(i):\n@@ -177,7 +177,7 @@ def main(args):\n             tasks = [get_one_answer_async(k) for k in bt]\n             loop.run_until_complete(asyncio.gather(*tasks))\n \n-    latency = time.time() - tic\n+    latency = time.perf_counter() - tic\n \n     answers_text = []\n     for s in states:\ndiff --git a/benchmark/tree_of_thought_deep/bench_sglang.py b/benchmark/tree_of_thought_deep/bench_sglang.py\nindex bfb2a4113..bcdb6e54d 100644\n--- a/benchmark/tree_of_thought_deep/bench_sglang.py\n+++ b/benchmark/tree_of_thought_deep/bench_sglang.py\n@@ -119,7 +119,7 @@ def main(args):\n     backend = select_sglang_backend(args)\n \n     # Run requests\n-    tic = time.time()\n+    tic = time.perf_counter()\n     states = tree_search.run_batch(\n         arguments,\n         temperature=0,\n@@ -127,7 +127,7 @@ def main(args):\n         num_threads=args.parallel,\n         progress_bar=True,\n     )\n-    latency = time.time() - tic\n+    latency = time.perf_counter() - tic\n     answers_text = []\n     for s in states:\n         answers_text.append([x for xs in s.ret_value for x in xs])\ndiff --git a/benchmark/tree_of_thought_v0/bench_other.py b/benchmark/tree_of_thought_v0/bench_other.py\nindex 86e133577..703ecd7f4 100644\n--- a/benchmark/tree_of_thought_v0/bench_other.py\n+++ b/benchmark/tree_of_thought_v0/bench_other.py\n@@ -121,7 +121,7 @@ def main(args):\n     def get_one_answer(i):\n         states[i] = tree_search(**arguments[i], call_generate=call_generate)\n \n-    tic = time.time()\n+    tic = time.perf_counter()\n     if args.parallel == 1:\n         for i in tqdm(range(len(questions))):\n             get_one_answer(i)\n@@ -134,7 +134,7 @@ def main(args):\n                 )\n             )\n \n-    latency = time.time() - tic\n+    latency = time.perf_counter() - tic\n \n     answers_text = []\n     for s in states:\ndiff --git a/benchmark/tree_of_thought_v0/bench_sglang.py b/benchmark/tree_of_thought_v0/bench_sglang.py\nindex f0d130778..6d7575f36 100644\n--- a/benchmark/tree_of_thought_v0/bench_sglang.py\n+++ b/benchmark/tree_of_thought_v0/bench_sglang.py\n@@ -107,7 +107,7 @@ def main(args):\n     backend = select_sglang_backend(args)\n \n     # Run requests\n-    tic = time.time()\n+    tic = time.perf_counter()\n     states = tree_search.run_batch(\n         arguments,\n         temperature=0,\n@@ -115,7 +115,7 @@ def main(args):\n         num_threads=args.parallel,\n         progress_bar=True,\n     )\n-    latency = time.time() - tic\n+    latency = time.perf_counter() - tic\n     answers_text = []\n     for s in states:\n         answers_text.append([x for xs in s[\"answer\"] for x in xs])\ndiff --git a/python/sglang/test/few_shot_gsm8k.py b/python/sglang/test/few_shot_gsm8k.py\nindex 4f655eb60..5aac87bd2 100644\n--- a/python/sglang/test/few_shot_gsm8k.py\n+++ b/python/sglang/test/few_shot_gsm8k.py\n@@ -90,7 +90,7 @@ def run_eval(args):\n     #####################################\n \n     # Run requests\n-    tic = time.time()\n+    tic = time.perf_counter()\n     states = few_shot_gsm8k.run_batch(\n         arguments,\n         temperature=args.temperature if hasattr(args, \"temperature\") else 0,\n@@ -99,7 +99,7 @@ def run_eval(args):\n         return_logprob=getattr(args, \"return_logprob\", None),\n         logprob_start_len=getattr(args, \"logprob_start_len\", None),\n     )\n-    latency = time.time() - tic\n+    latency = time.perf_counter() - tic\n \n     preds = []\n     for i in range(len(states)):\ndiff --git a/python/sglang/test/few_shot_gsm8k_engine.py b/python/sglang/test/few_shot_gsm8k_engine.py\nindex 67844e2f1..2453a91e4 100644\n--- a/python/sglang/test/few_shot_gsm8k_engine.py\n+++ b/python/sglang/test/few_shot_gsm8k_engine.py\n@@ -89,7 +89,7 @@ def run_eval(args):\n     }\n \n     # Run requests\n-    tic = time.time()\n+    tic = time.perf_counter()\n \n     loop = asyncio.get_event_loop()\n \n@@ -98,7 +98,7 @@ def run_eval(args):\n     )\n \n     # End requests\n-    latency = time.time() - tic\n+    latency = time.perf_counter() - tic\n \n     # Shutdown the engine\n     engine.shutdown()\ndiff --git a/python/sglang/test/run_eval.py b/python/sglang/test/run_eval.py\nindex fe88171ce..51743be09 100644\n--- a/python/sglang/test/run_eval.py\n+++ b/python/sglang/test/run_eval.py\n@@ -71,9 +71,9 @@ def run_eval(args):\n     )\n \n     # Run eval\n-    tic = time.time()\n+    tic = time.perf_counter()\n     result = eval_obj(sampler)\n-    latency = time.time() - tic\n+    latency = time.perf_counter() - tic\n \n     # Dump reports\n     metrics = result.metrics | {\"score\": result.score}\ndiff --git a/python/sglang/test/test_programs.py b/python/sglang/test/test_programs.py\nindex 262637eed..6756f2dd7 100644\n--- a/python/sglang/test/test_programs.py\n+++ b/python/sglang/test/test_programs.py\n@@ -503,7 +503,7 @@ def test_hellaswag_select():\n     #####################################\n \n     # Run requests\n-    tic = time.time()\n+    tic = time.perf_counter()\n     rets = few_shot_hellaswag.run_batch(\n         arguments,\n         temperature=0,\n@@ -514,13 +514,13 @@ def test_hellaswag_select():\n     preds = []\n     for i, ret in enumerate(rets):\n         preds.append(choices[i].index(ret[\"answer\"]))\n-    latency = time.time() - tic\n+    latency = time.perf_counter() - tic\n \n     # Compute accuracy\n     accuracy = np.mean(np.array(preds) == np.array(labels))\n \n     # Test generator style of run_batch\n-    tic = time.time()\n+    tic = time.perf_counter()\n     rets = few_shot_hellaswag.run_batch(\n         arguments,\n         temperature=0,\n@@ -531,7 +531,7 @@ def test_hellaswag_select():\n     preds_gen = []\n     for i, ret in enumerate(rets):\n         preds_gen.append(choices[i].index(ret[\"answer\"]))\n-    latency_gen = time.time() - tic\n+    latency_gen = time.perf_counter() - tic\n \n     # Compute accuracy\n     accuracy_gen = np.mean(np.array(preds_gen) == np.array(labels))\ndiff --git a/python/sglang/test/test_utils.py b/python/sglang/test/test_utils.py\nindex 1e78d6dc1..499551007 100644\n--- a/python/sglang/test/test_utils.py\n+++ b/python/sglang/test/test_utils.py\n@@ -449,9 +449,9 @@ def popen_launch_server(\n     else:\n         process = subprocess.Popen(command, stdout=None, stderr=None, env=env)\n \n-    start_time = time.time()\n+    start_time = time.perf_counter()\n     with requests.Session() as session:\n-        while time.time() - start_time < timeout:\n+        while time.perf_counter() - start_time < timeout:\n             try:\n                 headers = {\n                     \"Content-Type\": \"application/json; charset=utf-8\",\n@@ -526,9 +526,9 @@ def popen_launch_pd_server(\n     else:\n         process = subprocess.Popen(command, stdout=None, stderr=None, env=env)\n \n-    start_time = time.time()\n+    start_time = time.perf_counter()\n     with requests.Session() as session:\n-        while time.time() - start_time < timeout:\n+        while time.perf_counter() - start_time < timeout:\n             try:\n                 headers = {\n                     \"Content-Type\": \"application/json; charset=utf-8\",\n@@ -584,7 +584,7 @@ class TestFile:\n \n \n def run_unittest_files(files: List[TestFile], timeout_per_file: float):\n-    tic = time.time()\n+    tic = time.perf_counter()\n     success = True\n \n     for i, file in enumerate(files):\n@@ -599,13 +599,13 @@ def run_unittest_files(files: List[TestFile], timeout_per_file: float):\n                 f\".\\n.\\nBegin ({i}/{len(files) - 1}):\\npython3 {filename}\\n.\\n.\\n\",\n                 flush=True,\n             )\n-            tic = time.time()\n+            tic = time.perf_counter()\n \n             process = subprocess.Popen(\n                 [\"python3\", filename], stdout=None, stderr=None, env=os.environ\n             )\n             process.wait()\n-            elapsed = time.time() - tic\n+            elapsed = time.perf_counter() - tic\n \n             print(\n                 f\".\\n.\\nEnd ({i}/{len(files) - 1}):\\n{filename=}, {elapsed=:.0f}, {estimated_time=}\\n.\\n.\\n\",\n@@ -631,9 +631,9 @@ def run_unittest_files(files: List[TestFile], timeout_per_file: float):\n             break\n \n     if success:\n-        print(f\"Success. Time elapsed: {time.time() - tic:.2f}s\", flush=True)\n+        print(f\"Success. Time elapsed: {time.perf_counter() - tic:.2f}s\", flush=True)\n     else:\n-        print(f\"Fail. Time elapsed: {time.time() - tic:.2f}s\", flush=True)\n+        print(f\"Fail. Time elapsed: {time.perf_counter() - tic:.2f}s\", flush=True)\n \n     return 0 if success else -1\n \ndiff --git a/sgl-router/py_test/test_launch_server.py b/sgl-router/py_test/test_launch_server.py\nindex 33dd3e854..afffe334f 100644\n--- a/sgl-router/py_test/test_launch_server.py\n+++ b/sgl-router/py_test/test_launch_server.py\n@@ -92,9 +92,9 @@ def popen_launch_router(\n \n     process = subprocess.Popen(command, stdout=None, stderr=None)\n \n-    start_time = time.time()\n+    start_time = time.perf_counter()\n     with requests.Session() as session:\n-        while time.time() - start_time < timeout:\n+        while time.perf_counter() - start_time < timeout:\n             try:\n                 response = session.get(f\"{base_url}/health\")\n                 if response.status_code == 200:\n@@ -155,11 +155,11 @@ def terminate_and_wait(process, timeout=300):\n         return\n \n     process.terminate()\n-    start_time = time.time()\n+    start_time = time.perf_counter()\n \n     while process.poll() is None:\n         print(f\"Terminating process {process.pid}\")\n-        if time.time() - start_time > timeout:\n+        if time.perf_counter() - start_time > timeout:\n             raise TimeoutError(\n                 f\"Process {process.pid} failed to terminate within {timeout}s\"\n             )\ndiff --git a/test/srt/experiment_runner.py b/test/srt/experiment_runner.py\nindex 7feeef1aa..f32f61d3b 100644\n--- a/test/srt/experiment_runner.py\n+++ b/test/srt/experiment_runner.py\n@@ -184,9 +184,9 @@ class ExperimentRunner:\n         self.logger = logging.getLogger(__name__)\n \n     def wait_for_server(self, port: int, timeout: int = 300) -> bool:\n-        start_time = time.time()\n+        start_time = time.perf_counter()\n \n-        while time.time() - start_time < timeout:\n+        while time.perf_counter() - start_time < timeout:\n             try:\n                 response = requests.get(f\"http://localhost:{port}/health\")\n                 if response.status_code == 200:\n@@ -197,7 +197,7 @@ class ExperimentRunner:\n         return False\n \n     def run_task(self, config: TaskConfig) -> TaskResult:\n-        start_time = time.time()\n+        start_time = time.perf_counter()\n         client_output = []\n \n         try:\n@@ -247,7 +247,7 @@ class ExperimentRunner:\n                 name=config.name,\n                 success=True,\n                 output=formatted_output,\n-                runtime=time.time() - start_time,\n+                runtime=time.perf_counter() - start_time,\n                 timestamp=datetime.now().isoformat(),\n             )\n \n@@ -256,7 +256,7 @@ class ExperimentRunner:\n                 name=config.name,\n                 success=False,\n                 output=str(e),\n-                runtime=time.time() - start_time,\n+                runtime=time.perf_counter() - start_time,\n                 timestamp=datetime.now().isoformat(),\n             )\n \ndiff --git a/test/srt/models/test_encoder_embedding_models.py b/test/srt/models/test_encoder_embedding_models.py\nindex 5202917c4..bea5d4aff 100644\n--- a/test/srt/models/test_encoder_embedding_models.py\n+++ b/test/srt/models/test_encoder_embedding_models.py\n@@ -79,9 +79,9 @@ class TestEncoderEmbeddingModels(CustomTestCase):\n             # warm up\n             hf_outputs = hf_runner.forward(truncated_prompts)\n \n-            st_start_time = time.time()\n+            st_start_time = time.perf_counter()\n             hf_outputs = hf_runner.forward(truncated_prompts)\n-            st_end_time = time.time()\n+            st_end_time = time.perf_counter()\n \n         with SRTRunner(\n             model_path,\n@@ -95,9 +95,9 @@ class TestEncoderEmbeddingModels(CustomTestCase):\n             # warm up\n             srt_outputs = srt_runner.forward(truncated_prompts)\n \n-            sgl_start_time = time.time()\n+            sgl_start_time = time.perf_counter()\n             srt_outputs = srt_runner.forward(truncated_prompts)\n-            sgl_end_time = time.time()\n+            sgl_end_time = time.perf_counter()\n \n         transformer_time = st_end_time - st_start_time\n         sgl_time = sgl_end_time - sgl_start_time\ndiff --git a/test/srt/test_gptqmodel_dynamic.py b/test/srt/test_gptqmodel_dynamic.py\nindex 27ccd9a4b..284465b8b 100644\n--- a/test/srt/test_gptqmodel_dynamic.py\n+++ b/test/srt/test_gptqmodel_dynamic.py\n@@ -130,9 +130,9 @@ class TestGPTQModelDynamic(CustomTestCase):\n     def test_throughput(self):\n         max_tokens = 256\n \n-        tic = time.time()\n+        tic = time.perf_counter()\n         result = self.run_decode(max_tokens)\n-        tok = time.time()\n+        tok = time.perf_counter()\n \n         print(f\"result = `{result}`\")\n \n@@ -185,9 +185,9 @@ class TestGPTQModelDynamicWithMarlin(CustomTestCase):\n     def test_throughput(self):\n         max_tokens = 256\n \n-        tic = time.time()\n+        tic = time.perf_counter()\n         result = self.run_decode(max_tokens)\n-        tok = time.time()\n+        tok = time.perf_counter()\n \n         print(f\"result = `{result}`\")\n \ndiff --git a/test/srt/test_release_memory_occupation.py b/test/srt/test_release_memory_occupation.py\nindex 7ccd9f1f7..7a7659280 100644\n--- a/test/srt/test_release_memory_occupation.py\n+++ b/test/srt/test_release_memory_occupation.py\n@@ -42,10 +42,10 @@ class TestReleaseMemoryOccupation(CustomTestCase):\n         )\n \n         print(\"release_memory_occupation start\")\n-        t = time.time()\n+        t = time.perf_counter()\n         engine.release_memory_occupation()\n         if _DEBUG_EXTRA:\n-            print(\"release_memory_occupation\", time.time() - t)\n+            print(\"release_memory_occupation\", time.perf_counter() - t)\n \n         if _DEBUG_EXTRA:\n             time.sleep(5)\n@@ -60,10 +60,10 @@ class TestReleaseMemoryOccupation(CustomTestCase):\n             time.sleep(5)\n \n         print(\"resume_memory_occupation start\")\n-        t = time.time()\n+        t = time.perf_counter()\n         engine.resume_memory_occupation()\n         if _DEBUG_EXTRA:\n-            print(\"resume_memory_occupation\", time.time() - t)\n+            print(\"resume_memory_occupation\", time.perf_counter() - t)\n \n         self.assertEqual(\n             _try_allocate_big_tensor(),\ndiff --git a/test/srt/test_torch_compile.py b/test/srt/test_torch_compile.py\nindex 760cec84b..904e49f9d 100644\n--- a/test/srt/test_torch_compile.py\n+++ b/test/srt/test_torch_compile.py\n@@ -62,9 +62,9 @@ class TestTorchCompile(CustomTestCase):\n         res = self.run_decode(16)\n \n         max_tokens = 256\n-        tic = time.time()\n+        tic = time.perf_counter()\n         res = self.run_decode(max_tokens)\n-        tok = time.time()\n+        tok = time.perf_counter()\n         print(f\"{res=}\")\n         throughput = max_tokens / (tok - tic)\n         print(f\"Throughput: {throughput} tokens/s\")\ndiff --git a/test/srt/test_torch_compile_moe.py b/test/srt/test_torch_compile_moe.py\nindex 42415b155..63423af43 100644\n--- a/test/srt/test_torch_compile_moe.py\n+++ b/test/srt/test_torch_compile_moe.py\n@@ -62,9 +62,9 @@ class TestTorchCompileMoe(CustomTestCase):\n         res = self.run_decode(16)\n \n         max_tokens = 256\n-        tic = time.time()\n+        tic = time.perf_counter()\n         res = self.run_decode(max_tokens)\n-        tok = time.time()\n+        tok = time.perf_counter()\n         print(f\"{res=}\")\n         throughput = max_tokens / (tok - tic)\n         self.assertGreaterEqual(throughput, 285)\ndiff --git a/test/srt/test_torchao.py b/test/srt/test_torchao.py\nindex 77ec0a570..ead127f4f 100644\n--- a/test/srt/test_torchao.py\n+++ b/test/srt/test_torchao.py\n@@ -61,9 +61,9 @@ class TestTorchAO(CustomTestCase):\n \n         max_tokens = 256\n \n-        tic = time.time()\n+    tic = time.perf_counter()\n         res = self.run_decode(max_tokens)\n-        tok = time.time()\n+    tok = time.perf_counter()\n         print(res[\"text\"])\n         throughput = max_tokens / (tok - tic)\n         print(f\"Throughput: {throughput} tokens/s\")\ndiff --git a/test/srt/test_update_weights_from_distributed.py b/test/srt/test_update_weights_from_distributed.py\nindex e558a56e3..064406703 100644\n--- a/test/srt/test_update_weights_from_distributed.py\n+++ b/test/srt/test_update_weights_from_distributed.py\n@@ -164,7 +164,7 @@ def init_process_hf(\n     )\n     dist.barrier(group=group, device_ids=[rank])\n     torch.cuda.synchronize()\n-    time_begin_broadcast = time.time()\n+    time_begin_broadcast = time.perf_counter()\n \n     # The last parameter is lm_head.weight, which is tied\n     # with embed_tokens.weight. Actually, we only need\n@@ -182,7 +182,7 @@ def init_process_hf(\n             group=group,\n         )\n     torch.cuda.synchronize()\n-    time_end_broadcast = time.time()\n+    time_end_broadcast = time.perf_counter()\n \n     # Measure the latency of broadcasting/weights update.\n     broadcast_time = time_end_broadcast - time_begin_broadcast\n@@ -282,7 +282,7 @@ def init_process_sgl(\n         )\n \n     torch.cuda.synchronize()\n-    time_begin_update = time.time()\n+    time_begin_update = time.perf_counter()\n \n     # The last parameter is lm_head.weight, which is tied\n     # with embed_tokens.weight. Actually, we only need\n@@ -312,7 +312,7 @@ def init_process_sgl(\n                 },\n             )\n     torch.cuda.synchronize()\n-    time_end_update = time.time()\n+    time_end_update = time.perf_counter()\n \n     # Measure the latency of broadcast/weights update.\n     update_time = time_end_update - time_begin_update\ndiff --git a/test/srt/test_update_weights_from_tensor.py b/test/srt/test_update_weights_from_tensor.py\nindex 1f3592447..38187652b 100644\n--- a/test/srt/test_update_weights_from_tensor.py\n+++ b/test/srt/test_update_weights_from_tensor.py\n@@ -21,9 +21,9 @@ def test_update_weights_from_tensor(tp_size):\n     memory_before = torch.cuda.memory_allocated()\n     new_tensor = torch.full((16384, 2048), 1.5, device=\"cuda\")\n \n-    time_start = time.time()\n+    time_start = time.perf_counter()\n     engine.update_weights_from_tensor([(x, new_tensor) for x in param_names])\n-    print(f\"Time delta: {time.time() - time_start:.03f}\")\n+    print(f\"Time delta: {time.perf_counter() - time_start:.03f}\")\n \n     for param_name in param_names[:3]:\n         _check_param(engine, param_name, [1.5] * 5)\ndiff --git a/test/srt/test_w8a8_quantization.py b/test/srt/test_w8a8_quantization.py\nindex 2cb2fa073..3d4ce1afa 100644\n--- a/test/srt/test_w8a8_quantization.py\n+++ b/test/srt/test_w8a8_quantization.py\n@@ -62,9 +62,9 @@ class TestW8A8(CustomTestCase):\n     def test_throughput(self):\n         max_tokens = 256\n \n-        tic = time.time()\n+        tic = time.perf_counter()\n         res = self.run_decode(max_tokens)\n-        tok = time.time()\n+        tok = time.perf_counter()\n         print(res[\"text\"])\n         throughput = max_tokens / (tok - tic)\n         print(f\"Throughput: {throughput} tokens/s\")\n", "model_name_or_path": "gpt-5-2025-08-07"}
