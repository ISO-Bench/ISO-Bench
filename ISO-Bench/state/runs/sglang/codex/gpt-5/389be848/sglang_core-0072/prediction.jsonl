{"instance_id": "/home/ubuntu/OmniPerf-Bench/sglan__repo-b04df75", "model_patch": "diff --git a/python/sglang/srt/layers/quantization/fp8_utils.py b/python/sglang/srt/layers/quantization/fp8_utils.py\nindex 0e1640fcf..2f9dbe0cb 100644\n--- a/python/sglang/srt/layers/quantization/fp8_utils.py\n+++ b/python/sglang/srt/layers/quantization/fp8_utils.py\n@@ -332,22 +332,16 @@ def block_quant_to_tensor_quant(\n     assert n_tiles == x_s.shape[0]\n     assert k_tiles == x_s.shape[1]\n \n-    x_dq_block = x_q_block.to(torch.float32)\n-\n-    x_dq_block_tiles = [\n-        [\n-            x_dq_block[\n-                j * block_n : min((j + 1) * block_n, n),\n-                i * block_k : min((i + 1) * block_k, k),\n-            ]\n-            for i in range(k_tiles)\n-        ]\n-        for j in range(n_tiles)\n-    ]\n-\n-    for i in range(k_tiles):\n-        for j in range(n_tiles):\n-            x_dq_block_tiles[j][i][:, :] = x_dq_block_tiles[j][i] * x_s[j][i]\n+    # Vectorized dequantization: expand block scales to elementwise scales\n+    # Shape: (n, k)\n+    scale_expanded = (\n+        x_s.repeat_interleave(block_n, dim=0)\n+        .repeat_interleave(block_k, dim=1)\n+        [:n, :k]\n+    )\n+\n+    # Dequantize in one shot\n+    x_dq_block = x_q_block.to(torch.float32) * scale_expanded\n \n     x_q_tensor, scale = (\n         scaled_fp8_quant(x_dq_block)\n@@ -375,21 +369,14 @@ def block_quant_dequant(\n     assert n_tiles == x_s.shape[0]\n     assert k_tiles == x_s.shape[1]\n \n-    x_dq_block = torch.empty_like(x_q_block, dtype=dtype)\n-\n-    for j in range(n_tiles):\n-        for i in range(k_tiles):\n-            x_q_block_tile = x_q_block[\n-                j * block_n : min((j + 1) * block_n, n),\n-                i * block_k : min((i + 1) * block_k, k),\n-            ]\n-            x_dq_block_tile = x_dq_block[\n-                j * block_n : min((j + 1) * block_n, n),\n-                i * block_k : min((i + 1) * block_k, k),\n-            ]\n-            x_dq_block_tile[:, :] = x_q_block_tile.to(torch.float32) * x_s[j][i]\n-\n-    return x_dq_block\n+    # Expand per-block scales to elementwise scales and dequantize vectorially\n+    scale_expanded = (\n+        x_s.repeat_interleave(block_n, dim=0)\n+        .repeat_interleave(block_k, dim=1)\n+        [:n, :k]\n+    )\n+    x_dq = x_q_block.to(torch.float32) * scale_expanded\n+    return x_dq.to(dtype)\n \n \n def channel_quant_to_tensor_quant(\n", "model_name_or_path": "gpt-5-2025-08-07"}
