diff --git a/python/sglang/srt/disaggregation/mooncake/conn.py b/python/sglang/srt/disaggregation/mooncake/conn.py
index 8ab5066ec..4a25b79b9 100644
--- a/python/sglang/srt/disaggregation/mooncake/conn.py
+++ b/python/sglang/srt/disaggregation/mooncake/conn.py
@@ -31,6 +31,7 @@ from sglang.srt.disaggregation.base.conn import (
 from sglang.srt.disaggregation.mooncake.transfer_engine import MooncakeTransferEngine
 from sglang.srt.disaggregation.utils import (
     DisaggregationMode,
+    FastQueue,
     group_concurrent_contiguous,
 )
 from sglang.srt.server_args import ServerArgs
@@ -151,7 +152,8 @@ class MooncakeKVManager(BaseKVManager):
         self.server_socket = zmq.Context().socket(zmq.PULL)
         self.register_buffer_to_engine()
         if self.disaggregation_mode == DisaggregationMode.PREFILL:
-            self.transfer_queue = queue.Queue()
+            # Use FastQueue to reduce contention and overhead in transfer path
+            self.transfer_queue = FastQueue()
             self.transfer_infos: Dict[int, Dict[str, TransferInfo]] = {}
             self.decode_kv_args_table: Dict[str, KVArgsRegisterInfo] = {}
             self.start_prefill_thread()
diff --git a/python/sglang/srt/disaggregation/utils.py b/python/sglang/srt/disaggregation/utils.py
index 8841d5f1a..6147ac29e 100644
--- a/python/sglang/srt/disaggregation/utils.py
+++ b/python/sglang/srt/disaggregation/utils.py
@@ -14,6 +14,9 @@ import torch
 import torch.distributed as dist
 
 from sglang.srt.utils import get_ip, get_local_ip_by_remote
+import threading
+import time
+import queue as _queue
 
 if TYPE_CHECKING:
     from sglang.srt.managers.schedule_batch import Req
@@ -207,17 +210,19 @@ class MetadataBuffers:
 
         # We transfer the metadata of first output token to decode
         # The minimal size for RDMA is 64Bytes, so we pad it to > 64Bytes
-        self.output_ids = torch.zeros((size, 16), dtype=torch.int32, device="cpu")
-        self.output_token_logprobs_val = torch.zeros(
+        # Use torch.empty to avoid unnecessary zero-initialization overhead.
+        # Callers populate only the used fields before transfer.
+        self.output_ids = torch.empty((size, 16), dtype=torch.int32, device="cpu")
+        self.output_token_logprobs_val = torch.empty(
             (size, 16), dtype=torch.float32, device="cpu"
         )
-        self.output_token_logprobs_idx = torch.zeros(
+        self.output_token_logprobs_idx = torch.empty(
             (size, 16), dtype=torch.int32, device="cpu"
         )
-        self.output_top_logprobs_val = torch.zeros(
+        self.output_top_logprobs_val = torch.empty(
             (size, max_top_logprobs_num), dtype=torch.float32, device="cpu"
         )
-        self.output_top_logprobs_idx = torch.zeros(
+        self.output_top_logprobs_idx = torch.empty(
             (size, max_top_logprobs_num), dtype=torch.int32, device="cpu"
         )
 
@@ -289,10 +294,41 @@ def group_concurrent_contiguous(
         return [], []
 
     brk = np.where((np.diff(src_indices) != 1) | (np.diff(dst_indices) != 1))[0] + 1
-    src_groups = np.split(src_indices, brk)
-    dst_groups = np.split(dst_indices, brk)
+    # Keep groups as NumPy arrays to reduce Python list allocations and conversions
+    src_groups = list(np.split(src_indices, brk))
+    dst_groups = list(np.split(dst_indices, brk))
+    return src_groups, dst_groups
 
-    src_groups = [g.tolist() for g in src_groups]
-    dst_groups = [g.tolist() for g in dst_groups]
 
-    return src_groups, dst_groups
+class FastQueue:
+    """
+    A lightweight, thread-safe queue optimized for high-throughput puts/gets.
+    Implements a minimal subset of queue.Queue with timeout support for get().
+    """
+
+    def __init__(self):
+        self._deque = deque()
+        self._cond = threading.Condition()
+
+    def put(self, item) -> None:
+        with self._cond:
+            self._deque.append(item)
+            self._cond.notify()
+
+    def get(self, timeout: Optional[float] = None):
+        end = None if timeout is None else (time.time() + timeout)
+        with self._cond:
+            while not self._deque:
+                remaining = None if end is None else (end - time.time())
+                if remaining is not None and remaining <= 0:
+                    raise _queue.Empty
+                self._cond.wait(timeout=remaining)
+            return self._deque.popleft()
+
+    def empty(self) -> bool:
+        with self._cond:
+            return not self._deque
+
+    def qsize(self) -> int:
+        with self._cond:
+            return len(self._deque)
