diff --git a/docs/backend/native_api.ipynb b/docs/backend/native_api.ipynb
index 04b8ec0ed..189c678c0 100644
--- a/docs/backend/native_api.ipynb
+++ b/docs/backend/native_api.ipynb
@@ -390,7 +390,7 @@
    "outputs": [],
    "source": [
     "expert_record_server_process, port = launch_server_cmd(\n",
-    "    \"python3 -m sglang.launch_server --model-path Qwen/Qwen1.5-MoE-A2.7B --host 0.0.0.0\"\n",
+    "    \"python3 -m sglang.launch_server --model-path Qwen/Qwen1.5-MoE-A2.7B --host 0.0.0.0 --expert-distribution-recorder-mode stat\"\n",
     ")\n",
     "\n",
     "wait_for_server(f\"http://localhost:{port}\")"
@@ -415,19 +415,7 @@
     "print_highlight(response)\n",
     "\n",
     "response = requests.post(f\"http://localhost:{port}/dump_expert_distribution_record\")\n",
-    "print_highlight(response)\n",
-    "\n",
-    "import glob\n",
-    "\n",
-    "output_file = glob.glob(\"expert_distribution_*.csv\")[0]\n",
-    "with open(output_file, \"r\") as f:\n",
-    "    print_highlight(\"\\n| Layer ID | Expert ID | Count |\")\n",
-    "    print_highlight(\"|----------|-----------|--------|\")\n",
-    "    next(f)\n",
-    "    for i, line in enumerate(f):\n",
-    "        if i < 9:\n",
-    "            layer_id, expert_id, count = line.strip().split(\",\")\n",
-    "            print_highlight(f\"| {layer_id:8} | {expert_id:9} | {count:6} |\")"
+    "print_highlight(response)"
    ]
   },
   {
diff --git a/python/sglang/srt/layers/moe/ep_moe/token_dispatcher.py b/python/sglang/srt/layers/moe/ep_moe/token_dispatcher.py
index 4d165dbd2..c3810641b 100644
--- a/python/sglang/srt/layers/moe/ep_moe/token_dispatcher.py
+++ b/python/sglang/srt/layers/moe/ep_moe/token_dispatcher.py
@@ -260,6 +260,8 @@ class _DeepEPDispatcherImplNormal(_DeepEPDispatcherImplBase):
                 reorder_topk_ids = torch.empty(
                     (0,), device=hidden_states.device, dtype=torch.int64
                 )
+                # For the empty case, seg_indptr content does not affect compute,
+                # but keep it zero-sized-initialized for correctness.
                 seg_indptr = torch.zeros(
                     (self.num_experts + 1,),
                     device=hidden_states.device,
@@ -400,7 +402,8 @@ class _DeepEPDispatcherImplNormal(_DeepEPDispatcherImplBase):
                     BLOCK_SIZE=512,
                 )
             else:
-                output = torch.zeros(
+                # Use empty() for zero-length allocation to avoid unnecessary init
+                output = torch.empty(
                     (0, hidden_states.shape[1]),
                     device=hidden_states.device,
                     dtype=hidden_states.dtype,
diff --git a/python/sglang/srt/layers/moe/topk.py b/python/sglang/srt/layers/moe/topk.py
index 4c065e4e5..007c08a35 100644
--- a/python/sglang/srt/layers/moe/topk.py
+++ b/python/sglang/srt/layers/moe/topk.py
@@ -44,10 +44,6 @@ def fused_topk_native(
         hidden_states.shape[0] == gating_output.shape[0]
     ), f"Number of tokens mismatch, {hidden_states.shape=} vs {gating_output.shape=}"
     M, _ = hidden_states.shape
-    topk_weights = torch.empty(
-        M, topk, dtype=torch.float32, device=hidden_states.device
-    )
-    topk_ids = torch.empty(M, topk, dtype=torch.int32, device=hidden_states.device)
     topk_weights = F.softmax(gating_output.float(), dim=-1)
     topk_weights, topk_ids = torch.topk(topk_weights, topk, dim=-1)
     if renormalize:
@@ -353,6 +349,14 @@ def select_experts(
             renormalize=renormalize,
         )
 
+    # Prepare expert recorder (for efficient stat-mode counting) and record
+    try:
+        expert_distribution_recorder.prepare_layer(
+            num_experts=router_logits.shape[1], device=topk_ids.device
+        )
+    except Exception:
+        # Be robust if recorder does not support prepare_layer on some paths
+        pass
     expert_distribution_recorder.record_new_token(topk_ids)
 
     return topk_weights, topk_ids
diff --git a/python/sglang/srt/managers/expert_distribution.py b/python/sglang/srt/managers/expert_distribution.py
index 226256ed2..901064c1c 100644
--- a/python/sglang/srt/managers/expert_distribution.py
+++ b/python/sglang/srt/managers/expert_distribution.py
@@ -18,36 +18,79 @@ class ExpertDistributionRecorder:
         return cls.instance
 
     def __init__(self):
-        # the length of the dictionary is the number of layers
-        # the length of the list is the number of tokens
-        # the length of the tuple is topk's k value
-        self._expert_distribution_record: Dict[int, List[Tuple[int]]] = defaultdict(
-            list
-        )
+        # Per-layer expert counts recorded efficiently on device
+        self._expert_counts: Dict[int, torch.Tensor] = {}
+        # Optional raw records for debugging (kept empty for performance)
+        self._raw_records: Dict[int, List[torch.Tensor]] = defaultdict(list)
         self._record = False
         self._current_layer_id = "UNKNOWN"
+        self._layer_num_experts: Dict[int, int] = {}
 
     def set_current_layer(self, layer_idx):
         self._current_layer_id = layer_idx
 
-    def record_new_token(self, topk_ids):
+    def prepare_layer(self, num_experts: int, device: torch.device):
+        """Prepare per-layer buffers for efficient counting.
+
+        This avoids repeated CPU sync by keeping counts on device and updating them
+        incrementally using index_add_/bincount.
+        """
+        if not self._record:
+            return
+        layer_id = self._current_layer_id
+        if layer_id not in self._expert_counts:
+            # counts tensor lives on the same device as tokens
+            self._expert_counts[layer_id] = torch.zeros(
+                (num_experts,), dtype=torch.int64, device=device
+            )
+            self._layer_num_experts[layer_id] = num_experts
+        else:
+            # If num_experts changes (shouldn't), resize conservatively
+            if num_experts > self._expert_counts[layer_id].numel():
+                new_counts = torch.zeros(
+                    (num_experts,), dtype=torch.int64, device=device
+                )
+                old = self._expert_counts[layer_id]
+                new_counts[: old.numel()] = old
+                self._expert_counts[layer_id] = new_counts
+                self._layer_num_experts[layer_id] = num_experts
+
+    def record_new_token(self, topk_ids: torch.Tensor):
         if not self._record:
             return
-        topk_ids_list = topk_ids.to("cpu", non_blocking=True).numpy().tolist()
-        torch.cuda.synchronize()
-        for i in topk_ids_list:
-            self._expert_distribution_record[self._current_layer_id].append(tuple(i))
+
+        layer_id = self._current_layer_id
+        # Fast path: accumulate counts on device without host sync
+        if layer_id in self._expert_counts:
+            flat = topk_ids.reshape(-1).to(torch.int64)
+            if flat.numel() == 0:
+                return
+            # filter out padded region marked as -1
+            valid = flat >= 0
+            if valid.any():
+                idx = flat[valid]
+                # Prefer index_add_ over bincount to avoid temp allocations for small vocab
+                ones = torch.ones_like(idx, dtype=torch.int64, device=idx.device)
+                self._expert_counts[layer_id].index_add_(0, idx, ones)
+        else:
+            # Fallback: defer CPU processing without explicit synchronize
+            # Keep CPU tensors to aggregate at dump time
+            self._raw_records[layer_id].append(
+                topk_ids.detach().to("cpu", non_blocking=True)
+            )
 
     def reset(self):
         """Reset the expert distribution recorder."""
         logger.info("Resetting expert distribution record...")
         self._record = False
-        self._expert_distribution_record.clear()
+        self._expert_counts.clear()
+        self._raw_records.clear()
+        self._layer_num_experts.clear()
         self._current_layer_id = "UNKNOWN"
 
     def start_record(self):
-        """Start recording the expert distribution. Reset the recorder and set the recording flag to True."""
-        if self._record == True:
+        """Start recording the expert distribution. Reset then enable recording."""
+        if self._record is True:
             logger.warning(
                 "SGLang server is already recording expert ids. Did you forget to dump the expert ids recorded so far by sending requests to the `/stop_expert_distribution_record` and `/dump_expert_distribution_record` endpoints?"
             )
@@ -56,26 +99,58 @@ class ExpertDistributionRecorder:
 
     def stop_record(self):
         """Stop recording the expert distribution. Set the recording flag to False."""
-        if self._record == False:
+        if self._record is False:
             logger.warning(
                 "SGLang server has not been recording expert ids. Did you forget to start recording by sending request to the `/start_expert_distribution_record` endpoint?"
             )
         self._record = False
 
+    def _aggregate_raw_records(self) -> Dict[int, Dict[int, int]]:
+        """Aggregate raw CPU records into per-layer counts (slow path)."""
+        results: Dict[int, Dict[int, int]] = {}
+        for layer_idx, cpu_tensors in self._raw_records.items():
+            layer_counts: Dict[int, int] = defaultdict(int)
+            for t in cpu_tensors:
+                # accessing .tolist() will implicitly wait for async copies
+                for row in t.tolist():
+                    for expert_idx in row:
+                        if expert_idx >= 0:
+                            layer_counts[int(expert_idx)] += 1
+            results[layer_idx] = layer_counts
+        return results
+
     def dump_record(self):
         """Dump the expert distribution record to a file. Reset the recorder after dumping."""
-        results = {}
-        for layer_idx, layer_record in self._expert_distribution_record.items():
-            results[layer_idx] = defaultdict(int)
-            for token_record in layer_record:
-                for expert_idx in token_record:
-                    results[layer_idx][expert_idx] += 1
+        # Merge fast-path GPU counters and any raw fallback records
+        results: Dict[int, Dict[int, int]] = {}
+
+        # GPU/device counts
+        for layer_idx, counts in self._expert_counts.items():
+            # Bring to CPU once at dump time
+            cpu_counts = counts.to("cpu")
+            layer_counts: Dict[int, int] = {}
+            for expert_idx, cnt in enumerate(cpu_counts.tolist()):
+                if cnt > 0:
+                    layer_counts[int(expert_idx)] = int(cnt)
+            results[layer_idx] = layer_counts
+
+        # Fallback raw records (if any)
+        raw_results = self._aggregate_raw_records()
+        for layer_idx, layer_counts in raw_results.items():
+            if layer_idx not in results:
+                results[layer_idx] = dict(layer_counts)
+            else:
+                for expert_idx, cnt in layer_counts.items():
+                    results[layer_idx][expert_idx] = (
+                        results[layer_idx].get(expert_idx, 0) + cnt
+                    )
+
         with open(
             f"expert_distribution_rank{torch.distributed.get_rank()}_timestamp{time.time()}.csv",
             "w",
         ) as fd:
             fd.write("layer_id,expert_id,count\n")
-            for layer_idx, layer_results in results.items():
-                for expert_idx, count in layer_results.items():
+            for layer_idx in sorted(results.keys()):
+                for expert_idx, count in results[layer_idx].items():
                     fd.write(f"{layer_idx},{expert_idx},{count}\n")
         self.reset()
diff --git a/python/sglang/srt/server_args.py b/python/sglang/srt/server_args.py
index 1e650fe71..3d61e6ce1 100644
--- a/python/sglang/srt/server_args.py
+++ b/python/sglang/srt/server_args.py
@@ -167,6 +167,8 @@ class ServerArgs:
     enable_mixed_chunk: bool = False
     enable_dp_attention: bool = False
     enable_dp_lm_head: bool = False
+    # Expert distribution recorder mode: 'raw' or 'stat' (lightweight counting)
+    expert_distribution_recorder_mode: Optional[str] = None
     enable_ep_moe: bool = False
     enable_deepep_moe: bool = False
     deepep_mode: Optional[Literal["auto", "normal", "low_latency"]] = "auto"
@@ -1129,6 +1131,13 @@ class ServerArgs:
             action="store_true",
             help="Enabling expert parallelism for moe. The ep size is equal to the tp size.",
         )
+        parser.add_argument(
+            "--expert-distribution-recorder-mode",
+            type=str,
+            default=ServerArgs.expert_distribution_recorder_mode,
+            choices=["raw", "stat"],
+            help="Set expert distribution recorder mode. 'stat' minimizes overhead by accumulating counts instead of per-token logging.",
+        )
         parser.add_argument(
             "--enable-torch-compile",
             action="store_true",
diff --git a/python/sglang/srt/utils.py b/python/sglang/srt/utils.py
index 0c16667ef..d03d91c33 100644
--- a/python/sglang/srt/utils.py
+++ b/python/sglang/srt/utils.py
@@ -2100,7 +2100,8 @@ def is_fa3_default_architecture(hf_config):
 # Can be more general if it is used in multiple places (keep it simple and thus not general now)
 class BumpAllocator:
     def __init__(self, buffer_size: int, dtype, device):
-        self._buffer = torch.zeros((buffer_size,), dtype=dtype, device=device)
+        # Use uninitialized buffer; callers are expected to write before read.
+        self._buffer = torch.empty((buffer_size,), dtype=dtype, device=device)
         self._pointer = 0
 
     def allocate(self, size: int):
