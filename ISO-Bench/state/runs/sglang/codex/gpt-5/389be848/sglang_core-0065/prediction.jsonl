{"instance_id": "/home/ubuntu/OmniPerf-Bench/sglan__repo-48efec7", "model_patch": "diff --git a/python/sglang/srt/entrypoints/http_server.py b/python/sglang/srt/entrypoints/http_server.py\nindex 32a11c15c..4af19e6da 100644\n--- a/python/sglang/srt/entrypoints/http_server.py\n+++ b/python/sglang/srt/entrypoints/http_server.py\n@@ -19,6 +19,7 @@ This file implements HTTP APIs for the inference engine via fastapi.\n \n import asyncio\n import dataclasses\n+import json\n import logging\n import multiprocessing as multiprocessing\n import os\n@@ -259,6 +260,43 @@ async def generate_request(obj: GenerateReqInput, request: Request):\n             return _create_error_response(e)\n \n \n+@app.api_route(\"/generate_from_file\", methods=[\"POST\"])\n+async def generate_from_file_request(file: UploadFile, request: Request):\n+    \"\"\"Handle a generate request by reading input_embeds from an uploaded file.\n+\n+    This endpoint exists to speed up and simplify processing of large\n+    input_embeds payloads by avoiding JSON coercion into the request body.\n+    The uploaded file content should be a JSON array compatible with\n+    GenerateReqInput.input_embeds.\n+    \"\"\"\n+    try:\n+        content = await file.read()\n+        # Use orjson if available (imported above) for faster loads; fall back to json\n+        try:\n+            input_embeds = orjson.loads(content)\n+        except Exception:\n+            # Fallback to stdlib json if orjson fails due to unexpected encoding\n+            input_embeds = json.loads(content.decode(\"utf-8\"))\n+\n+        obj = GenerateReqInput(\n+            input_embeds=input_embeds,\n+            sampling_params={\n+                # Reasonable defaults for non-stream generation\n+                \"repetition_penalty\": 1.2,\n+                \"temperature\": 0.2,\n+                \"max_new_tokens\": 512,\n+            },\n+        )\n+\n+        ret = await _global_state.tokenizer_manager.generate_request(\n+            obj, request\n+        ).__anext__()\n+        return ret\n+    except ValueError as e:\n+        logger.error(f\"Error in generate_from_file: {e}\")\n+        return _create_error_response(e)\n+\n+\n @app.api_route(\"/encode\", methods=[\"POST\", \"PUT\"])\n async def encode_request(obj: EmbeddingReqInput, request: Request):\n     \"\"\"Handle an embedding request.\"\"\"\ndiff --git a/test/srt/test_input_embeddings.py b/test/srt/test_input_embeddings.py\nindex 015aabe78..26248c5bc 100644\n--- a/test/srt/test_input_embeddings.py\n+++ b/test/srt/test_input_embeddings.py\n@@ -1,4 +1,5 @@\n import json\n+import io\n import unittest\n \n import requests\n@@ -50,6 +51,27 @@ class TestInputEmbeds(unittest.TestCase):\n             \"error\": f\"Request failed with status {response.status_code}: {response.text}\"\n         }\n \n+    def send_request_file(self, embeddings):\n+        \"\"\"Send a POST request with embeddings as an uploaded file.\n+\n+        This avoids the overhead of embedding JSON in the request body and\n+        leverages the optimized /generate_from_file endpoint.\n+        \"\"\"\n+        content = json.dumps(embeddings).encode(\"utf-8\")\n+        files = {\n+            \"file\": (\"input_embeds.json\", io.BytesIO(content), \"application/json\")\n+        }\n+        response = requests.post(\n+            self.base_url + \"/generate_from_file\",\n+            files=files,\n+            timeout=30,\n+        )\n+        if response.status_code == 200:\n+            return response.json()\n+        return {\n+            \"error\": f\"Request failed with status {response.status_code}: {response.text}\"\n+        }\n+\n     def test_text_based_response(self):\n         \"\"\"Print API response using text-based input.\"\"\"\n         for text in self.texts:\n@@ -77,6 +99,15 @@ class TestInputEmbeds(unittest.TestCase):\n                 f\"Embeddings Input (for text '{text}'):\\nResponse: {json.dumps(response, indent=2)}\\n{'-' * 80}\"\n             )\n \n+    def test_embedding_based_response_from_file(self):\n+        \"\"\"Print API response using input embeddings via file upload.\"\"\"\n+        for text in self.texts:\n+            embeddings = self.generate_input_embeddings(text)\n+            response = self.send_request_file(embeddings)\n+            print(\n+                f\"Embeddings Input via file (for text '{text}'):\\nResponse: {json.dumps(response, indent=2)}\\n{'-' * 80}\"\n+            )\n+\n     def test_compare_text_vs_embedding(self):\n         \"\"\"Print responses for both text-based and embedding-based inputs.\"\"\"\n         for text in self.texts:\n@@ -96,6 +127,8 @@ class TestInputEmbeds(unittest.TestCase):\n             # Get responses\n             text_response = self.send_request(text_payload)\n             embed_response = self.send_request(embed_payload)\n+            # Also exercise file-based path\n+            _ = self.send_request_file(embeddings)\n             # Print responses\n             print(\n                 f\"Text Input: {text}\\nText-Based Response: {json.dumps(text_response, indent=2)}\\n\"\n", "model_name_or_path": "gpt-5-2025-08-07"}
