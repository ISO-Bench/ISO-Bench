{"instance_id": "/home/ubuntu/OmniPerf-Bench/sglan__repo-9d5fa68", "model_patch": "diff --git a/python/sglang/srt/lora/layers.py b/python/sglang/srt/lora/layers.py\nindex aa10ef6b7..0bfb75cdf 100644\n--- a/python/sglang/srt/lora/layers.py\n+++ b/python/sglang/srt/lora/layers.py\n@@ -137,18 +137,18 @@ class MergedColumnParallelLinearWithLoRA(ColumnParallelLinearWithLoRA):\n         self.A_buffer_gate_up = A_buffer\n         if self.lora_backend.fuse_stacked_lora_b:\n             # B_buffer_gate_up: (num_lora, 2 * output_dim, r)\n-            if not hasattr(self, \"B_buffer_gate_up\") or self.B_buffer_gate_up is None:\n+            B0, B1 = B_buffer\n+            num_lora, output_dim, r = B0.shape\n+            target_shape = (num_lora, 2 * output_dim, r)\n+            if getattr(self, \"B_buffer_gate_up\", None) is None or self.B_buffer_gate_up.shape != target_shape:\n                 self.B_buffer_gate_up = torch.empty(\n-                    (\n-                        B_buffer[0].shape[0],\n-                        2 * B_buffer[0].shape[1],\n-                        B_buffer[0].shape[2],\n-                    ),\n-                    dtype=B_buffer[0].dtype,\n-                    device=B_buffer[0].device,\n+                    target_shape,\n+                    dtype=B0.dtype,\n+                    device=B0.device,\n                 )\n-            self.B_buffer_gate_up[:, : B_buffer[0].shape[1], :].copy_(B_buffer[0])\n-            self.B_buffer_gate_up[:, B_buffer[0].shape[1] :, :].copy_(B_buffer[1])\n+            # Copy without creating intermediate tensors\n+            self.B_buffer_gate_up[:, :output_dim, :].copy_(B0)\n+            self.B_buffer_gate_up[:, output_dim:, :].copy_(B1)\n         else:\n             self.B_buffer_gate_up = (B_buffer[0], B_buffer[1])\n \n@@ -202,13 +202,11 @@ class QKVParallelLinearWithLoRA(ColumnParallelLinearWithLoRA):\n             output_dim_q, output_dim_kv = B_buffer_q.shape[-2], B_buffer_kv.shape[-2]\n \n             # B_buffer_qkv: (num_lora, output_dim_q + 2 * output_dim_kv, r)\n-            if not hasattr(self, \"B_buffer_qkv\") or self.B_buffer_qkv is None:\n+            num_lora, r = B_buffer_q[0].shape[0], B_buffer_q[0].shape[2]\n+            target_shape = (num_lora, output_dim_q + 2 * output_dim_kv, r)\n+            if getattr(self, \"B_buffer_qkv\", None) is None or self.B_buffer_qkv.shape != target_shape:\n                 self.B_buffer_qkv = torch.empty(\n-                    (\n-                        B_buffer_q[0].shape[0],\n-                        output_dim_q + 2 * output_dim_kv,\n-                        B_buffer_q[0].shape[2],\n-                    ),\n+                    target_shape,\n                     dtype=B_buffer_q[0].dtype,\n                     device=B_buffer_q[0].device,\n                 )\n@@ -221,20 +219,15 @@ class QKVParallelLinearWithLoRA(ColumnParallelLinearWithLoRA):\n             )\n \n             # Offsets of q/k/v in output dimension\n-            if not hasattr(self, \"output_offset\") or self.output_offset is None:\n+            if getattr(self, \"output_offset\", None) is None:\n                 self.output_offset = torch.empty(\n                     4, dtype=torch.int32, device=B_buffer_q.device\n                 )\n-            self.output_offset[:4] = torch.tensor(\n-                [\n-                    0,\n-                    output_dim_q,\n-                    output_dim_q + output_dim_kv,\n-                    output_dim_q + 2 * output_dim_kv,\n-                ],\n-                dtype=torch.int32,\n-                device=B_buffer_q.device,\n-            )\n+            # Avoid creating temporary tensors which may cause stream sync\n+            self.output_offset[0] = 0\n+            self.output_offset[1] = int(output_dim_q)\n+            self.output_offset[2] = int(output_dim_q + output_dim_kv)\n+            self.output_offset[3] = int(output_dim_q + 2 * output_dim_kv)\n             # For computing number of launched blocks\n             self.max_qkv_out_dim = max(output_dim_q, output_dim_kv)\n         else:\n", "model_name_or_path": "gpt-5-2025-08-07"}
