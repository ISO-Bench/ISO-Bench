{"instance_id": "/home/ubuntu/OmniPerf-Bench/sglan__repo-83452db", "model_patch": "diff --git a/python/sglang/srt/managers/schedule_batch.py b/python/sglang/srt/managers/schedule_batch.py\nindex faf05a7ff..bc9bd0295 100644\n--- a/python/sglang/srt/managers/schedule_batch.py\n+++ b/python/sglang/srt/managers/schedule_batch.py\n@@ -32,6 +32,7 @@ ScheduleBatch -> ModelWorkerBatch -> ForwardBatch\n import dataclasses\n import logging\n from typing import TYPE_CHECKING, List, Optional, Set, Tuple, Union\n+from itertools import chain\n \n import numpy as np\n import torch\n@@ -668,8 +669,8 @@ class ScheduleBatch:\n                     or len(req.prefix_indices) >= im.num_image_tokens\n                 )\n \n-        self.encoder_lens = torch.tensor(self.encoder_lens_cpu, dtype=torch.int32).to(\n-            self.device, non_blocking=True\n+        self.encoder_lens = torch.tensor(\n+            self.encoder_lens_cpu, dtype=torch.int32, device=self.device\n         )\n \n         # Strip encoder infos\n@@ -699,24 +700,18 @@ class ScheduleBatch:\n             pt += req.extend_input_len\n \n         # Reassign\n-        self.input_ids = torch.tensor(sum(input_ids, []), dtype=torch.int32).to(\n-            self.device, non_blocking=True\n-        )\n-        self.seq_lens = torch.tensor(seq_lens, dtype=torch.int32).to(\n-            self.device, non_blocking=True\n+        self.input_ids = torch.tensor(\n+            list(chain.from_iterable(input_ids)), dtype=torch.int32, device=self.device\n         )\n+        self.seq_lens = torch.tensor(seq_lens, dtype=torch.int32, device=self.device)\n \n         if not decoder_out_cache_loc:\n-            self.out_cache_loc = torch.zeros(0, dtype=torch.int32).to(\n-                self.device, non_blocking=True\n-            )\n+            self.out_cache_loc = torch.empty(0, dtype=torch.int32, device=self.device)\n         else:\n             self.out_cache_loc = torch.cat(decoder_out_cache_loc)\n \n         if not encoder_out_cache_loc:\n-            self.encoder_out_cache_loc = torch.zeros(0, dtype=torch.int32).to(\n-                self.device, non_blocking=True\n-            )\n+            self.encoder_out_cache_loc = torch.empty(0, dtype=torch.int32, device=self.device)\n         else:\n             self.encoder_out_cache_loc = torch.cat(encoder_out_cache_loc)\n \n@@ -775,19 +770,15 @@ class ScheduleBatch:\n             pre_lens.append(pre_len)\n \n         # Set fields\n-        self.input_ids = torch.tensor(sum(input_ids, []), dtype=torch.int32).to(\n-            self.device, non_blocking=True\n-        )\n-        self.req_pool_indices = torch.tensor(req_pool_indices, dtype=torch.int32).to(\n-            self.device, non_blocking=True\n+        self.input_ids = torch.tensor(\n+            list(chain.from_iterable(input_ids)), dtype=torch.int32, device=self.device\n         )\n-        self.seq_lens = torch.tensor(seq_lens, dtype=torch.int32).to(\n-            self.device, non_blocking=True\n+        self.req_pool_indices = torch.tensor(\n+            req_pool_indices, dtype=torch.int32, device=self.device\n         )\n+        self.seq_lens = torch.tensor(seq_lens, dtype=torch.int32, device=self.device)\n         self.input_embeds = (\n-            torch.tensor(input_embeds).to(self.device, non_blocking=True)\n-            if input_embeds\n-            else None\n+            torch.tensor(input_embeds, device=self.device) if input_embeds else None\n         )\n \n         self.out_cache_loc = out_cache_loc\n@@ -801,12 +792,8 @@ class ScheduleBatch:\n         self.extend_logprob_start_lens = [r.extend_logprob_start_len for r in reqs]\n \n         # Write to req_to_token_pool\n-        pre_lens = torch.tensor(pre_lens, dtype=torch.int32).to(\n-            self.device, non_blocking=True\n-        )\n-        extend_lens = torch.tensor(self.extend_lens, dtype=torch.int32).to(\n-            self.device, non_blocking=True\n-        )\n+        pre_lens = torch.tensor(pre_lens, dtype=torch.int32, device=self.device)\n+        extend_lens = torch.tensor(self.extend_lens, dtype=torch.int32, device=self.device)\n         if global_server_args_dict[\"attention_backend\"] != \"torch_native\":\n             write_req_to_token_pool_triton[(bs,)](\n                 self.req_to_token_pool.req_to_token,\n@@ -1084,9 +1071,7 @@ class ScheduleBatch:\n             self.encoder_lens_cpu = [self.encoder_lens_cpu[i] for i in keep_indices]\n \n         self.reqs = [self.reqs[i] for i in keep_indices]\n-        new_indices = torch.tensor(keep_indices, dtype=torch.int32).to(\n-            self.device, non_blocking=True\n-        )\n+        new_indices = torch.tensor(keep_indices, dtype=torch.int32, device=self.device)\n         self.req_pool_indices = self.req_pool_indices[new_indices]\n         self.seq_lens = self.seq_lens[new_indices]\n         self.out_cache_loc = None\n", "model_name_or_path": "gpt-5-2025-08-07"}
