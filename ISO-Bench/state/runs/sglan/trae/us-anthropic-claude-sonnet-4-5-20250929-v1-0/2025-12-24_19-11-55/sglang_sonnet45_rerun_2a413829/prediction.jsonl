{"instance_id": "/home/ubuntu/OmniPerf-Bench/sglan__repo-d5c097a", "model_patch": "diff --git a/python/sglang/srt/layers/moe/fused_moe_triton/fused_moe.py b/python/sglang/srt/layers/moe/fused_moe_triton/fused_moe.py\nindex bd1432d..4561791 100644\n--- a/python/sglang/srt/layers/moe/fused_moe_triton/fused_moe.py\n+++ b/python/sglang/srt/layers/moe/fused_moe_triton/fused_moe.py\n@@ -613,10 +613,12 @@ def moe_align_block_size_triton(\n ) -> None:\n     numel = topk_ids.numel()\n     grid = (num_experts,)\n+    # tokens_cnts needs to be zero-initialized because stage1 reads and increments\n     tokens_cnts = torch.zeros(\n         (num_experts + 1, num_experts), dtype=torch.int32, device=topk_ids.device\n     )\n-    cumsum = torch.zeros((num_experts + 1,), dtype=torch.int32, device=topk_ids.device)\n+    # cumsum is only written to, so can use empty\n+    cumsum = torch.empty((num_experts + 1,), dtype=torch.int32, device=topk_ids.device)\n     tokens_per_thread = ceil_div(numel, num_experts)\n \n     moe_align_block_size_stage1[grid](\n@@ -691,10 +693,9 @@ def moe_align_block_size(\n         by block_size for proper block matrix operations.\n     \"\"\"\n     max_num_tokens_padded = topk_ids.numel() + num_experts * (block_size - 1)\n-    sorted_ids = torch.empty(\n-        (max_num_tokens_padded,), dtype=torch.int32, device=topk_ids.device\n+    sorted_ids = torch.full(\n+        (max_num_tokens_padded,), topk_ids.numel(), dtype=torch.int32, device=topk_ids.device\n     )\n-    sorted_ids.fill_(topk_ids.numel())\n     max_num_m_blocks = triton.cdiv(max_num_tokens_padded, block_size)\n     expert_ids = torch.empty(\n         (max_num_m_blocks,), dtype=torch.int32, device=topk_ids.device\n", "model_name_or_path": "gpt-5-2025-08-07"}
