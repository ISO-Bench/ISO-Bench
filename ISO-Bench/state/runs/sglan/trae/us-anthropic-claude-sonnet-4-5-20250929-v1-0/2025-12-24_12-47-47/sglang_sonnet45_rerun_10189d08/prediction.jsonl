{"instance_id": "/home/ubuntu/OmniPerf-Bench/sglan__repo-c4336b2", "model_patch": "diff --git a/python/sglang/srt/managers/scheduler.py b/python/sglang/srt/managers/scheduler.py\nindex 1d1cf36..b53395a 100644\n--- a/python/sglang/srt/managers/scheduler.py\n+++ b/python/sglang/srt/managers/scheduler.py\n@@ -425,8 +425,8 @@ class Scheduler:\n         else:\n             num_tokens = local_batch.extend_num_tokens\n \n-        local_num_tokens = torch.tensor([num_tokens], dtype=torch.int64)\n-        global_num_tokens = torch.empty(self.tp_size, dtype=torch.int64)\n+        local_num_tokens = torch.tensor([num_tokens], dtype=torch.int64, device='cpu')\n+        global_num_tokens = torch.empty(self.tp_size, dtype=torch.int64, device='cpu')\n         torch.distributed.all_gather_into_tensor(\n             global_num_tokens,\n             local_num_tokens,\n@@ -449,6 +449,7 @@ class Scheduler:\n                         else 0\n                     ),\n                     dtype=torch.int32,\n+                    device='cpu',\n                 )\n                 torch.distributed.all_reduce(\n                     forward_mode_state,\n@@ -556,7 +557,8 @@ class Scheduler:\n                 req.origin_input_ids_unpadded, req.image_inputs\n             )\n \n-            if len(req.origin_input_ids) > self.max_req_input_len:\n+            origin_input_ids_len = len(req.origin_input_ids)\n+            if origin_input_ids_len > self.max_req_input_len:\n                 req.finished_reason = FINISH_ABORT(\n                     \"Image request length is longer than the KV cache pool size or \"\n                     \"the max context length aborting because you cannot truncate the image embeds\"\n@@ -575,12 +577,14 @@ class Scheduler:\n             req.logprob_start_len = len(recv_req.input_ids) - 1\n \n         # Truncate prompts that are too long\n-        if len(req.origin_input_ids) > self.max_req_input_len:\n+        origin_input_ids_len = len(req.origin_input_ids)\n+        if origin_input_ids_len > self.max_req_input_len:\n             logger.warning(\n                 \"Request length is longer than the KV cache pool size or \"\n                 \"the max context length. Truncated!!!\"\n             )\n             req.origin_input_ids = req.origin_input_ids[: self.max_req_input_len]\n+            origin_input_ids_len = self.max_req_input_len\n \n         req.sampling_params.max_new_tokens = min(\n             (\n@@ -588,7 +592,7 @@ class Scheduler:\n                 if req.sampling_params.max_new_tokens is not None\n                 else 1 << 30\n             ),\n-            self.max_req_len - len(req.origin_input_ids) - 1,\n+            self.max_req_len - origin_input_ids_len - 1,\n         )\n \n         # Init grammar cache for this request\n@@ -977,13 +981,14 @@ class Scheduler:\n \n             # Check finish conditions\n             logprob_pt = 0\n+            batch_reqs_len = len(batch.reqs)\n             for i, (req, next_token_id) in enumerate(zip(batch.reqs, next_token_ids)):\n                 if req.is_retracted:\n                     continue\n \n                 if self.is_mixed_chunk and self.enable_overlap and req.finished():\n                     # Free the one delayed token for the mixed decode batch\n-                    j = len(batch.out_cache_loc) - len(batch.reqs) + i\n+                    j = len(batch.out_cache_loc) - batch_reqs_len + i\n                     self.token_to_kv_pool.free(batch.out_cache_loc[j : j + 1])\n                     continue\n \n@@ -1287,7 +1292,7 @@ class Scheduler:\n \n         if self.tp_size > 1:\n             # Sync across TP ranks to make sure they have the same number of ready requests\n-            tensor = torch.tensor(num_ready_reqs, dtype=torch.int32)\n+            tensor = torch.tensor(num_ready_reqs, dtype=torch.int32, device='cpu')\n             torch.distributed.all_reduce(\n                 tensor, op=torch.distributed.ReduceOp.MAX, group=self.tp_cpu_group\n             )\ndiff --git a/python/sglang/srt/utils.py b/python/sglang/srt/utils.py\nindex e947d1a..fb8848e 100644\n--- a/python/sglang/srt/utils.py\n+++ b/python/sglang/srt/utils.py\n@@ -183,8 +183,8 @@ def get_available_gpu_memory(device, gpu_id, distributed=False):\n         free_gpu_memory = total_gpu_memory - used_memory\n \n     if distributed:\n-        tensor = torch.tensor(free_gpu_memory, dtype=torch.float32).to(\n-            torch.device(device, gpu_id)\n+        tensor = torch.tensor(\n+            free_gpu_memory, dtype=torch.float32, device=torch.device(device, gpu_id)\n         )\n         torch.distributed.all_reduce(tensor, op=torch.distributed.ReduceOp.MIN)\n         free_gpu_memory = tensor.item()\n@@ -687,7 +687,7 @@ def broadcast_pyobj(\n \n     if rank == 0:\n         if len(data) == 0:\n-            tensor_size = torch.tensor([0], dtype=torch.long)\n+            tensor_size = torch.tensor([0], dtype=torch.long, device='cpu')\n             dist.broadcast(tensor_size, src=0, group=dist_group)\n         else:\n             serialized_data = pickle.dumps(data)\n@@ -695,13 +695,13 @@ def broadcast_pyobj(\n             tensor_data = torch.ByteTensor(\n                 np.frombuffer(serialized_data, dtype=np.uint8)\n             )\n-            tensor_size = torch.tensor([size], dtype=torch.long)\n+            tensor_size = torch.tensor([size], dtype=torch.long, device='cpu')\n \n             dist.broadcast(tensor_size, src=0, group=dist_group)\n             dist.broadcast(tensor_data, src=0, group=dist_group)\n         return data\n     else:\n-        tensor_size = torch.tensor([0], dtype=torch.long)\n+        tensor_size = torch.tensor([0], dtype=torch.long, device='cpu')\n         dist.broadcast(tensor_size, src=0, group=dist_group)\n         size = tensor_size.item()\n \n", "model_name_or_path": "gpt-5-2025-08-07"}
