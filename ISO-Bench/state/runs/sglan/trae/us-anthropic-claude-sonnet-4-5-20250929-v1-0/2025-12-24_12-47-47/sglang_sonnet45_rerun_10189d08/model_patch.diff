diff --git a/python/sglang/srt/managers/scheduler.py b/python/sglang/srt/managers/scheduler.py
index 1d1cf36..b53395a 100644
--- a/python/sglang/srt/managers/scheduler.py
+++ b/python/sglang/srt/managers/scheduler.py
@@ -425,8 +425,8 @@ class Scheduler:
         else:
             num_tokens = local_batch.extend_num_tokens
 
-        local_num_tokens = torch.tensor([num_tokens], dtype=torch.int64)
-        global_num_tokens = torch.empty(self.tp_size, dtype=torch.int64)
+        local_num_tokens = torch.tensor([num_tokens], dtype=torch.int64, device='cpu')
+        global_num_tokens = torch.empty(self.tp_size, dtype=torch.int64, device='cpu')
         torch.distributed.all_gather_into_tensor(
             global_num_tokens,
             local_num_tokens,
@@ -449,6 +449,7 @@ class Scheduler:
                         else 0
                     ),
                     dtype=torch.int32,
+                    device='cpu',
                 )
                 torch.distributed.all_reduce(
                     forward_mode_state,
@@ -556,7 +557,8 @@ class Scheduler:
                 req.origin_input_ids_unpadded, req.image_inputs
             )
 
-            if len(req.origin_input_ids) > self.max_req_input_len:
+            origin_input_ids_len = len(req.origin_input_ids)
+            if origin_input_ids_len > self.max_req_input_len:
                 req.finished_reason = FINISH_ABORT(
                     "Image request length is longer than the KV cache pool size or "
                     "the max context length aborting because you cannot truncate the image embeds"
@@ -575,12 +577,14 @@ class Scheduler:
             req.logprob_start_len = len(recv_req.input_ids) - 1
 
         # Truncate prompts that are too long
-        if len(req.origin_input_ids) > self.max_req_input_len:
+        origin_input_ids_len = len(req.origin_input_ids)
+        if origin_input_ids_len > self.max_req_input_len:
             logger.warning(
                 "Request length is longer than the KV cache pool size or "
                 "the max context length. Truncated!!!"
             )
             req.origin_input_ids = req.origin_input_ids[: self.max_req_input_len]
+            origin_input_ids_len = self.max_req_input_len
 
         req.sampling_params.max_new_tokens = min(
             (
@@ -588,7 +592,7 @@ class Scheduler:
                 if req.sampling_params.max_new_tokens is not None
                 else 1 << 30
             ),
-            self.max_req_len - len(req.origin_input_ids) - 1,
+            self.max_req_len - origin_input_ids_len - 1,
         )
 
         # Init grammar cache for this request
@@ -977,13 +981,14 @@ class Scheduler:
 
             # Check finish conditions
             logprob_pt = 0
+            batch_reqs_len = len(batch.reqs)
             for i, (req, next_token_id) in enumerate(zip(batch.reqs, next_token_ids)):
                 if req.is_retracted:
                     continue
 
                 if self.is_mixed_chunk and self.enable_overlap and req.finished():
                     # Free the one delayed token for the mixed decode batch
-                    j = len(batch.out_cache_loc) - len(batch.reqs) + i
+                    j = len(batch.out_cache_loc) - batch_reqs_len + i
                     self.token_to_kv_pool.free(batch.out_cache_loc[j : j + 1])
                     continue
 
@@ -1287,7 +1292,7 @@ class Scheduler:
 
         if self.tp_size > 1:
             # Sync across TP ranks to make sure they have the same number of ready requests
-            tensor = torch.tensor(num_ready_reqs, dtype=torch.int32)
+            tensor = torch.tensor(num_ready_reqs, dtype=torch.int32, device='cpu')
             torch.distributed.all_reduce(
                 tensor, op=torch.distributed.ReduceOp.MAX, group=self.tp_cpu_group
             )
diff --git a/python/sglang/srt/utils.py b/python/sglang/srt/utils.py
index e947d1a..fb8848e 100644
--- a/python/sglang/srt/utils.py
+++ b/python/sglang/srt/utils.py
@@ -183,8 +183,8 @@ def get_available_gpu_memory(device, gpu_id, distributed=False):
         free_gpu_memory = total_gpu_memory - used_memory
 
     if distributed:
-        tensor = torch.tensor(free_gpu_memory, dtype=torch.float32).to(
-            torch.device(device, gpu_id)
+        tensor = torch.tensor(
+            free_gpu_memory, dtype=torch.float32, device=torch.device(device, gpu_id)
         )
         torch.distributed.all_reduce(tensor, op=torch.distributed.ReduceOp.MIN)
         free_gpu_memory = tensor.item()
@@ -687,7 +687,7 @@ def broadcast_pyobj(
 
     if rank == 0:
         if len(data) == 0:
-            tensor_size = torch.tensor([0], dtype=torch.long)
+            tensor_size = torch.tensor([0], dtype=torch.long, device='cpu')
             dist.broadcast(tensor_size, src=0, group=dist_group)
         else:
             serialized_data = pickle.dumps(data)
@@ -695,13 +695,13 @@ def broadcast_pyobj(
             tensor_data = torch.ByteTensor(
                 np.frombuffer(serialized_data, dtype=np.uint8)
             )
-            tensor_size = torch.tensor([size], dtype=torch.long)
+            tensor_size = torch.tensor([size], dtype=torch.long, device='cpu')
 
             dist.broadcast(tensor_size, src=0, group=dist_group)
             dist.broadcast(tensor_data, src=0, group=dist_group)
         return data
     else:
-        tensor_size = torch.tensor([0], dtype=torch.long)
+        tensor_size = torch.tensor([0], dtype=torch.long, device='cpu')
         dist.broadcast(tensor_size, src=0, group=dist_group)
         size = tensor_size.item()
 
