{"instance_id": "/home/ubuntu/OmniPerf-Bench/sglan__repo-c4336b2", "model_patch": "diff --git a/python/sglang/srt/managers/scheduler.py b/python/sglang/srt/managers/scheduler.py\nindex 1d1cf36..9a7f16b 100644\n--- a/python/sglang/srt/managers/scheduler.py\n+++ b/python/sglang/srt/managers/scheduler.py\n@@ -337,6 +337,12 @@ class Scheduler:\n                     # TODO: Add lora name/path in the future,\n                 },\n             )\n+        \n+        # Pre-allocate frequently used tensors for performance optimization\n+        self._local_num_tokens_buffer = torch.empty(1, dtype=torch.int64)\n+        self._global_num_tokens_buffer = torch.empty(self.tp_size, dtype=torch.int64)\n+        self._forward_mode_state_buffer = torch.empty(1, dtype=torch.int32)\n+        self._num_ready_reqs_buffer = torch.empty(1, dtype=torch.int32)\n \n     def watchdog_thread(self):\n         self.watchdog_last_forward_ct = 0\n@@ -425,13 +431,14 @@ class Scheduler:\n         else:\n             num_tokens = local_batch.extend_num_tokens\n \n-        local_num_tokens = torch.tensor([num_tokens], dtype=torch.int64)\n-        global_num_tokens = torch.empty(self.tp_size, dtype=torch.int64)\n+        # Reuse pre-allocated buffers for better performance\n+        self._local_num_tokens_buffer[0] = num_tokens\n         torch.distributed.all_gather_into_tensor(\n-            global_num_tokens,\n-            local_num_tokens,\n+            self._global_num_tokens_buffer,\n+            self._local_num_tokens_buffer,\n             group=self.tp_cpu_group,\n         )\n+        global_num_tokens = self._global_num_tokens_buffer\n \n         if local_batch is None and global_num_tokens.max().item() > 0:\n             local_batch = self.get_idle_batch()\n@@ -441,21 +448,19 @@ class Scheduler:\n \n             # Check forward mode for cuda graph\n             if not self.server_args.disable_cuda_graph:\n-                forward_mode_state = torch.tensor(\n-                    (\n-                        1\n-                        if local_batch.forward_mode.is_decode()\n-                        or local_batch.forward_mode.is_idle()\n-                        else 0\n-                    ),\n-                    dtype=torch.int32,\n+                # Reuse pre-allocated buffer for better performance\n+                self._forward_mode_state_buffer[0] = (\n+                    1\n+                    if local_batch.forward_mode.is_decode()\n+                    or local_batch.forward_mode.is_idle()\n+                    else 0\n                 )\n                 torch.distributed.all_reduce(\n-                    forward_mode_state,\n+                    self._forward_mode_state_buffer,\n                     op=torch.distributed.ReduceOp.MIN,\n                     group=self.tp_cpu_group,\n                 )\n-                local_batch.can_run_dp_cuda_graph = forward_mode_state.item() == 1\n+                local_batch.can_run_dp_cuda_graph = self._forward_mode_state_buffer.item() == 1\n \n         return local_batch\n \n@@ -875,7 +880,9 @@ class Scheduler:\n             return None\n \n         # Check if decode out of memory\n-        if not batch.check_decode_mem() or (test_retract and batch.batch_size() > 10):\n+        # Optimize: cache batch_size() call to avoid repeated computation\n+        current_bs = batch.batch_size()\n+        if not batch.check_decode_mem() or (test_retract and current_bs > 10):\n             old_ratio = self.new_token_ratio\n \n             retracted_reqs, new_token_ratio = batch.retract_decode()\n@@ -887,6 +894,8 @@ class Scheduler:\n                 f\"#new_token_ratio: {old_ratio:.4f} -> {self.new_token_ratio:.4f}\"\n             )\n             self.waiting_queue.extend(retracted_reqs)\n+            # Update current_bs after retraction\n+            current_bs = batch.batch_size()\n         else:\n             self.new_token_ratio = max(\n                 self.new_token_ratio - self.new_token_ratio_decay,\n@@ -900,8 +909,10 @@ class Scheduler:\n             if batch.is_empty():\n                 self.batch_is_full = False\n                 return None\n+            # Update current_bs after jump-forward\n+            current_bs = batch.batch_size()\n \n-        if batch.batch_size() < initial_bs:\n+        if current_bs < initial_bs:\n             self.batch_is_full = False\n \n         # Update batch tensors\n@@ -925,11 +936,13 @@ class Scheduler:\n             else:\n                 logits_output = None\n                 if self.skip_tokenizer_init:\n-                    next_token_ids = torch.full(\n-                        (batch.batch_size(),), self.tokenizer.eos_token_id\n-                    )\n+                    # Optimize: use zeros + fill for better performance\n+                    batch_size = batch.batch_size()\n+                    next_token_ids = torch.empty(batch_size, dtype=torch.long)\n+                    next_token_ids.fill_(self.tokenizer.eos_token_id)\n                 else:\n-                    next_token_ids = torch.full((batch.batch_size(),), 0)\n+                    # Optimize: zeros is faster than full for zero values\n+                    next_token_ids = torch.zeros(batch.batch_size(), dtype=torch.long)\n             batch.output_ids = next_token_ids\n             ret = logits_output, next_token_ids, model_worker_batch.bid\n         else:  # embedding or reward model\n@@ -1287,11 +1300,12 @@ class Scheduler:\n \n         if self.tp_size > 1:\n             # Sync across TP ranks to make sure they have the same number of ready requests\n-            tensor = torch.tensor(num_ready_reqs, dtype=torch.int32)\n+            # Reuse pre-allocated buffer for better performance\n+            self._num_ready_reqs_buffer[0] = num_ready_reqs\n             torch.distributed.all_reduce(\n-                tensor, op=torch.distributed.ReduceOp.MAX, group=self.tp_cpu_group\n+                self._num_ready_reqs_buffer, op=torch.distributed.ReduceOp.MAX, group=self.tp_cpu_group\n             )\n-            num_ready_reqs_max = tensor.item()\n+            num_ready_reqs_max = self._num_ready_reqs_buffer.item()\n             for i in range(num_ready_reqs, num_ready_reqs_max):\n                 self.grammar_queue[i].grammar = self.grammar_queue[i].grammar.result()\n             num_ready_reqs = num_ready_reqs_max\ndiff --git a/python/sglang/srt/utils.py b/python/sglang/srt/utils.py\nindex e947d1a..ab10c4f 100644\n--- a/python/sglang/srt/utils.py\n+++ b/python/sglang/srt/utils.py\n@@ -183,9 +183,8 @@ def get_available_gpu_memory(device, gpu_id, distributed=False):\n         free_gpu_memory = total_gpu_memory - used_memory\n \n     if distributed:\n-        tensor = torch.tensor(free_gpu_memory, dtype=torch.float32).to(\n-            torch.device(device, gpu_id)\n-        )\n+        # Optimize: create tensor directly on device to avoid data transfer\n+        tensor = torch.tensor([free_gpu_memory], dtype=torch.float32, device=torch.device(device, gpu_id))\n         torch.distributed.all_reduce(tensor, op=torch.distributed.ReduceOp.MIN)\n         free_gpu_memory = tensor.item()\n \n@@ -687,21 +686,26 @@ def broadcast_pyobj(\n \n     if rank == 0:\n         if len(data) == 0:\n-            tensor_size = torch.tensor([0], dtype=torch.long)\n+            # Optimize: use zeros instead of tensor for single value\n+            tensor_size = torch.zeros(1, dtype=torch.long)\n             dist.broadcast(tensor_size, src=0, group=dist_group)\n         else:\n             serialized_data = pickle.dumps(data)\n             size = len(serialized_data)\n-            tensor_data = torch.ByteTensor(\n+            # Optimize: directly create from numpy buffer without intermediate copy\n+            tensor_data = torch.from_numpy(\n                 np.frombuffer(serialized_data, dtype=np.uint8)\n-            )\n-            tensor_size = torch.tensor([size], dtype=torch.long)\n+            ).clone()\n+            # Optimize: use empty + fill for single value tensor\n+            tensor_size = torch.empty(1, dtype=torch.long)\n+            tensor_size[0] = size\n \n             dist.broadcast(tensor_size, src=0, group=dist_group)\n             dist.broadcast(tensor_data, src=0, group=dist_group)\n         return data\n     else:\n-        tensor_size = torch.tensor([0], dtype=torch.long)\n+        # Optimize: use zeros instead of tensor for single value\n+        tensor_size = torch.zeros(1, dtype=torch.long)\n         dist.broadcast(tensor_size, src=0, group=dist_group)\n         size = tensor_size.item()\n \n", "model_name_or_path": "gpt-5-2025-08-07"}
