{"instance_id": "/home/ubuntu/OmniPerf-Bench/sglan__repo-777688b", "model_patch": "diff --git a/python/sglang/srt/lora/lora_manager.py b/python/sglang/srt/lora/lora_manager.py\nindex 45050df..d60c835 100644\n--- a/python/sglang/srt/lora/lora_manager.py\n+++ b/python/sglang/srt/lora/lora_manager.py\n@@ -77,16 +77,16 @@ class LoRAManager:\n         with torch.device(\"cuda\"):\n             self.cuda_graph_batch_info = LoRABatchInfo(\n                 bs=self.max_bs_in_cuda_graph,\n-                seg_lens=torch.zeros(self.max_bs_in_cuda_graph, dtype=torch.int32),\n-                seg_indptr=torch.zeros(\n+                seg_lens=torch.empty(self.max_bs_in_cuda_graph, dtype=torch.int32),\n+                seg_indptr=torch.empty(\n                     self.max_bs_in_cuda_graph + 1, dtype=torch.int32\n                 ),\n                 max_len=0,\n-                weight_indices=torch.zeros(\n+                weight_indices=torch.empty(\n                     self.max_bs_in_cuda_graph, dtype=torch.int32\n                 ),\n-                lora_ranks=torch.zeros(self.max_loras_per_batch, dtype=torch.int32),\n-                scalings=torch.zeros(self.max_loras_per_batch, dtype=torch.float),\n+                lora_ranks=torch.empty(self.max_loras_per_batch, dtype=torch.int32),\n+                scalings=torch.empty(self.max_loras_per_batch, dtype=torch.float),\n             )\n \n     def init_loras(self):\n@@ -167,10 +167,12 @@ class LoRAManager:\n             # Do in-place updates when CUDA graph is enabled and the batch forward mode\n             # could use CUDA graph.\n             self.cuda_graph_batch_info.bs = bs\n-            self.cuda_graph_batch_info.seg_lens[:bs].fill_(1)\n-            torch.cumsum(\n-                self.cuda_graph_batch_info.seg_lens[:bs],\n-                dim=0,\n+            # Optimize: directly set seg_indptr values instead of fill + cumsum\n+            # Since seg_lens would all be 1, cumsum produces [1, 2, 3, ..., bs]\n+            torch.arange(\n+                1, bs + 1,\n+                dtype=torch.int32,\n+                device=self.cuda_graph_batch_info.seg_indptr.device,\n                 out=self.cuda_graph_batch_info.seg_indptr[1 : bs + 1],\n             )\n             self.cuda_graph_batch_info.max_len = 1\n@@ -194,15 +196,16 @@ class LoRAManager:\n                 if forward_batch.forward_mode.is_extend()\n                 else torch.ones(bs, device=self.device)\n             )\n-            seg_indptr = torch.zeros((bs + 1,), dtype=torch.int32, device=self.device)\n-            seg_indptr[1:] = torch.cumsum(seg_lens, dim=0)\n+            seg_indptr = torch.empty((bs + 1,), dtype=torch.int32, device=self.device)\n+            seg_indptr[0] = 0\n+            torch.cumsum(seg_lens, dim=0, out=seg_indptr[1:])\n             max_len = int(torch.max(seg_lens))\n             weight_indices = torch.empty((bs,), dtype=torch.int64, device=self.device)\n \n-            lora_ranks = torch.zeros(\n+            lora_ranks = torch.empty(\n                 (self.max_loras_per_batch,), dtype=torch.int64, device=\"cuda\"\n             )\n-            scalings = torch.zeros(\n+            scalings = torch.empty(\n                 (self.max_loras_per_batch,), dtype=torch.float, device=\"cuda\"\n             )\n             for i, lora_path in enumerate(forward_batch.lora_paths):\ndiff --git a/python/sglang/srt/lora/mem_pool.py b/python/sglang/srt/lora/mem_pool.py\nindex 8b8d213..9f5733f 100644\n--- a/python/sglang/srt/lora/mem_pool.py\n+++ b/python/sglang/srt/lora/mem_pool.py\n@@ -129,15 +129,21 @@ class LoRAMemoryPool:\n     ):\n \n         def get_available_buffer_slot():\n+            evictable_buffer_id = None\n+            evictable_uid = None\n+            \n             for buffer_id in range(self.max_loras_per_batch):\n+                uid = self.buffer_id_to_uid[buffer_id]\n                 # Prioritize empty slots\n-                if self.buffer_id_to_uid[buffer_id] == \"\":\n+                if uid == \"\":\n                     return buffer_id, \"\"\n-\n-            for buffer_id in range(self.max_loras_per_batch):\n-                # Evict unneeded lora\n-                if self.buffer_id_to_uid[buffer_id] not in cur_uids:\n-                    return buffer_id, self.buffer_id_to_uid[buffer_id]\n+                # Track first evictable slot\n+                if evictable_buffer_id is None and uid not in cur_uids:\n+                    evictable_buffer_id = buffer_id\n+                    evictable_uid = uid\n+            \n+            if evictable_buffer_id is not None:\n+                return evictable_buffer_id, evictable_uid\n \n             raise ValueError(\n                 \"No available buffer slots found. Please ensure the number of active loras is less than max_loras_per_batch.\"\n@@ -165,7 +171,7 @@ class LoRAMemoryPool:\n         if uid is None:\n             for i in range(self.num_layer):\n                 for k in self.A_buffer.keys():\n-                    self.A_buffer[k][i][buffer_id] = 0\n+                    self.A_buffer[k][i][buffer_id].zero_()\n             return\n \n         assert lora_adapter is not None\n", "model_name_or_path": "gpt-5-2025-08-07"}
