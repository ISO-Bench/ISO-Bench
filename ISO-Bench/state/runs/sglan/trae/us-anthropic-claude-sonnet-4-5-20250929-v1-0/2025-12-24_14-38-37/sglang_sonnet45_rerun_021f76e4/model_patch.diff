diff --git a/python/sglang/srt/lora/lora_manager.py b/python/sglang/srt/lora/lora_manager.py
index 45050df..1e85365 100644
--- a/python/sglang/srt/lora/lora_manager.py
+++ b/python/sglang/srt/lora/lora_manager.py
@@ -77,16 +77,16 @@ class LoRAManager:
         with torch.device("cuda"):
             self.cuda_graph_batch_info = LoRABatchInfo(
                 bs=self.max_bs_in_cuda_graph,
-                seg_lens=torch.zeros(self.max_bs_in_cuda_graph, dtype=torch.int32),
-                seg_indptr=torch.zeros(
+                seg_lens=torch.empty(self.max_bs_in_cuda_graph, dtype=torch.int32),
+                seg_indptr=torch.empty(
                     self.max_bs_in_cuda_graph + 1, dtype=torch.int32
                 ),
                 max_len=0,
-                weight_indices=torch.zeros(
+                weight_indices=torch.empty(
                     self.max_bs_in_cuda_graph, dtype=torch.int32
                 ),
-                lora_ranks=torch.zeros(self.max_loras_per_batch, dtype=torch.int32),
-                scalings=torch.zeros(self.max_loras_per_batch, dtype=torch.float),
+                lora_ranks=torch.empty(self.max_loras_per_batch, dtype=torch.int32),
+                scalings=torch.empty(self.max_loras_per_batch, dtype=torch.float),
             )
 
     def init_loras(self):
@@ -194,15 +194,16 @@ class LoRAManager:
                 if forward_batch.forward_mode.is_extend()
                 else torch.ones(bs, device=self.device)
             )
-            seg_indptr = torch.zeros((bs + 1,), dtype=torch.int32, device=self.device)
-            seg_indptr[1:] = torch.cumsum(seg_lens, dim=0)
+            seg_indptr = torch.empty((bs + 1,), dtype=torch.int32, device=self.device)
+            seg_indptr[0] = 0
+            torch.cumsum(seg_lens, dim=0, out=seg_indptr[1:])
             max_len = int(torch.max(seg_lens))
             weight_indices = torch.empty((bs,), dtype=torch.int64, device=self.device)
 
-            lora_ranks = torch.zeros(
+            lora_ranks = torch.empty(
                 (self.max_loras_per_batch,), dtype=torch.int64, device="cuda"
             )
-            scalings = torch.zeros(
+            scalings = torch.empty(
                 (self.max_loras_per_batch,), dtype=torch.float, device="cuda"
             )
             for i, lora_path in enumerate(forward_batch.lora_paths):
diff --git a/python/sglang/srt/lora/mem_pool.py b/python/sglang/srt/lora/mem_pool.py
index 8b8d213..336aee6 100644
--- a/python/sglang/srt/lora/mem_pool.py
+++ b/python/sglang/srt/lora/mem_pool.py
@@ -45,6 +45,9 @@ class LoRAMemoryPool:
         #   (stacked_num, max_loras_per_batch, output_dim, max_lora_dim)
         self.A_buffer: Dict[str, List[torch.Tensor]] = {}
         self.B_buffer: Dict[str, List[torch.Tensor]] = {}
+        
+        # Cache for stacked multiply values to avoid repeated lookups
+        self._stacked_multiply_cache: Dict[str, int] = {}
 
         # Lora uid -> buffer idx in memory pool
         self.uid_to_buffer_id: Dict[Optional[str], int] = {}
@@ -165,7 +168,7 @@ class LoRAMemoryPool:
         if uid is None:
             for i in range(self.num_layer):
                 for k in self.A_buffer.keys():
-                    self.A_buffer[k][i][buffer_id] = 0
+                    self.A_buffer[k][i][buffer_id].zero_()
             return
 
         assert lora_adapter is not None
@@ -211,7 +214,9 @@ class LoRAMemoryPool:
                         )
 
             for name, weights in temp_A_buffer.items():
-                c = get_stacked_multiply(name)
+                if name not in self._stacked_multiply_cache:
+                    self._stacked_multiply_cache[name] = get_stacked_multiply(name)
+                c = self._stacked_multiply_cache[name]
                 buffer_view = self.A_buffer[name][layer_id][buffer_id][
                     : lora_rank * c, :
                 ]
@@ -219,7 +224,9 @@ class LoRAMemoryPool:
                 buffer_view.copy_(weights)
 
             for name, weights in temp_B_buffer.items():
-                c = get_stacked_multiply(name)
+                if name not in self._stacked_multiply_cache:
+                    self._stacked_multiply_cache[name] = get_stacked_multiply(name)
+                c = self._stacked_multiply_cache[name]
                 if c > 1:
                     for stacked_id in range(c):
                         buffer_view = self.B_buffer[name][layer_id][stacked_id][
