{"instance_id": "/home/ubuntu/OmniPerf-Bench/sglan__repo-a4d6d6f", "model_patch": "diff --git a/docker/Dockerfile.rocm b/docker/Dockerfile.rocm\nindex 84ea69c..d1bd70f 100644\n--- a/docker/Dockerfile.rocm\n+++ b/docker/Dockerfile.rocm\n@@ -60,5 +60,7 @@ ENV VLLM_FP8_WEIGHT_PADDING=1\n ENV VLLM_FP8_REDUCE_CONV=1\n ENV TORCHINDUCTOR_MAX_AUTOTUNE=1\n ENV TORCHINDUCTOR_MAX_AUTOTUNE_POINTWISE=1\n+ENV PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True\n+ENV CUDA_LAUNCH_BLOCKING=0\n \n CMD [\"/bin/bash\"]\ndiff --git a/python/sglang/srt/layers/moe/fused_moe_triton/fused_moe.py b/python/sglang/srt/layers/moe/fused_moe_triton/fused_moe.py\nindex cbacd90..da2adb8 100644\n--- a/python/sglang/srt/layers/moe/fused_moe_triton/fused_moe.py\n+++ b/python/sglang/srt/layers/moe/fused_moe_triton/fused_moe.py\n@@ -263,10 +263,9 @@ def moe_align_block_size(\n         by block_size for proper block matrix operations.\n     \"\"\"\n     max_num_tokens_padded = topk_ids.numel() + num_experts * (block_size - 1)\n-    sorted_ids = torch.empty(\n-        (max_num_tokens_padded,), dtype=torch.int32, device=topk_ids.device\n+    sorted_ids = torch.full(\n+        (max_num_tokens_padded,), topk_ids.numel(), dtype=torch.int32, device=topk_ids.device\n     )\n-    sorted_ids.fill_(topk_ids.numel())\n     max_num_m_blocks = triton.cdiv(max_num_tokens_padded, block_size)\n     expert_ids = torch.empty(\n         (max_num_m_blocks,), dtype=torch.int32, device=topk_ids.device\n", "model_name_or_path": "gpt-5-2025-08-07"}
