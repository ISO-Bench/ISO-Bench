{"instance_id": "/home/ubuntu/OmniPerf-Bench/sglan__repo-777688b", "model_patch": "diff --git a/python/sglang/srt/lora/lora_manager.py b/python/sglang/srt/lora/lora_manager.py\nindex 45050df..f3a30c4 100644\n--- a/python/sglang/srt/lora/lora_manager.py\n+++ b/python/sglang/srt/lora/lora_manager.py\n@@ -75,18 +75,20 @@ class LoRAManager:\n     def init_cuda_graph_batch_info(self, max_bs_in_cuda_graph: int):\n         self.max_bs_in_cuda_graph = max_bs_in_cuda_graph\n         with torch.device(\"cuda\"):\n+            seg_indptr = torch.empty(\n+                self.max_bs_in_cuda_graph + 1, dtype=torch.int32\n+            )\n+            seg_indptr[0] = 0\n             self.cuda_graph_batch_info = LoRABatchInfo(\n                 bs=self.max_bs_in_cuda_graph,\n-                seg_lens=torch.zeros(self.max_bs_in_cuda_graph, dtype=torch.int32),\n-                seg_indptr=torch.zeros(\n-                    self.max_bs_in_cuda_graph + 1, dtype=torch.int32\n-                ),\n+                seg_lens=torch.empty(self.max_bs_in_cuda_graph, dtype=torch.int32),\n+                seg_indptr=seg_indptr,\n                 max_len=0,\n-                weight_indices=torch.zeros(\n+                weight_indices=torch.empty(\n                     self.max_bs_in_cuda_graph, dtype=torch.int32\n                 ),\n-                lora_ranks=torch.zeros(self.max_loras_per_batch, dtype=torch.int32),\n-                scalings=torch.zeros(self.max_loras_per_batch, dtype=torch.float),\n+                lora_ranks=torch.empty(self.max_loras_per_batch, dtype=torch.int32),\n+                scalings=torch.empty(self.max_loras_per_batch, dtype=torch.float),\n             )\n \n     def init_loras(self):\n@@ -189,20 +191,21 @@ class LoRAManager:\n                     ] = lora.scaling\n             batch_info = self.cuda_graph_batch_info\n         else:\n-            seg_lens = (\n-                forward_batch.extend_seq_lens\n-                if forward_batch.forward_mode.is_extend()\n-                else torch.ones(bs, device=self.device)\n-            )\n-            seg_indptr = torch.zeros((bs + 1,), dtype=torch.int32, device=self.device)\n-            seg_indptr[1:] = torch.cumsum(seg_lens, dim=0)\n+            if forward_batch.forward_mode.is_extend():\n+                seg_lens = forward_batch.extend_seq_lens\n+            else:\n+                seg_lens = torch.empty(bs, device=self.device)\n+                seg_lens.fill_(1)\n+            seg_indptr = torch.empty((bs + 1,), dtype=torch.int32, device=self.device)\n+            seg_indptr[0] = 0\n+            torch.cumsum(seg_lens, dim=0, out=seg_indptr[1:])\n             max_len = int(torch.max(seg_lens))\n             weight_indices = torch.empty((bs,), dtype=torch.int64, device=self.device)\n \n-            lora_ranks = torch.zeros(\n+            lora_ranks = torch.empty(\n                 (self.max_loras_per_batch,), dtype=torch.int64, device=\"cuda\"\n             )\n-            scalings = torch.zeros(\n+            scalings = torch.empty(\n                 (self.max_loras_per_batch,), dtype=torch.float, device=\"cuda\"\n             )\n             for i, lora_path in enumerate(forward_batch.lora_paths):\ndiff --git a/python/sglang/srt/lora/mem_pool.py b/python/sglang/srt/lora/mem_pool.py\nindex 8b8d213..9f5733f 100644\n--- a/python/sglang/srt/lora/mem_pool.py\n+++ b/python/sglang/srt/lora/mem_pool.py\n@@ -129,15 +129,21 @@ class LoRAMemoryPool:\n     ):\n \n         def get_available_buffer_slot():\n+            evictable_buffer_id = None\n+            evictable_uid = None\n+            \n             for buffer_id in range(self.max_loras_per_batch):\n+                uid = self.buffer_id_to_uid[buffer_id]\n                 # Prioritize empty slots\n-                if self.buffer_id_to_uid[buffer_id] == \"\":\n+                if uid == \"\":\n                     return buffer_id, \"\"\n-\n-            for buffer_id in range(self.max_loras_per_batch):\n-                # Evict unneeded lora\n-                if self.buffer_id_to_uid[buffer_id] not in cur_uids:\n-                    return buffer_id, self.buffer_id_to_uid[buffer_id]\n+                # Track first evictable slot\n+                if evictable_buffer_id is None and uid not in cur_uids:\n+                    evictable_buffer_id = buffer_id\n+                    evictable_uid = uid\n+            \n+            if evictable_buffer_id is not None:\n+                return evictable_buffer_id, evictable_uid\n \n             raise ValueError(\n                 \"No available buffer slots found. Please ensure the number of active loras is less than max_loras_per_batch.\"\n@@ -165,7 +171,7 @@ class LoRAMemoryPool:\n         if uid is None:\n             for i in range(self.num_layer):\n                 for k in self.A_buffer.keys():\n-                    self.A_buffer[k][i][buffer_id] = 0\n+                    self.A_buffer[k][i][buffer_id].zero_()\n             return\n \n         assert lora_adapter is not None\n", "model_name_or_path": "gpt-5-2025-08-07"}
