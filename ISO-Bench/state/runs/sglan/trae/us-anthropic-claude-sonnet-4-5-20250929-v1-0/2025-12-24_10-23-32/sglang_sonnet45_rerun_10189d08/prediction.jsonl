{"instance_id": "/home/ubuntu/OmniPerf-Bench/sglan__repo-c4336b2", "model_patch": "diff --git a/python/sglang/srt/managers/scheduler.py b/python/sglang/srt/managers/scheduler.py\nindex 1d1cf36..c12de3d 100644\n--- a/python/sglang/srt/managers/scheduler.py\n+++ b/python/sglang/srt/managers/scheduler.py\n@@ -425,7 +425,9 @@ class Scheduler:\n         else:\n             num_tokens = local_batch.extend_num_tokens\n \n-        local_num_tokens = torch.tensor([num_tokens], dtype=torch.int64)\n+        # Optimize: Use torch.empty + fill instead of torch.tensor for better performance\n+        local_num_tokens = torch.empty(1, dtype=torch.int64)\n+        local_num_tokens[0] = num_tokens\n         global_num_tokens = torch.empty(self.tp_size, dtype=torch.int64)\n         torch.distributed.all_gather_into_tensor(\n             global_num_tokens,\n@@ -441,14 +443,13 @@ class Scheduler:\n \n             # Check forward mode for cuda graph\n             if not self.server_args.disable_cuda_graph:\n-                forward_mode_state = torch.tensor(\n-                    (\n-                        1\n-                        if local_batch.forward_mode.is_decode()\n-                        or local_batch.forward_mode.is_idle()\n-                        else 0\n-                    ),\n-                    dtype=torch.int32,\n+                # Optimize: Use torch.empty + fill instead of torch.tensor\n+                forward_mode_state = torch.empty(1, dtype=torch.int32)\n+                forward_mode_state[0] = (\n+                    1\n+                    if local_batch.forward_mode.is_decode()\n+                    or local_batch.forward_mode.is_idle()\n+                    else 0\n                 )\n                 torch.distributed.all_reduce(\n                     forward_mode_state,\n@@ -819,8 +820,10 @@ class Scheduler:\n         can_run_list = adder.can_run_list\n         if len(can_run_list) == 0:\n             return None\n+        # Optimize: Create set once instead of in list comprehension\n+        can_run_set = set(can_run_list)\n         self.waiting_queue = [\n-            x for x in self.waiting_queue if x not in set(can_run_list)\n+            x for x in self.waiting_queue if x not in can_run_set\n         ]\n \n         if adder.new_inflight_req is not None:\n@@ -924,12 +927,13 @@ class Scheduler:\n                 return\n             else:\n                 logits_output = None\n+                # Optimize: Use torch.empty + fill_ instead of torch.full for better performance\n+                batch_size = batch.batch_size()\n+                next_token_ids = torch.empty(batch_size, dtype=torch.long)\n                 if self.skip_tokenizer_init:\n-                    next_token_ids = torch.full(\n-                        (batch.batch_size(),), self.tokenizer.eos_token_id\n-                    )\n+                    next_token_ids.fill_(self.tokenizer.eos_token_id)\n                 else:\n-                    next_token_ids = torch.full((batch.batch_size(),), 0)\n+                    next_token_ids.fill_(0)\n             batch.output_ids = next_token_ids\n             ret = logits_output, next_token_ids, model_worker_batch.bid\n         else:  # embedding or reward model\n@@ -1122,10 +1126,12 @@ class Scheduler:\n             input_token_logprobs = output.input_token_logprobs[\n                 pt : pt + num_input_logprobs - 1 - req.last_update_decode_tokens\n             ]\n+            # Optimize: Cache len(req.fill_ids) to avoid redundant calls\n+            fill_ids_len = len(req.fill_ids)\n             input_token_ids = req.fill_ids[\n-                len(req.fill_ids)\n+                fill_ids_len\n                 - num_input_logprobs\n-                + 1 : len(req.fill_ids)\n+                + 1 : fill_ids_len\n                 - req.last_update_decode_tokens\n             ]\n             req.input_token_logprobs = list(zip(input_token_logprobs, input_token_ids))\n@@ -1139,6 +1145,8 @@ class Scheduler:\n \n         if req.last_update_decode_tokens != 0:\n             # Some decode tokens are re-computed in an extend batch\n+            # Optimize: Cache len(req.fill_ids) to avoid redundant calls\n+            fill_ids_len = len(req.fill_ids)\n             req.output_token_logprobs.extend(\n                 list(\n                     zip(\n@@ -1151,8 +1159,8 @@ class Scheduler:\n                             - 1\n                         ],\n                         req.fill_ids[\n-                            len(req.fill_ids)\n-                            - req.last_update_decode_tokens : len(req.fill_ids)\n+                            fill_ids_len\n+                            - req.last_update_decode_tokens : fill_ids_len\n                         ],\n                     )\n                 )\n@@ -1287,7 +1295,9 @@ class Scheduler:\n \n         if self.tp_size > 1:\n             # Sync across TP ranks to make sure they have the same number of ready requests\n-            tensor = torch.tensor(num_ready_reqs, dtype=torch.int32)\n+            # Optimize: Use torch.empty + fill instead of torch.tensor\n+            tensor = torch.empty(1, dtype=torch.int32)\n+            tensor[0] = num_ready_reqs\n             torch.distributed.all_reduce(\n                 tensor, op=torch.distributed.ReduceOp.MAX, group=self.tp_cpu_group\n             )\ndiff --git a/python/sglang/srt/utils.py b/python/sglang/srt/utils.py\nindex e947d1a..7f2a4c0 100644\n--- a/python/sglang/srt/utils.py\n+++ b/python/sglang/srt/utils.py\n@@ -183,9 +183,9 @@ def get_available_gpu_memory(device, gpu_id, distributed=False):\n         free_gpu_memory = total_gpu_memory - used_memory\n \n     if distributed:\n-        tensor = torch.tensor(free_gpu_memory, dtype=torch.float32).to(\n-            torch.device(device, gpu_id)\n-        )\n+        # Optimize: Use torch.empty + fill instead of torch.tensor for better performance\n+        tensor = torch.empty(1, dtype=torch.float32, device=torch.device(device, gpu_id))\n+        tensor[0] = free_gpu_memory\n         torch.distributed.all_reduce(tensor, op=torch.distributed.ReduceOp.MIN)\n         free_gpu_memory = tensor.item()\n \n@@ -687,7 +687,9 @@ def broadcast_pyobj(\n \n     if rank == 0:\n         if len(data) == 0:\n-            tensor_size = torch.tensor([0], dtype=torch.long)\n+            # Optimize: Use torch.empty + fill instead of torch.tensor\n+            tensor_size = torch.empty(1, dtype=torch.long)\n+            tensor_size[0] = 0\n             dist.broadcast(tensor_size, src=0, group=dist_group)\n         else:\n             serialized_data = pickle.dumps(data)\n@@ -695,13 +697,17 @@ def broadcast_pyobj(\n             tensor_data = torch.ByteTensor(\n                 np.frombuffer(serialized_data, dtype=np.uint8)\n             )\n-            tensor_size = torch.tensor([size], dtype=torch.long)\n+            # Optimize: Use torch.empty + fill instead of torch.tensor\n+            tensor_size = torch.empty(1, dtype=torch.long)\n+            tensor_size[0] = size\n \n             dist.broadcast(tensor_size, src=0, group=dist_group)\n             dist.broadcast(tensor_data, src=0, group=dist_group)\n         return data\n     else:\n-        tensor_size = torch.tensor([0], dtype=torch.long)\n+        # Optimize: Use torch.empty + fill instead of torch.tensor\n+        tensor_size = torch.empty(1, dtype=torch.long)\n+        tensor_size[0] = 0\n         dist.broadcast(tensor_size, src=0, group=dist_group)\n         size = tensor_size.item()\n \n", "model_name_or_path": "gpt-5-2025-08-07"}
