{"instance_id": "/home/ubuntu/OmniPerf-Bench/sglan__repo-2cea614", "model_patch": "diff --git a/python/sglang/srt/layers/logits_processor.py b/python/sglang/srt/layers/logits_processor.py\nindex 53d6620..c3177ef 100644\n--- a/python/sglang/srt/layers/logits_processor.py\n+++ b/python/sglang/srt/layers/logits_processor.py\n@@ -49,8 +49,8 @@ class LogitsProcessor(nn.Module):\n         else:\n             prefill_top_logprobs, decode_top_logprobs = [], []\n             pt = 0\n-            # NOTE: the GPU-CPU overhead can be reduced\n-            extend_seq_lens_cpu = input_metadata.extend_seq_lens.cpu().numpy()\n+            # Optimize: use tolist() instead of cpu().numpy() for better performance\n+            extend_seq_lens_cpu = input_metadata.extend_seq_lens.tolist()\n             for i in range(len(extend_seq_lens_cpu)):\n                 if extend_seq_lens_cpu[i] == 0:\n                     prefill_top_logprobs.append([])\n@@ -101,9 +101,9 @@ class LogitsProcessor(nn.Module):\n                     all_logits = tensor_model_parallel_all_gather(all_logits)\n                 all_logits = all_logits[:, : self.config.vocab_size]\n \n-            all_logprobs = all_logits.float()\n+            # Optimize: compute log_softmax directly without intermediate copy\n+            all_logprobs = torch.nn.functional.log_softmax(all_logits.float(), dim=-1)\n             del all_logits\n-            all_logprobs[:] = torch.nn.functional.log_softmax(all_logprobs, dim=-1)\n \n             return_top_logprob = any(x > 0 for x in input_metadata.top_logprobs_nums)\n             if return_top_logprob:\ndiff --git a/python/sglang/srt/managers/detokenizer_manager.py b/python/sglang/srt/managers/detokenizer_manager.py\nindex eeefbe0..8d42217 100644\n--- a/python/sglang/srt/managers/detokenizer_manager.py\n+++ b/python/sglang/srt/managers/detokenizer_manager.py\n@@ -50,24 +50,25 @@ class DetokenizerManager:\n \n                 # Trim stop str\n                 # TODO(lmzheng): handle the case where multiple stop strs are hit\n-                for i in range(len(output_strs)):\n-                    if recv_obj.hit_stop_str[i] is not None:\n-                        pos = output_strs[i].find(recv_obj.hit_stop_str[i])\n+                # Optimize: use enumerate and reduce list indexing overhead\n+                for i, output_str in enumerate(output_strs):\n+                    hit_stop = recv_obj.hit_stop_str[i]\n+                    if hit_stop is not None:\n+                        pos = output_str.find(hit_stop)\n                         if pos != -1:\n-                            output_strs[i] = output_strs[i][:pos]\n+                            output_str = output_str[:pos]\n \n-                    if len(output_tokens[i]) > 0:\n+                    tokens_i = output_tokens[i]\n+                    if len(tokens_i) > 0:\n                         first_token = self.tokenizer.convert_ids_to_tokens(\n-                            int(output_tokens[i][0])\n+                            int(tokens_i[0])\n                         )\n                         if not isinstance(first_token, str):\n                             first_token = first_token.decode(\"utf-8\", errors=\"ignore\")\n                         if first_token.startswith(\"\u2581\"):\n-                            output_strs[i] = \" \" + output_strs[i]\n+                            output_str = \" \" + output_str\n \n-                    output_strs[i] = (\n-                        recv_obj.output_and_jump_forward_strs[i] + output_strs[i]\n-                    )\n+                    output_strs[i] = recv_obj.output_and_jump_forward_strs[i] + output_str\n \n                 self.send_to_tokenizer.send_pyobj(\n                     BatchStrOut(\ndiff --git a/python/sglang/srt/managers/router/infer_batch.py b/python/sglang/srt/managers/router/infer_batch.py\nindex dbe9437..ae4f618 100644\n--- a/python/sglang/srt/managers/router/infer_batch.py\n+++ b/python/sglang/srt/managers/router/infer_batch.py\n@@ -250,7 +250,8 @@ class Batch:\n \n             seq_lens.append(prefix_lens[-1] + extend_lens[-1])\n \n-        position_ids_offsets = torch.zeros((bs,), dtype=torch.int32, device=device)\n+        position_ids_offsets = torch.empty((bs,), dtype=torch.int32, device=device)\n+        position_ids_offsets.fill_(0)\n \n         # Alloc mem\n         seq_lens, prefix_lens = np.array(seq_lens), np.array(prefix_lens)\n@@ -277,9 +278,10 @@ class Batch:\n         for i in range(bs):\n             if reqs[i].sampling_params.dtype == \"int\":\n                 if logit_bias is None:\n-                    logit_bias = torch.zeros(\n+                    logit_bias = torch.empty(\n                         (bs, vocab_size), dtype=torch.float32, device=device\n                     )\n+                    logit_bias.fill_(0)\n                 logit_bias[i] = int_token_logit_bias\n \n         # Set fields\n@@ -292,8 +294,9 @@ class Batch:\n             r.image_offset - p_len for r, p_len in zip(reqs, prefix_lens)\n         ]\n         self.req_pool_indices = req_pool_indices\n-        self.seq_lens = torch.tensor(seq_lens, dtype=torch.int32, device=device)\n-        self.prefix_lens = torch.tensor(prefix_lens, dtype=torch.int32, device=device)\n+        # Optimize: use torch.from_numpy for better performance with numpy arrays\n+        self.seq_lens = torch.from_numpy(seq_lens).to(device=device, dtype=torch.int32)\n+        self.prefix_lens = torch.from_numpy(prefix_lens).to(device=device, dtype=torch.int32)\n         self.position_ids_offsets = position_ids_offsets\n         self.extend_num_tokens = extend_num_tokens\n         self.out_cache_loc = out_cache_loc\n@@ -495,13 +498,15 @@ class Batch:\n                 else other.logit_bias.shape[1]\n             )\n             if self.logit_bias is None:\n-                self.logit_bias = torch.zeros(\n+                self.logit_bias = torch.empty(\n                     (len(self.reqs), vocab_size), dtype=torch.float32, device=\"cuda\"\n                 )\n+                self.logit_bias.fill_(0)\n             if other.logit_bias is None:\n-                other.logit_bias = torch.zeros(\n+                other.logit_bias = torch.empty(\n                     (len(other.reqs), vocab_size), dtype=torch.float32, device=\"cuda\"\n                 )\n+                other.logit_bias.fill_(0)\n             self.logit_bias = torch.concat([self.logit_bias, other.logit_bias])\n \n     def sample(self, logits: torch.Tensor):\ndiff --git a/python/sglang/srt/managers/router/model_rpc.py b/python/sglang/srt/managers/router/model_rpc.py\nindex 6abb20b..55b007a 100644\n--- a/python/sglang/srt/managers/router/model_rpc.py\n+++ b/python/sglang/srt/managers/router/model_rpc.py\n@@ -566,30 +566,37 @@ class ModelRpcServer:\n         output_finished = []\n         finished_indices = []\n         unfinished_indices = []\n+        \n+        # Optimize: cache frequently accessed attributes\n+        decode_forward_ct = self.decode_forward_ct\n+        stream_interval = self.stream_interval\n+        \n         for i, req in enumerate(batch.reqs):\n-            if req.finished:\n+            req_finished = req.finished\n+            if req_finished:\n                 finished_indices.append(i)\n             else:\n                 unfinished_indices.append(i)\n \n-            if req.finished or (\n+            if req_finished or (\n                 (\n                     req.stream\n                     and (\n-                        self.decode_forward_ct % self.stream_interval == 0\n+                        decode_forward_ct % stream_interval == 0\n                         or len(req.output_ids) == 1\n                     )\n                 )\n             ):\n+                sampling_params = req.sampling_params\n                 output_rids.append(req.rid)\n                 output_tokens.append(req.output_ids)\n                 output_and_jump_forward_strs.append(req.output_and_jump_forward_str)\n                 output_hit_stop_str.append(req.hit_stop_str)\n                 output_skip_special_tokens.append(\n-                    req.sampling_params.skip_special_tokens\n+                    sampling_params.skip_special_tokens\n                 )\n                 output_spaces_between_special_tokens.append(\n-                    req.sampling_params.spaces_between_special_tokens\n+                    sampling_params.spaces_between_special_tokens\n                 )\n \n                 meta_info = {\n", "model_name_or_path": "gpt-5-2025-08-07"}
