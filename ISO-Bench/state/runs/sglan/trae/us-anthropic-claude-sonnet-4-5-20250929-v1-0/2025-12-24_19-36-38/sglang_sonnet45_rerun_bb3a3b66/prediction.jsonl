{"instance_id": "/home/ubuntu/OmniPerf-Bench/sglan__repo-45d6592", "model_patch": "diff --git a/python/sglang/srt/managers/router/infer_batch.py b/python/sglang/srt/managers/router/infer_batch.py\nindex 88f6031..97bf301 100644\n--- a/python/sglang/srt/managers/router/infer_batch.py\n+++ b/python/sglang/srt/managers/router/infer_batch.py\n@@ -7,6 +7,9 @@ import torch\n from sglang.srt.managers.router.radix_cache import RadixCache\n from sglang.srt.memory_pool import ReqToTokenPool, TokenToKVPool\n \n+# Cache for arange tensors to avoid repeated allocations\n+_arange_cache = {}\n+\n \n class ForwardMode(Enum):\n     PREFILL = auto()\n@@ -208,7 +211,8 @@ class Batch:\n \n             seq_lens.append(prefix_lens[-1] + extend_lens[-1])\n \n-        position_ids_offsets = torch.zeros((bs,), dtype=torch.int32, device=device)\n+        position_ids_offsets = torch.empty((bs,), dtype=torch.int32, device=device)\n+        position_ids_offsets.fill_(0)\n \n         # Alloc mem\n         seq_lens, prefix_lens = np.array(seq_lens), np.array(prefix_lens)\n@@ -247,8 +251,8 @@ class Batch:\n             r.image_offset - p_len for r, p_len in zip(reqs, prefix_lens)\n         ]\n         self.req_pool_indices = req_pool_indices\n-        self.seq_lens = torch.tensor(seq_lens, dtype=torch.int32, device=device)\n-        self.prefix_lens = torch.tensor(prefix_lens, dtype=torch.int32, device=device)\n+        self.seq_lens = torch.from_numpy(seq_lens).to(dtype=torch.int32, device=device)\n+        self.prefix_lens = torch.from_numpy(prefix_lens).to(dtype=torch.int32, device=device)\n         self.position_ids_offsets = position_ids_offsets\n         self.extend_num_tokens = extend_num_tokens\n         self.out_cache_loc = out_cache_loc\n@@ -482,8 +486,14 @@ def _top_p_top_k(probs: torch.Tensor, top_ps: torch.Tensor, top_ks: torch.Tensor\n     probs_sort, probs_idx = probs.sort(dim=-1, descending=True)\n     probs_sum = torch.cumsum(probs_sort, dim=-1)\n     probs_sort[(probs_sum - probs_sort) > top_ps] = 0.0\n-    probs_sort[\n-        torch.arange(0, probs.shape[-1], device=probs.device).view(1, -1) >= top_ks\n-    ] = 0.0\n+    \n+    # Cache arange tensor to avoid repeated allocations\n+    vocab_size = probs.shape[-1]\n+    cache_key = (vocab_size, probs.device)\n+    if cache_key not in _arange_cache:\n+        _arange_cache[cache_key] = torch.arange(0, vocab_size, device=probs.device).view(1, -1)\n+    arange_tensor = _arange_cache[cache_key]\n+    \n+    probs_sort[arange_tensor >= top_ks] = 0.0\n     probs_sort.div_(probs_sort.max(dim=-1, keepdim=True)[0])\n     return probs_sort, probs_idx\ndiff --git a/python/sglang/srt/managers/router/model_rpc.py b/python/sglang/srt/managers/router/model_rpc.py\nindex 8b7adf9..301d6e1 100644\n--- a/python/sglang/srt/managers/router/model_rpc.py\n+++ b/python/sglang/srt/managers/router/model_rpc.py\n@@ -293,11 +293,10 @@ class ModelRpcServer(rpyc.Service):\n             self.token_to_kv_pool.available_size() + self.tree_cache.evictable_size()\n         )\n         if self.running_batch:\n+            # Optimize by avoiding repeated method calls\n             available_size -= sum(\n-                [\n-                    (r.max_new_tokens() - len(r.output_ids)) * self.new_token_ratio\n-                    for r in self.running_batch.reqs\n-                ]\n+                (r.max_new_tokens() - len(r.output_ids)) * self.new_token_ratio\n+                for r in self.running_batch.reqs\n             )\n \n         for req in self.forward_queue:\n@@ -309,7 +308,10 @@ class ModelRpcServer(rpyc.Service):\n                     req.prefix_indices = req.prefix_indices[:-delta]\n                     if req.image_offset is not None:\n                         req.image_offset += delta\n-            if req.extend_input_len == 0 and req.max_new_tokens() > 0:\n+            \n+            # Cache max_new_tokens to avoid repeated method calls\n+            max_new_tokens = req.max_new_tokens()\n+            if req.extend_input_len == 0 and max_new_tokens > 0:\n                 # Need at least one token to compute logits\n                 req.extend_input_len = 1\n                 req.prefix_indices = req.prefix_indices[:-1]\n@@ -317,7 +319,7 @@ class ModelRpcServer(rpyc.Service):\n                     req.image_offset += 1\n \n             if (\n-                req.extend_input_len + req.max_new_tokens() + new_batch_total_tokens\n+                req.extend_input_len + max_new_tokens + new_batch_total_tokens\n                 < available_size\n                 and req.extend_input_len + new_batch_input_tokens\n                 < self.max_prefill_num_token\n@@ -326,7 +328,7 @@ class ModelRpcServer(rpyc.Service):\n                 available_size += delta\n \n                 if not (\n-                    req.extend_input_len + req.max_new_tokens() + new_batch_total_tokens\n+                    req.extend_input_len + max_new_tokens + new_batch_total_tokens\n                     < available_size\n                 ):\n                     # Undo the insertion\n@@ -336,9 +338,7 @@ class ModelRpcServer(rpyc.Service):\n                     # Add this request to the running batch\n                     self.token_to_kv_pool.add_refs(req.prefix_indices)\n                     can_run_list.append(req)\n-                    new_batch_total_tokens += (\n-                        req.extend_input_len + req.max_new_tokens()\n-                    )\n+                    new_batch_total_tokens += req.extend_input_len + max_new_tokens\n                     new_batch_input_tokens += req.extend_input_len\n \n         if len(can_run_list) == 0:\n@@ -453,9 +453,9 @@ class ModelRpcServer(rpyc.Service):\n \n         # Check finish condition\n         reqs = batch.reqs\n-        for i in range(len(reqs)):\n-            reqs[i].output_ids.append(next_token_ids[i])\n-            reqs[i].check_finished()\n+        for i, req in enumerate(reqs):\n+            req.output_ids.append(next_token_ids[i])\n+            req.check_finished()\n \n         self.handle_finished_requests(batch)\n \n", "model_name_or_path": "gpt-5-2025-08-07"}
