{"instance_id": "/home/ubuntu/OmniPerf-Bench/sglan__repo-62f15ee", "model_patch": "diff --git a/python/sglang/srt/managers/policy_scheduler.py b/python/sglang/srt/managers/policy_scheduler.py\nindex 04169e8..696c909 100644\n--- a/python/sglang/srt/managers/policy_scheduler.py\n+++ b/python/sglang/srt/managers/policy_scheduler.py\n@@ -64,31 +64,34 @@ class PolicyScheduler:\n         elif self.policy == \"random\":\n             random.shuffle(waiting_queue)\n         elif self.policy == \"dfs-weight\":\n-            last_node_to_reqs = defaultdict(list)\n+            # Optimize: Use dict comprehension with setdefault for better performance\n+            last_node_to_reqs = {}\n             for req in waiting_queue:\n-                last_node_to_reqs[req.last_node].append(req)\n+                last_node_to_reqs.setdefault(req.last_node, []).append(req)\n \n-            node_to_weight = defaultdict(int)\n-            for node in last_node_to_reqs:\n-                node_to_weight[node] = len(last_node_to_reqs[node])\n+            # Optimize: Direct dict comprehension instead of defaultdict + loop\n+            node_to_weight = {node: len(reqs) for node, reqs in last_node_to_reqs.items()}\n             self.calc_weight(self.tree_cache.root_node, node_to_weight)\n \n-            waiting_queue.clear()\n+            # Optimize: Use list comprehension instead of clear() + extend()\n+            new_queue = []\n             self.get_dfs_priority(\n                 self.tree_cache.root_node,\n                 node_to_weight,\n                 last_node_to_reqs,\n-                waiting_queue,\n+                new_queue,\n             )\n+            waiting_queue[:] = new_queue\n         else:\n             raise ValueError(f\"Unknown schedule_policy: {self.policy}\")\n \n         return prefix_computed\n \n     def calc_weight(self, cur_node: TreeNode, node_to_weight: Dict):\n+        # Optimize: Use get() with default 0 to avoid KeyError and improve performance\n         for child in cur_node.children.values():\n             self.calc_weight(child, node_to_weight)\n-            node_to_weight[cur_node] += node_to_weight[child]\n+            node_to_weight[cur_node] = node_to_weight.get(cur_node, 0) + node_to_weight.get(child, 0)\n \n     def get_dfs_priority(\n         self,\n@@ -97,11 +100,12 @@ class PolicyScheduler:\n         last_node_to_reqs: Dict,\n         q: List,\n     ):\n-        childs = [child for child in cur_node.children.values()]\n-        childs.sort(key=lambda x: -node_to_priority[x])\n+        # Optimize: Use sorted() directly instead of list + sort for better performance\n+        childs = sorted(cur_node.children.values(), key=lambda x: -node_to_priority.get(x, 0))\n         for child in childs:\n             self.get_dfs_priority(child, node_to_priority, last_node_to_reqs, q)\n-        q.extend(last_node_to_reqs[cur_node])\n+        # Optimize: Use get() with default empty list to avoid KeyError\n+        q.extend(last_node_to_reqs.get(cur_node, []))\n \n \n class PrefillAdder:\ndiff --git a/python/sglang/srt/managers/tp_worker.py b/python/sglang/srt/managers/tp_worker.py\nindex 8fc03b8..f0c4f81 100644\n--- a/python/sglang/srt/managers/tp_worker.py\n+++ b/python/sglang/srt/managers/tp_worker.py\n@@ -487,7 +487,9 @@ class ModelTpServer:\n             self.token_to_kv_pool,\n             self.tree_cache,\n         )\n-        self.waiting_queue = [x for x in self.waiting_queue if x not in can_run_list]\n+        # Optimize: Use set for O(1) lookup instead of O(n) for each item\n+        can_run_set = set(can_run_list)\n+        self.waiting_queue = [x for x in self.waiting_queue if x not in can_run_set]\n         return new_batch\n \n     def forward_prefill_batch(self, batch: ScheduleBatch):\n@@ -532,11 +534,11 @@ class ModelTpServer:\n                 next_token_ids = next_token_ids.tolist()\n             else:\n                 if self.tokenizer is None:\n-                    next_token_ids = []\n-                    for req in batch.reqs:\n-                        next_token_ids.append(\n-                            next(iter(req.sampling_params.stop_token_ids))\n-                        )\n+                    # Optimize: Use list comprehension instead of loop with append\n+                    next_token_ids = [\n+                        next(iter(req.sampling_params.stop_token_ids))\n+                        for req in batch.reqs\n+                    ]\n                 else:\n                     next_token_ids = [self.tokenizer.eos_token_id] * len(batch.reqs)\n \n@@ -844,15 +846,8 @@ class ModelTpServer:\n         return if_success\n \n     def abort_request(self, recv_req):\n-        # Delete requests in the waiting queue\n-        to_del = None\n-        for i, req in enumerate(self.waiting_queue):\n-            if req.rid == recv_req.rid:\n-                to_del = i\n-                break\n-\n-        if to_del is not None:\n-            del self.waiting_queue[to_del]\n+        # Optimize: Use list comprehension to filter out request instead of del\n+        self.waiting_queue = [req for req in self.waiting_queue if req.rid != recv_req.rid]\n \n         # Delete requests in the running batch\n         if self.running_batch:\n", "model_name_or_path": "gpt-5-2025-08-07"}
