{"instance_id": "/home/ubuntu/OmniPerf-Bench/sglan__repo-9d5fa68", "model_patch": "--- /tmp/original_layers.py\t2025-12-24 23:01:43.182999353 +0000\n+++ /tmp/current_layers.py\t2025-12-24 23:01:43.180999312 +0000\n@@ -94,19 +94,21 @@\n \n     def forward(self, input_: torch.Tensor):\n         # duplicate the logic in ColumnParallelLinear\n-        bias = self.base_layer.bias if not self.base_layer.skip_bias_add else None\n-        output_parallel = self.base_layer.quant_method.apply(\n-            self.base_layer, input_, bias\n+        base_layer = self.base_layer\n+        skip_bias_add = base_layer.skip_bias_add\n+        bias = None if skip_bias_add else base_layer.bias\n+        output_parallel = base_layer.quant_method.apply(\n+            base_layer, input_, bias\n         )\n \n         if self.set_lora:\n             output_parallel = self.apply_lora(output_parallel, input_)\n \n-        if self.base_layer.gather_output:\n+        if base_layer.gather_output:\n             output = tensor_model_parallel_all_gather(output_parallel)\n         else:\n             output = output_parallel\n-        output_bias = self.base_layer.bias if self.base_layer.skip_bias_add else None\n+        output_bias = base_layer.bias if skip_bias_add else None\n         return output, output_bias\n \n     def slice_lora_a_weights(self, A: torch.Tensor, tp_rank: int):\n@@ -115,9 +117,7 @@\n     def slice_lora_b_weights(self, B: torch.Tensor, tp_rank: int):\n         shard_size = self.base_layer.output_partition_sizes[0]\n         start_idx = tp_rank * shard_size\n-        end_idx = (tp_rank + 1) * shard_size\n-        B = B[start_idx:end_idx, :]\n-        return B\n+        return B[start_idx:start_idx + shard_size, :]\n \n \n class MergedColumnParallelLinearWithLoRA(ColumnParallelLinearWithLoRA):\n@@ -137,18 +137,8 @@\n         self.A_buffer_gate_up = A_buffer\n         if self.lora_backend.fuse_stacked_lora_b:\n             # B_buffer_gate_up: (num_lora, 2 * output_dim, r)\n-            if not hasattr(self, \"B_buffer_gate_up\") or self.B_buffer_gate_up is None:\n-                self.B_buffer_gate_up = torch.empty(\n-                    (\n-                        B_buffer[0].shape[0],\n-                        2 * B_buffer[0].shape[1],\n-                        B_buffer[0].shape[2],\n-                    ),\n-                    dtype=B_buffer[0].dtype,\n-                    device=B_buffer[0].device,\n-                )\n-            self.B_buffer_gate_up[:, : B_buffer[0].shape[1], :].copy_(B_buffer[0])\n-            self.B_buffer_gate_up[:, B_buffer[0].shape[1] :, :].copy_(B_buffer[1])\n+            # Use torch.cat for more efficient concatenation\n+            self.B_buffer_gate_up = torch.cat([B_buffer[0], B_buffer[1]], dim=1)\n         else:\n             self.B_buffer_gate_up = (B_buffer[0], B_buffer[1])\n \n@@ -174,8 +164,7 @@\n         # Since the outputs for both gate and up are identical, we use a random one.\n         shard_size = self.base_layer.output_partition_sizes[0]\n         start_idx = tp_rank * shard_size\n-        end_idx = (tp_rank + 1) * shard_size\n-        return B[:, start_idx:end_idx, :]\n+        return B[:, start_idx:start_idx + shard_size, :]\n \n \n class QKVParallelLinearWithLoRA(ColumnParallelLinearWithLoRA):\n@@ -202,30 +191,12 @@\n             output_dim_q, output_dim_kv = B_buffer_q.shape[-2], B_buffer_kv.shape[-2]\n \n             # B_buffer_qkv: (num_lora, output_dim_q + 2 * output_dim_kv, r)\n-            if not hasattr(self, \"B_buffer_qkv\") or self.B_buffer_qkv is None:\n-                self.B_buffer_qkv = torch.empty(\n-                    (\n-                        B_buffer_q[0].shape[0],\n-                        output_dim_q + 2 * output_dim_kv,\n-                        B_buffer_q[0].shape[2],\n-                    ),\n-                    dtype=B_buffer_q[0].dtype,\n-                    device=B_buffer_q[0].device,\n-                )\n-            self.B_buffer_qkv[:, :output_dim_q, :].copy_(B_buffer_q[0])\n-            self.B_buffer_qkv[:, output_dim_q : output_dim_q + output_dim_kv, :].copy_(\n-                B_buffer_kv[0]\n-            )\n-            self.B_buffer_qkv[:, output_dim_q + output_dim_kv :, :].copy_(\n-                B_buffer_kv[1]\n-            )\n+            # Use torch.cat for more efficient concatenation\n+            self.B_buffer_qkv = torch.cat([B_buffer_q[0], B_buffer_kv[0], B_buffer_kv[1]], dim=1)\n \n             # Offsets of q/k/v in output dimension\n-            if not hasattr(self, \"output_offset\") or self.output_offset is None:\n-                self.output_offset = torch.empty(\n-                    4, dtype=torch.int32, device=B_buffer_q.device\n-                )\n-            self.output_offset[:4] = torch.tensor(\n+            # Create tensor directly instead of empty + assignment\n+            self.output_offset = torch.tensor(\n                 [\n                     0,\n                     output_dim_q,\n@@ -271,16 +242,13 @@\n         base_layer = self.base_layer\n         q_proj_shard_size = base_layer.q_proj_shard_size\n         kv_proj_shard_size = base_layer.kv_proj_shard_size\n-        num_kv_head_replicas = base_layer.num_kv_head_replicas\n \n         q_start_idx = q_proj_shard_size * tp_rank\n-        q_end_idx = q_start_idx + q_proj_shard_size\n \n-        kv_shard_id = tp_rank // num_kv_head_replicas\n+        kv_shard_id = tp_rank // base_layer.num_kv_head_replicas\n         kv_start_idx = kv_proj_shard_size * kv_shard_id\n-        kv_end_idx = kv_start_idx + kv_proj_shard_size\n \n-        return B_q[q_start_idx:q_end_idx, :], B_kv[:, kv_start_idx:kv_end_idx, :]\n+        return B_q[q_start_idx:q_start_idx + q_proj_shard_size, :], B_kv[:, kv_start_idx:kv_start_idx + kv_proj_shard_size, :]\n \n \n class RowParallelLinearWithLoRA(BaseLayerWithLoRA):\n@@ -312,44 +280,41 @@\n \n     def forward(self, input_: torch.Tensor):\n         # duplicate the logic in RowParallelLinear\n-        if self.base_layer.input_is_parallel:\n+        base_layer = self.base_layer\n+        if base_layer.input_is_parallel:\n             input_parallel = input_\n         else:\n             tp_rank = get_tensor_model_parallel_rank()\n             splitted_input = split_tensor_along_last_dim(\n-                input_, num_partitions=self.base_layer.tp_size\n+                input_, num_partitions=base_layer.tp_size\n             )\n             input_parallel = splitted_input[tp_rank].contiguous()\n-        output_parallel = self.base_layer.quant_method.apply(\n-            self.base_layer, input_parallel\n+        output_parallel = base_layer.quant_method.apply(\n+            base_layer, input_parallel\n         )\n \n         if self.set_lora:\n             output_parallel = self.apply_lora(output_parallel, input_parallel)\n \n-        if self.base_layer.reduce_results and self.base_layer.tp_size > 1:\n+        if base_layer.reduce_results and base_layer.tp_size > 1:\n             output_ = tensor_model_parallel_all_reduce(output_parallel)\n         else:\n             output_ = output_parallel\n \n-        if not self.base_layer.skip_bias_add:\n-            output = (\n-                output_ + self.base_layer.bias\n-                if self.base_layer.bias is not None\n-                else output_\n-            )\n+        skip_bias_add = base_layer.skip_bias_add\n+        if not skip_bias_add:\n+            bias = base_layer.bias\n+            output = output_ + bias if bias is not None else output_\n             output_bias = None\n         else:\n             output = output_\n-            output_bias = self.base_layer.bias\n+            output_bias = base_layer.bias\n         return output, output_bias\n \n     def slice_lora_a_weights(self, A: torch.Tensor, tp_rank: int):\n         shard_size = self.base_layer.input_size_per_partition\n         start_idx = tp_rank * shard_size\n-        end_idx = (tp_rank + 1) * shard_size\n-        A = A[:, start_idx:end_idx].contiguous()\n-        return A\n+        return A[:, start_idx:start_idx + shard_size].contiguous()\n \n     def slice_lora_b_weights(self, B: torch.Tensor, tp_rank: int):\n         return B\n", "model_name_or_path": "gpt-5-2025-08-07"}
