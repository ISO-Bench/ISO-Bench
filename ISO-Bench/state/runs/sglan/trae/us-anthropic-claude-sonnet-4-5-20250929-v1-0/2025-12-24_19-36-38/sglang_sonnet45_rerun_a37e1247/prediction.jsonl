{"instance_id": "/home/ubuntu/OmniPerf-Bench/sglan__repo-136c6e0", "model_patch": "diff --git a/python/sglang/bench_serving.py b/python/sglang/bench_serving.py\nindex 04c2202..bd579cf 100644\n--- a/python/sglang/bench_serving.py\n+++ b/python/sglang/bench_serving.py\n@@ -138,6 +138,7 @@ async def async_request_trt_llm(\n         ttft = 0.0\n         st = time.perf_counter()\n         most_recent_timestamp = st\n+        generated_text_parts = []\n         try:\n             async with session.post(url=api_url, json=payload) as response:\n                 if response.status == 200:\n@@ -149,7 +150,7 @@ async def async_request_trt_llm(\n                         chunk = remove_prefix(chunk_bytes.decode(\"utf-8\"), \"data:\")\n \n                         data = json.loads(chunk)\n-                        output.generated_text += data[\"text_output\"]\n+                        generated_text_parts.append(data[\"text_output\"])\n                         timestamp = time.perf_counter()\n                         # First token\n                         if ttft == 0.0:\n@@ -162,6 +163,7 @@ async def async_request_trt_llm(\n \n                         most_recent_timestamp = timestamp\n \n+                    output.generated_text = \"\".join(generated_text_parts)\n                     output.latency = most_recent_timestamp - st\n                     output.success = True\n                     output.output_len = request_func_input.output_len\n@@ -206,7 +208,7 @@ async def async_request_openai_completions(\n \n         output = RequestFuncOutput.init_new(request_func_input)\n \n-        generated_text = \"\"\n+        generated_text_parts = []\n         output_len = request_func_input.output_len\n         ttft = 0.0\n         st = time.perf_counter()\n@@ -231,7 +233,9 @@ async def async_request_openai_completions(\n                             # NOTE: Some completion API might have a last\n                             # usage summary response without a token so we\n                             # want to check a token was generated\n-                            if data[\"choices\"][0][\"text\"]:\n+                            choice = data[\"choices\"][0]\n+                            text = choice[\"text\"]\n+                            if text:\n                                 timestamp = time.perf_counter()\n                                 # First token\n                                 if ttft == 0.0:\n@@ -243,12 +247,12 @@ async def async_request_openai_completions(\n                                     output.itl.append(timestamp - most_recent_timestamp)\n \n                                 most_recent_timestamp = timestamp\n-                                generated_text += data[\"choices\"][0][\"text\"]\n+                                generated_text_parts.append(text)\n                                 output_len = (data.get(\"usage\") or {}).get(\n                                     \"completion_tokens\", output_len\n                                 )\n \n-                    output.generated_text = generated_text\n+                    output.generated_text = \"\".join(generated_text_parts)\n                     output.success = True\n                     output.latency = latency\n                     output.output_len = output_len\n@@ -317,7 +321,7 @@ async def async_request_openai_chat_completions(\n \n         output = RequestFuncOutput.init_new(request_func_input)\n \n-        generated_text = \"\"\n+        generated_text_parts = []\n         output_len = request_func_input.output_len\n         ttft = 0.0\n         st = time.perf_counter()\n@@ -373,14 +377,14 @@ async def async_request_openai_chat_completions(\n                                         )\n \n                                     most_recent_timestamp = timestamp\n-                                    generated_text += content\n+                                    generated_text_parts.append(content)\n \n                                 # Check for usage info in final chunk\n                                 output_len = (data.get(\"usage\") or {}).get(\n                                     \"completion_tokens\", output_len\n                                 )\n \n-                        output.generated_text = generated_text\n+                        output.generated_text = \"\".join(generated_text_parts)\n                         output.success = True\n                         output.latency = latency\n                         output.output_len = output_len\n@@ -420,7 +424,7 @@ async def async_request_truss(\n \n         output = RequestFuncOutput.init_new(request_func_input)\n \n-        generated_text = \"\"\n+        generated_text_parts = []\n         ttft = 0.0\n         st = time.perf_counter()\n         most_recent_timestamp = st\n@@ -444,7 +448,9 @@ async def async_request_truss(\n                             # NOTE: Some completion API might have a last\n                             # usage summary response without a token so we\n                             # want to check a token was generated\n-                            if data[\"choices\"][0][\"text\"]:\n+                            choice = data[\"choices\"][0]\n+                            text = choice[\"text\"]\n+                            if text:\n                                 timestamp = time.perf_counter()\n                                 # First token\n                                 if ttft == 0.0:\n@@ -456,9 +462,9 @@ async def async_request_truss(\n                                     output.itl.append(timestamp - most_recent_timestamp)\n \n                                 most_recent_timestamp = timestamp\n-                                generated_text += data[\"choices\"][0][\"text\"]\n+                                generated_text_parts.append(text)\n \n-                    output.generated_text = generated_text\n+                    output.generated_text = \"\".join(generated_text_parts)\n                     output.success = True\n                     output.latency = latency\n                     output.output_len = request_func_input.output_len\ndiff --git a/python/sglang/srt/entrypoints/http_server_engine.py b/python/sglang/srt/entrypoints/http_server_engine.py\nindex b2edf1a..2ea2b59 100644\n--- a/python/sglang/srt/entrypoints/http_server_engine.py\n+++ b/python/sglang/srt/entrypoints/http_server_engine.py\n@@ -25,15 +25,16 @@ def launch_server_process(server_args: ServerArgs) -> multiprocessing.Process:\n     base_url = server_args.url()\n     timeout = 300.0  # Increased timeout to 5 minutes for downloading large models\n     start_time = time.perf_counter()\n+    health_url = f\"{base_url}/health_generate\"\n+    headers = {\n+        \"Content-Type\": \"application/json; charset=utf-8\",\n+        \"Authorization\": f\"Bearer {server_args.api_key}\",\n+    }\n \n     with requests.Session() as session:\n         while time.perf_counter() - start_time < timeout:\n             try:\n-                headers = {\n-                    \"Content-Type\": \"application/json; charset=utf-8\",\n-                    \"Authorization\": f\"Bearer {server_args.api_key}\",\n-                }\n-                response = session.get(f\"{base_url}/health_generate\", headers=headers)\n+                response = session.get(health_url, headers=headers)\n                 if response.status_code == 200:\n                     return p\n             except requests.RequestException:\ndiff --git a/python/sglang/srt/utils.py b/python/sglang/srt/utils.py\nindex 052e732..8635113 100644\n--- a/python/sglang/srt/utils.py\n+++ b/python/sglang/srt/utils.py\n@@ -2354,7 +2354,7 @@ def is_fa3_default_architecture(hf_config):\n # Can be more general if it is used in multiple places (keep it simple and thus not general now)\n class BumpAllocator:\n     def __init__(self, buffer_size: int, dtype, device):\n-        self._buffer = torch.zeros((buffer_size,), dtype=dtype, device=device)\n+        self._buffer = torch.empty((buffer_size,), dtype=dtype, device=device)\n         self._pointer = 0\n \n     def allocate(self, size: int):\n", "model_name_or_path": "gpt-5-2025-08-07"}
