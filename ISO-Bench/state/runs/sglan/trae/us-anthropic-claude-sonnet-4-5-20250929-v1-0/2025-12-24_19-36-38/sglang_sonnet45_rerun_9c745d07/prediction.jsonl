{"instance_id": "/home/ubuntu/OmniPerf-Bench/sglan__repo-ebaa2f3", "model_patch": "diff --git a/python/sglang/srt/constrained/outlines_backend.py b/python/sglang/srt/constrained/outlines_backend.py\nindex cc68b97..51907e6 100644\n--- a/python/sglang/srt/constrained/outlines_backend.py\n+++ b/python/sglang/srt/constrained/outlines_backend.py\n@@ -60,15 +60,18 @@ class OutlinesGrammar(BaseGrammarObject):\n         suffix_bytes = []\n         continuation_range = range(0x80, 0xC0)\n         cur_state = self.state\n-        while (\n-            len(jump_forward_bytes) and jump_forward_bytes[0][0] in continuation_range\n-        ):\n+        idx = 0\n+        while idx < len(jump_forward_bytes) and jump_forward_bytes[idx][0] in continuation_range:\n             # continuation bytes\n-            byte_edge = jump_forward_bytes.pop(0)\n+            byte_edge = jump_forward_bytes[idx]\n             suffix_bytes.append(byte_edge[0])\n             cur_state = byte_edge[1]\n+            idx += 1\n \n-        suffix_tokens = [f\"<0x{hex(b)[2:].upper()}>\" for b in suffix_bytes]\n+        # Remove processed bytes from the beginning\n+        del jump_forward_bytes[:idx]\n+\n+        suffix_tokens = [f\"<0x{b:02X}>\" for b in suffix_bytes]\n         suffix_ids = tokenizer.convert_tokens_to_ids(suffix_tokens)\n         return suffix_ids, cur_state\n \ndiff --git a/python/sglang/srt/constrained/xgrammar_backend.py b/python/sglang/srt/constrained/xgrammar_backend.py\nindex ab4df5c..74e05dd 100644\n--- a/python/sglang/srt/constrained/xgrammar_backend.py\n+++ b/python/sglang/srt/constrained/xgrammar_backend.py\n@@ -66,10 +66,11 @@ class XGrammarGrammar(BaseGrammarObject):\n     def jump_and_retokenize(\n         self, old_output_ids: List[int], new_output_ids: List[int], next_state: int\n     ):\n+        # Find the first position where tokens differ\n         k = 0\n-        for i, old_id in enumerate(old_output_ids):\n-            if old_id == new_output_ids[i]:\n-                k = i + 1\n+        for old_id, new_id in zip(old_output_ids, new_output_ids):\n+            if old_id == new_id:\n+                k += 1\n             else:\n                 break\n \ndiff --git a/python/sglang/srt/sampling/sampling_batch_info.py b/python/sglang/srt/sampling/sampling_batch_info.py\nindex a341c2b..bdf36be 100644\n--- a/python/sglang/srt/sampling/sampling_batch_info.py\n+++ b/python/sglang/srt/sampling/sampling_batch_info.py\n@@ -49,23 +49,26 @@ class SamplingBatchInfo:\n     ):\n         reqs = batch.reqs\n         device = batch.device\n-        temperatures = (\n-            torch.tensor(\n-                [r.sampling_params.temperature for r in reqs],\n-                dtype=torch.float,\n-            )\n-            .view(-1, 1)\n-            .to(device, non_blocking=True)\n-        )\n+        temperatures = torch.tensor(\n+            [r.sampling_params.temperature for r in reqs],\n+            dtype=torch.float,\n+            device=device,\n+        ).view(-1, 1)\n         top_ps = torch.tensor(\n-            [r.sampling_params.top_p for r in reqs], dtype=torch.float\n-        ).to(device, non_blocking=True)\n+            [r.sampling_params.top_p for r in reqs],\n+            dtype=torch.float,\n+            device=device,\n+        )\n         top_ks = torch.tensor(\n-            [r.sampling_params.top_k for r in reqs], dtype=torch.int32\n-        ).to(device, non_blocking=True)\n+            [r.sampling_params.top_k for r in reqs],\n+            dtype=torch.int32,\n+            device=device,\n+        )\n         min_ps = torch.tensor(\n-            [r.sampling_params.min_p for r in reqs], dtype=torch.float\n-        ).to(device, non_blocking=True)\n+            [r.sampling_params.min_p for r in reqs],\n+            dtype=torch.float,\n+            device=device,\n+        )\n \n         ret = cls(\n             temperatures=temperatures,\n@@ -137,8 +140,9 @@ class SamplingBatchInfo:\n             self.vocab_mask = None\n             return\n \n+        batch_size = len(self.temperatures)\n         self.vocab_mask = torch.zeros(\n-            len(self.temperatures),\n+            batch_size,\n             self.vocab_size,\n             dtype=torch.bool,\n             device=self.device,\n@@ -178,11 +182,10 @@ class SamplingBatchInfo:\n                 shape, dtype = lhs.shape[1:], lhs.dtype\n             else:\n                 shape, dtype = rhs.shape[1:], rhs.dtype\n-            with torch.dtype(dtype):\n-                if lhs is None:\n-                    lhs = torch.empty((bs1, *shape), device=device).fill_(default)\n-                if rhs is None:\n-                    rhs = torch.empty((bs2, *shape), device=device).fill_(default)\n+            if lhs is None:\n+                lhs = torch.full((bs1, *shape), default, dtype=dtype, device=device)\n+            if rhs is None:\n+                rhs = torch.full((bs2, *shape), default, dtype=dtype, device=device)\n             return torch.cat([lhs, rhs])\n \n         return None\n", "model_name_or_path": "gpt-5-2025-08-07"}
