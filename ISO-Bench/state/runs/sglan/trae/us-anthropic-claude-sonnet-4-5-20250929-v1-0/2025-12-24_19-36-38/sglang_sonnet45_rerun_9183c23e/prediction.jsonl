{"instance_id": "/home/ubuntu/OmniPerf-Bench/sglan__repo-148254d", "model_patch": "diff --git a/python/sglang/srt/managers/io_struct.py b/python/sglang/srt/managers/io_struct.py\nindex 13eb233..3ae80fd 100644\n--- a/python/sglang/srt/managers/io_struct.py\n+++ b/python/sglang/srt/managers/io_struct.py\n@@ -148,8 +148,6 @@ class GenerateReqInput:\n                 self.image_data = [None] * num\n             elif not isinstance(self.image_data, list):\n                 self.image_data = [self.image_data] * num\n-            elif isinstance(self.image_data, list):\n-                pass\n \n             if self.sampling_params is None:\n                 self.sampling_params = [{}] * num\ndiff --git a/python/sglang/srt/model_executor/model_runner.py b/python/sglang/srt/model_executor/model_runner.py\nindex 786f654..3e1ea95 100644\n--- a/python/sglang/srt/model_executor/model_runner.py\n+++ b/python/sglang/srt/model_executor/model_runner.py\n@@ -577,8 +577,10 @@ class ModelRunner:\n         \"\"\"We need to run a small matmul to init cublas. Otherwise, it will raise some errors later.\"\"\"\n         dtype = torch.float16\n         device = \"cuda\"\n-        a = torch.ones((16, 16), dtype=dtype, device=device)\n-        b = torch.ones((16, 16), dtype=dtype, device=device)\n+        # Use torch.empty instead of torch.ones for faster initialization\n+        # The actual values don't matter, we just need to trigger cublas initialization\n+        a = torch.empty((16, 16), dtype=dtype, device=device)\n+        b = torch.empty((16, 16), dtype=dtype, device=device)\n         c = a @ b\n         return c\n \n@@ -609,20 +611,19 @@ class ModelRunner:\n     def init_double_sparsity_channel_config(self, selected_channel):\n \n         selected_channel = \".\" + selected_channel + \"_proj\"\n-        self.sorted_channels = []\n         # load channel config\n         with open(self.server_args.ds_channel_config_path, \"r\") as f:\n             channel_config = json.load(f)\n \n-        for i in range(self.model_config.num_hidden_layers):\n-            key = \"model.layers.\" + str(i) + \".self_attn\" + selected_channel\n-            self.sorted_channels.append(\n-                torch.tensor(channel_config[key])[\n-                    :, : self.server_args.ds_heavy_channel_num\n-                ]\n-                .contiguous()\n-                .cuda()\n-            )\n+        # Use list comprehension instead of repeated append for better performance\n+        self.sorted_channels = [\n+            torch.tensor(channel_config[f\"model.layers.{i}.self_attn{selected_channel}\"])[\n+                :, : self.server_args.ds_heavy_channel_num\n+            ]\n+            .contiguous()\n+            .cuda()\n+            for i in range(self.model_config.num_hidden_layers)\n+        ]\n \n     def init_cuda_graphs(self):\n         \"\"\"Capture cuda graphs.\"\"\"\ndiff --git a/python/sglang/srt/utils.py b/python/sglang/srt/utils.py\nindex 7c3efa9..0eb638b 100644\n--- a/python/sglang/srt/utils.py\n+++ b/python/sglang/srt/utils.py\n@@ -1179,13 +1179,13 @@ def set_gpu_proc_affinity(\n     end_cpu_id = start_cpu_id + num_cores_bind\n \n     if psutil.cpu_count() != psutil.cpu_count(logical=False):\n-        # HT on\n-        upper_cpu_ids = [id for id in range(start_cpu_id, end_cpu_id)]\n-        lower_cpu_ids = [id + total_pcores for id in range(start_cpu_id, end_cpu_id)]\n-        bind_cpu_ids = list(itertools.chain(upper_cpu_ids, lower_cpu_ids))\n+        # HT on - use list() with range for better performance\n+        upper_cpu_ids = list(range(start_cpu_id, end_cpu_id))\n+        lower_cpu_ids = list(range(start_cpu_id + total_pcores, end_cpu_id + total_pcores))\n+        bind_cpu_ids = upper_cpu_ids + lower_cpu_ids\n     else:\n-        # HT off\n-        bind_cpu_ids = [id for id in range(start_cpu_id, end_cpu_id)]\n+        # HT off - use list() with range for better performance\n+        bind_cpu_ids = list(range(start_cpu_id, end_cpu_id))\n \n     # set cpu_affinity to current process\n     p.cpu_affinity(bind_cpu_ids)\n", "model_name_or_path": "gpt-5-2025-08-07"}
