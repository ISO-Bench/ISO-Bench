{"instance_id": "/home/ubuntu/OmniPerf-Bench/sglan__repo-8c7279c", "model_patch": "diff --git a/python/sglang/srt/two_batch_overlap.py b/python/sglang/srt/two_batch_overlap.py\nindex 0fbc3c8..5e876e1 100644\n--- a/python/sglang/srt/two_batch_overlap.py\n+++ b/python/sglang/srt/two_batch_overlap.py\n@@ -40,13 +40,13 @@ def compute_split_seq_index(\n \n def _split_array_by_half_sum(arr: Sequence[int]) -> int:\n     overall_sum = sum(arr)\n-    accumulator, split_index = 0, 0\n-    for value in arr[:-1]:\n+    half_sum = overall_sum >> 1  # Use bit shift instead of division\n+    accumulator = 0\n+    for split_index, value in enumerate(arr[:-1]):\n         accumulator += value\n-        split_index += 1\n-        if accumulator >= overall_sum // 2:\n-            break\n-    return split_index\n+        if accumulator >= half_sum:\n+            return split_index + 1\n+    return len(arr) - 1\n \n \n def compute_split_token_index(\n@@ -56,7 +56,11 @@ def compute_split_token_index(\n ) -> int:\n     if forward_mode.is_extend():\n         assert extend_seq_lens is not None\n-        return sum(extend_seq_lens[:split_seq_index])\n+        # Avoid slice allocation by manually summing\n+        result = 0\n+        for i in range(split_seq_index):\n+            result += extend_seq_lens[i]\n+        return result\n     elif forward_mode.is_decode():\n         return split_seq_index\n     elif forward_mode.is_idle():\n@@ -209,20 +213,20 @@ class TboForwardBatchPreparer:\n         num_tokens = batch.input_ids.shape[0]\n         num_seqs = batch.batch_size\n \n-        output_dict = dict()\n+        output_dict = {}\n \n-        for key in [\n+        for key in (\n             \"input_ids\",\n             \"positions\",\n             \"out_cache_loc\",\n-        ]:\n+        ):\n             old_value = getattr(batch, key)\n             assert (\n                 old_value.shape[0] == num_tokens\n             ), f\"{key=} {old_value=} {num_tokens=} {batch=}\"\n             output_dict[key] = old_value[start_token_index:end_token_index]\n \n-        for key in [\n+        for key in (\n             \"req_pool_indices\",\n             \"seq_lens\",\n             \"seq_lens_cpu\",\n@@ -233,7 +237,7 @@ class TboForwardBatchPreparer:\n             \"extend_seq_lens_cpu\",\n             \"extend_logprob_start_lens_cpu\",\n             \"lora_paths\",\n-        ]:\n+        ):\n             old_value = getattr(batch, key)\n             if old_value is None:\n                 continue\n@@ -242,7 +246,7 @@ class TboForwardBatchPreparer:\n             ), f\"{key=} {old_value=} {num_seqs=} {batch=}\"\n             output_dict[key] = old_value[start_seq_index:end_seq_index]\n \n-        for key in [\n+        for key in (\n             \"forward_mode\",\n             \"return_logprob\",\n             \"req_to_token_pool\",\n@@ -254,7 +258,7 @@ class TboForwardBatchPreparer:\n             \"capture_hidden_mode\",\n             \"padded_static_len\",\n             \"mrope_positions\",  # only used by qwen2-vl, thus not care\n-        ]:\n+        ):\n             output_dict[key] = getattr(batch, key)\n \n         assert (\n@@ -268,7 +272,7 @@ class TboForwardBatchPreparer:\n         # TODO improve, e.g. unify w/ `init_raw`\n         if global_server_args_dict[\"moe_dense_tp_size\"] == 1:\n             sum_len = end_token_index - start_token_index\n-            gathered_buffer = torch.zeros(\n+            gathered_buffer = torch.empty(\n                 (sum_len, batch.gathered_buffer.shape[1]),\n                 dtype=batch.gathered_buffer.dtype,\n                 device=batch.gathered_buffer.device,\n", "model_name_or_path": "gpt-5-2025-08-07"}
