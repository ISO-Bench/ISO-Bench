{"instance_id": "/home/ubuntu/OmniPerf-Bench/sglan__repo-2fce449", "model_patch": "diff --git a/python/sglang/srt/mem_cache/memory_pool.py b/python/sglang/srt/mem_cache/memory_pool.py\nindex 181ac7e..5fd5647 100644\n--- a/python/sglang/srt/mem_cache/memory_pool.py\n+++ b/python/sglang/srt/mem_cache/memory_pool.py\n@@ -22,6 +22,7 @@ BaseTokenToKVPool maps a token location to its KV cache data.\n \"\"\"\n \n import logging\n+from collections import deque\n from typing import List, Tuple, Union\n \n import torch\n@@ -41,7 +42,8 @@ class ReqToTokenPool:\n         self.req_to_token = torch.empty(\n             (size, max_context_len), dtype=torch.int32, device=device\n         )\n-        self.free_slots = list(range(size))\n+        # Use deque for O(1) append/pop operations instead of list\n+        self.free_slots = deque(range(size))\n         self.write_records = []\n         self.use_records = use_records\n \n@@ -61,8 +63,8 @@ class ReqToTokenPool:\n         if need_size > len(self.free_slots):\n             return None\n \n-        select_index = self.free_slots[:need_size]\n-        self.free_slots = self.free_slots[need_size:]\n+        # Efficiently pop from left side of deque\n+        select_index = [self.free_slots.popleft() for _ in range(need_size)]\n \n         return select_index\n \n@@ -73,7 +75,7 @@ class ReqToTokenPool:\n             self.free_slots.extend(free_index)\n \n     def clear(self):\n-        self.free_slots = list(range(self.size))\n+        self.free_slots = deque(range(self.size))\n         self.write_records = []\n \n     def write_without_records(self, indices, values):\n@@ -110,6 +112,8 @@ class BaseTokenToKVPool:\n         else:\n             self.store_dtype = dtype\n         self.device = device\n+        # Cache the dtype comparison result to avoid repeated checks\n+        self._needs_dtype_conversion = (self.store_dtype != self.dtype)\n \n         self.free_slots = None\n         self.is_not_in_free_group = True\n@@ -130,7 +134,8 @@ class BaseTokenToKVPool:\n \n     def free(self, free_index: torch.Tensor):\n         if self.is_not_in_free_group:\n-            self.free_slots = torch.concat((self.free_slots, free_index.cpu()))\n+            # Use torch.cat instead of torch.concat for better performance\n+            self.free_slots = torch.cat((self.free_slots, free_index.cpu()))\n         else:\n             self.free_group.append(free_index)\n \n@@ -141,7 +146,8 @@ class BaseTokenToKVPool:\n     def free_group_end(self):\n         self.is_not_in_free_group = True\n         if self.free_group:\n-            self.free(torch.concat(self.free_group))\n+            # Use torch.cat instead of torch.concat for better performance\n+            self.free(torch.cat(self.free_group))\n \n     def clear(self):\n         # The padded slot 0 is used for writing dummy outputs from padded tokens.\n@@ -201,17 +207,20 @@ class MHATokenToKVPool(BaseTokenToKVPool):\n         ]\n \n     def get_key_buffer(self, layer_id: int):\n-        if self.store_dtype != self.dtype:\n+        if self._needs_dtype_conversion:\n             return self.k_buffer[layer_id].view(self.dtype)\n         return self.k_buffer[layer_id]\n \n     def get_value_buffer(self, layer_id: int):\n-        if self.store_dtype != self.dtype:\n+        if self._needs_dtype_conversion:\n             return self.v_buffer[layer_id].view(self.dtype)\n         return self.v_buffer[layer_id]\n \n     def get_kv_buffer(self, layer_id: int):\n-        return self.get_key_buffer(layer_id), self.get_value_buffer(layer_id)\n+        # Optimize to avoid double dtype check\n+        if self._needs_dtype_conversion:\n+            return self.k_buffer[layer_id].view(self.dtype), self.v_buffer[layer_id].view(self.dtype)\n+        return self.k_buffer[layer_id], self.v_buffer[layer_id]\n \n     def set_kv_buffer(\n         self,\n@@ -263,17 +272,22 @@ class MLATokenToKVPool(BaseTokenToKVPool):\n         ]\n \n     def get_key_buffer(self, layer_id: int):\n-        if self.store_dtype != self.dtype:\n+        if self._needs_dtype_conversion:\n             return self.kv_buffer[layer_id].view(self.dtype)\n         return self.kv_buffer[layer_id]\n \n     def get_value_buffer(self, layer_id: int):\n-        if self.store_dtype != self.dtype:\n+        if self._needs_dtype_conversion:\n             return self.kv_buffer[layer_id][..., : self.kv_lora_rank].view(self.dtype)\n         return self.kv_buffer[layer_id][..., : self.kv_lora_rank]\n \n     def get_kv_buffer(self, layer_id: int):\n-        return self.get_key_buffer(layer_id), self.get_value_buffer(layer_id)\n+        # Optimize to avoid double dtype check and buffer access\n+        if self._needs_dtype_conversion:\n+            buf_view = self.kv_buffer[layer_id].view(self.dtype)\n+            return buf_view, buf_view[..., : self.kv_lora_rank]\n+        buf = self.kv_buffer[layer_id]\n+        return buf, buf[..., : self.kv_lora_rank]\n \n     def set_kv_buffer(\n         self,\n@@ -285,7 +299,7 @@ class MLATokenToKVPool(BaseTokenToKVPool):\n         layer_id = layer.layer_id\n         if cache_k.dtype != self.dtype:\n             cache_k = cache_k.to(self.dtype)\n-        if self.store_dtype != self.dtype:\n+        if self._needs_dtype_conversion:\n             self.kv_buffer[layer_id][loc] = cache_k.view(self.store_dtype)\n         else:\n             self.kv_buffer[layer_id][loc] = cache_k\n", "model_name_or_path": "gpt-5-2025-08-07"}
