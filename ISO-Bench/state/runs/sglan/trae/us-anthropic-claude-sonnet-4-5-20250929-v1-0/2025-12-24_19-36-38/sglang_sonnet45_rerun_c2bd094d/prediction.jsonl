{"instance_id": "/home/ubuntu/OmniPerf-Bench/sglan__repo-f8f9244", "model_patch": "diff --git a/python/sglang/srt/layers/moe/ep_moe/kernels.py b/python/sglang/srt/layers/moe/ep_moe/kernels.py\nindex 6d6c432..ca86d89 100644\n--- a/python/sglang/srt/layers/moe/ep_moe/kernels.py\n+++ b/python/sglang/srt/layers/moe/ep_moe/kernels.py\n@@ -42,7 +42,7 @@ def deepep_compute_src2dst_triton_kernel(\n \n def deepep_run_moe_deep_preprocess(topk_ids: torch.Tensor, num_experts: int):\n     reorder_topk_ids, reorder_ids = torch.sort(topk_ids.view(-1), stable=True)\n-    seg_indptr = torch.zeros(num_experts + 1, device=topk_ids.device, dtype=torch.int64)\n+    seg_indptr = torch.empty(num_experts + 1, device=topk_ids.device, dtype=torch.int64)\n     src2dst = torch.empty(topk_ids.numel(), device=topk_ids.device, dtype=torch.int32)\n \n     # Find offet\n@@ -51,7 +51,7 @@ def deepep_run_moe_deep_preprocess(topk_ids: torch.Tensor, num_experts: int):\n     )\n     torch.searchsorted(reorder_topk_ids, expert_ids, out=seg_indptr)\n     num_minus_one = seg_indptr[0]\n-    seg_indptr = seg_indptr - num_minus_one\n+    seg_indptr.sub_(num_minus_one)\n \n     BLOCK_SIZE = 512\n     grid = (triton.cdiv(topk_ids.numel(), BLOCK_SIZE),)\n@@ -147,7 +147,8 @@ def compute_seg_indptr_triton_kernel(reorder_topk_ids, seg_indptr, num_toks):\n \n def run_moe_ep_preproess(topk_ids: torch.Tensor, num_experts: int):\n     reorder_topk_ids, reorder_ids = torch.sort(topk_ids.view(-1), stable=True)\n-    seg_indptr = torch.zeros(num_experts + 1, device=topk_ids.device, dtype=torch.int64)\n+    seg_indptr = torch.empty(num_experts + 1, device=topk_ids.device, dtype=torch.int64)\n+    seg_indptr[0] = 0\n     src2dst = torch.empty(topk_ids.numel(), device=topk_ids.device, dtype=torch.int32)\n \n     compute_seg_indptr_triton_kernel[(num_experts,)](\n@@ -531,7 +532,8 @@ def grouped_gemm_triton(\n         \"BLOCK_SIZE_K\": 128,\n     }\n \n-    m_num_tiles_indptr = torch.zeros(batch_size + 1, device=a.device, dtype=torch.int64)\n+    m_num_tiles_indptr = torch.empty(batch_size + 1, device=a.device, dtype=torch.int64)\n+    m_num_tiles_indptr[0] = 0\n     compute_m_num_tiles_indptr[(1,)](\n         m_num_tiles_indptr, seg_indptr, batch_size, config[\"BLOCK_SIZE_M\"]\n     )\ndiff --git a/python/sglang/srt/layers/moe/ep_moe/layer.py b/python/sglang/srt/layers/moe/ep_moe/layer.py\nindex a9b443a..a9b244d 100644\n--- a/python/sglang/srt/layers/moe/ep_moe/layer.py\n+++ b/python/sglang/srt/layers/moe/ep_moe/layer.py\n@@ -851,14 +851,8 @@ class DeepEPMoE(EPMoE):\n             self.grouped_gemm_runner = GroupedGemmRunner(\n                 hidden_states.device, use_flashinfer=False  # TODO: use flashinfer\n             )\n-        seg_indptr_cur_rank = torch.cat(\n-            [\n-                torch.zeros(\n-                    1, device=tokens_per_expert.device, dtype=tokens_per_expert.dtype\n-                ),\n-                torch.cumsum(tokens_per_expert, dim=0),\n-            ]\n-        )\n+        cumsum_tokens = torch.cumsum(tokens_per_expert, dim=0)\n+        seg_indptr_cur_rank = torch.nn.functional.pad(cumsum_tokens, (1, 0), value=0)\n         reorder_topk_ids = torch.repeat_interleave(tokens_per_expert)\n         if self.activation_scheme == \"dynamic\" and not self.use_block_quant:\n             max_value = (\n", "model_name_or_path": "gpt-5-2025-08-07"}
