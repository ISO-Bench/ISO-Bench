{"instance_id": "/home/ubuntu/OmniPerf-Bench/sglan__repo-96c503e", "model_patch": "diff --git a/python/sglang/srt/managers/controller/model_runner.py b/python/sglang/srt/managers/controller/model_runner.py\nindex e415147..e706cff 100644\n--- a/python/sglang/srt/managers/controller/model_runner.py\n+++ b/python/sglang/srt/managers/controller/model_runner.py\n@@ -69,13 +69,15 @@ class InputMetadata:\n     flashinfer_decode_wrapper: \"BatchDecodeWithPagedKVCacheWrapper\" = None\n \n     def init_flashinfer_args(self, num_qo_heads, num_kv_heads, head_dim):\n-        self.kv_indptr = torch.zeros(\n+        self.kv_indptr = torch.empty(\n             (self.batch_size + 1,), dtype=torch.int32, device=\"cuda\"\n         )\n+        self.kv_indptr[0] = 0\n         self.kv_indptr[1:] = torch.cumsum(self.seq_lens, dim=0)\n-        self.kv_last_page_len = torch.ones(\n+        self.kv_last_page_len = torch.empty(\n             (self.batch_size,), dtype=torch.int32, device=\"cuda\"\n         )\n+        self.kv_last_page_len.fill_(1)\n         req_pool_indices_cpu = self.req_pool_indices.cpu().numpy()\n         seq_lens_cpu = self.seq_lens.cpu().numpy()\n         self.kv_indices = torch.cat(\n@@ -92,9 +94,10 @@ class InputMetadata:\n             self.forward_mode == ForwardMode.PREFILL\n             or self.forward_mode == ForwardMode.EXTEND\n         ):\n-            self.qo_indptr = torch.zeros(\n+            self.qo_indptr = torch.empty(\n                 (self.batch_size + 1,), dtype=torch.int32, device=\"cuda\"\n             )\n+            self.qo_indptr[0] = 0\n             self.qo_indptr[1:] = torch.cumsum(self.extend_seq_lens, dim=0)\n \n             self.flashinfer_prefill_wrapper.end_forward()\n@@ -124,7 +127,8 @@ class InputMetadata:\n \n     def init_extend_args(self):\n         self.extend_seq_lens = self.seq_lens - self.prefix_lens\n-        self.extend_start_loc = torch.zeros_like(self.seq_lens)\n+        self.extend_start_loc = torch.empty_like(self.seq_lens)\n+        self.extend_start_loc[0] = 0\n         self.extend_start_loc[1:] = torch.cumsum(self.extend_seq_lens[:-1], dim=0)\n         self.max_extend_len = int(torch.max(self.extend_seq_lens))\n \n@@ -147,7 +151,8 @@ class InputMetadata:\n         flashinfer_decode_wrapper=None,\n     ):\n         batch_size = len(req_pool_indices)\n-        start_loc = torch.zeros((batch_size,), dtype=torch.int32, device=\"cuda\")\n+        start_loc = torch.empty((batch_size,), dtype=torch.int32, device=\"cuda\")\n+        start_loc[0] = 0\n         start_loc[1:] = torch.cumsum(seq_lens[:-1], dim=0)\n         total_num_tokens = int(torch.sum(seq_lens))\n         max_seq_len = int(torch.max(seq_lens))\n", "model_name_or_path": "gpt-5-2025-08-07"}
