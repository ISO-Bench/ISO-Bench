{"instance_id": "/home/ubuntu/OmniPerf-Bench/sglan__repo-db45276", "model_patch": "diff --git a/python/sglang/srt/layers/attention/flashattention_backend.py b/python/sglang/srt/layers/attention/flashattention_backend.py\nindex 62604fe..9498817 100644\n--- a/python/sglang/srt/layers/attention/flashattention_backend.py\n+++ b/python/sglang/srt/layers/attention/flashattention_backend.py\n@@ -720,13 +720,13 @@ class FlashAttentionBackend(AttentionBackend):\n         \"\"\"\n         self.decode_cuda_graph_metadata = {\n             # Page table for token mapping (batch_size, max_context_len)\n-            \"page_table\": torch.zeros(\n+            \"page_table\": torch.empty(\n                 max_bs,\n                 (self.max_context_len + self.page_size - 1) // self.page_size,\n                 dtype=torch.int32,\n                 device=self.device,\n             ),\n-            \"page_table_draft_decode\": torch.zeros(\n+            \"page_table_draft_decode\": torch.empty(\n                 max_bs,\n                 (self.max_context_len + self.page_size - 1) // self.page_size,\n                 dtype=torch.int32,\n@@ -735,27 +735,27 @@ class FlashAttentionBackend(AttentionBackend):\n             \"strided_indices\": torch.arange(\n                 0, self.max_context_len, self.page_size, device=self.device\n             ),\n-            \"cache_seqlens\": torch.zeros(max_bs, dtype=torch.int32, device=self.device),\n+            \"cache_seqlens\": torch.empty(max_bs, dtype=torch.int32, device=self.device),\n             \"cu_seqlens_q\": torch.arange(\n                 0, max_bs + 128, dtype=torch.int32, device=self.device\n             ),\n-            \"cu_seqlens_k\": torch.zeros(\n+            \"cu_seqlens_k\": torch.empty(\n                 max_bs + 128, dtype=torch.int32, device=self.device\n             ),\n         }\n \n         self.target_verify_metadata = {\n-            \"page_table\": torch.zeros(\n+            \"page_table\": torch.empty(\n                 max_bs,\n                 (self.max_context_len + self.page_size - 1) // self.page_size,\n                 dtype=torch.int32,\n                 device=self.device,\n             ),\n-            \"cache_seqlens\": torch.zeros(max_bs, dtype=torch.int32, device=self.device),\n-            \"cu_seqlens_q\": torch.zeros(\n+            \"cache_seqlens\": torch.empty(max_bs, dtype=torch.int32, device=self.device),\n+            \"cu_seqlens_q\": torch.empty(\n                 max_bs + 128, dtype=torch.int32, device=self.device\n             ),\n-            \"cu_seqlens_k\": torch.zeros(\n+            \"cu_seqlens_k\": torch.empty(\n                 max_bs + 128, dtype=torch.int32, device=self.device\n             ),\n             \"max_seqlen_q\": 0,\n", "model_name_or_path": "gpt-5-2025-08-07"}
