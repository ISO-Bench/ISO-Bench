{"instance_id": "/home/ubuntu/OmniPerf-Bench/sglan__repo-36fc926", "model_patch": "diff --git a/python/sglang/srt/layers/moe/ep_moe/kernels.py b/python/sglang/srt/layers/moe/ep_moe/kernels.py\nindex d3ec90a..e5d28c6 100644\n--- a/python/sglang/srt/layers/moe/ep_moe/kernels.py\n+++ b/python/sglang/srt/layers/moe/ep_moe/kernels.py\n@@ -148,7 +148,8 @@ def compute_seg_indptr_triton_kernel(reorder_topk_ids, seg_indptr, num_toks):\n def run_moe_ep_preproess(topk_ids: torch.Tensor, num_experts: int):\n     reorder_topk_ids, reorder_ids = torch.sort(topk_ids.view(-1), stable=True)\n \n-    seg_indptr = torch.zeros(num_experts + 1, device=topk_ids.device, dtype=torch.int64)\n+    seg_indptr = torch.empty(num_experts + 1, device=topk_ids.device, dtype=torch.int64)\n+    seg_indptr[0] = 0\n     src2dst = torch.empty(topk_ids.numel(), device=topk_ids.device, dtype=torch.int32)\n \n     compute_seg_indptr_triton_kernel[(num_experts,)](\n@@ -759,7 +760,8 @@ def grouped_gemm_triton(\n         \"BLOCK_SIZE_K\": 128,\n     }\n \n-    m_num_tiles_indptr = torch.zeros(batch_size + 1, device=a.device, dtype=torch.int64)\n+    m_num_tiles_indptr = torch.empty(batch_size + 1, device=a.device, dtype=torch.int64)\n+    m_num_tiles_indptr[0] = 0\n     compute_m_num_tiles_indptr[(1,)](\n         m_num_tiles_indptr, seg_indptr, batch_size, config[\"BLOCK_SIZE_M\"]\n     )\n@@ -1253,9 +1255,10 @@ def moe_ep_deepgemm_preprocess(\n     output_dtype: torch.dtype = torch.float8_e4m3fn,\n ):\n     reorder_topk_ids, reorder_ids = torch.sort(topk_ids.view(-1), stable=True)\n-    seg_indptr = torch.zeros(num_experts + 1, device=topk_ids.device, dtype=torch.int64)\n+    seg_indptr = torch.empty(num_experts + 1, device=topk_ids.device, dtype=torch.int64)\n+    seg_indptr[0] = 0\n     src2dst = torch.empty(topk_ids.numel(), device=topk_ids.device, dtype=torch.int32)\n-    masked_m = torch.zeros(num_experts, device=topk_ids.device, dtype=torch.int32)\n+    masked_m = torch.empty(num_experts, device=topk_ids.device, dtype=torch.int32)\n \n     compute_seg_indptr_triton_kernel[(num_experts,)](\n         reorder_topk_ids, seg_indptr, topk_ids.numel()\ndiff --git a/python/sglang/srt/layers/moe/ep_moe/layer.py b/python/sglang/srt/layers/moe/ep_moe/layer.py\nindex 66fbb36..33dc094 100644\n--- a/python/sglang/srt/layers/moe/ep_moe/layer.py\n+++ b/python/sglang/srt/layers/moe/ep_moe/layer.py\n@@ -351,13 +351,13 @@ class DeepEPMoE(EPMoE):\n             # for instance, if we have 4 experts on this rank, we would have a expert_mask like:\n             #     self.expert_mask = [1, 1, 1, 1, 0]\n             # idx from 0-3 is valid and will be processed, while idx == 4 will be masked out\n-            self.expert_mask = torch.zeros(\n+            self.expert_mask = torch.ones(\n                 (self.num_local_experts + 1),\n                 device=torch.cuda.current_device(),\n                 dtype=torch.int,\n             )\n             # the last one is invalid rank_id\n-            self.expert_mask[:-1] = 1\n+            self.expert_mask[-1] = 0\n         else:\n             self.w13_weight_fp8 = (\n                 self.w13_weight,\n@@ -502,12 +502,11 @@ class DeepEPMoE(EPMoE):\n                 dtype=hidden_states_fp8.dtype,\n             ),\n             (\n-                # TODO check whether need `zeros`\n-                torch.zeros(\n-                    (ceil_div(K // 128, 4), all_tokens),\n+                torch.empty(\n+                    (all_tokens, ceil_div(K // 128, 4)),\n                     device=hidden_states_fp8.device,\n                     dtype=torch.int,\n-                ).transpose(0, 1)\n+                )\n                 if deep_gemm_wrapper.DEEPGEMM_SCALE_UE8M0\n                 else torch.empty(\n                     (all_tokens, K // 128),\ndiff --git a/python/sglang/srt/managers/schedule_batch.py b/python/sglang/srt/managers/schedule_batch.py\nindex 03faea6..bf0d041 100644\n--- a/python/sglang/srt/managers/schedule_batch.py\n+++ b/python/sglang/srt/managers/schedule_batch.py\n@@ -1099,14 +1099,14 @@ class ScheduleBatch(ScheduleBatchDisaggregationDecodeMixin):\n         )\n \n         if not decoder_out_cache_loc:\n-            self.out_cache_loc = torch.zeros(0, dtype=torch.int64).to(\n+            self.out_cache_loc = torch.empty(0, dtype=torch.int64).to(\n                 self.device, non_blocking=True\n             )\n         else:\n             self.out_cache_loc = torch.cat(decoder_out_cache_loc)\n \n         if not encoder_out_cache_loc:\n-            self.encoder_out_cache_loc = torch.zeros(0, dtype=torch.int64).to(\n+            self.encoder_out_cache_loc = torch.empty(0, dtype=torch.int64).to(\n                 self.device, non_blocking=True\n             )\n         else:\n", "model_name_or_path": "gpt-5-2025-08-07"}
