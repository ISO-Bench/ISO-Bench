{"instance_id": "/home/ubuntu/OmniPerf-Bench/sglan__repo-05b3bf5", "model_patch": "diff --git a/python/sglang/srt/hf_transformers_utils.py b/python/sglang/srt/hf_transformers_utils.py\nindex 56d7c8a..c8ee023 100644\n--- a/python/sglang/srt/hf_transformers_utils.py\n+++ b/python/sglang/srt/hf_transformers_utils.py\n@@ -91,10 +91,10 @@ def get_context_length(config):\n     \"\"\"Get the context length of a model from a huggingface model configs.\"\"\"\n     rope_scaling = getattr(config, \"rope_scaling\", None)\n     if rope_scaling:\n-        rope_scaling_factor = config.rope_scaling.get(\"factor\", 1)\n+        rope_scaling_factor = rope_scaling.get(\"factor\", 1)\n         if \"original_max_position_embeddings\" in rope_scaling:\n             rope_scaling_factor = 1\n-        if config.rope_scaling.get(\"rope_type\", None) == \"llama3\":\n+        elif rope_scaling.get(\"rope_type\", None) == \"llama3\":\n             rope_scaling_factor = 1\n     else:\n         rope_scaling_factor = 1\ndiff --git a/python/sglang/srt/layers/sampler.py b/python/sglang/srt/layers/sampler.py\nindex 9ae5801..ba96a2e 100644\n--- a/python/sglang/srt/layers/sampler.py\n+++ b/python/sglang/srt/layers/sampler.py\n@@ -80,7 +80,8 @@ class Sampler(nn.Module):\n \n                 if not torch.all(success):\n                     logger.warning(\"Detected errors during sampling!\")\n-                    batch_next_token_ids = torch.zeros_like(batch_next_token_ids)\n+                    batch_next_token_ids = torch.empty_like(batch_next_token_ids)\n+                    batch_next_token_ids.fill_(0)\n             elif global_server_args_dict[\"sampling_backend\"] == \"pytorch\":\n                 # A slower fallback implementation with torch native operations.\n                 batch_next_token_ids = top_k_top_p_min_p_sampling_from_probs_torch(\ndiff --git a/python/sglang/srt/managers/schedule_batch.py b/python/sglang/srt/managers/schedule_batch.py\nindex fac008d..5094ad2 100644\n--- a/python/sglang/srt/managers/schedule_batch.py\n+++ b/python/sglang/srt/managers/schedule_batch.py\n@@ -30,6 +30,7 @@ ScheduleBatch -> ModelWorkerBatch -> ForwardBatch\n \"\"\"\n \n import dataclasses\n+import itertools\n import logging\n from typing import List, Optional, Tuple, Union\n \n@@ -574,7 +575,7 @@ class ScheduleBatch:\n             pt += req.extend_input_len\n \n         # Reassign\n-        self.input_ids = torch.tensor(sum(input_ids, []), dtype=torch.int32).to(\n+        self.input_ids = torch.tensor(list(itertools.chain.from_iterable(input_ids)), dtype=torch.int32).to(\n             self.device, non_blocking=True\n         )\n         self.seq_lens = torch.tensor(seq_lens, dtype=torch.int32).to(\n@@ -645,7 +646,7 @@ class ScheduleBatch:\n             pt += req.extend_input_len\n \n         # Set fields\n-        self.input_ids = torch.tensor(sum(input_ids, []), dtype=torch.int32).to(\n+        self.input_ids = torch.tensor(list(itertools.chain.from_iterable(input_ids)), dtype=torch.int32).to(\n             self.device, non_blocking=True\n         )\n         self.req_pool_indices = torch.tensor(req_pool_indices, dtype=torch.int32).to(\ndiff --git a/python/sglang/srt/sampling/penaltylib/penalizers/min_new_tokens.py b/python/sglang/srt/sampling/penaltylib/penalizers/min_new_tokens.py\nindex c9e0f07..3504aee 100644\n--- a/python/sglang/srt/sampling/penaltylib/penalizers/min_new_tokens.py\n+++ b/python/sglang/srt/sampling/penaltylib/penalizers/min_new_tokens.py\n@@ -43,11 +43,13 @@ class BatchedMinNewTokensPenalizer(_BatchedPenalizer):\n             batch_first=True,\n             padding_value=self.orchestrator.vocab_size,\n         )\n-        self.stop_token_penalties = torch.zeros(\n+        self.stop_token_penalties = torch.empty(\n             size=(self.orchestrator.batch_size(), self.orchestrator.vocab_size + 1),\n             dtype=torch.float32,\n             device=self.orchestrator.device,\n-        ).scatter_add_(\n+        )\n+        self.stop_token_penalties.fill_(0)\n+        self.stop_token_penalties.scatter_add_(\n             dim=1,\n             index=padded_stop_token_ids,\n             src=torch.full_like(\n@@ -56,15 +58,17 @@ class BatchedMinNewTokensPenalizer(_BatchedPenalizer):\n                 fill_value=float(\"-inf\"),\n                 device=self.orchestrator.device,\n             ),\n-        )[\n+        )\n+        self.stop_token_penalties = self.stop_token_penalties[\n             :, : self.orchestrator.vocab_size\n         ]\n \n-        self.len_output_tokens = torch.zeros(\n+        self.len_output_tokens = torch.empty(\n             size=(self.orchestrator.batch_size(), 1),\n             dtype=torch.int32,\n             device=self.orchestrator.device,\n         )\n+        self.len_output_tokens.fill_(0)\n \n     def _teardown(self):\n         del self.min_new_tokens\ndiff --git a/python/sglang/srt/sampling/sampling_params.py b/python/sglang/srt/sampling/sampling_params.py\nindex b0863b5..f0f0982 100644\n--- a/python/sglang/srt/sampling/sampling_params.py\n+++ b/python/sglang/srt/sampling/sampling_params.py\n@@ -125,14 +125,17 @@ class SamplingParams:\n             if isinstance(self.stop_strs, str):\n                 self.stop_strs = [self.stop_strs]\n \n-            stop_str_max_len = 0\n-            for stop_str in self.stop_strs:\n-                if tokenizer is not None:\n-                    stop_str_ids = tokenizer.encode(stop_str, add_special_tokens=False)\n-                    stop_str_max_len = max(stop_str_max_len, len(stop_str_ids))\n-                else:\n-                    stop_str_max_len = max(stop_str_max_len, len(stop_str))\n-            self.stop_str_max_len = stop_str_max_len\n+            if tokenizer is not None:\n+                self.stop_str_max_len = max(\n+                    (len(tokenizer.encode(stop_str, add_special_tokens=False)) \n+                     for stop_str in self.stop_strs),\n+                    default=0\n+                )\n+            else:\n+                self.stop_str_max_len = max(\n+                    (len(stop_str) for stop_str in self.stop_strs),\n+                    default=0\n+                )\n \n         # Process stop token ids\n         if tokenizer and tokenizer.additional_stop_token_ids:\n", "model_name_or_path": "gpt-5-2025-08-07"}
