{"instance_id": "/home/ubuntu/OmniPerf-Bench/sglan__repo-7360067", "model_patch": "diff --git a/python/sglang/bench_serving.py b/python/sglang/bench_serving.py\nindex 9b27221..79d79a0 100644\n--- a/python/sglang/bench_serving.py\n+++ b/python/sglang/bench_serving.py\n@@ -217,7 +217,7 @@ async def async_request_openai_completions(\n                                 timestamp = time.perf_counter()\n                                 # First token\n                                 if ttft == 0.0:\n-                                    ttft = time.perf_counter() - st\n+                                    ttft = timestamp - st\n                                     output.ttft = ttft\n \n                                 # Decoding phase\n@@ -299,7 +299,7 @@ async def async_request_truss(\n                                 timestamp = time.perf_counter()\n                                 # First token\n                                 if ttft == 0.0:\n-                                    ttft = time.perf_counter() - st\n+                                    ttft = timestamp - st\n                                     output.ttft = ttft\n \n                                 # Decoding phase\n@@ -386,7 +386,7 @@ async def async_request_sglang_generate(\n \n                                 # First token\n                                 if ttft == 0.0:\n-                                    ttft = time.perf_counter() - st\n+                                    ttft = timestamp - st\n                                     output.ttft = ttft\n \n                                 # Decoding phase\n@@ -617,32 +617,26 @@ def sample_sharegpt_requests(\n     with open(dataset_path) as f:\n         dataset = json.load(f)\n \n-    # Filter out the conversations with less than 2 turns.\n-    dataset = [\n-        data\n-        for data in dataset\n-        if len(data.get(\"conversations\", data.get(\"conversation\", []))) >= 2\n-    ]\n-    # Only keep the first two turns of each conversation.\n-    dataset = [\n-        (\n-            data.get(\"conversations\", data.get(\"conversation\", []))[0][\"value\"],\n-            data.get(\"conversations\", data.get(\"conversation\", []))[1][\"value\"],\n-        )\n-        for data in dataset\n-    ]\n+    # Filter out the conversations with less than 2 turns and keep only first two turns.\n+    # Optimized: single pass with cached conversation lookup\n+    filtered_dataset = []\n+    for data in dataset:\n+        conv = data.get(\"conversations\") or data.get(\"conversation\", [])\n+        if len(conv) >= 2:\n+            filtered_dataset.append((conv[0][\"value\"], conv[1][\"value\"]))\n+    dataset = filtered_dataset\n \n     # Shuffle the dataset.\n     random.shuffle(dataset)\n \n     # Filter out sequences that are too long or too short\n     filtered_dataset: List[Tuple[str, int, int]] = []\n-    for i in range(len(dataset)):\n+    for prompt_data, completion_data in dataset:\n         if len(filtered_dataset) == num_requests:\n             break\n \n         # Tokenize the prompts and completions.\n-        prompt = dataset[i][0]\n+        prompt = prompt_data\n         if prompt_suffix:\n             prompt = (\n                 remove_suffix(prompt, ASSISTANT_SUFFIX)\n@@ -659,7 +653,7 @@ def sample_sharegpt_requests(\n             prompt = prompt.replace(tokenizer.bos_token, \"\")\n \n         prompt_token_ids = tokenizer.encode(prompt)\n-        completion = dataset[i][1]\n+        completion = completion_data\n         completion_token_ids = tokenizer.encode(completion)\n         prompt_len = len(prompt_token_ids)\n         output_len = (\n@@ -711,20 +705,14 @@ def sample_random_requests(\n         # Load the dataset.\n         with open(dataset_path) as f:\n             dataset = json.load(f)\n-        # Filter out the conversations with less than 2 turns.\n-        dataset = [\n-            data\n-            for data in dataset\n-            if len(data.get(\"conversations\", data.get(\"conversation\", []))) >= 2\n-        ]\n-        # Only keep the first two turns of each conversation.\n-        dataset = [\n-            (\n-                data.get(\"conversations\", data.get(\"conversation\", []))[0][\"value\"],\n-                data.get(\"conversations\", data.get(\"conversation\", []))[1][\"value\"],\n-            )\n-            for data in dataset\n-        ]\n+        # Filter out the conversations with less than 2 turns and keep only first two turns.\n+        # Optimized: single pass with cached conversation lookup\n+        filtered_dataset = []\n+        for data in dataset:\n+            conv = data.get(\"conversations\") or data.get(\"conversation\", [])\n+            if len(conv) >= 2:\n+                filtered_dataset.append((conv[0][\"value\"], conv[1][\"value\"]))\n+        dataset = filtered_dataset\n         # Shuffle the dataset.\n         random.shuffle(dataset)\n \n@@ -898,21 +886,21 @@ def calculate_metrics(\n     tpots: List[float] = []\n     ttfts: List[float] = []\n     e2e_latencies: List[float] = []\n-    for i in range(len(outputs)):\n-        if outputs[i].success:\n-            output_len = outputs[i].output_len\n+    for i, output in enumerate(outputs):\n+        if output.success:\n+            output_len = output.output_len\n             output_lens.append(output_len)\n             retokenized_output_len = len(\n-                tokenizer.encode(outputs[i].generated_text, add_special_tokens=False)\n+                tokenizer.encode(output.generated_text, add_special_tokens=False)\n             )\n             retokenized_output_lens.append(retokenized_output_len)\n             total_input += input_requests[i][1]\n             if output_len > 1:\n-                tpots.append((outputs[i].latency - outputs[i].ttft) / (output_len - 1))\n-            itls += outputs[i].itl\n-            ttfts.append(outputs[i].ttft)\n+                tpots.append((output.latency - output.ttft) / (output_len - 1))\n+            itls.extend(output.itl)\n+            ttfts.append(output.ttft)\n \n-            e2e_latencies.append(outputs[i].latency)\n+            e2e_latencies.append(output.latency)\n \n             completed += 1\n         else:\ndiff --git a/python/sglang/test/test_utils.py b/python/sglang/test/test_utils.py\nindex 6bcacb4..52efc92 100644\n--- a/python/sglang/test/test_utils.py\n+++ b/python/sglang/test/test_utils.py\n@@ -449,17 +449,16 @@ def popen_launch_server(\n         process = subprocess.Popen(command, stdout=None, stderr=None, env=env)\n \n     start_time = time.time()\n+    # Optimize: create headers once outside the loop\n+    headers = {\n+        \"Content-Type\": \"application/json; charset=utf-8\",\n+        \"Authorization\": f\"Bearer {api_key}\",\n+    }\n+    health_url = f\"{base_url}/health_generate\"\n     with requests.Session() as session:\n         while time.time() - start_time < timeout:\n             try:\n-                headers = {\n-                    \"Content-Type\": \"application/json; charset=utf-8\",\n-                    \"Authorization\": f\"Bearer {api_key}\",\n-                }\n-                response = session.get(\n-                    f\"{base_url}/health_generate\",\n-                    headers=headers,\n-                )\n+                response = session.get(health_url, headers=headers)\n                 if response.status_code == 200:\n                     return process\n             except requests.RequestException:\n", "model_name_or_path": "gpt-5-2025-08-07"}
