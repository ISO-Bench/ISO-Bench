diff --git a/python/sglang/srt/managers/mm_utils.py b/python/sglang/srt/managers/mm_utils.py
index 13ca29c..925d8c0 100644
--- a/python/sglang/srt/managers/mm_utils.py
+++ b/python/sglang/srt/managers/mm_utils.py
@@ -433,7 +433,8 @@ def embed_mm_inputs(
                 device=input_ids.device,
             )
             # calculate per request items length offset
-            items_size = torch.zeros(len(mm_inputs_list) + 1, dtype=int)
+            items_size = torch.empty(len(mm_inputs_list) + 1, dtype=int)
+            items_size[0] = 0  # Initialize first element
             items_offsets = []
             for i, mm_inputs in enumerate(mm_inputs_list):
                 mm_items = [
@@ -591,7 +592,7 @@ def get_multimodal_data_bounds(
     valid_mm_data_nums = min(len(data_start_tokens_cpu), len(data_end_tokens_cpu))
 
     if valid_mm_data_nums == 0:
-        return torch.zeros((0, 2), device=input_ids.device)
+        return torch.empty((0, 2), device=input_ids.device)
 
     # Filter out pairs where start_token >= end_token
     valid_pairs = []
@@ -602,7 +603,7 @@ def get_multimodal_data_bounds(
             valid_pairs.append((start_token + 1, end_token - 1))
 
     if not valid_pairs:
-        return torch.zeros((0, 2), device=input_ids.device)
+        return torch.empty((0, 2), device=input_ids.device)
 
     # Convert valid pairs to tensor
     valid_pairs_tensor = torch.as_tensor(valid_pairs, device=input_ids.device)
diff --git a/python/sglang/srt/managers/schedule_batch.py b/python/sglang/srt/managers/schedule_batch.py
index ad8bcf1..1a92356 100644
--- a/python/sglang/srt/managers/schedule_batch.py
+++ b/python/sglang/srt/managers/schedule_batch.py
@@ -1096,14 +1096,14 @@ class ScheduleBatch(ScheduleBatchDisaggregationDecodeMixin):
         )
 
         if not decoder_out_cache_loc:
-            self.out_cache_loc = torch.zeros(0, dtype=torch.int64).to(
+            self.out_cache_loc = torch.empty(0, dtype=torch.int64).to(
                 self.device, non_blocking=True
             )
         else:
             self.out_cache_loc = torch.cat(decoder_out_cache_loc)
 
         if not encoder_out_cache_loc:
-            self.encoder_out_cache_loc = torch.zeros(0, dtype=torch.int64).to(
+            self.encoder_out_cache_loc = torch.empty(0, dtype=torch.int64).to(
                 self.device, non_blocking=True
             )
         else:
