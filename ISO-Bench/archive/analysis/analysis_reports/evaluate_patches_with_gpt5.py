#!/usr/bin/env python3
"""
GPT-5 as Judge: Evaluate TRAE agent patches against human optimizations.

This script uses GPT-5 to critically evaluate each generated patch by:
1. Comparing it to the human optimization commit
2. Checking for syntax errors
3. Evaluating if optimization logic matches
4. Assessing patch quality and completeness
5. Determining true success/failure

Usage:
    export OPENAI_API_KEY="your_key"
    python evaluate_patches_with_gpt5.py
"""

import json
import os
import sys
from pathlib import Path
from typing import Dict, Any, List, Optional
from openai import OpenAI
import time

# Initialize OpenAI client
api_key = os.getenv("OPENAI_API_KEY")
# Try loading from .env file if not in environment
if not api_key:
    # Try multiple possible .env locations
    possible_env_files = [
        Path(__file__).parent.parent / ".env",
        Path(__file__).parent / ".env",
        Path("/home/raven/coding-mess/kernel-corp/ISO-Bench/.env")
    ]
    for env_file in possible_env_files:
        if env_file.exists():
            for line in env_file.read_text().splitlines():
                line = line.strip()
                if line.startswith("OPENAI_API_KEY="):
                    # Handle both OPENAI_API_KEY="key" and OPENAI_API_KEY=key formats
                    value = line.split("=", 1)[1].strip()
                    # Remove quotes if present
                    if value.startswith('"') and value.endswith('"'):
                        api_key = value[1:-1]
                    elif value.startswith("'") and value.endswith("'"):
                        api_key = value[1:-1]
                    else:
                        api_key = value
                    break
            if api_key:
                break

if not api_key:
    print("ERROR: OPENAI_API_KEY environment variable not set")
    print("Please set it with: export OPENAI_API_KEY='your-key'")
    sys.exit(1)

client = OpenAI(api_key=api_key)

def load_commit_data() -> Dict[str, Any]:
    """Load human commit optimization data."""
    commit_data = {}
    # Try multiple possible paths
    possible_paths = [
        Path("../../tmp_single_commit"),
        Path("../tmp_single_commit"),
        Path("tmp_single_commit"),
        Path("/home/raven/coding-mess/kernel-corp/ISO-Bench/tmp_single_commit")
    ]
    commit_dir = None
    for path in possible_paths:
        if path.exists():
            commit_dir = path
            break
    
    if not commit_dir:
        print("Warning: Could not find tmp_single_commit directory")
        return commit_data
    
    for cf in commit_dir.glob("*.json"):
        try:
            data = json.loads(cf.read_text())
            commit_hash = data.get("head_commit") or data.get("commit_hash", "")
            if commit_hash:
                commit_data[commit_hash] = data
        except Exception as e:
            print(f"Warning: Could not load {cf}: {e}")
    return commit_data

def load_attempt_data() -> List[Dict[str, Any]]:
    """Load all attempt data."""
    with open("all_attempts_comprehensive.json") as f:
        return json.load(f)

def evaluate_patch_with_gpt5(
    human_commit: Dict[str, Any],
    generated_patch: str,
    task_prompt: Dict[str, Any],
    attempt_info: Dict[str, Any]
) -> Dict[str, Any]:
    """
    Use GPT-5 to evaluate if generated patch matches human optimization.
    
    Returns evaluation with:
    - matches_human_intent: bool
    - syntax_correct: bool
    - optimization_correct: bool
    - quality_score: float (0-1)
    - issues: List[str]
    - reasoning: str
    """
    
    human_subject = human_commit.get("subject", "Unknown")
    human_patch = human_commit.get("patch") or human_commit.get("diff_text", "") or human_commit.get("diff", "")
    human_files = human_commit.get("files_changed", [])
    
    has_human_data = human_subject != "Unknown" and human_patch
    
    if has_human_data:
        human_section = f"""## Human Optimization (Ground Truth)

**Commit Subject:** {human_subject}

**Files Changed:** {', '.join(human_files) if human_files else 'Unknown'}

**Human Patch:**
```
{human_patch[:3000]}
```
"""
    else:
        human_section = """## Human Optimization (Ground Truth)

**Note:** Human commit data not available. Evaluate patch quality independently.
"""

    evaluation_prompt = f"""You are an expert code reviewer evaluating a performance optimization patch generated by an AI agent.

{human_section}

## Task Given to Agent

**Target Files:** {', '.join(task_prompt.get('target_files', []))}

**Task Description:** {task_prompt.get('description', 'N/A')}

## Generated Patch

```
{generated_patch[:5000] if generated_patch else "No patch generated"}
```

## Evaluation Criteria

Evaluate the generated patch on:

1. **Syntax Correctness**: Does the patch have any syntax errors? (Python syntax, indentation, etc.)
2. **Files Match**: {"Does the patch modify the same files as the human optimization?" if has_human_data else "Are the modified files reasonable for the task?"}
3. **Optimization Logic Match**: {"Does the patch implement the same or equivalent optimization as the human commit?" if has_human_data else "Does the patch implement a reasonable optimization?"}
4. **Completeness**: Does the patch address the full optimization, or is it incomplete?
5. **Code Quality**: Is the code correct, readable, and follows best practices?
6. **Template/Example Code**: Does the patch contain template code, example code, or placeholder code rather than actual implementation?

## Your Task

Provide a JSON evaluation with:
- "syntax_correct": boolean (true if no syntax errors)
- "files_match": boolean (true if modifies correct files)
- "optimization_logic_match": boolean (true if implements correct optimization)
- "is_complete": boolean (true if addresses full optimization)
- "is_template_code": boolean (true if contains template/example code)
- "quality_score": float (0.0-1.0, where 1.0 is perfect)
- "issues": array of strings (list specific issues found)
- "reasoning": string (detailed explanation of evaluation)

Be CRITICAL and Meticulous. A patch is only truly successful if it:
- Has no syntax errors
- Modifies the correct files
- Implements correct optimization logic
- Is complete (not partial)
- Contains actual code (not templates)

{"If human commit data is available, compare against it. Otherwise, evaluate patch quality independently." if has_human_data else "Evaluate patch quality independently since human commit data is not available."}

Return ONLY valid JSON, no markdown formatting.
"""

    try:
        response = client.chat.completions.create(
            model="gpt-5-2025-08-07",
            messages=[
                {"role": "system", "content": "You are an expert code reviewer. Evaluate patches critically and return only valid JSON."},
                {"role": "user", "content": evaluation_prompt}
            ],
            max_completion_tokens=2000
        )
        
        result_text = response.choices[0].message.content.strip()
        
        # Try to extract JSON
        if result_text.startswith("```json"):
            result_text = result_text[7:]
        if result_text.startswith("```"):
            result_text = result_text[3:]
        if result_text.endswith("```"):
            result_text = result_text[:-3]
        result_text = result_text.strip()
        
        # Try to find JSON object in the response
        try:
            evaluation = json.loads(result_text)
        except json.JSONDecodeError:
            # Try to extract JSON from text
            import re
            json_match = re.search(r'\{.*\}', result_text, re.DOTALL)
            if json_match:
                try:
                    evaluation = json.loads(json_match.group())
                except:
                    raise ValueError(f"Could not parse JSON from response: {result_text[:200]}")
            else:
                raise ValueError(f"No JSON found in response: {result_text[:200]}")
        
        # Calculate true success
        evaluation["true_success"] = (
            evaluation.get("syntax_correct", False) and
            evaluation.get("files_match", False) and
            evaluation.get("optimization_logic_match", False) and
            evaluation.get("is_complete", False) and
            not evaluation.get("is_template_code", False)
        )
        
        return evaluation
        
    except Exception as e:
        print(f"Error evaluating patch: {e}")
        return {
            "syntax_correct": False,
            "files_match": False,
            "optimization_logic_match": False,
            "is_complete": False,
            "is_template_code": True,
            "quality_score": 0.0,
            "issues": [f"Evaluation error: {str(e)}"],
            "reasoning": f"Could not evaluate: {str(e)}",
            "true_success": False
        }

def main():
    print("Loading data...")
    commit_data = load_commit_data()
    all_attempts = load_attempt_data()
    
    print(f"Loaded {len(commit_data)} commits")
    print(f"Loaded {len(all_attempts)} attempts")
    
    # Group attempts by commit
    attempts_by_commit = {}
    for attempt in all_attempts:
        commit = attempt["commit"]
        if commit:
            if commit not in attempts_by_commit:
                attempts_by_commit[commit] = []
            attempts_by_commit[commit].append(attempt)
    
    print(f"\nFound {len(attempts_by_commit)} unique commits")
    
    # Evaluate commits that we have data for
    evaluated_results = []
    
    commits_to_evaluate = list(attempts_by_commit.items())
    print(f"\nEvaluating ALL {len(commits_to_evaluate)} commits...")
    
    for commit_idx, (commit_hash, attempts) in enumerate(commits_to_evaluate, 1):
        # Use commit data if available, otherwise use empty dict
        human_commit = commit_data.get(commit_hash, {})
        if not human_commit:
            print(f"[{commit_idx}/{len(commits_to_evaluate)}] Commit {commit_hash[:8]}: No human commit data (will evaluate patch quality only)")
        
        human_subject = human_commit.get("subject", "Unknown")
        human_files = human_commit.get("files_changed", [])
        human_patch = human_commit.get("diff_text") or human_commit.get("patch", "")
        print(f"\n[{commit_idx}/{len(commits_to_evaluate)}] Evaluating commit {commit_hash[:8]} ({len(attempts)} attempts)...")
        
        commit_evaluations = []
        
        for i, attempt in enumerate(attempts, 1):  # Evaluate ALL attempts per commit
            print(f"  Attempt {i}/{len(attempts)}: {attempt['run_id']}/{attempt['item_id']}...", end=" ", flush=True)
            
            generated_patch = attempt.get("patch", {}).get("content", "")
            task_prompt = attempt.get("prompt", {})
            
            if not generated_patch:
                print("SKIP (no patch)")
                continue
            
            # Evaluate patch (with or without human commit data)
            evaluation = evaluate_patch_with_gpt5(
                human_commit if human_commit else {
                    "subject": "Unknown",
                    "patch": "",
                    "files_changed": []
                },
                generated_patch,
                task_prompt,
                attempt
            )
            
            commit_evaluations.append({
                "run_id": attempt["run_id"],
                "item_id": attempt["item_id"],
                "status": attempt["status"],
                "patch_size": attempt.get("patch", {}).get("size_bytes", 0),
                "evaluation": evaluation
            })
            
            true_success = evaluation.get("true_success", False)
            marked_success = attempt["status"] == "success"
            
            if true_success:
                print(f"✅ TRUE SUCCESS")
            elif marked_success:
                print(f"⚠️  FALSE POSITIVE (marked success but evaluation failed)")
            else:
                print(f"❌ FAILED")
            
            time.sleep(0.5)  # Rate limiting (reduced for faster processing)
        
        evaluated_results.append({
            "commit": commit_hash,
            "subject": human_subject,
            "human_files": human_files,
            "total_attempts": len(attempts),
            "evaluated_attempts": commit_evaluations,
            "summary": {
                "marked_successful": sum(1 for a in attempts if a["status"] == "success"),
                "true_successful": sum(1 for e in commit_evaluations if e["evaluation"].get("true_success", False))
            }
        })
    
    # Save results
    with open("gpt5_evaluations.json", "w") as f:
        json.dump(evaluated_results, f, indent=2, default=str)
    
    # Print summary
    print("\n" + "="*70)
    print("GPT-5 EVALUATION SUMMARY")
    print("="*70)
    
    total_marked_success = 0
    total_true_success = 0
    
    for result in evaluated_results:
        commit = result["commit"][:8]
        marked = result["summary"]["marked_successful"]
        true = result["summary"]["true_successful"]
        total_marked_success += marked
        total_true_success += true
        
        print(f"\nCommit {commit}: {result['subject'][:60]}")
        print(f"  Marked successful: {marked}/{result['total_attempts']}")
        print(f"  True successful: {true}/{len(result['evaluated_attempts'])}")
    
    print(f"\nOverall:")
    print(f"  Marked successful: {total_marked_success}")
    print(f"  True successful: {total_true_success}")
    if total_marked_success > 0:
        print(f"  True success rate: {total_true_success/total_marked_success*100:.1f}%")
    
    print(f"\nResults saved to: gpt5_evaluations.json")

if __name__ == "__main__":
    main()

