diff --git a/model_patch.diff b/model_patch.diff
new file mode 100644
index 000000000..e69de29bb
diff --git a/tests/v1/test_serial_utils.py b/tests/v1/test_serial_utils.py
index bc0e0cbd8..cd31e3e9b 100644
--- a/tests/v1/test_serial_utils.py
+++ b/tests/v1/test_serial_utils.py
@@ -50,7 +50,7 @@ def test_encode_decode():
         large_non_contig_tensor=torch.rand(1024, 512)[:, 10:20],
     )
 
-    encoder = MsgpackEncoder()
+    encoder = MsgpackEncoder(size_threshold=256)
     decoder = MsgpackDecoder(MyType)
 
     encoded = encoder.encode(obj)
diff --git a/vllm/envs.py b/vllm/envs.py
index f80bf878f..bd7e3c7d1 100644
--- a/vllm/envs.py
+++ b/vllm/envs.py
@@ -605,6 +605,11 @@ environment_variables: dict[str, Callable[[], Any]] = {
     "VLLM_V1_OUTPUT_PROC_CHUNK_SIZE":
     lambda: int(os.getenv("VLLM_V1_OUTPUT_PROC_CHUNK_SIZE", "128")),
 
+    # Threshold (in bytes) below which small numpy/tensor buffers are inlined
+    # in the V1 msgpack serializer to reduce aux buffer overhead.
+    "VLLM_V1_MSGBUF_INLINE_THRESHOLD":
+    lambda: int(os.getenv("VLLM_V1_MSGBUF_INLINE_THRESHOLD", "512")),
+
     # If set, vLLM will disable the MLA attention optimizations.
     "VLLM_MLA_DISABLE":
     lambda: bool(int(os.getenv("VLLM_MLA_DISABLE", "0"))),
diff --git a/vllm/v1/serial_utils.py b/vllm/v1/serial_utils.py
index 3af6793fd..6de19586f 100644
--- a/vllm/v1/serial_utils.py
+++ b/vllm/v1/serial_utils.py
@@ -1,5 +1,6 @@
 # SPDX-License-Identifier: Apache-2.0
 
+import os
 import pickle
 from collections.abc import Sequence
 from inspect import isclass
@@ -16,7 +17,8 @@ CUSTOM_TYPE_PICKLE = 1
 CUSTOM_TYPE_CLOUDPICKLE = 2
 CUSTOM_TYPE_RAW_VIEW = 3
 
-# TODO calibrate this size
+# Default inline threshold for small buffers. Can be overridden via
+# constructor or the env var VLLM_V1_MSGBUF_INLINE_THRESHOLD.
 MIN_NOCOPY_BUF_SIZE = 512
 
 bytestr = Union[bytes, bytearray, memoryview, zmq.Frame]
@@ -29,7 +31,14 @@ class MsgpackEncoder:
     not thread-safe when encoding tensors / numpy arrays.
     """
 
-    def __init__(self):
+    def __init__(self, size_threshold: Optional[int] = None):
+        # Resolve the size threshold once during initialization to avoid repeated
+        # environment lookups during encoding.
+        if size_threshold is None:
+            size_threshold = int(
+                os.getenv("VLLM_V1_MSGBUF_INLINE_THRESHOLD", MIN_NOCOPY_BUF_SIZE))
+        self.size_threshold = int(size_threshold)
+
         self.encoder = msgpack.Encoder(enc_hook=self.enc_hook)
         # This is used as a local stash of buffers that we can then access from
         # our custom `msgspec` hook, `enc_hook`. We don't have a way to
@@ -38,7 +47,7 @@ class MsgpackEncoder:
 
     def encode(self, obj: Any) -> Sequence[bytestr]:
         try:
-            self.aux_buffers = bufs = [b'']
+            self.aux_buffers = bufs = [b""]
             bufs[0] = self.encoder.encode(obj)
             # This `bufs` list allows us to collect direct pointers to backing
             # buffers of tensors and np arrays, and return them along with the
@@ -62,30 +71,48 @@ class MsgpackEncoder:
             return self._encode_ndarray(obj.numpy())
 
         # Fall back to pickle for object or void kind ndarrays.
-        if isinstance(obj, np.ndarray) and obj.dtype.kind not in ('O', 'V'):
+        if isinstance(obj, np.ndarray) and obj.dtype.kind not in ("O", "V"):
             return self._encode_ndarray(obj)
 
         if isinstance(obj, FunctionType):
-            # `pickle` is generally faster than cloudpickle, but can have
-            # problems serializing methods.
+            # Prefer pickle for top-level functions as it's faster; fall back to
+            # cloudpickle for methods/closures where pickle may fail.
+            if getattr(obj, "__qualname__", None) == getattr(obj, "__name__", None):
+                try:
+                    return msgpack.Ext(
+                        CUSTOM_TYPE_PICKLE,
+                        pickle.dumps(obj, protocol=pickle.HIGHEST_PROTOCOL),
+                    )
+                except Exception:
+                    pass
             return msgpack.Ext(CUSTOM_TYPE_CLOUDPICKLE, cloudpickle.dumps(obj))
 
-        return msgpack.Ext(CUSTOM_TYPE_PICKLE,
-                           pickle.dumps(obj, protocol=pickle.HIGHEST_PROTOCOL))
+        return msgpack.Ext(
+            CUSTOM_TYPE_PICKLE,
+            pickle.dumps(obj, protocol=pickle.HIGHEST_PROTOCOL))
 
     def _encode_ndarray(
         self, obj: np.ndarray
     ) -> tuple[str, tuple[int, ...], Union[int, memoryview]]:
         assert self.aux_buffers is not None
-        arr_data = obj.data if obj.data.c_contiguous else obj.tobytes()
-        if not obj.shape or obj.nbytes < MIN_NOCOPY_BUF_SIZE:
-            # Encode small arrays and scalars inline. Using this extension type
-            # ensures we can avoid copying when decoding.
-            data = msgpack.Ext(CUSTOM_TYPE_RAW_VIEW, arr_data)
+        bufs = self.aux_buffers  # local alias to avoid attribute lookups in hot path
+        threshold = self.size_threshold
+        # Fast path: reuse underlying contiguous buffer when available.
+        # Use ndarray.flags to avoid creating an intermediate buffer proxy.
+        if obj.flags.c_contiguous:
+            arr_data: Union[memoryview, bytes] = memoryview(obj)
+        else:
+            # Create a contiguous copy only when necessary.
+            arr_data = obj.tobytes()
+        # Inline small arrays and scalars; otherwise append to aux_buffers.
+        if not obj.shape or obj.nbytes < threshold:
+            # If we already have a memoryview, avoid wrapping it again.
+            mv = arr_data if isinstance(arr_data, memoryview) else memoryview(arr_data)
+            data: Union[int, memoryview] = msgpack.Ext(CUSTOM_TYPE_RAW_VIEW, mv)
         else:
             # Otherwise encode index of backing buffer to avoid copy.
-            data = len(self.aux_buffers)
-            self.aux_buffers.append(arr_data)
+            data = len(bufs)
+            bufs.append(arr_data)
 
         # We serialize the ndarray as a tuple of native types.
         # The data is either inlined if small, or an index into a list of
@@ -102,12 +129,12 @@ class MsgpackDecoder:
 
     def __init__(self, t: Optional[Any] = None):
         args = () if t is None else (t, )
-        self.decoder = msgpack.Decoder(*args,
-                                       ext_hook=self.ext_hook,
-                                       dec_hook=self.dec_hook)
+        self.decoder = msgpack.Decoder(
+            *args, ext_hook=self.ext_hook, dec_hook=self.dec_hook)
         self.aux_buffers: Sequence[bytestr] = ()
 
     def decode(self, bufs: Union[bytestr, Sequence[bytestr]]) -> Any:
+        # Fast path for single-buffer inputs.
         if isinstance(bufs, (bytes, bytearray, memoryview, zmq.Frame)):
             # TODO - This check can become `isinstance(bufs, bytestr)`
             # as of Python 3.10.
@@ -130,7 +157,8 @@ class MsgpackDecoder:
 
     def _decode_ndarray(self, arr: Any) -> np.ndarray:
         dtype, shape, data = arr
-        buffer = self.aux_buffers[data] if isinstance(data, int) else data
+        aux = self.aux_buffers
+        buffer = aux[data] if isinstance(data, int) else data
         return np.ndarray(buffer=buffer, dtype=np.dtype(dtype), shape=shape)
 
     def ext_hook(self, code: int, data: memoryview) -> Any:
