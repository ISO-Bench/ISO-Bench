{"instance_id": "/home/ubuntu/OmniPerf-Bench/sglan__repo-a53fe42", "model_patch": "diff --git a/sgl-kernel/csrc/allreduce/trt_reduce_internal.cu b/sgl-kernel/csrc/allreduce/trt_reduce_internal.cu\nindex f1ee5d40e..d3b995492 100644\n--- a/sgl-kernel/csrc/allreduce/trt_reduce_internal.cu\n+++ b/sgl-kernel/csrc/allreduce/trt_reduce_internal.cu\n@@ -156,7 +156,9 @@ __inline__ __device__ void block_barrier(\n     int const tidx,\n     int const bidx,\n     int const grid_size) {\n-  if constexpr (!start) {\n+  // Only synchronize threads when entering the first barrier of the kernel\n+  // or when a fence is explicitly required to ensure ordering.\n+  if constexpr (start || need_fence) {\n     __syncthreads();\n   }\n   // After this function, the block of id == bidx of each GPU has reached the barrier\n@@ -183,7 +185,11 @@ __inline__ __device__ void block_barrier(\n     }\n   }\n \n-  __syncthreads();\n+  // Avoid an unconditional sync here; rely on conditional syncs above\n+  // to reduce barrier overhead when unnecessary.\n+  if constexpr (start || need_fence) {\n+    __syncthreads();\n+  }\n }\n \n template <typename T, int RANKS_PER_NODE, bool COPY_INPUT = true>\n@@ -251,10 +257,12 @@ static __global__ void __launch_bounds__(512, 1) oneShotAllReduceKernel(AllReduc\n     }\n \n     // Sum the values from the different ranks.\n+    // Initialize from the first rank to avoid extra zero-initialization\n+    // and reduce the number of additions.\n     PackedStruct sums;\n-    sums.packed = {0, 0, 0, 0};\n+    sums.packed = vals[0].packed;\n #pragma unroll\n-    for (int rank = 0; rank < RANKS_PER_NODE; ++rank) {\n+    for (int rank = 1; rank < RANKS_PER_NODE; ++rank) {\n       // Always reduce from rank 0 to ensure stable reduce order.\n       sums.packed = add128b(sums, vals[rank]);\n     }\n@@ -262,6 +270,10 @@ static __global__ void __launch_bounds__(512, 1) oneShotAllReduceKernel(AllReduc\n     // Store to the destination buffer.\n     *reinterpret_cast<int4*>(&reinterpret_cast<T*>(params.local_output_buffer_ptr)[iter_offset]) = sums.packed;\n   }\n+  // Ensure all participating blocks have completed their writes and\n+  // signal completion to peers before exiting the kernel.\n+  block_barrier<false>(\n+      params.peer_barrier_ptrs_out, params.barrier_flag, params.local_rank, RANKS_PER_NODE, tidx, bidx, grid_size);\n }\n \n template <typename T, int RANKS_PER_NODE, bool COPY_INPUT = true>\n", "model_name_or_path": "gpt-5-2025-08-07"}
