{"instance_id": "/home/ubuntu/OmniPerf-Bench/sglan__repo-c4336b2", "model_patch": "diff --git a/python/sglang/srt/managers/scheduler.py b/python/sglang/srt/managers/scheduler.py\nindex 1d1cf3688..f404312f3 100644\n--- a/python/sglang/srt/managers/scheduler.py\n+++ b/python/sglang/srt/managers/scheduler.py\n@@ -72,6 +72,7 @@ from sglang.srt.utils import (\n     configure_logger,\n     crash_on_warnings,\n     get_zmq_socket,\n+    gpu_proc_affinity,\n     kill_parent_process,\n     set_random_seed,\n     suppress_other_loggers,\n@@ -1393,6 +1394,13 @@ def run_scheduler_process(\n     dp_rank: Optional[int],\n     pipe_writer,\n ):\n+    # Pin this scheduler process to a subset of CPU cores based on GPU id.\n+    # This reduces cross-socket memory traffic and improves locality.\n+    try:\n+        gpu_proc_affinity(server_args.tp_size, server_args.nnodes, gpu_id)\n+    except Exception:\n+        # Affinity setup is best-effort; ignore failures on unsupported platforms.\n+        pass\n     # [For Router] if env var \"DP_RANK\" exist, set dp_rank to the value of the env var\n     if dp_rank is None and \"DP_RANK\" in os.environ:\n         dp_rank = int(os.environ[\"DP_RANK\"])\ndiff --git a/python/sglang/srt/utils.py b/python/sglang/srt/utils.py\nindex e947d1a92..8a3380df0 100644\n--- a/python/sglang/srt/utils.py\n+++ b/python/sglang/srt/utils.py\n@@ -35,6 +35,7 @@ from typing import Any, Callable, Dict, List, Optional, Protocol, Tuple, Union\n \n import numpy as np\n import psutil\n+import functools\n import requests\n import torch\n import torch.distributed as dist\n@@ -692,9 +693,7 @@ def broadcast_pyobj(\n         else:\n             serialized_data = pickle.dumps(data)\n             size = len(serialized_data)\n-            tensor_data = torch.ByteTensor(\n-                np.frombuffer(serialized_data, dtype=np.uint8)\n-            )\n+            tensor_data = _bytes_to_uint8_tensor(serialized_data)\n             tensor_size = torch.tensor([size], dtype=torch.long)\n \n             dist.broadcast(tensor_size, src=0, group=dist_group)\n@@ -716,6 +715,22 @@ def broadcast_pyobj(\n         return data\n \n \n+def _bytes_to_uint8_tensor(b: bytes) -> torch.Tensor:\n+    \"\"\"Convert bytes to a CPU uint8 tensor with minimal copies.\n+\n+    Uses torch.frombuffer when available; otherwise falls back to numpy.\n+    \"\"\"\n+    try:\n+        mv = memoryview(b)\n+        t = torch.frombuffer(mv, dtype=torch.uint8)  # type: ignore[arg-type]\n+        # keep a reference to avoid GC of the underlying buffer\n+        t._mv_ref = mv  # type: ignore[attr-defined]\n+        return t\n+    except Exception:\n+        arr = np.frombuffer(b, dtype=np.uint8)\n+        return torch.from_numpy(arr)\n+\n+\n step_counter = 0\n \n \n@@ -755,13 +770,8 @@ def first_rank_print(*args, **kwargs):\n \n \n def get_zmq_socket(context: zmq.Context, socket_type: zmq.SocketType, endpoint: str):\n-    mem = psutil.virtual_memory()\n-    total_mem = mem.total / 1024**3\n-    available_mem = mem.available / 1024**3\n-    if total_mem > 32 and available_mem > 16:\n-        buf_size = int(0.5 * 1024**3)\n-    else:\n-        buf_size = -1\n+    # Use cached buffer size heuristic to avoid repeated psutil calls.\n+    buf_size = _zmq_default_buf_size()\n \n     socket = context.socket(socket_type)\n     if socket_type == zmq.PUSH:\n@@ -778,6 +788,20 @@ def get_zmq_socket(context: zmq.Context, socket_type: zmq.SocketType, endpoint:\n     return socket\n \n \n+@functools.lru_cache(maxsize=1)\n+def _zmq_default_buf_size() -> int:\n+    \"\"\"Heuristic for ZMQ socket buffer size; cached to avoid psutil overhead.\"\"\"\n+    try:\n+        mem = psutil.virtual_memory()\n+        total_mem_gb = mem.total / (1024**3)\n+        available_mem_gb = mem.available / (1024**3)\n+        if total_mem_gb > 32 and available_mem_gb > 16:\n+            return int(0.5 * (1024**3))\n+    except Exception:\n+        pass\n+    return -1\n+\n+\n def dump_to_file(dirpath, name, value):\n     from vllm.distributed import get_tensor_model_parallel_rank\n \n@@ -947,6 +971,104 @@ def get_device_name(device_id: int = 0) -> str:\n         return torch.hpu.get_device_name(device_id)\n \n \n+def _parse_cpulist(cpulist: str) -> List[int]:\n+    \"\"\"Parse Linux cpulist format like '0-3,8,10-12' -> [0,1,2,3,8,10,11,12].\"\"\"\n+    res: List[int] = []\n+    for part in filter(None, cpulist.split(\",\")):\n+        if \"-\" in part:\n+            a, b = part.split(\"-\", 1)\n+            start, end = int(a), int(b)\n+            res.extend(range(start, end + 1))\n+        else:\n+            res.append(int(part))\n+    return res\n+\n+\n+def _read_numa_cpu_lists() -> List[List[int]]:\n+    \"\"\"Read Linux NUMA node CPU lists, return list of CPU id lists per node.\n+\n+    Falls back to a single group with all available CPUs when not available.\n+    \"\"\"\n+    nodes: List[List[int]] = []\n+    sysfs_nodes = \"/sys/devices/system/node\"\n+    try:\n+        if os.path.isdir(sysfs_nodes):\n+            entries = sorted([d for d in os.listdir(sysfs_nodes) if d.startswith(\"node\")])\n+            for d in entries:\n+                path = os.path.join(sysfs_nodes, d, \"cpulist\")\n+                if not os.path.exists(path):\n+                    continue\n+                with open(path, \"r\") as f:\n+                    cpulist = f.read().strip()\n+                cpus = _parse_cpulist(cpulist)\n+                if cpus:\n+                    nodes.append(cpus)\n+    except Exception:\n+        nodes = []\n+    if not nodes:\n+        # Fallback to all allowed CPUs\n+        try:\n+            allowed = sorted(list(os.sched_getaffinity(0)))  # type: ignore[attr-defined]\n+        except Exception:\n+            allowed = list(range(psutil.cpu_count(logical=True) or 1))\n+        nodes = [allowed]\n+    return nodes\n+\n+\n+def gpu_proc_affinity(tp_size: int, nnodes: Optional[int], gpu_id: int) -> None:\n+    \"\"\"Pin current process to a subset of CPUs for the given GPU.\n+\n+    - Splits available CPUs per NUMA node when possible for locality.\n+    - Evenly partitions CPUs across the number of local GPUs.\n+    - Best-effort; silently returns on unsupported platforms.\n+    \"\"\"\n+    if os.getenv(\"SGLANG_DISABLE_CPU_AFFINITY\", \"\").lower() in {\"1\", \"true\", \"yes\"}:\n+        return\n+\n+    try:\n+        local_gpus = torch.cuda.device_count() if torch.cuda.is_available() else 1\n+        if local_gpus <= 0:\n+            return\n+\n+        # Derive local GPU index (best-effort)\n+        local_gpu_idx = gpu_id % max(1, local_gpus)\n+\n+        numa_nodes = _read_numa_cpu_lists()  # list of cpu-id lists per NUMA node\n+        # Flatten NUMA nodes into a single ordered list while keeping node groups\n+        per_node_cpus = [list(cpus) for cpus in numa_nodes if cpus]\n+        all_cpus: List[int] = [cpu for group in per_node_cpus for cpu in group]\n+        if not all_cpus:\n+            return\n+\n+        # Partition CPUs across local GPUs. Prefer node-granularity if possible.\n+        if len(per_node_cpus) >= local_gpus:\n+            # Assign whole nodes to GPUs in order\n+            groups: List[List[int]] = [[] for _ in range(local_gpus)]\n+            for i, node in enumerate(per_node_cpus):\n+                groups[i % local_gpus].extend(node)\n+            target_cpus = sorted(set(groups[local_gpu_idx]))\n+        else:\n+            # Evenly slice all CPUs\n+            chunk = max(1, len(all_cpus) // local_gpus)\n+            start = local_gpu_idx * chunk\n+            end = len(all_cpus) if local_gpu_idx == local_gpus - 1 else start + chunk\n+            target_cpus = sorted(set(all_cpus[start:end]))\n+\n+        if not target_cpus:\n+            return\n+\n+        p = psutil.Process(os.getpid())\n+        p.cpu_affinity(target_cpus)\n+        try:\n+            # Limit torch intraop threads to stay within the CPU set\n+            torch.set_num_threads(min(len(target_cpus), os.cpu_count() or 1))\n+        except Exception:\n+            pass\n+    except Exception:\n+        # Best effort only; ignore failures on unsupported systems\n+        return\n+\n+\n sglang_lib = Library(\"sglang\", \"FRAGMENT\")  # noqa\n \n \n", "model_name_or_path": "gpt-5-2025-08-07"}
