{"instance_id": "/home/ubuntu/OmniPerf-Bench/sglan__repo-9c064bf", "model_patch": "diff --git a/python/sglang/srt/layers/sampler.py b/python/sglang/srt/layers/sampler.py\nindex ad7f0a1f3..95ac45edf 100644\n--- a/python/sglang/srt/layers/sampler.py\n+++ b/python/sglang/srt/layers/sampler.py\n@@ -37,13 +37,14 @@ class Sampler(nn.Module):\n         logits = None\n         del logits\n \n-        if torch.any(torch.isnan(probs)):\n+        if probs.isnan().any():\n             logger.warning(\"Detected errors during sampling! NaN in the probability.\")\n-            probs = torch.where(\n-                torch.isnan(probs), torch.full_like(probs, 1e-10), probs\n-            )\n-\n-        if global_server_args_dict[\"sampling_backend\"] == \"flashinfer\":\n+            # Avoid extra allocations by fixing NaNs in-place\n+            probs.nan_to_num_(nan=1e-10)\n+        # Fast path: if all requests use greedy sampling, directly take argmax\n+        if sampling_info.top_ks.max().item() <= 1:\n+            batch_next_token_ids = torch.argmax(probs, -1)\n+        elif global_server_args_dict[\"sampling_backend\"] == \"flashinfer\":\n             max_top_k_round, batch_size = 32, probs.shape[0]\n             uniform_samples = torch.rand(\n                 (max_top_k_round, batch_size), device=probs.device\n@@ -65,7 +66,7 @@ class Sampler(nn.Module):\n \n             if not torch.all(success):\n                 logger.warning(\"Detected errors during sampling!\")\n-                batch_next_token_ids = torch.zeros_like(batch_next_token_ids)\n+                batch_next_token_ids.zero_()\n         elif global_server_args_dict[\"sampling_backend\"] == \"pytorch\":\n             # Here we provide a slower fallback implementation.\n             batch_next_token_ids = top_k_top_p_min_p_sampling_from_probs_torch(\ndiff --git a/test/srt/test_bench_serving.py b/test/srt/test_bench_serving.py\nindex 056483487..6955d4917 100644\n--- a/test/srt/test_bench_serving.py\n+++ b/test/srt/test_bench_serving.py\n@@ -27,11 +27,11 @@ class TestBenchServing(unittest.TestCase):\n             model=DEFAULT_MODEL_NAME_FOR_TEST,\n             num_prompts=200,\n             request_rate=float(\"inf\"),\n+            other_server_args=[\"--max-running-requests\", \"10\"],\n             dataset_name=\"sharegpt\",\n             random_input_len=None,\n             random_output_len=None,\n             disable_stream=True,\n-            other_server_args=[\"--max-running-requests\", \"10\"],\n         )\n \n         if is_in_ci():\ndiff --git a/test/srt/test_pytorch_sampling_backend.py b/test/srt/test_pytorch_sampling_backend.py\nindex ddd744149..8ecf0a2ae 100644\n--- a/test/srt/test_pytorch_sampling_backend.py\n+++ b/test/srt/test_pytorch_sampling_backend.py\n@@ -39,6 +39,28 @@ class TestPyTorchSamplingBackend(unittest.TestCase):\n         metrics = run_eval(args)\n         assert metrics[\"score\"] >= 0.65\n \n+    def test_mmlu_greedy(self):\n+        \"\"\"\n+        Sanity check with the PyTorch sampling backend under a typical\n+        greedy-like configuration. This ensures the backend path works\n+        correctly when requests are effectively greedy (top-k=1).\n+        \"\"\"\n+        # Reuse the same server started with --sampling-backend pytorch.\n+        # The eval driver will issue requests with default sampling params\n+        # which include greedy-like settings for many prompts.\n+        args = SimpleNamespace(\n+            base_url=self.base_url,\n+            model=self.model,\n+            eval_name=\"mmlu\",\n+            num_examples=32,\n+            num_threads=16,\n+        )\n+\n+        metrics = run_eval(args)\n+        # Only require that the evaluation runs and produces a reasonable score.\n+        assert \"score\" in metrics\n+        assert 0.0 <= metrics[\"score\"] <= 1.0\n+\n \n if __name__ == \"__main__\":\n     unittest.main()\n", "model_name_or_path": "gpt-5-2025-08-07"}
