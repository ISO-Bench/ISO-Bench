{"instance_id": "/home/ubuntu/OmniPerf-Bench/sglan__repo-e1792cc", "model_patch": "diff --git a/python/sglang/srt/server_args.py b/python/sglang/srt/server_args.py\nindex 7c0317fc0..989fa347a 100644\n--- a/python/sglang/srt/server_args.py\n+++ b/python/sglang/srt/server_args.py\n@@ -33,7 +33,8 @@ class ServerArgs:\n \n     # Other runtime options\n     tp_size: int = 1\n-    stream_interval: int = 8\n+    # Smaller interval improves responsiveness and reduces TTFT.\n+    stream_interval: int = 1\n     random_seed: Optional[int] = None\n \n     # Logging\n@@ -116,7 +117,7 @@ class ServerArgs:\n             \"--tokenizer-mode\",\n             type=str,\n             default=ServerArgs.tokenizer_mode,\n-            choices=[\"auto\", \"slow\"],\n+            choices=(\"auto\", \"slow\"),\n             help=\"Tokenizer mode. 'auto' will use the fast \"\n             \"tokenizer if available, and 'slow' will \"\n             \"always use the slow tokenizer.\",\n@@ -125,7 +126,7 @@ class ServerArgs:\n             \"--load-format\",\n             type=str,\n             default=ServerArgs.load_format,\n-            choices=[\"auto\", \"pt\", \"safetensors\", \"npcache\", \"dummy\"],\n+            choices=(\"auto\", \"pt\", \"safetensors\", \"npcache\", \"dummy\"),\n             help=\"The format of the model weights to load. \"\n             '\"auto\" will try to load the weights in the safetensors format '\n             \"and fall back to the pytorch bin format if safetensors format \"\n@@ -141,7 +142,7 @@ class ServerArgs:\n             \"--dtype\",\n             type=str,\n             default=ServerArgs.dtype,\n-            choices=[\"auto\", \"half\", \"float16\", \"bfloat16\", \"float\", \"float32\"],\n+            choices=(\"auto\", \"half\", \"float16\", \"bfloat16\", \"float\", \"float32\"),\n             help=\"Data type for model weights and activations.\\n\\n\"\n             '* \"auto\" will use FP16 precision for FP32 and FP16 models, and '\n             \"BF16 precision for BF16 models.\\n\"\n@@ -196,7 +197,7 @@ class ServerArgs:\n             \"--schedule-heuristic\",\n             type=str,\n             default=ServerArgs.schedule_heuristic,\n-            choices=[\"lpm\", \"random\", \"fcfs\", \"dfs-weight\"],\n+            choices=(\"lpm\", \"random\", \"fcfs\", \"dfs-weight\"),\n             help=\"The scheduling heuristic.\",\n         )\n         parser.add_argument(\n@@ -264,10 +265,10 @@ class ServerArgs:\n             type=str,\n             default=ServerArgs.load_balance_method,\n             help=\"The load balancing strategy for data parallelism.\",\n-            choices=[\n+            choices=(\n                 \"round_robin\",\n                 \"shortest_queue\",\n-            ],\n+            ),\n         )\n \n         # Multi-node distributed serving args\n@@ -321,7 +322,11 @@ class ServerArgs:\n \n     @classmethod\n     def from_cli_args(cls, args: argparse.Namespace):\n-        attrs = [attr.name for attr in dataclasses.fields(cls)]\n+        # Use precomputed field names to avoid repeated dataclasses.fields calls\n+        try:\n+            attrs = cls._FIELD_NAMES  # type: ignore[attr-defined]\n+        except AttributeError:\n+            attrs = tuple(f.name for f in dataclasses.fields(cls))\n         return cls(**{attr: getattr(args, attr) for attr in attrs})\n \n     def url(self):\n@@ -343,3 +348,7 @@ class PortArgs:\n     controller_port: int\n     detokenizer_port: int\n     nccl_ports: List[int]\n+\n+# Precompute dataclass field names once to speed up from_cli_args\n+# and avoid repeated introspection overhead.\n+ServerArgs._FIELD_NAMES = tuple(ServerArgs.__dataclass_fields__.keys())  # type: ignore[attr-defined]\n", "model_name_or_path": "gpt-5-2025-08-07"}
