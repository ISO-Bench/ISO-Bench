diff --git a/python/sglang/srt/managers/schedule_batch.py b/python/sglang/srt/managers/schedule_batch.py
index faf05a7ff..bc9bd0295 100644
--- a/python/sglang/srt/managers/schedule_batch.py
+++ b/python/sglang/srt/managers/schedule_batch.py
@@ -32,6 +32,7 @@ ScheduleBatch -> ModelWorkerBatch -> ForwardBatch
 import dataclasses
 import logging
 from typing import TYPE_CHECKING, List, Optional, Set, Tuple, Union
+from itertools import chain
 
 import numpy as np
 import torch
@@ -668,8 +669,8 @@ class ScheduleBatch:
                     or len(req.prefix_indices) >= im.num_image_tokens
                 )
 
-        self.encoder_lens = torch.tensor(self.encoder_lens_cpu, dtype=torch.int32).to(
-            self.device, non_blocking=True
+        self.encoder_lens = torch.tensor(
+            self.encoder_lens_cpu, dtype=torch.int32, device=self.device
         )
 
         # Strip encoder infos
@@ -699,24 +700,18 @@ class ScheduleBatch:
             pt += req.extend_input_len
 
         # Reassign
-        self.input_ids = torch.tensor(sum(input_ids, []), dtype=torch.int32).to(
-            self.device, non_blocking=True
-        )
-        self.seq_lens = torch.tensor(seq_lens, dtype=torch.int32).to(
-            self.device, non_blocking=True
+        self.input_ids = torch.tensor(
+            list(chain.from_iterable(input_ids)), dtype=torch.int32, device=self.device
         )
+        self.seq_lens = torch.tensor(seq_lens, dtype=torch.int32, device=self.device)
 
         if not decoder_out_cache_loc:
-            self.out_cache_loc = torch.zeros(0, dtype=torch.int32).to(
-                self.device, non_blocking=True
-            )
+            self.out_cache_loc = torch.empty(0, dtype=torch.int32, device=self.device)
         else:
             self.out_cache_loc = torch.cat(decoder_out_cache_loc)
 
         if not encoder_out_cache_loc:
-            self.encoder_out_cache_loc = torch.zeros(0, dtype=torch.int32).to(
-                self.device, non_blocking=True
-            )
+            self.encoder_out_cache_loc = torch.empty(0, dtype=torch.int32, device=self.device)
         else:
             self.encoder_out_cache_loc = torch.cat(encoder_out_cache_loc)
 
@@ -775,19 +770,15 @@ class ScheduleBatch:
             pre_lens.append(pre_len)
 
         # Set fields
-        self.input_ids = torch.tensor(sum(input_ids, []), dtype=torch.int32).to(
-            self.device, non_blocking=True
-        )
-        self.req_pool_indices = torch.tensor(req_pool_indices, dtype=torch.int32).to(
-            self.device, non_blocking=True
+        self.input_ids = torch.tensor(
+            list(chain.from_iterable(input_ids)), dtype=torch.int32, device=self.device
         )
-        self.seq_lens = torch.tensor(seq_lens, dtype=torch.int32).to(
-            self.device, non_blocking=True
+        self.req_pool_indices = torch.tensor(
+            req_pool_indices, dtype=torch.int32, device=self.device
         )
+        self.seq_lens = torch.tensor(seq_lens, dtype=torch.int32, device=self.device)
         self.input_embeds = (
-            torch.tensor(input_embeds).to(self.device, non_blocking=True)
-            if input_embeds
-            else None
+            torch.tensor(input_embeds, device=self.device) if input_embeds else None
         )
 
         self.out_cache_loc = out_cache_loc
@@ -801,12 +792,8 @@ class ScheduleBatch:
         self.extend_logprob_start_lens = [r.extend_logprob_start_len for r in reqs]
 
         # Write to req_to_token_pool
-        pre_lens = torch.tensor(pre_lens, dtype=torch.int32).to(
-            self.device, non_blocking=True
-        )
-        extend_lens = torch.tensor(self.extend_lens, dtype=torch.int32).to(
-            self.device, non_blocking=True
-        )
+        pre_lens = torch.tensor(pre_lens, dtype=torch.int32, device=self.device)
+        extend_lens = torch.tensor(self.extend_lens, dtype=torch.int32, device=self.device)
         if global_server_args_dict["attention_backend"] != "torch_native":
             write_req_to_token_pool_triton[(bs,)](
                 self.req_to_token_pool.req_to_token,
@@ -1084,9 +1071,7 @@ class ScheduleBatch:
             self.encoder_lens_cpu = [self.encoder_lens_cpu[i] for i in keep_indices]
 
         self.reqs = [self.reqs[i] for i in keep_indices]
-        new_indices = torch.tensor(keep_indices, dtype=torch.int32).to(
-            self.device, non_blocking=True
-        )
+        new_indices = torch.tensor(keep_indices, dtype=torch.int32, device=self.device)
         self.req_pool_indices = self.req_pool_indices[new_indices]
         self.seq_lens = self.seq_lens[new_indices]
         self.out_cache_loc = None
