{"instance_id": "/home/ubuntu/OmniPerf-Bench/sglan__repo-aac531c", "model_patch": "diff --git a/python/sglang/srt/layers/moe/ep_moe/token_dispatcher.py b/python/sglang/srt/layers/moe/ep_moe/token_dispatcher.py\nindex 100fa57fb..8762e3847 100644\n--- a/python/sglang/srt/layers/moe/ep_moe/token_dispatcher.py\n+++ b/python/sglang/srt/layers/moe/ep_moe/token_dispatcher.py\n@@ -156,7 +156,9 @@ class _DeepEPDispatcherImplNormal(_DeepEPDispatcherImplBase):\n         num_experts: int,\n         num_max_dispatch_tokens_per_rank: int,\n     ):\n-        topk_idx = topk_idx.to(torch.int64)\n+        # Avoid redundant dtype conversion if already int64\n+        if topk_idx.dtype is not torch.int64:\n+            topk_idx = topk_idx.to(torch.int64)\n         previous_event = Buffer.capture() if self.async_finish else None\n         return hidden_states, topk_idx, topk_weights, num_experts, previous_event\n \n@@ -311,7 +313,8 @@ class _DeepEPDispatcherImplNormal(_DeepEPDispatcherImplBase):\n                 BLOCK_SIZE=512,\n             )\n         else:\n-            output = torch.zeros(\n+            # For zero tokens, allocate an empty tensor to avoid memset overhead\n+            output = torch.empty(\n                 (0, hidden_states.shape[1]),\n                 device=hidden_states.device,\n                 dtype=hidden_states.dtype,\n@@ -363,7 +366,9 @@ class _DeepEPDispatcherImplLowLatency(_DeepEPDispatcherImplBase):\n         num_experts: int,\n         num_max_dispatch_tokens_per_rank: int,\n     ):\n-        topk_idx = topk_idx.to(torch.int64)\n+        # Avoid redundant dtype conversion if already int64\n+        if topk_idx.dtype is not torch.int64:\n+            topk_idx = topk_idx.to(torch.int64)\n         expected_m = (\n             hidden_states.shape[0]\n             * self.buffer_low_latency.group_size\ndiff --git a/python/sglang/srt/model_executor/forward_batch_info.py b/python/sglang/srt/model_executor/forward_batch_info.py\nindex 96a13a999..02618920f 100644\n--- a/python/sglang/srt/model_executor/forward_batch_info.py\n+++ b/python/sglang/srt/model_executor/forward_batch_info.py\n@@ -280,7 +280,8 @@ class ForwardBatch:\n             ).to(device, non_blocking=True)\n \n             sum_len = sum(batch.global_num_tokens)\n-            ret.gathered_buffer = torch.zeros(\n+            # Preallocate without zero-initialization; downstream ops fully write this buffer\n+            ret.gathered_buffer = torch.empty(\n                 (sum_len, model_runner.model_config.hidden_size),\n                 dtype=model_runner.dtype,\n                 device=device,\ndiff --git a/python/sglang/srt/models/deepseek_v2.py b/python/sglang/srt/models/deepseek_v2.py\nindex d973f1b88..38da8169b 100644\n--- a/python/sglang/srt/models/deepseek_v2.py\n+++ b/python/sglang/srt/models/deepseek_v2.py\n@@ -298,9 +298,8 @@ class DeepseekV2MoE(nn.Module):\n         self, hidden_states: torch.Tensor, forward_mode: ForwardMode\n     ) -> torch.Tensor:\n         shared_output = None\n-        topk_idx = torch.full(\n-            (0, self.top_k), -1, dtype=torch.int, device=hidden_states.device\n-        )\n+        # For empty batches, avoid filling memory with a constant; an empty tensor suffices\n+        topk_idx = torch.empty((0, self.top_k), dtype=torch.int, device=hidden_states.device)\n         topk_weights = torch.empty(\n             (0, self.top_k), dtype=torch.float32, device=hidden_states.device\n         )\n", "model_name_or_path": "gpt-5-2025-08-07"}
