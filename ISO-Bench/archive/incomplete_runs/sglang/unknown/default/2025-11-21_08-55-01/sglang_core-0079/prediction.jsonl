{"instance_id": "/home/ubuntu/OmniPerf-Bench/sglan__repo-2fce449", "model_patch": "diff --git a/python/sglang/srt/mem_cache/memory_pool.py b/python/sglang/srt/mem_cache/memory_pool.py\nindex 181ac7eef..763689ad0 100644\n--- a/python/sglang/srt/mem_cache/memory_pool.py\n+++ b/python/sglang/srt/mem_cache/memory_pool.py\n@@ -129,8 +129,11 @@ class BaseTokenToKVPool:\n         return select_index.to(self.device, non_blocking=True)\n \n     def free(self, free_index: torch.Tensor):\n+        # Move to CPU only if needed; reduce redundant host copies\n+        if free_index.device.type != \"cpu\":\n+            free_index = free_index.cpu()\n         if self.is_not_in_free_group:\n-            self.free_slots = torch.concat((self.free_slots, free_index.cpu()))\n+            self.free_slots = torch.concat((self.free_slots, free_index))\n         else:\n             self.free_group.append(free_index)\n \n@@ -141,11 +144,20 @@ class BaseTokenToKVPool:\n     def free_group_end(self):\n         self.is_not_in_free_group = True\n         if self.free_group:\n-            self.free(torch.concat(self.free_group))\n+            # Merge once and free; indices may already be on CPU\n+            merged = torch.concat(self.free_group)\n+            self.free(merged)\n \n     def clear(self):\n         # The padded slot 0 is used for writing dummy outputs from padded tokens.\n-        self.free_slots = torch.arange(1, self.size + 1, dtype=torch.int32)\n+        free = torch.arange(1, self.size + 1, dtype=torch.int32)\n+        # Pin host memory to accelerate non_blocking device transfers when using CUDA\n+        if isinstance(self.device, str) and self.device.startswith(\"cuda\"):\n+            try:\n+                free = free.pin_memory()\n+            except Exception:\n+                pass\n+        self.free_slots = free\n         self.is_in_free_group = False\n         self.free_group = []\n \n@@ -221,21 +233,22 @@ class MHATokenToKVPool(BaseTokenToKVPool):\n         cache_v: torch.Tensor,\n     ):\n         layer_id = layer.layer_id\n-        copy_two_array(\n-            loc,\n-            self.k_buffer[layer_id],\n-            cache_k,\n-            self.v_buffer[layer_id],\n-            cache_v,\n-            self.dtype,\n-            self.store_dtype,\n-        )\n+        # Avoid compiled helper overhead; conditionally cast then assign\n+        if cache_k.dtype != self.dtype:\n+            cache_k = cache_k.to(self.dtype)\n+            cache_v = cache_v.to(self.dtype)\n+        if self.store_dtype != self.dtype:\n+            self.k_buffer[layer_id][loc] = cache_k.view(self.store_dtype)\n+            self.v_buffer[layer_id][loc] = cache_v.view(self.store_dtype)\n+        else:\n+            self.k_buffer[layer_id][loc] = cache_k\n+            self.v_buffer[layer_id][loc] = cache_v\n \n \n-@torch.compile(dynamic=True)\n-def copy_two_array(loc, dst_1, src_1, dst_2, src_2, dtype, store_dtype):\n-    dst_1[loc] = src_1.to(dtype).view(store_dtype)\n-    dst_2[loc] = src_2.to(dtype).view(store_dtype)\n+# Note: A previously used compiled helper was removed since the\n+# compilation overhead and dependency on system headers could slow\n+# things down or fail in minimal environments. Direct assignment in\n+# set_kv_buffer with light dtype handling is preferred.\n \n \n class MLATokenToKVPool(BaseTokenToKVPool):\n", "model_name_or_path": "gpt-5-2025-08-07"}
