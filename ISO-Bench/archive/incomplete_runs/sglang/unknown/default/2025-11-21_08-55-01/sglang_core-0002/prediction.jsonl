{"instance_id": "/home/ubuntu/OmniPerf-Bench/sglan__repo-33b242d", "model_patch": "diff --git a/python/sglang/srt/layers/logits_processor.py b/python/sglang/srt/layers/logits_processor.py\nindex f95c30786..0981192f9 100644\n--- a/python/sglang/srt/layers/logits_processor.py\n+++ b/python/sglang/srt/layers/logits_processor.py\n@@ -49,9 +49,13 @@ class LogitsProcessor(nn.Module):\n         else:\n             prefill_top_logprobs, decode_top_logprobs = [], []\n             pt = 0\n-            # NOTE: the GPU-CPU overhead can be reduced\n-            extend_seq_lens_cpu = input_metadata.extend_seq_lens\n-            for i in range(len(input_metadata.extend_seq_lens)):\n+            # Reduce per-iteration device transfers by converting once to CPU list\n+            extend_seq_lens_cpu = (\n+                input_metadata.extend_seq_lens.cpu().tolist()\n+                if isinstance(input_metadata.extend_seq_lens, torch.Tensor)\n+                else input_metadata.extend_seq_lens\n+            )\n+            for i in range(len(extend_seq_lens_cpu)):\n                 if extend_seq_lens_cpu[i] == 0:\n                     continue\n                 k = input_metadata.top_logprobs_nums[i]\n@@ -98,7 +102,10 @@ class LogitsProcessor(nn.Module):\n                     all_logits = tensor_model_parallel_all_gather(all_logits)\n                 all_logits = all_logits[:, : self.config.vocab_size]\n \n-            all_logprobs = torch.log(torch.softmax(all_logits.float(), dim=-1) + 1e-6)\n+            # Use in-place log_softmax on a reused buffer to reduce allocations\n+            all_logprobs = all_logits.float()\n+            all_logits = None  # release reference early\n+            all_logprobs[:] = torch.nn.functional.log_softmax(all_logprobs, dim=-1)\n \n             prefill_top_logprobs, decode_top_logprobs = self._get_top_logprobs(\n                 all_logprobs, input_metadata\n@@ -119,10 +126,12 @@ class LogitsProcessor(nn.Module):\n \n                 # Compute the logprobs and normalized logprobs for the prefill tokens.\n                 # Note that we pad a zero at the end of each sequence for easy computation.\n-                prefill_token_logprobs = all_logprobs[\n-                    torch.arange(all_logprobs.shape[0], device=\"cuda\"),\n-                    torch.cat([input_ids[1:], torch.tensor([0], device=\"cuda\")]),\n-                ]\n+                # Build indices efficiently without redundant tensor creations\n+                row_ids = torch.arange(\n+                    all_logprobs.shape[0], device=input_ids.device\n+                )\n+                next_ids = torch.cat([input_ids[1:], input_ids.new_zeros((1,) )])\n+                prefill_token_logprobs = all_logprobs[row_ids, next_ids]\n \n                 normalized_prompt_logprobs = self._get_normalized_prompt_logprobs(\n                     prefill_token_logprobs, input_metadata\ndiff --git a/python/sglang/srt/managers/router/model_rpc.py b/python/sglang/srt/managers/router/model_rpc.py\nindex f283635c3..86e7eb277 100644\n--- a/python/sglang/srt/managers/router/model_rpc.py\n+++ b/python/sglang/srt/managers/router/model_rpc.py\n@@ -589,7 +589,8 @@ class ModelRpcServer:\n                     + len(req.output_ids)\n                     - req.prompt_tokens,\n                     \"completion_tokens_wo_jump_forward\": req.completion_tokens_wo_jump_forward,\n-                    \"finish_reason\": req.finish_reason,\n+                    # Ensure a cheap, JSON-serializable finish reason\n+                    \"finish_reason\": str(req.finish_reason),\n                     \"hit_stop_str\": req.hit_stop_str,\n                 }\n                 if req.return_logprob:\n", "model_name_or_path": "gpt-5-2025-08-07"}
