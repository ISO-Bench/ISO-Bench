diff --git a/benchmark/latency_throughput/bench_one.py b/benchmark/latency_throughput/bench_one.py
index b912d3a02..7bc1eca5d 100644
--- a/benchmark/latency_throughput/bench_one.py
+++ b/benchmark/latency_throughput/bench_one.py
@@ -17,7 +17,13 @@ def run_one_batch_size(bs):
 
     if args.input_len:
         input_ids = [
-            [int(x) for x in np.random.randint(0, high=16384, size=(args.input_len,))] for _ in range(bs)
+            [
+                int(x)
+                for x in np.random.randint(
+                    0, high=16384, size=(args.input_len,)
+                )
+            ]
+            for _ in range(bs)
         ]
     else:
         text = [f"{i, }" for i in range(bs)]
@@ -116,9 +122,11 @@ if __name__ == "__main__":
     parser.add_argument("--port", type=int, default=None)
     parser.add_argument("--backend", type=str, default="srt")
     parser.add_argument("--input-len", type=int, default=None)
-    parser.add_argument("--batch-size", type=int, nargs='*', default=[1])
+    parser.add_argument("--batch-size", type=int, nargs="*", default=[1])
     parser.add_argument("--max-tokens", type=int, default=256)
-    parser.add_argument("--vllm-model-name", type=str, default="meta-llama/Meta-Llama-3-70B")
+    parser.add_argument(
+        "--vllm-model-name", type=str, default="meta-llama/Meta-Llama-3-70B"
+    )
     args = parser.parse_args()
 
     if args.port is None:
diff --git a/python/sglang/srt/managers/controller/cuda_graph_runner.py b/python/sglang/srt/managers/controller/cuda_graph_runner.py
index 2e37e55b5..648462d69 100644
--- a/python/sglang/srt/managers/controller/cuda_graph_runner.py
+++ b/python/sglang/srt/managers/controller/cuda_graph_runner.py
@@ -23,6 +23,7 @@ class CudaGraphRunner:
 
         # Common inputs
         self.max_bs = max_batch_size_to_capture
+        # Use valid default values for warmup during graph capture
         self.input_ids = torch.zeros((self.max_bs,), dtype=torch.int32, device="cuda")
         self.req_pool_indices = torch.zeros((self.max_bs,), dtype=torch.int32, device="cuda")
         self.seq_lens = torch.ones((self.max_bs,), dtype=torch.int32, device="cuda")
@@ -31,6 +32,7 @@ class CudaGraphRunner:
 
         # FlashInfer inputs
         self.flashinfer_workspace_buffer = self.model_runner.flashinfer_workspace_buffers[0]
+        # Keep these zero-initialized to ensure safe values during capture warmup
         self.flashinfer_kv_indptr = torch.zeros(
             (self.max_bs + 1,), dtype=torch.int32, device="cuda"
         )
@@ -132,9 +134,10 @@ class CudaGraphRunner:
         index = bisect.bisect_left(self.batch_size_list, raw_bs)
         bs = self.batch_size_list[index]
         if bs != raw_bs:
-            self.seq_lens.zero_()
-            self.position_ids_offsets.fill_(1)
-            self.out_cache_loc.zero_()
+            # Only initialize the padded region to reduce unnecessary writes
+            self.seq_lens[raw_bs:bs].zero_()
+            self.position_ids_offsets[raw_bs:bs].fill_(1)
+            self.out_cache_loc[raw_bs:bs].zero_()
 
         # Common inputs
         self.input_ids[:raw_bs] = batch.input_ids
diff --git a/python/sglang/srt/managers/controller/infer_batch.py b/python/sglang/srt/managers/controller/infer_batch.py
index d89e9786e..ade0e7b05 100644
--- a/python/sglang/srt/managers/controller/infer_batch.py
+++ b/python/sglang/srt/managers/controller/infer_batch.py
@@ -781,8 +781,11 @@ class InputMetadata:
                 device="cuda",
             )
             extend_seq_lens = seq_lens - prefix_lens
-            extend_start_loc = torch.zeros_like(seq_lens)
-            extend_start_loc[1:] = torch.cumsum(extend_seq_lens[:-1], dim=0)
+            extend_start_loc = torch.empty_like(seq_lens)
+            if len(seq_lens) > 0:
+                extend_start_loc[0] = 0
+            if len(seq_lens) > 1:
+                extend_start_loc[1:] = torch.cumsum(extend_seq_lens[:-1], dim=0)
             extend_no_prefix = torch.all(prefix_lens == 0)
             total_num_tokens = int(torch.sum(seq_lens))
 
@@ -827,9 +830,9 @@ def init_flashinfer_args(forward_mode, model_runner, req_pool_indices, seq_lens,
     else:
         paged_kernel_lens = prefix_lens
 
-    kv_indptr = torch.zeros(
-        (batch_size + 1,), dtype=torch.int32, device="cuda"
-    )
+    # Avoid zero-initialization; set head explicitly then cumsum
+    kv_indptr = torch.empty((batch_size + 1,), dtype=torch.int32, device="cuda")
+    kv_indptr[0] = 0
     kv_indptr[1:] = torch.cumsum(paged_kernel_lens, dim=0)
     req_pool_indices_cpu = req_pool_indices.cpu().numpy()
     paged_kernel_lens_cpu = paged_kernel_lens.cpu().numpy()
@@ -859,9 +862,8 @@ def init_flashinfer_args(forward_mode, model_runner, req_pool_indices, seq_lens,
         )
     else:
         # extend part
-        qo_indptr = torch.zeros(
-            (batch_size + 1,), dtype=torch.int32, device="cuda"
-        )
+        qo_indptr = torch.empty((batch_size + 1,), dtype=torch.int32, device="cuda")
+        qo_indptr[0] = 0
         qo_indptr[1:] = torch.cumsum(seq_lens - prefix_lens, dim=0)
 
         model_runner.flashinfer_prefill_wrapper_ragged.end_forward()
@@ -890,8 +892,11 @@ def init_flashinfer_args(forward_mode, model_runner, req_pool_indices, seq_lens,
 def init_triton_args(forward_mode, seq_lens, prefix_lens):
     batch_size = len(seq_lens)
     max_seq_len = int(torch.max(seq_lens))
-    start_loc = torch.zeros((batch_size,), dtype=torch.int32, device="cuda")
-    start_loc[1:] = torch.cumsum(seq_lens[:-1], dim=0)
+    start_loc = torch.empty((batch_size,), dtype=torch.int32, device="cuda")
+    if batch_size > 0:
+        start_loc[0] = 0
+    if batch_size > 1:
+        start_loc[1:] = torch.cumsum(seq_lens[:-1], dim=0)
 
     if forward_mode == ForwardMode.DECODE:
         max_extend_len = None
diff --git a/python/sglang/srt/utils.py b/python/sglang/srt/utils.py
index 03a2d60ab..13a3ab4c8 100644
--- a/python/sglang/srt/utils.py
+++ b/python/sglang/srt/utils.py
@@ -610,7 +610,8 @@ def receive_addrs(model_port_args, server_args):
     )
 
     for src_rank in range(1, server_args.nnodes):
-        tensor = torch.zeros(4 + num_tp_ports, dtype=torch.int)
+        # Allocate without zero fill; recv will overwrite all elements
+        tensor = torch.empty(4 + num_tp_ports, dtype=torch.int)
         dist.recv(tensor, src=src_rank)
         ip = ".".join([str(x) for x in tensor[:4].tolist()])
         ports = tensor[4:].tolist()
