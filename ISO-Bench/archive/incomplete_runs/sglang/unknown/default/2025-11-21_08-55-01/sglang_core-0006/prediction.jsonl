{"instance_id": "/home/ubuntu/OmniPerf-Bench/sglan__repo-ceba0ce", "model_patch": "diff --git a/python/sglang/srt/disaggregation/mooncake/memory_pool.py b/python/sglang/srt/disaggregation/mooncake/memory_pool.py\nindex 6e8edaf92..6dfaeb019 100644\n--- a/python/sglang/srt/disaggregation/mooncake/memory_pool.py\n+++ b/python/sglang/srt/disaggregation/mooncake/memory_pool.py\n@@ -11,30 +11,52 @@ from torch.cuda.memory import CUDAPluggableAllocator\n class MooncakeNVLinkAllocator:\n     _instances: Dict[torch.device, CUDAPluggableAllocator] = {}\n     _lock: Final = threading.Lock()\n+    _so_path_cache: Optional[str] = None\n \n     @classmethod\n     def _get_so_path(cls) -> str:\n-        \"\"\"Dynamically locate hook.so in the mooncake package installation\"\"\"\n+        \"\"\"Locate mooncake's hook.so efficiently with caching and env override.\"\"\"\n+        # Cached result\n+        if cls._so_path_cache is not None:\n+            return cls._so_path_cache\n+\n+        # Env override for fast-path\n+        env_path = os.environ.get(\"SGLANG_MOONCAKE_HOOK_SO\")\n+        if env_path and os.path.exists(env_path):\n+            cls._so_path_cache = env_path\n+            return env_path\n+\n+        # Try importlib.resources (modern API if available)\n         try:\n-            # Attempt to locate package resource\n-            with resources.path(\"mooncake\", \"hook.so\") as so_path:\n-                if so_path.exists():\n-                    return str(so_path)\n-        except (ImportError, FileNotFoundError, TypeError):\n+            files = getattr(resources, \"files\", None)\n+            if files is not None:\n+                so_path = files(\"mooncake\").joinpath(\"hook.so\")\n+                if so_path.is_file():\n+                    cls._so_path_cache = str(so_path)\n+                    return cls._so_path_cache\n+            else:\n+                with resources.path(\"mooncake\", \"hook.so\") as so:\n+                    if so.exists():\n+                        cls._so_path_cache = str(so)\n+                        return cls._so_path_cache\n+        except (ImportError, FileNotFoundError, TypeError, AttributeError):\n             pass\n \n-        # Fallback strategy: check in package location via import metadata\n+        # Fallback: derive from installed package location\n         try:\n-            import mooncake\n+            import mooncake  # type: ignore\n \n             base_path = os.path.dirname(os.path.abspath(mooncake.__file__))\n             so_path = os.path.join(base_path, \"hook.so\")\n             if os.path.exists(so_path):\n+                cls._so_path_cache = so_path\n                 return so_path\n         except (ImportError, FileNotFoundError, TypeError):\n-            raise ImportError(\n-                \"SGLANG_MOONCAKE_CUSTOM_MEM_POOL require mooncake-transfer-engine >= 0.3.3.post2.\"\n-            )\n+            pass\n+\n+        raise ImportError(\n+            \"SGLANG_MOONCAKE_CUSTOM_MEM_POOL requires mooncake-transfer-engine >= 0.3.3.post2.\"\n+        )\n \n     @classmethod\n     def get_allocator(cls, device: torch.device) -> CUDAPluggableAllocator:\ndiff --git a/python/sglang/srt/mem_cache/memory_pool.py b/python/sglang/srt/mem_cache/memory_pool.py\nindex c01807f1b..9d5cc4be3 100644\n--- a/python/sglang/srt/mem_cache/memory_pool.py\n+++ b/python/sglang/srt/mem_cache/memory_pool.py\n@@ -302,22 +302,17 @@ class MHATokenToKVPool(KVCache):\n             ):\n                 # [size, head_num, head_dim] for each layer\n                 # The padded slot 0 is used for writing dummy outputs from padded tokens.\n-                self.k_buffer = [\n-                    torch.zeros(\n-                        (self.size + self.page_size, self.head_num, self.head_dim),\n-                        dtype=self.store_dtype,\n-                        device=self.device,\n-                    )\n-                    for _ in range(self.layer_num)\n-                ]\n-                self.v_buffer = [\n-                    torch.zeros(\n-                        (self.size + self.page_size, self.head_num, self.head_dim),\n-                        dtype=self.store_dtype,\n-                        device=self.device,\n-                    )\n-                    for _ in range(self.layer_num)\n-                ]\n+                shape = (self.size + self.page_size, self.head_num, self.head_dim)\n+                self.k_buffer = []\n+                self.v_buffer = []\n+                for _ in range(self.layer_num):\n+                    k = torch.empty(shape, dtype=self.store_dtype, device=self.device)\n+                    v = torch.empty(shape, dtype=self.store_dtype, device=self.device)\n+                    # Keep semantics for padded slot 0 only\n+                    k[0].zero_()\n+                    v[0].zero_()\n+                    self.k_buffer.append(k)\n+                    self.v_buffer.append(v)\n \n         self.data_ptrs = torch.tensor(\n             [x.data_ptr() for x in self.k_buffer + self.v_buffer],\n@@ -619,14 +614,12 @@ class MLATokenToKVPool(KVCache):\n                 else nullcontext()\n             ):\n                 # The padded slot 0 is used for writing dummy outputs from padded tokens.\n-                self.kv_buffer = [\n-                    torch.zeros(\n-                        (size + page_size, 1, kv_lora_rank + qk_rope_head_dim),\n-                        dtype=self.store_dtype,\n-                        device=device,\n-                    )\n-                    for _ in range(layer_num)\n-                ]\n+                shape = (size + page_size, 1, kv_lora_rank + qk_rope_head_dim)\n+                self.kv_buffer = []\n+                for _ in range(layer_num):\n+                    buf = torch.empty(shape, dtype=self.store_dtype, device=device)\n+                    buf[0].zero_()\n+                    self.kv_buffer.append(buf)\n \n         self.layer_transfer_counter = None\n \n@@ -757,26 +750,23 @@ class DoubleSparseTokenToKVPool(KVCache):\n \n         with self.memory_saver_adapter.region(GPU_MEMORY_TYPE_KV_CACHE):\n             # [size, head_num, head_dim] for each layer\n-            self.k_buffer = [\n-                torch.zeros(\n-                    (size + page_size, head_num, head_dim), dtype=dtype, device=device\n-                )\n-                for _ in range(layer_num)\n-            ]\n-            self.v_buffer = [\n-                torch.zeros(\n-                    (size + page_size, head_num, head_dim), dtype=dtype, device=device\n-                )\n-                for _ in range(layer_num)\n-            ]\n+            shape_kv = (size + page_size, head_num, head_dim)\n+            self.k_buffer, self.v_buffer = [], []\n+            for _ in range(layer_num):\n+                kb = torch.empty(shape_kv, dtype=dtype, device=device)\n+                vb = torch.empty(shape_kv, dtype=dtype, device=device)\n+                kb[0].zero_()\n+                vb[0].zero_()\n+                self.k_buffer.append(kb)\n+                self.v_buffer.append(vb)\n \n             # [size, head_num, heavy_channel_num] for each layer\n-            self.label_buffer = [\n-                torch.zeros(\n-                    (size + 1, head_num, heavy_channel_num), dtype=dtype, device=device\n-                )\n-                for _ in range(layer_num)\n-            ]\n+            shape_lbl = (size + 1, head_num, heavy_channel_num)\n+            self.label_buffer = []\n+            for _ in range(layer_num):\n+                lb = torch.empty(shape_lbl, dtype=dtype, device=device)\n+                lb[0].zero_()\n+                self.label_buffer.append(lb)\n \n     def get_key_buffer(self, layer_id: int):\n         return self.k_buffer[layer_id - self.start_layer]\ndiff --git a/scripts/ci_install_dependency.sh b/scripts/ci_install_dependency.sh\nindex 922c886c4..85b497b6d 100755\n--- a/scripts/ci_install_dependency.sh\n+++ b/scripts/ci_install_dependency.sh\n@@ -23,7 +23,7 @@ pip install -e \"python[dev]\"\n pip list\n \n # Install additional dependencies\n-pip install mooncake-transfer-engine==0.3.2.post1 nvidia-cuda-nvrtc-cu12\n+pip install mooncake-transfer-engine==0.3.3.post2 nvidia-cuda-nvrtc-cu12\n \n # For lmms_evals evaluating MMMU\n git clone --branch v0.3.3 --depth 1 https://github.com/EvolvingLMMs-Lab/lmms-eval.git\n", "model_name_or_path": "gpt-5-2025-08-07"}
