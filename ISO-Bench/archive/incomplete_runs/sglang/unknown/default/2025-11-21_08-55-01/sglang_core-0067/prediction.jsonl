{"instance_id": "/home/ubuntu/OmniPerf-Bench/sglan__repo-10bfce7", "model_patch": "diff --git a/python/sglang/srt/layers/radix_attention.py b/python/sglang/srt/layers/radix_attention.py\nindex a449d7188..7592688b2 100644\n--- a/python/sglang/srt/layers/radix_attention.py\n+++ b/python/sglang/srt/layers/radix_attention.py\n@@ -47,8 +47,9 @@ class RadixAttention(nn.Module):\n         self.logit_cap = logit_cap\n         self.sliding_window_size = sliding_window_size or -1\n         self.is_cross_attention = is_cross_attention\n-        self.k_scale = 1.0\n-        self.v_scale = 1.0\n+        # None indicates no explicit scaling (fast path)\n+        self.k_scale = None\n+        self.v_scale = None\n \n     def forward(\n         self,\ndiff --git a/python/sglang/srt/mem_cache/memory_pool.py b/python/sglang/srt/mem_cache/memory_pool.py\nindex e30736722..ce89437cb 100644\n--- a/python/sglang/srt/mem_cache/memory_pool.py\n+++ b/python/sglang/srt/mem_cache/memory_pool.py\n@@ -27,7 +27,7 @@ import logging\n import threading\n from enum import IntEnum\n from functools import wraps\n-from typing import List, Tuple, Union\n+from typing import List, Optional, Tuple, Union\n \n import numpy as np\n import psutil\n@@ -270,13 +270,21 @@ class MHATokenToKVPool(BaseTokenToKVPool):\n         loc: torch.Tensor,\n         cache_k: torch.Tensor,\n         cache_v: torch.Tensor,\n-        k_scale: float = 1.0,\n-        v_scale: float = 1.0,\n+        k_scale: Optional[float] = None,\n+        v_scale: Optional[float] = None,\n     ):\n         layer_id = layer.layer_id\n         if cache_k.dtype != self.dtype:\n-            cache_k = (cache_k / k_scale).to(self.dtype)\n-            cache_v = (cache_v / v_scale).to(self.dtype)\n+            # Avoid unnecessary division when no scaling is provided or scale == 1.0\n+            if k_scale is not None and k_scale != 1.0:\n+                cache_k = (cache_k / k_scale).to(self.dtype)\n+            else:\n+                cache_k = cache_k.to(self.dtype)\n+\n+            if v_scale is not None and v_scale != 1.0:\n+                cache_v = (cache_v / v_scale).to(self.dtype)\n+            else:\n+                cache_v = cache_v.to(self.dtype)\n         if self.store_dtype != self.dtype:\n             self.k_buffer[layer_id][loc] = cache_k.view(self.store_dtype)\n             self.v_buffer[layer_id][loc] = cache_v.view(self.store_dtype)\n", "model_name_or_path": "gpt-5-2025-08-07"}
