diff --git a/python/sglang/bench_latency.py b/python/sglang/bench_latency.py
index 7a03e162f..ac6b1fb6f 100644
--- a/python/sglang/bench_latency.py
+++ b/python/sglang/bench_latency.py
@@ -260,7 +260,7 @@ def correctness_test(
 
     # Decode
     output_ids = [input_ids[i] + [next_token_ids[i]] for i in range(len(input_ids))]
-    for _ in range(bench_args.output_len[0]):
+    for _ in range(bench_args.output_len[0] - 1):
         next_token_ids, _ = decode(next_token_ids, batch, model_runner)
         for i in range(len(reqs)):
             output_ids[i].append(next_token_ids[i])
@@ -311,7 +311,7 @@ def latency_test_run_once(
 
     # Decode
     decode_latencies = []
-    for i in range(output_len):
+    for i in range(output_len - 1):
         torch.cuda.synchronize()
         tic = time.time()
         next_token_ids, _ = decode(next_token_ids, batch, model_runner)
diff --git a/python/sglang/srt/managers/schedule_batch.py b/python/sglang/srt/managers/schedule_batch.py
index 2ab041726..de95808c0 100644
--- a/python/sglang/srt/managers/schedule_batch.py
+++ b/python/sglang/srt/managers/schedule_batch.py
@@ -471,7 +471,8 @@ class ScheduleBatch:
             self.input_ids = torch.tensor(sum(input_ids, []), dtype=torch.int32)
             self.req_pool_indices = torch.tensor(req_pool_indices_cpu)
             self.seq_lens = torch.tensor(seq_lens, dtype=torch.int32)
-            self.position_ids_offsets = torch.zeros((bs,), dtype=torch.int64)
+            # Initialize on-device offsets without extra kernel work
+            self.position_ids_offsets = torch.zeros_like(self.seq_lens, dtype=torch.int64)
 
         self.extend_num_tokens = extend_num_tokens
         self.out_cache_loc = out_cache_loc
@@ -681,7 +682,12 @@ class ScheduleBatch:
                 for r in self.reqs
             ]
         else:
-            self.sampling_info.penalizer_orchestrator.cumulate_input_tokens(input_ids)
+            # Avoid unnecessary overhead if no penalizer is active
+            orchestrator = self.sampling_info.penalizer_orchestrator
+            if orchestrator is not None:
+                penalizers = getattr(orchestrator, "penalizers", {})
+                if any(p.is_prepared() for p in penalizers.values()):
+                    orchestrator.cumulate_input_tokens(input_ids)
 
         self.input_ids = torch.tensor(input_ids, dtype=torch.int32, device="cuda")
         self.seq_lens.add_(1)
diff --git a/python/sglang/srt/model_executor/forward_batch_info.py b/python/sglang/srt/model_executor/forward_batch_info.py
index 4815fbc56..c429ad4af 100644
--- a/python/sglang/srt/model_executor/forward_batch_info.py
+++ b/python/sglang/srt/model_executor/forward_batch_info.py
@@ -100,39 +100,20 @@ class InputMetadata:
         position_ids_offsets = batch.position_ids_offsets
 
         if self.forward_mode.is_decode():
-            if True:
-                self.positions = self.seq_lens - 1
-            else:
-                # Deprecated
-                self.positions = (self.seq_lens - 1) + position_ids_offsets
+            # Decode positions are simply seq_lens - 1
+            self.positions = self.seq_lens - 1
         else:
-            if True:
-                self.positions = torch.tensor(
-                    np.concatenate(
-                        [
-                            np.arange(batch.prefix_lens_cpu[i], len(req.fill_ids))
-                            for i, req in enumerate(batch.reqs)
-                        ],
-                        axis=0,
-                    ),
-                    device="cuda",
-                )
-            else:
-                # Deprecated
-                position_ids_offsets_cpu = position_ids_offsets.cpu().numpy()
-                self.positions = torch.tensor(
-                    np.concatenate(
-                        [
-                            np.arange(
-                                batch.prefix_lens_cpu[i] + position_ids_offsets_cpu[i],
-                                len(req.fill_ids) + position_ids_offsets_cpu[i],
-                            )
-                            for i, req in enumerate(batch.reqs)
-                        ],
-                        axis=0,
-                    ),
+            # Build positions directly on GPU to avoid NumPy host work
+            ranges = [
+                torch.arange(
+                    start=batch.prefix_lens_cpu[i],
+                    end=len(req.fill_ids),
                     device="cuda",
+                    dtype=torch.int64,
                 )
+                for i, req in enumerate(batch.reqs)
+            ]
+            self.positions = torch.cat(ranges, dim=0) if ranges else torch.empty(0, dtype=torch.int64, device="cuda")
 
         # Positions should be in long type
         self.positions = self.positions.to(torch.int64)
diff --git a/python/sglang/srt/model_executor/model_runner.py b/python/sglang/srt/model_executor/model_runner.py
index 049a43840..8900b073a 100644
--- a/python/sglang/srt/model_executor/model_runner.py
+++ b/python/sglang/srt/model_executor/model_runner.py
@@ -552,15 +552,26 @@ class ModelRunner:
     def sample(
         self, logits_output: LogitsProcessorOutput, batch: ScheduleBatch
     ) -> torch.Tensor:
-        # Put CPU-heavy tasks here. They will be overlapped with the forward pass.
-        batch.sampling_info.update_regex_vocab_mask(batch)
-        batch.sampling_info.update_penalties()
-        logits = self._apply_logits_bias(
-            logits_output.next_token_logits, batch.sampling_info
-        )
+        # Fast-path: skip penalty and regex work when not used
+        sampling_info = batch.sampling_info
+        has_regex = any(req.regex_fsm is not None for req in batch.reqs)
+        orchestrator = getattr(sampling_info, "penalizer_orchestrator", None)
+        penalizers_prepared = False
+        if orchestrator is not None:
+            penalizers_prepared = any(
+                p.is_prepared() for p in orchestrator.penalizers.values()
+            )
+
+        if not has_regex and not penalizers_prepared and sampling_info.logit_bias is None:
+            logits = logits_output.next_token_logits
+        else:
+            # Put CPU-heavy tasks here. They will be overlapped with the forward pass.
+            sampling_info.update_regex_vocab_mask(batch)
+            sampling_info.update_penalties()
+            logits = self._apply_logits_bias(logits_output.next_token_logits, sampling_info)
 
         # Sample the next tokens.
-        next_token_ids = self.sampler(logits, batch.sampling_info)
+        next_token_ids = self.sampler(logits, sampling_info)
         return next_token_ids
 
 
diff --git a/scripts/playground/reference_hf.py b/scripts/playground/reference_hf.py
index 1eb7b0dd2..ff582bc2c 100644
--- a/scripts/playground/reference_hf.py
+++ b/scripts/playground/reference_hf.py
@@ -81,17 +81,16 @@ def synthetic_tokens(args):
 
     for p in prompts:
         input_ids = p
-        for i in range(output_len + 1):
-            prefill_logits = m.forward(torch.tensor([input_ids], device="cuda")).logits[
-                0
-            ][-1]
-
-            if i == 0:
-                print("prefill logits", prefill_logits)
-            else:
-                print("decode", i - 1, prefill_logits)
-
-            input_ids.append(torch.argmax(prefill_logits).item())
+        # Prefill step
+        prefill_logits = m.forward(torch.tensor([input_ids], device="cuda")).logits[0][-1]
+        print("prefill logits", prefill_logits)
+        input_ids.append(torch.argmax(prefill_logits).item())
+
+        # Decode steps
+        for i in range(output_len - 1):
+            decode_logits = m.forward(torch.tensor([input_ids], device="cuda")).logits[0][-1]
+            print("decode", i, decode_logits)
+            input_ids.append(torch.argmax(decode_logits).item())
 
 
 if __name__ == "__main__":
