{"instance_id": "/home/ubuntu/OmniPerf-Bench/sglan__repo-3d7f7a4", "model_patch": "diff --git a/python/sglang/srt/layers/attention/flashattention_backend.py b/python/sglang/srt/layers/attention/flashattention_backend.py\nindex 2f974ea9a..528797222 100644\n--- a/python/sglang/srt/layers/attention/flashattention_backend.py\n+++ b/python/sglang/srt/layers/attention/flashattention_backend.py\n@@ -327,6 +327,13 @@ class FlashAttentionBackend(AttentionBackend):\n         )\n         self.speculative_step_id = speculative_step_id\n \n+        # Caches for replay-time fast paths to reduce unnecessary work\n+        # - Track the last used number of KV pages per batch size to avoid\n+        #   repeatedly zero-filling large page_table tails when unchanged or growing.\n+        # - Track previous local-attention buffer extents to minimize zeroing.\n+        self._last_max_seq_pages_by_bs = {}\n+        self._local_attn_prev_sizes_by_bs = {}\n+\n         # Local attention settings\n         self.attention_chunk_size = (\n             model_runner.attention_chunk_size\n@@ -1135,8 +1142,12 @@ class FlashAttentionBackend(AttentionBackend):\n             \"cu_seqlens_q\": torch.arange(\n                 0, max_bs + 1, dtype=torch.int32, device=self.device\n             ),\n-            \"cu_seqlens_k\": torch.zeros(\n-                max_bs + 1, dtype=torch.int32, device=self.device\n+            # This buffer is updated in-place during replay (index 1:). It does not\n+            # need an expensive zero-initialization across the whole tensor.\n+            # Keep it uninitialized and only enforce the required convention that\n+            # the first element is zero.\n+            \"cu_seqlens_k\": (lambda t: (t.zero_(), t)[1])(\n+                torch.empty(max_bs + 1, dtype=torch.int32, device=self.device)\n             ),\n             \"page_table\": torch.zeros(\n                 max_bs,\n@@ -1241,7 +1252,9 @@ class FlashAttentionBackend(AttentionBackend):\n             and self.speculative_num_draft_tokens > 0\n         ):\n             self.target_verify_metadata = {\n-                \"cache_seqlens\": torch.zeros(\n+                # Will be overwritten via copy_() before use during capture/replay.\n+                # Avoid redundant zero-initialization here.\n+                \"cache_seqlens\": torch.empty(\n                     max_bs, dtype=torch.int32, device=self.device\n                 ),\n                 \"cu_seqlens_q\": torch.arange(\n@@ -1639,7 +1652,13 @@ class FlashAttentionBackend(AttentionBackend):\n                 ]\n                 page_indices //= self.page_size\n                 metadata.page_table[:, :max_seq_pages].copy_(page_indices)\n-                metadata.page_table[:, max_seq_pages:].fill_(0)\n+                # Zero-fill only when the new required pages shrink compared to the\n+                # previous replay for this batch size. This avoids repeatedly clearing\n+                # large tails when max_seq_pages stays the same or increases.\n+                prev_pages = self._last_max_seq_pages_by_bs.get(bs)\n+                if prev_pages is not None and prev_pages > max_seq_pages:\n+                    metadata.page_table[:, max_seq_pages:prev_pages].zero_()\n+                self._last_max_seq_pages_by_bs[bs] = max_seq_pages\n \n                 self._update_local_attn_metadata_for_replay(metadata, bs)\n         elif forward_mode.is_target_verify():\n@@ -1866,14 +1885,28 @@ class FlashAttentionBackend(AttentionBackend):\n         k_len = seqlens_k_local.shape[0]\n         b0, b1 = block_table_local.shape\n \n-        # In-place updates into preallocated tensors and zero out the unused space\n+        # In-place updates into preallocated tensors. Only clear regions that were\n+        # previously used but are no longer needed in this replay to minimize work.\n+        prev_q_len, prev_k_len, prev_b0, prev_b1 = self._local_attn_prev_sizes_by_bs.get(\n+            bs, (0, 0, 0, 0)\n+        )\n+\n         local_q_buf[:q_len].copy_(cu_seqlens_q_local)\n-        local_q_buf[q_len:].fill_(0)\n+        if prev_q_len > q_len:\n+            local_q_buf[q_len:prev_q_len].zero_()\n+\n         local_k_buf[:k_len].copy_(seqlens_k_local)\n-        local_k_buf[k_len:].fill_(0)\n+        if prev_k_len > k_len:\n+            local_k_buf[k_len:prev_k_len].zero_()\n+\n         local_block_buf[:b0, :b1].copy_(block_table_local)\n-        local_block_buf[b0:, :].fill_(0)\n-        local_block_buf[:b0, b1:].fill_(0)\n+        if prev_b0 > b0:\n+            local_block_buf[b0:prev_b0, :].zero_()\n+        if prev_b1 > b1:\n+            local_block_buf[:b0, b1:prev_b1].zero_()\n+\n+        # Record the latest extents for the next replay.\n+        self._local_attn_prev_sizes_by_bs[bs] = (q_len, k_len, b0, b1)\n \n         if metadata.local_attn_metadata is not None:\n             lam = metadata.local_attn_metadata\n", "model_name_or_path": "gpt-5-2025-08-07"}
