{"instance_id": "/home/ubuntu/OmniPerf-Bench/sglan__repo-6ea1e6a", "model_patch": "diff --git a/python/sglang/srt/layers/attention/flashattention_backend.py b/python/sglang/srt/layers/attention/flashattention_backend.py\nindex 9579b19f2..4ec241a4b 100644\n--- a/python/sglang/srt/layers/attention/flashattention_backend.py\n+++ b/python/sglang/srt/layers/attention/flashattention_backend.py\n@@ -338,7 +338,9 @@ class FlashAttentionBackend(AttentionBackend):\n         \"\"\"Initialize forward metadata hence all layers in the forward pass can reuse it.\"\"\"\n         metadata = FlashAttentionMetadata()\n         seqlens_in_batch = forward_batch.seq_lens\n-        batch_size = len(seqlens_in_batch)\n+        # Avoid len(seqlens_in_batch) which may materialize a Python int from a tensor\n+        # and instead use the batch size already computed by the forward batch\n+        batch_size = forward_batch.batch_size\n         device = seqlens_in_batch.device\n \n         if forward_batch.forward_mode.is_decode_or_idle():\n@@ -354,12 +356,18 @@ class FlashAttentionBackend(AttentionBackend):\n                     metadata.cu_seqlens_q = torch.arange(\n                         0, batch_size + 1, dtype=torch.int32, device=device\n                     )\n-                    metadata.cu_seqlens_k = torch.nn.functional.pad(\n-                        torch.cumsum(\n-                            metadata.cache_seqlens_int32, dim=0, dtype=torch.int32\n-                        ),\n-                        (1, 0),\n+                    # Build cu_seqlens_k using a preallocated buffer to avoid\n+                    # an intermediate allocation from pad(cumsum(...)).\n+                    cu = torch.empty(\n+                        metadata.cache_seqlens_int32.shape[0] + 1,\n+                        dtype=torch.int32,\n+                        device=device,\n+                    )\n+                    cu[0] = 0\n+                    torch.cumsum(\n+                        metadata.cache_seqlens_int32, dim=0, dtype=torch.int32, out=cu[1:]\n                     )\n+                    metadata.cu_seqlens_k = cu\n                     metadata.page_table = forward_batch.req_to_token_pool.req_to_token[\n                         forward_batch.req_pool_indices, : metadata.max_seq_len_k\n                     ]\n@@ -374,12 +382,16 @@ class FlashAttentionBackend(AttentionBackend):\n                         dtype=torch.int32,\n                         device=device,\n                     )\n-                    metadata.cu_seqlens_k = torch.nn.functional.pad(\n-                        torch.cumsum(\n-                            metadata.cache_seqlens_int32, dim=0, dtype=torch.int32\n-                        ),\n-                        (1, 0),\n+                    cu = torch.empty(\n+                        metadata.cache_seqlens_int32.shape[0] + 1,\n+                        dtype=torch.int32,\n+                        device=device,\n+                    )\n+                    cu[0] = 0\n+                    torch.cumsum(\n+                        metadata.cache_seqlens_int32, dim=0, dtype=torch.int32, out=cu[1:]\n                     )\n+                    metadata.cu_seqlens_k = cu\n                     metadata.page_table = forward_batch.req_to_token_pool.req_to_token[\n                         forward_batch.req_pool_indices, : metadata.max_seq_len_k\n                     ]\n@@ -421,9 +433,12 @@ class FlashAttentionBackend(AttentionBackend):\n                 metadata.cu_seqlens_q = torch.arange(\n                     0, batch_size + 1, dtype=torch.int32, device=device\n                 )\n-                metadata.cu_seqlens_k = torch.nn.functional.pad(\n-                    torch.cumsum(seqlens_in_batch, dim=0, dtype=torch.int32), (1, 0)\n+                cu = torch.empty(\n+                    seqlens_in_batch.shape[0] + 1, dtype=torch.int32, device=device\n                 )\n+                cu[0] = 0\n+                torch.cumsum(seqlens_in_batch, dim=0, dtype=torch.int32, out=cu[1:])\n+                metadata.cu_seqlens_k = cu\n                 metadata.page_table = forward_batch.req_to_token_pool.req_to_token[\n                     forward_batch.req_pool_indices, : metadata.max_seq_len_k\n                 ]\n@@ -556,9 +571,12 @@ class FlashAttentionBackend(AttentionBackend):\n         elif forward_batch.forward_mode.is_extend_or_draft_extend_or_mixed():\n             metadata.cache_seqlens_int32 = seqlens_in_batch.to(torch.int32)\n             metadata.max_seq_len_k = forward_batch.seq_lens_cpu.max().item()\n-            metadata.cu_seqlens_k = torch.nn.functional.pad(\n-                torch.cumsum(seqlens_in_batch, dim=0, dtype=torch.int32), (1, 0)\n+            cu = torch.empty(\n+                seqlens_in_batch.shape[0] + 1, dtype=torch.int32, device=device\n             )\n+            cu[0] = 0\n+            torch.cumsum(seqlens_in_batch, dim=0, dtype=torch.int32, out=cu[1:])\n+            metadata.cu_seqlens_k = cu\n             metadata.page_table = forward_batch.req_to_token_pool.req_to_token[\n                 forward_batch.req_pool_indices, : metadata.max_seq_len_k\n             ]\n@@ -569,9 +587,12 @@ class FlashAttentionBackend(AttentionBackend):\n             ):\n                 extend_seq_lens = forward_batch.extend_seq_lens\n                 metadata.max_seq_len_q = max(forward_batch.extend_seq_lens_cpu)\n-                metadata.cu_seqlens_q = torch.nn.functional.pad(\n-                    torch.cumsum(extend_seq_lens, dim=0, dtype=torch.int32), (1, 0)\n+                cuq = torch.empty(\n+                    extend_seq_lens.shape[0] + 1, dtype=torch.int32, device=device\n                 )\n+                cuq[0] = 0\n+                torch.cumsum(extend_seq_lens, dim=0, dtype=torch.int32, out=cuq[1:])\n+                metadata.cu_seqlens_q = cuq\n             else:\n                 metadata.max_seq_len_q = metadata.max_seq_len_k\n                 metadata.cu_seqlens_q = metadata.cu_seqlens_k\n@@ -587,10 +608,16 @@ class FlashAttentionBackend(AttentionBackend):\n             ), \"Only encoder size 1 is supported for now\"\n \n             metadata.encoder_lens_int32 = forward_batch.encoder_lens.to(torch.int32)\n-            metadata.encoder_cu_seqlens_k = torch.nn.functional.pad(\n-                torch.cumsum(metadata.encoder_lens_int32, dim=0, dtype=torch.int32),\n-                (1, 0),\n+            cuk = torch.empty(\n+                metadata.encoder_lens_int32.shape[0] + 1,\n+                dtype=torch.int32,\n+                device=device,\n+            )\n+            cuk[0] = 0\n+            torch.cumsum(\n+                metadata.encoder_lens_int32, dim=0, dtype=torch.int32, out=cuk[1:]\n             )\n+            metadata.encoder_cu_seqlens_k = cuk\n             metadata.encoder_max_seq_len_k = metadata.encoder_lens_int32.max().item()\n             metadata.encoder_page_table = forward_batch.req_to_token_pool.req_to_token[\n                 forward_batch.req_pool_indices, : metadata.encoder_max_seq_len_k\n@@ -1331,12 +1358,16 @@ class FlashAttentionBackend(AttentionBackend):\n                     metadata.cu_seqlens_q = self.decode_cuda_graph_metadata[\n                         \"cu_seqlens_q\"\n                     ][: bs + 1]\n-                    metadata.cu_seqlens_k = torch.nn.functional.pad(\n-                        torch.cumsum(\n-                            metadata.cache_seqlens_int32, dim=0, dtype=torch.int32\n-                        ),\n-                        (1, 0),\n+                    cu = torch.empty(\n+                        metadata.cache_seqlens_int32.shape[0] + 1,\n+                        dtype=torch.int32,\n+                        device=device,\n+                    )\n+                    cu[0] = 0\n+                    torch.cumsum(\n+                        metadata.cache_seqlens_int32, dim=0, dtype=torch.int32, out=cu[1:]\n                     )\n+                    metadata.cu_seqlens_k = cu\n                     metadata.page_table = self.decode_cuda_graph_metadata[\n                         \"page_table_draft_decode\"\n                     ][req_pool_indices, :]\n@@ -1388,11 +1419,12 @@ class FlashAttentionBackend(AttentionBackend):\n                 # Normal Decode\n                 # Get sequence information\n                 metadata.cache_seqlens_int32 = seq_lens.to(torch.int32)\n-                batch_size = len(seq_lens)\n+                batch_size = bs\n                 device = seq_lens.device\n-                metadata.cu_seqlens_k = torch.nn.functional.pad(\n-                    torch.cumsum(seq_lens, dim=0, dtype=torch.int32), (1, 0)\n-                )\n+                cu = torch.empty(batch_size + 1, dtype=torch.int32, device=device)\n+                cu[0] = 0\n+                torch.cumsum(seq_lens[:batch_size], dim=0, dtype=torch.int32, out=cu[1:])\n+                metadata.cu_seqlens_k = cu\n                 # Precompute maximum sequence length\n                 metadata.max_seq_len_k = seq_lens.max().item()\n                 # Precompute page table\n@@ -1616,13 +1648,9 @@ class FlashAttentionBackend(AttentionBackend):\n                 metadata.max_seq_len_k = (\n                     seq_lens_cpu.max().item() + self.speculative_num_draft_tokens\n                 )\n-                metadata.cu_seqlens_k.copy_(\n-                    torch.nn.functional.pad(\n-                        torch.cumsum(\n-                            metadata.cache_seqlens_int32, dim=0, dtype=torch.int32\n-                        ),\n-                        (1, 0),\n-                    )\n+                metadata.cu_seqlens_k[0] = 0\n+                torch.cumsum(\n+                    metadata.cache_seqlens_int32, dim=0, dtype=torch.int32, out=metadata.cu_seqlens_k[1:]\n                 )\n                 max_seq_pages = (\n                     metadata.max_seq_len_k + self.page_size - 1\n@@ -1641,13 +1669,9 @@ class FlashAttentionBackend(AttentionBackend):\n                 # metadata.max_seq_len_q = self.speculative_num_draft_tokens, already set in capture\n                 metadata.max_seq_len_k = seq_lens_cpu.max().item()\n                 # metadata.cu_seqlens_q already set in capture\n-                metadata.cu_seqlens_k.copy_(\n-                    torch.nn.functional.pad(\n-                        torch.cumsum(\n-                            metadata.cache_seqlens_int32, dim=0, dtype=torch.int32\n-                        ),\n-                        (1, 0),\n-                    )\n+                metadata.cu_seqlens_k[0] = 0\n+                torch.cumsum(\n+                    metadata.cache_seqlens_int32, dim=0, dtype=torch.int32, out=metadata.cu_seqlens_k[1:]\n                 )\n                 page_table = self.req_to_token[\n                     req_pool_indices, : metadata.max_seq_len_k\n@@ -1705,15 +1729,12 @@ class FlashAttentionBackend(AttentionBackend):\n                 metadata_expand.cache_seqlens_int32.copy_(\n                     mask.sum(dim=1).to(torch.int32)\n                 )\n-                metadata_expand.cu_seqlens_k.copy_(\n-                    torch.nn.functional.pad(\n-                        torch.cumsum(\n-                            metadata_expand.cache_seqlens_int32,\n-                            dim=0,\n-                            dtype=torch.int32,\n-                        ),\n-                        (1, 0),\n-                    )\n+                metadata_expand.cu_seqlens_k[0] = 0\n+                torch.cumsum(\n+                    metadata_expand.cache_seqlens_int32,\n+                    dim=0,\n+                    dtype=torch.int32,\n+                    out=metadata_expand.cu_seqlens_k[1:],\n                 )\n                 metadata_expand.max_seq_len_k = (\n                     metadata_expand.cache_seqlens_int32.max().item()\n@@ -1723,11 +1744,9 @@ class FlashAttentionBackend(AttentionBackend):\n             # Only support encoder size 1 for now\n             metadata.encoder_max_seq_len_k = encoder_lens[0]\n             metadata.encoder_lens_int32.copy_(encoder_lens[:1])\n-            metadata.encoder_cu_seqlens_k.copy_(\n-                torch.nn.functional.pad(\n-                    torch.cumsum(metadata.encoder_lens_int32, dim=0, dtype=torch.int32),\n-                    (1, 0),\n-                )\n+            metadata.encoder_cu_seqlens_k[0] = 0\n+            torch.cumsum(\n+                metadata.encoder_lens_int32, dim=0, dtype=torch.int32, out=metadata.encoder_cu_seqlens_k[1:]\n             )\n \n             metadata.encoder_page_table[:, : metadata.encoder_max_seq_len_k].copy_(\n@@ -1763,8 +1782,9 @@ class FlashAttentionBackend(AttentionBackend):\n             metadata.local_attn_metadata = None\n             return\n \n-        cu_seqlens_q_np = cu_seqlens_q.cpu().numpy()\n-        seq_lens_np = cache_seqlens_int32.cpu().numpy()\n+        # Prefer non_blocking CPU transfers when possible to reduce sync overhead\n+        cu_seqlens_q_np = cu_seqlens_q.to(\"cpu\", non_blocking=True).numpy()\n+        seq_lens_np = cache_seqlens_int32.to(\"cpu\", non_blocking=True).numpy()\n         (\n             seqlens_q_local_np,\n             cu_seqlens_q_local_np,\n@@ -1780,7 +1800,7 @@ class FlashAttentionBackend(AttentionBackend):\n         local_metadata = FlashAttentionMetadata.LocalAttentionMetadata(\n             local_query_start_loc=torch.from_numpy(cu_seqlens_q_local_np).to(device),\n             local_seqused_k=torch.from_numpy(seqlens_k_local_np).to(device),\n-            local_block_table=block_table_local.to(device),\n+            local_block_table=block_table_local.to(device, non_blocking=True),\n             local_max_query_len=int(seqlens_q_local_np.max()),\n             local_max_seq_len=int(seqlens_k_local_np.max()),\n         )\n", "model_name_or_path": "gpt-5-2025-08-07"}
