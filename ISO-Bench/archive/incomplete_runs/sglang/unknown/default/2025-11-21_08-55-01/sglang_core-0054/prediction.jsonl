{"instance_id": "/home/ubuntu/OmniPerf-Bench/sglan__repo-39d9044", "model_patch": "diff --git a/python/sglang/srt/models/deepseek_v2.py b/python/sglang/srt/models/deepseek_v2.py\nindex 42ae9d293..9ede95a08 100644\n--- a/python/sglang/srt/models/deepseek_v2.py\n+++ b/python/sglang/srt/models/deepseek_v2.py\n@@ -283,12 +283,15 @@ class DeepseekV2MoE(nn.Module):\n         shared_output = self._forward_shared_experts(hidden_states)\n         # router_logits: (num_tokens, n_experts)\n         router_logits = self.gate(hidden_states)\n-        final_hidden_states = (\n-            self.experts(hidden_states=hidden_states, router_logits=router_logits)\n-            * self.routed_scaling_factor\n+        # Compute experts output first, then scale in-place to avoid\n+        # unnecessary intermediate tensor allocations\n+        final_hidden_states = self.experts(\n+            hidden_states=hidden_states, router_logits=router_logits\n         )\n+        final_hidden_states.mul_(self.routed_scaling_factor)\n         if shared_output is not None:\n-            final_hidden_states = final_hidden_states + shared_output\n+            # In-place add to reduce memory traffic\n+            final_hidden_states.add_(shared_output)\n         if self.tp_size > 1:\n             final_hidden_states = tensor_model_parallel_all_reduce(final_hidden_states)\n         return final_hidden_states\n@@ -297,12 +300,9 @@ class DeepseekV2MoE(nn.Module):\n         self, hidden_states: torch.Tensor, forward_mode: ForwardMode\n     ) -> torch.Tensor:\n         shared_output = None\n-        topk_idx = torch.full(\n-            (0, self.top_k), -1, dtype=torch.int, device=hidden_states.device\n-        )\n-        topk_weights = torch.empty(\n-            (0, self.top_k), dtype=torch.float32, device=hidden_states.device\n-        )\n+        # Use new_* factory functions to avoid redundant dtype/device plumbing\n+        topk_idx = hidden_states.new_full((0, self.top_k), -1, dtype=torch.int)\n+        topk_weights = hidden_states.new_empty((0, self.top_k), dtype=torch.float32)\n         if (\n             forward_mode is not None\n             and not forward_mode.is_idle()\n@@ -337,16 +337,15 @@ class DeepseekV2MoE(nn.Module):\n                 topk_weights,\n                 forward_mode=forward_mode,\n             )\n-        final_hidden_states = (\n-            self.experts(\n-                hidden_states=hidden_states,\n-                reorder_topk_ids=reorder_topk_ids,\n-                seg_indptr=seg_indptr,\n-                masked_m=masked_m,\n-                expected_m=expected_m,\n-                forward_mode=forward_mode,\n-            )\n-            * self.routed_scaling_factor\n+        # Compute experts output first without scaling to minimize the\n+        # number of elements multiplied (pre-combine tensor is larger).\n+        final_hidden_states = self.experts(\n+            hidden_states=hidden_states,\n+            reorder_topk_ids=reorder_topk_ids,\n+            seg_indptr=seg_indptr,\n+            masked_m=masked_m,\n+            expected_m=expected_m,\n+            forward_mode=forward_mode,\n         )\n         if self.ep_size > 1:\n             final_hidden_states = self.deepep_dispatcher.combine(\n@@ -355,8 +354,10 @@ class DeepseekV2MoE(nn.Module):\n                 topk_weights,\n                 forward_mode,\n             )\n+        # Apply routed scaling after combine; same result with fewer ops\n+        final_hidden_states.mul_(self.routed_scaling_factor)\n         if shared_output is not None:\n-            final_hidden_states = final_hidden_states + shared_output\n+            final_hidden_states.add_(shared_output)\n \n         return final_hidden_states\n \n", "model_name_or_path": "gpt-5-2025-08-07"}
