{"instance_id": "/home/ubuntu/OmniPerf-Bench/sglan__repo-30643fe", "model_patch": "diff --git a/python/sglang/srt/constrained/__init__.py b/python/sglang/srt/constrained/__init__.py\nindex c47c5c8dd..398becc31 100644\n--- a/python/sglang/srt/constrained/__init__.py\n+++ b/python/sglang/srt/constrained/__init__.py\n@@ -24,31 +24,92 @@ try:\n     from outlines.caching import cache as disk_cache\n     from outlines.caching import disable_cache\n     from outlines.fsm.guide import RegexGuide\n-    from outlines.fsm.regex import FSMInfo, make_byte_level_fsm, make_deterministic_fsm\n+    from outlines.fsm.regex import (\n+        FSMInfo,\n+        make_byte_level_fsm,\n+        make_deterministic_fsm,\n+    )\n     from outlines.models.transformers import TransformerTokenizer\n except ImportError as e:\n-    print(\n-        f'\\nError: {e}. Please install a new version of outlines by `pip install \"outlines>=0.0.44\"`\\n'\n-    )\n-    raise\n+    # Fall back to lightweight dummies so importing modules that reference\n+    # constrained decoding does not hard-require outlines unless actually used.\n+    # A clear error will be raised upon invocation of these dummies.\n+    def _missing_outlines(*args, **kwargs):  # pragma: no cover\n+        raise ImportError(\n+            f\"{e}. Please install a new version of outlines by `pip install \\\"outlines>=0.0.44\\\"`\"\n+        )\n+\n+    class _DummyRegexGuide:\n+        def __init__(self, *args, **kwargs):  # pragma: no cover\n+            _missing_outlines()\n+\n+    class _DummyFSMInfo:\n+        pass\n+\n+    def _dummy_make_byte_level_fsm(*args, **kwargs):  # pragma: no cover\n+        _missing_outlines()\n+\n+    def _dummy_make_deterministic_fsm(*args, **kwargs):  # pragma: no cover\n+        _missing_outlines()\n+\n+    class _DummyTransformerTokenizer:  # pragma: no cover\n+        def __init__(self, *args, **kwargs):\n+            _missing_outlines()\n+\n+    def _dummy_cache(*args, **kwargs):  # pragma: no cover\n+        _missing_outlines()\n+\n+    def _dummy_disable_cache(*args, **kwargs):  # pragma: no cover\n+        _missing_outlines()\n+\n+    disk_cache = _dummy_cache\n+    disable_cache = _dummy_disable_cache\n+    RegexGuide = _DummyRegexGuide\n+    FSMInfo = _DummyFSMInfo\n+    make_byte_level_fsm = _dummy_make_byte_level_fsm\n+    make_deterministic_fsm = _dummy_make_deterministic_fsm\n+    TransformerTokenizer = _DummyTransformerTokenizer\n \n try:\n     from outlines.fsm.json_schema import build_regex_from_object\n except ImportError:\n-    # Since outlines 0.0.32, build_regex_from_object is replaced by build_regex_from_schema,\n-    # which only accepts string schema as input.\n-    from outlines.fsm.json_schema import build_regex_from_schema\n-\n-    def build_regex_from_object(\n-        object: Union[str, BaseModel, Dict], whitespace_pattern: Optional[str] = None\n-    ):\n-        if isinstance(object, type(BaseModel)):\n-            schema = json.dumps(object.model_json_schema())\n-        elif isinstance(object, Dict):\n-            schema = json.dumps(object)\n-        else:\n-            schema = object\n-        return build_regex_from_schema(schema, whitespace_pattern)\n+    try:\n+        # Since outlines 0.0.32, build_regex_from_object is replaced by build_regex_from_schema,\n+        # which only accepts string schema as input.\n+        from outlines.fsm.json_schema import build_regex_from_schema\n+\n+        def build_regex_from_object(\n+            object: Union[str, BaseModel, Dict], whitespace_pattern: Optional[str] = None\n+        ):\n+            if isinstance(object, type(BaseModel)):\n+                schema = json.dumps(object.model_json_schema())\n+            elif isinstance(object, Dict):\n+                schema = json.dumps(object)\n+            else:\n+                schema = object\n+            return build_regex_from_schema(schema, whitespace_pattern)\n+    except ImportError:\n+        # outlines.fsm.json_schema is unavailable entirely\n+        def build_regex_from_object(*args, **kwargs):  # pragma: no cover\n+            raise ImportError(\n+                \"outlines.fsm.json_schema is not available. Install outlines>=0.0.44.\"\n+            )\n+\n+try:\n+    # Optional accelerated grammar backend (if available)\n+    from xgrammar import (\n+        GrammarMatcher,\n+        GrammarMatcherInitContext,\n+        GrammarMatcherInitContextCache,\n+    )\n+except ImportError:\n+    # Gracefully fall back if xgrammar is not installed\n+    class _Dummy:\n+        pass\n+\n+    GrammarMatcher = _Dummy\n+    GrammarMatcherInitContext = _Dummy\n+    GrammarMatcherInitContextCache = _Dummy\n \n \n __all__ = [\n@@ -60,4 +121,8 @@ __all__ = [\n     \"disk_cache\",\n     \"disable_cache\",\n     \"make_byte_level_fsm\",\n+    # Optional xgrammar exports\n+    \"GrammarMatcher\",\n+    \"GrammarMatcherInitContext\",\n+    \"GrammarMatcherInitContextCache\",\n ]\ndiff --git a/python/sglang/srt/constrained/bnf_cache.py b/python/sglang/srt/constrained/bnf_cache.py\nnew file mode 100644\nindex 000000000..e7884d12f\n--- /dev/null\n+++ b/python/sglang/srt/constrained/bnf_cache.py\n@@ -0,0 +1,64 @@\n+\"\"\"\n+Lightweight cache helpers for constrained-decoding grammars.\n+\n+This module provides a simple in-memory cache for BNF/JSON-schema to\n+regex conversions, avoiding repeated expensive builds when the same\n+schema is reused across requests.\n+\"\"\"\n+\n+from __future__ import annotations\n+\n+import hashlib\n+import json\n+from dataclasses import dataclass\n+from typing import Dict, Optional, Union\n+\n+from . import build_regex_from_object\n+\n+\n+def _hash_key(obj: Union[str, dict]) -> str:\n+    if isinstance(obj, str):\n+        data = obj.encode(\"utf-8\")\n+    else:\n+        data = json.dumps(obj, sort_keys=True).encode(\"utf-8\")\n+    return hashlib.sha256(data).hexdigest()\n+\n+\n+@dataclass\n+class CachedRegex:\n+    pattern: str\n+    whitespace_pattern: Optional[str]\n+\n+\n+class BNFRegexCache:\n+    \"\"\"A minimal LRU-like cache for schema->regex conversions.\n+\n+    This is intentionally simple and lock-free as it is read-heavy and\n+    per-process. If needed, it can be extended to an actual LRU.\n+    \"\"\"\n+\n+    def __init__(self, max_size: int = 256):\n+        self._max_size = max_size\n+        self._store: Dict[str, CachedRegex] = {}\n+\n+    def get_or_build(\n+        self, obj: Union[str, dict], whitespace_pattern: Optional[str] = None\n+    ) -> str:\n+        key = _hash_key({\"obj\": obj, \"ws\": whitespace_pattern})\n+        found = self._store.get(key)\n+        if found is not None:\n+            return found.pattern\n+\n+        # Build with outlines helper\n+        pattern = build_regex_from_object(obj, whitespace_pattern)\n+\n+        if len(self._store) >= self._max_size:\n+            # Pop an arbitrary item to keep memory bounded\n+            self._store.pop(next(iter(self._store)))\n+        self._store[key] = CachedRegex(pattern, whitespace_pattern)\n+        return pattern\n+\n+\n+# Module-level default cache\n+default_bnf_regex_cache = BNFRegexCache()\n+\ndiff --git a/python/sglang/srt/constrained/grammar.py b/python/sglang/srt/constrained/grammar.py\nnew file mode 100644\nindex 000000000..902591b68\n--- /dev/null\n+++ b/python/sglang/srt/constrained/grammar.py\n@@ -0,0 +1,59 @@\n+\"\"\"\n+Unified grammar utilities that can leverage either outlines or xgrammar.\n+\n+These helpers keep runtime flexible while enabling faster backends\n+when available, without changing public APIs elsewhere.\n+\"\"\"\n+\n+from __future__ import annotations\n+\n+from typing import Any, Optional, Sequence\n+\n+from . import (\n+    GrammarMatcher,\n+    GrammarMatcherInitContext,\n+    GrammarMatcherInitContextCache,\n+    RegexGuide,\n+)\n+from .bnf_cache import default_bnf_regex_cache\n+\n+\n+def compile_regex_from_schema(schema: Any, whitespace_pattern: Optional[str] = None) -> str:\n+    \"\"\"Compile a JSON schema or BNF-like object to a regex string with caching.\"\"\"\n+    return default_bnf_regex_cache.get_or_build(schema, whitespace_pattern)\n+\n+\n+def build_regex_guide_from_schema(schema: Any, whitespace_pattern: Optional[str] = None) -> RegexGuide:\n+    \"\"\"Build an outlines RegexGuide using a cached regex from schema.\"\"\"\n+    pattern = compile_regex_from_schema(schema, whitespace_pattern)\n+    return RegexGuide(pattern)\n+\n+\n+def maybe_build_xgrammar_context(\n+    tokenizer, grammar: str\n+) -> Optional[GrammarMatcherInitContext]:\n+    \"\"\"Attempt to initialize an xgrammar context if available.\n+\n+    If xgrammar is not installed, returns None. Callers can fall back to\n+    outlines-based RegexGuide in that case.\n+    \"\"\"\n+    try:\n+        if GrammarMatcherInitContext is object or GrammarMatcherInitContext is None:  # type: ignore[attr-defined]\n+            return None\n+    except Exception:\n+        return None\n+\n+    return GrammarMatcherInitContext(tokenizer, grammar)\n+\n+\n+def match_with_xgrammar(\n+    matcher_ctx: GrammarMatcherInitContext,\n+    init_cache: Optional[GrammarMatcherInitContextCache],\n+    token_ids: Sequence[int],\n+) -> Optional[GrammarMatcher]:\n+    \"\"\"Create a GrammarMatcher if xgrammar is available, else return None.\"\"\"\n+    try:\n+        return GrammarMatcher(matcher_ctx, init_cache, token_ids)\n+    except Exception:\n+        return None\n+\ndiff --git a/python/sglang/srt/managers/schedule_batch.py b/python/sglang/srt/managers/schedule_batch.py\nindex fcd06d8cc..f6b04fb4d 100644\n--- a/python/sglang/srt/managers/schedule_batch.py\n+++ b/python/sglang/srt/managers/schedule_batch.py\n@@ -548,8 +548,9 @@ class ScheduleBatch:\n                     or len(req.prefix_indices) >= im.num_image_tokens\n                 )\n \n-        self.encoder_lens = torch.tensor(self.encoder_lens_cpu, dtype=torch.int32).to(\n-            self.device, non_blocking=True\n+        # Allocate directly on the target device to avoid host->device copies\n+        self.encoder_lens = torch.tensor(\n+            self.encoder_lens_cpu, dtype=torch.int32, device=self.device\n         )\n \n         # Strip encoder infos\n@@ -579,23 +580,19 @@ class ScheduleBatch:\n             pt += req.extend_input_len\n \n         # Reassign\n-        self.input_ids = torch.tensor(sum(input_ids, []), dtype=torch.int32).to(\n-            self.device, non_blocking=True\n-        )\n-        self.seq_lens = torch.tensor(seq_lens, dtype=torch.int32).to(\n-            self.device, non_blocking=True\n+        self.input_ids = torch.tensor(\n+            sum(input_ids, []), dtype=torch.int32, device=self.device\n         )\n+        self.seq_lens = torch.tensor(seq_lens, dtype=torch.int32, device=self.device)\n \n         if not decoder_out_cache_loc:\n-            self.out_cache_loc = torch.empty(0, dtype=torch.int32).to(\n-                self.device, non_blocking=True\n-            )\n+            self.out_cache_loc = torch.empty(0, dtype=torch.int32, device=self.device)\n         else:\n             self.out_cache_loc = torch.cat(decoder_out_cache_loc)\n \n         if not encoder_out_cache_loc:\n-            self.encoder_out_cache_loc = torch.empty(0, dtype=torch.int32).to(\n-                self.device, non_blocking=True\n+            self.encoder_out_cache_loc = torch.empty(\n+                0, dtype=torch.int32, device=self.device\n             )\n         else:\n             self.encoder_out_cache_loc = torch.cat(encoder_out_cache_loc)\n@@ -650,15 +647,13 @@ class ScheduleBatch:\n             pt += req.extend_input_len\n \n         # Set fields\n-        self.input_ids = torch.tensor(sum(input_ids, []), dtype=torch.int32).to(\n-            self.device, non_blocking=True\n-        )\n-        self.req_pool_indices = torch.tensor(req_pool_indices, dtype=torch.int32).to(\n-            self.device, non_blocking=True\n+        self.input_ids = torch.tensor(\n+            sum(input_ids, []), dtype=torch.int32, device=self.device\n         )\n-        self.seq_lens = torch.tensor(seq_lens, dtype=torch.int32).to(\n-            self.device, non_blocking=True\n+        self.req_pool_indices = torch.tensor(\n+            req_pool_indices, dtype=torch.int32, device=self.device\n         )\n+        self.seq_lens = torch.tensor(seq_lens, dtype=torch.int32, device=self.device)\n \n         self.out_cache_loc = out_cache_loc\n \n@@ -931,8 +926,8 @@ class ScheduleBatch:\n             self.encoder_lens_cpu = [self.encoder_lens_cpu[i] for i in keep_indices]\n \n         self.reqs = [self.reqs[i] for i in keep_indices]\n-        new_indices = torch.tensor(keep_indices, dtype=torch.int32).to(\n-            self.device, non_blocking=True\n+        new_indices = torch.tensor(\n+            keep_indices, dtype=torch.int32, device=self.device\n         )\n         self.req_pool_indices = self.req_pool_indices[new_indices]\n         self.seq_lens = self.seq_lens[new_indices]\ndiff --git a/python/sglang/srt/sampling/sampling_batch_info.py b/python/sglang/srt/sampling/sampling_batch_info.py\nindex 27a2d07fb..09e22cbd2 100644\n--- a/python/sglang/srt/sampling/sampling_batch_info.py\n+++ b/python/sglang/srt/sampling/sampling_batch_info.py\n@@ -52,23 +52,21 @@ class SamplingBatchInfo:\n     ):\n         reqs = batch.reqs\n         device = batch.device\n-        temperatures = (\n-            torch.tensor(\n-                [r.sampling_params.temperature for r in reqs],\n-                dtype=torch.float,\n-            )\n-            .view(-1, 1)\n-            .to(device, non_blocking=True)\n-        )\n+        # Allocate tensors directly on the target device to avoid extra host->device copies\n+        temperatures = torch.tensor(\n+            [r.sampling_params.temperature for r in reqs],\n+            dtype=torch.float,\n+            device=device,\n+        ).view(-1, 1)\n         top_ps = torch.tensor(\n-            [r.sampling_params.top_p for r in reqs], dtype=torch.float\n-        ).to(device, non_blocking=True)\n+            [r.sampling_params.top_p for r in reqs], dtype=torch.float, device=device\n+        )\n         top_ks = torch.tensor(\n-            [r.sampling_params.top_k for r in reqs], dtype=torch.int32\n-        ).to(device, non_blocking=True)\n+            [r.sampling_params.top_k for r in reqs], dtype=torch.int32, device=device\n+        )\n         min_ps = torch.tensor(\n-            [r.sampling_params.min_p for r in reqs], dtype=torch.float\n-        ).to(device, non_blocking=True)\n+            [r.sampling_params.min_p for r in reqs], dtype=torch.float, device=device\n+        )\n \n         ret = cls(\n             temperatures=temperatures,\n@@ -128,10 +126,9 @@ class SamplingBatchInfo:\n             else:\n                 if self.linear_penalties is None:\n                     bs = self.penalizer_orchestrator.batch.batch_size()\n+                    # Start from zeros as the neutral element for additive penalties\n                     self.linear_penalties = torch.zeros(\n-                        (bs, self.vocab_size),\n-                        dtype=torch.float32,\n-                        device=self.device,\n+                        (bs, self.vocab_size), dtype=torch.float32, device=self.device\n                     )\n                 self.linear_penalties = penalizer.apply(self.linear_penalties)\n \n@@ -141,18 +138,16 @@ class SamplingBatchInfo:\n             self.vocab_mask = None\n             return\n \n-        self.vocab_mask = torch.zeros(\n-            len(self.temperatures),\n-            self.vocab_size,\n-            dtype=torch.bool,\n-            device=self.device,\n+        # Initialize mask with ones and then zero out allowed tokens.\n+        # This avoids creating all-zero tensors and per-row fill_ operations.\n+        bs = len(self.temperatures)\n+        self.vocab_mask = torch.ones(\n+            bs, self.vocab_size, dtype=torch.bool, device=self.device\n         )\n         for i, regex_fsm in enumerate(self.regex_fsms):\n             if regex_fsm is not None:\n-                self.vocab_mask[i].fill_(1)\n-                self.vocab_mask[i][\n-                    regex_fsm.get_next_instruction(self.regex_fsm_states[i]).tokens\n-                ] = 0\n+                tokens = regex_fsm.get_next_instruction(self.regex_fsm_states[i]).tokens\n+                self.vocab_mask[i][tokens] = 0\n \n     def filter_batch(self, unfinished_indices: List[int], new_indices: torch.Tensor):\n         if self.penalizer_orchestrator:\n@@ -180,16 +175,11 @@ class SamplingBatchInfo:\n     ):\n         # bias tensor can be None\n         if lhs is not None or rhs is not None:\n-            shape, dtype = None, None\n-            if lhs is not None:\n-                shape, dtype = lhs.shape[1:], lhs.dtype\n-            else:\n-                shape, dtype = rhs.shape[1:], rhs.dtype\n-            with torch.dtype(dtype):\n-                if lhs is None:\n-                    lhs = torch.empty((bs1, *shape), device=device).fill_(default)\n-                if rhs is None:\n-                    rhs = torch.empty((bs2, *shape), device=device).fill_(default)\n+            shape, dtype = (lhs.shape[1:], lhs.dtype) if lhs is not None else (rhs.shape[1:], rhs.dtype)\n+            if lhs is None:\n+                lhs = torch.full((bs1, *shape), fill_value=default, dtype=dtype, device=device)\n+            if rhs is None:\n+                rhs = torch.full((bs2, *shape), fill_value=default, dtype=dtype, device=device)\n             return torch.cat([lhs, rhs])\n \n         return None\ndiff --git a/python/sglang/srt/server_args.py b/python/sglang/srt/server_args.py\nindex 6ccd89185..2b7702884 100644\n--- a/python/sglang/srt/server_args.py\n+++ b/python/sglang/srt/server_args.py\n@@ -63,6 +63,8 @@ class ServerArgs:\n     stream_interval: int = 1\n     random_seed: Optional[int] = None\n     constrained_json_whitespace_pattern: Optional[str] = None\n+    # Constrained decoding backend: 'auto', 'outlines', or 'xgrammar'\n+    grammar_backend: str = \"auto\"\n \n     # Logging\n     log_level: str = \"info\"\n@@ -319,6 +321,13 @@ class ServerArgs:\n             action=\"store_true\",\n             help=\"Whether to use a CausalLM as an embedding model.\",\n         )\n+        parser.add_argument(\n+            \"--grammar-backend\",\n+            type=str,\n+            default=ServerArgs.grammar_backend,\n+            choices=[\"auto\", \"outlines\", \"xgrammar\"],\n+            help=\"Backend for constrained decoding grammar. Default is 'auto'.\",\n+        )\n         parser.add_argument(\n             \"--mem-fraction-static\",\n             type=float,\n", "model_name_or_path": "gpt-5-2025-08-07"}
