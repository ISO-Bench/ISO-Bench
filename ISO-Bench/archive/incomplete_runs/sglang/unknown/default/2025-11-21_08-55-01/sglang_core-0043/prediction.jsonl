{"instance_id": "/home/ubuntu/OmniPerf-Bench/sglan__repo-da19434", "model_patch": "diff --git a/python/sglang/srt/managers/router/model_rpc.py b/python/sglang/srt/managers/router/model_rpc.py\nindex 883bb12f9..3e032f291 100644\n--- a/python/sglang/srt/managers/router/model_rpc.py\n+++ b/python/sglang/srt/managers/router/model_rpc.py\n@@ -309,13 +309,20 @@ class ModelRpcServer:\n             self.token_to_kv_pool.available_size() + self.tree_cache.evictable_size()\n         )\n         if self.running_batch:\n+            # Avoid creating an intermediate list during summation\n             available_size -= sum(\n-                [\n+                (\n                     (r.max_new_tokens() - len(r.output_ids)) * self.new_token_ratio\n                     for r in self.running_batch.reqs\n-                ]\n+                )\n             )\n \n+        # Cache frequently used attributes/methods to reduce attribute lookups in the loop\n+        max_prefill = self.max_prefill_num_token\n+        inc_ref = self.tree_cache.inc_ref_counter\n+        dec_ref = self.tree_cache.dec_ref_counter\n+        add_refs = self.token_to_kv_pool.add_refs\n+\n         for req in self.forward_queue:\n             if req.return_logprob:\n                 # Need at least two tokens to compute normalized logprob\n@@ -325,7 +332,9 @@ class ModelRpcServer:\n                     req.prefix_indices = req.prefix_indices[:-delta]\n                     if req.image_offset is not None:\n                         req.image_offset += delta\n-            if req.extend_input_len == 0 and req.max_new_tokens() > 0:\n+            # Cache method result to avoid repeated calls\n+            mnt = req.max_new_tokens()\n+            if req.extend_input_len == 0 and mnt > 0:\n                 # Need at least one token to compute logits\n                 req.extend_input_len = 1\n                 req.prefix_indices = req.prefix_indices[:-1]\n@@ -333,34 +342,35 @@ class ModelRpcServer:\n                     req.image_offset += 1\n \n             if (\n-                req.extend_input_len + req.max_new_tokens() + new_batch_total_tokens\n+                req.extend_input_len + mnt + new_batch_total_tokens\n                 < available_size\n-                and req.extend_input_len + new_batch_input_tokens\n-                < self.max_prefill_num_token\n+                and req.extend_input_len + new_batch_input_tokens < max_prefill\n             ):\n-                delta = self.tree_cache.inc_ref_counter(req.last_node)\n+                delta = inc_ref(req.last_node)\n                 available_size += delta\n \n                 if not (\n-                    req.extend_input_len + req.max_new_tokens() + new_batch_total_tokens\n+                    req.extend_input_len + mnt + new_batch_total_tokens\n                     < available_size\n                 ):\n                     # Undo the insertion\n-                    delta = self.tree_cache.dec_ref_counter(req.last_node)\n+                    delta = dec_ref(req.last_node)\n                     available_size += delta\n+                    # No more space available; stop scanning further requests\n+                    break\n                 else:\n                     # Add this request to the running batch\n-                    self.token_to_kv_pool.add_refs(req.prefix_indices)\n+                    add_refs(req.prefix_indices)\n                     can_run_list.append(req)\n-                    new_batch_total_tokens += (\n-                        req.extend_input_len + req.max_new_tokens()\n-                    )\n+                    new_batch_total_tokens += req.extend_input_len + mnt\n                     new_batch_input_tokens += req.extend_input_len\n-\n+            else:\n+                # Further requests are lower-priority; stop scanning\n+                break\n         if len(can_run_list) == 0:\n             return None\n \n-        if self.tp_rank == 0:\n+        if self.tp_rank == 0 and logger.isEnabledFor(logging.INFO):\n             running_req = (\n                 0 if self.running_batch is None else len(self.running_batch.reqs)\n             )\n@@ -380,12 +390,13 @@ class ModelRpcServer:\n                 f\"#running_req: {running_req}. \"\n                 f\"tree_cache_hit_rate: {100.0 * tree_cache_hit_rate:.2f}%.\"\n             )\n-            logger.debug(\n-                f\"fsm_cache_hit_rate: {100.0 * self.regex_fsm_cache.get_cache_hit_rate():.2f}%. \"\n-                f\"fsm_cache_avg_init_time: {self.regex_fsm_cache.get_avg_init_time():.2f}s. \"\n-                f\"ff_cache_hit_rate: {100.0 * self.jump_forward_cache.get_cache_hit_rate():.2f}%. \"\n-                f\"ff_cache_avg_init_time: {self.jump_forward_cache.get_avg_init_time():.2f}s. \"\n-            )\n+            if logger.isEnabledFor(logging.DEBUG):\n+                logger.debug(\n+                    f\"fsm_cache_hit_rate: {100.0 * self.regex_fsm_cache.get_cache_hit_rate():.2f}%. \"\n+                    f\"fsm_cache_avg_init_time: {self.regex_fsm_cache.get_avg_init_time():.2f}s. \"\n+                    f\"ff_cache_hit_rate: {100.0 * self.jump_forward_cache.get_cache_hit_rate():.2f}%. \"\n+                    f\"ff_cache_avg_init_time: {self.jump_forward_cache.get_avg_init_time():.2f}s. \"\n+                )\n \n         new_batch = Batch.init_new(\n             can_run_list,\n@@ -393,7 +404,9 @@ class ModelRpcServer:\n             self.token_to_kv_pool,\n             self.tree_cache,\n         )\n-        self.forward_queue = [x for x in self.forward_queue if x not in can_run_list]\n+        # Use O(1) membership by comparing object identity to avoid O(n*m) filtering\n+        remove_ids = {id(x) for x in can_run_list}\n+        self.forward_queue = [x for x in self.forward_queue if id(x) not in remove_ids]\n         return new_batch\n \n     def forward_fill_batch(self, batch: Batch):\n", "model_name_or_path": "gpt-5-2025-08-07"}
