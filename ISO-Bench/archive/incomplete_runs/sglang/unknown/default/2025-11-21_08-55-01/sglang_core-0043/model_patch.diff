diff --git a/python/sglang/srt/managers/router/model_rpc.py b/python/sglang/srt/managers/router/model_rpc.py
index 883bb12f9..3e032f291 100644
--- a/python/sglang/srt/managers/router/model_rpc.py
+++ b/python/sglang/srt/managers/router/model_rpc.py
@@ -309,13 +309,20 @@ class ModelRpcServer:
             self.token_to_kv_pool.available_size() + self.tree_cache.evictable_size()
         )
         if self.running_batch:
+            # Avoid creating an intermediate list during summation
             available_size -= sum(
-                [
+                (
                     (r.max_new_tokens() - len(r.output_ids)) * self.new_token_ratio
                     for r in self.running_batch.reqs
-                ]
+                )
             )
 
+        # Cache frequently used attributes/methods to reduce attribute lookups in the loop
+        max_prefill = self.max_prefill_num_token
+        inc_ref = self.tree_cache.inc_ref_counter
+        dec_ref = self.tree_cache.dec_ref_counter
+        add_refs = self.token_to_kv_pool.add_refs
+
         for req in self.forward_queue:
             if req.return_logprob:
                 # Need at least two tokens to compute normalized logprob
@@ -325,7 +332,9 @@ class ModelRpcServer:
                     req.prefix_indices = req.prefix_indices[:-delta]
                     if req.image_offset is not None:
                         req.image_offset += delta
-            if req.extend_input_len == 0 and req.max_new_tokens() > 0:
+            # Cache method result to avoid repeated calls
+            mnt = req.max_new_tokens()
+            if req.extend_input_len == 0 and mnt > 0:
                 # Need at least one token to compute logits
                 req.extend_input_len = 1
                 req.prefix_indices = req.prefix_indices[:-1]
@@ -333,34 +342,35 @@ class ModelRpcServer:
                     req.image_offset += 1
 
             if (
-                req.extend_input_len + req.max_new_tokens() + new_batch_total_tokens
+                req.extend_input_len + mnt + new_batch_total_tokens
                 < available_size
-                and req.extend_input_len + new_batch_input_tokens
-                < self.max_prefill_num_token
+                and req.extend_input_len + new_batch_input_tokens < max_prefill
             ):
-                delta = self.tree_cache.inc_ref_counter(req.last_node)
+                delta = inc_ref(req.last_node)
                 available_size += delta
 
                 if not (
-                    req.extend_input_len + req.max_new_tokens() + new_batch_total_tokens
+                    req.extend_input_len + mnt + new_batch_total_tokens
                     < available_size
                 ):
                     # Undo the insertion
-                    delta = self.tree_cache.dec_ref_counter(req.last_node)
+                    delta = dec_ref(req.last_node)
                     available_size += delta
+                    # No more space available; stop scanning further requests
+                    break
                 else:
                     # Add this request to the running batch
-                    self.token_to_kv_pool.add_refs(req.prefix_indices)
+                    add_refs(req.prefix_indices)
                     can_run_list.append(req)
-                    new_batch_total_tokens += (
-                        req.extend_input_len + req.max_new_tokens()
-                    )
+                    new_batch_total_tokens += req.extend_input_len + mnt
                     new_batch_input_tokens += req.extend_input_len
-
+            else:
+                # Further requests are lower-priority; stop scanning
+                break
         if len(can_run_list) == 0:
             return None
 
-        if self.tp_rank == 0:
+        if self.tp_rank == 0 and logger.isEnabledFor(logging.INFO):
             running_req = (
                 0 if self.running_batch is None else len(self.running_batch.reqs)
             )
@@ -380,12 +390,13 @@ class ModelRpcServer:
                 f"#running_req: {running_req}. "
                 f"tree_cache_hit_rate: {100.0 * tree_cache_hit_rate:.2f}%."
             )
-            logger.debug(
-                f"fsm_cache_hit_rate: {100.0 * self.regex_fsm_cache.get_cache_hit_rate():.2f}%. "
-                f"fsm_cache_avg_init_time: {self.regex_fsm_cache.get_avg_init_time():.2f}s. "
-                f"ff_cache_hit_rate: {100.0 * self.jump_forward_cache.get_cache_hit_rate():.2f}%. "
-                f"ff_cache_avg_init_time: {self.jump_forward_cache.get_avg_init_time():.2f}s. "
-            )
+            if logger.isEnabledFor(logging.DEBUG):
+                logger.debug(
+                    f"fsm_cache_hit_rate: {100.0 * self.regex_fsm_cache.get_cache_hit_rate():.2f}%. "
+                    f"fsm_cache_avg_init_time: {self.regex_fsm_cache.get_avg_init_time():.2f}s. "
+                    f"ff_cache_hit_rate: {100.0 * self.jump_forward_cache.get_cache_hit_rate():.2f}%. "
+                    f"ff_cache_avg_init_time: {self.jump_forward_cache.get_avg_init_time():.2f}s. "
+                )
 
         new_batch = Batch.init_new(
             can_run_list,
@@ -393,7 +404,9 @@ class ModelRpcServer:
             self.token_to_kv_pool,
             self.tree_cache,
         )
-        self.forward_queue = [x for x in self.forward_queue if x not in can_run_list]
+        # Use O(1) membership by comparing object identity to avoid O(n*m) filtering
+        remove_ids = {id(x) for x in can_run_list}
+        self.forward_queue = [x for x in self.forward_queue if id(x) not in remove_ids]
         return new_batch
 
     def forward_fill_batch(self, batch: Batch):
