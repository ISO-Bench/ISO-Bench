{"instance_id": "/home/ubuntu/OmniPerf-Bench/sglan__repo-efb099c", "model_patch": "diff --git a/python/sglang/srt/managers/tp_worker_overlap_thread.py b/python/sglang/srt/managers/tp_worker_overlap_thread.py\nindex 5d78b97ce..082222995 100644\n--- a/python/sglang/srt/managers/tp_worker_overlap_thread.py\n+++ b/python/sglang/srt/managers/tp_worker_overlap_thread.py\n@@ -54,13 +54,28 @@ class TpModelWorkerClient:\n         self.future_token_ids_map = torch.empty(\n             (self.max_running_requests * 5,), dtype=torch.int32, device=self.device\n         )\n+        # Preallocate buffer for future next token ids to avoid per-iteration allocations\n+        self.future_next_token_ids_buf = torch.empty(\n+            (self.max_running_requests,), dtype=torch.int32, device=self.device\n+        )\n \n-        # Launch a thread\n+        # Launch threads\n+        # Queue for host->device copy stage\n         self.input_queue = Queue()\n+        # Queue for forward stage after copies are done\n+        self.forward_queue = Queue()\n         self.output_queue = Queue()\n+        # Use separate CUDA streams to overlap copy and compute\n+        self.copy_stream = torch.cuda.Stream()\n         self.forward_stream = torch.cuda.Stream()\n+        self.copy_thread = threading.Thread(\n+            target=self.copy_thread_func,\n+            daemon=True,\n+        )\n+        self.copy_thread.start()\n         self.forward_thread = threading.Thread(\n             target=self.forward_thread_func,\n+            daemon=True,\n         )\n         self.forward_thread.start()\n \n@@ -86,15 +101,17 @@ class TpModelWorkerClient:\n     @torch.inference_mode()\n     def forward_thread_func_(self):\n         while True:\n-            model_worker_batch, future_token_ids_ct = self.input_queue.get()\n+            model_worker_batch, future_token_ids_ct = self.forward_queue.get()\n+            # Ensure forward stream waits for pending H2D copies\n+            self.forward_stream.wait_stream(self.copy_stream)\n \n             # Resolve future tokens in the input\n             input_ids = model_worker_batch.input_ids\n-            input_ids[:] = torch.where(\n-                input_ids < 0,\n-                self.future_token_ids_map[torch.clamp(-input_ids, min=0)],\n-                input_ids,\n-            )\n+            # Use masked assignment to reduce temporary allocations\n+            neg_mask = input_ids < 0\n+            if torch.count_nonzero(neg_mask):\n+                idx = (-input_ids[neg_mask]).clamp_min_(0)\n+                input_ids[neg_mask] = self.future_token_ids_map[idx]\n \n             # Run forward\n             logits_output, next_token_ids = self.worker.forward_batch_generation(\n@@ -103,11 +120,13 @@ class TpModelWorkerClient:\n \n             # Update the future token ids map\n             bs = len(model_worker_batch.seq_lens)\n-            future_next_token_ids = torch.arange(\n+            future_next_token_ids = self.future_next_token_ids_buf[:bs]\n+            torch.arange(\n                 -(future_token_ids_ct + bs),\n                 -(future_token_ids_ct),\n                 dtype=torch.int32,\n                 device=self.device,\n+                out=future_next_token_ids,\n             )\n             self.future_token_ids_map[-future_next_token_ids] = next_token_ids.to(\n                 torch.int32\n@@ -139,6 +158,15 @@ class TpModelWorkerClient:\n         ) % self.future_token_ids_limit\n         return None, future_next_token_ids\n \n+    def copy_thread_func(self):\n+        \"\"\"Stage 1: move batch tensors to device on a dedicated stream.\"\"\"\n+        with torch.cuda.stream(self.copy_stream):\n+            while True:\n+                model_worker_batch, future_token_ids_ct = self.input_queue.get()\n+                # Non-blocking copies overlap with compute on a different stream\n+                model_worker_batch.to(self.device)\n+                self.forward_queue.put((model_worker_batch, future_token_ids_ct))\n+\n     def forward_batch_embedding(self, model_worker_batch: ModelWorkerBatch):\n         forward_batch = ForwardBatch.init_new(model_worker_batch, self.model_runner)\n         logits_output = self.model_runner.forward(forward_batch)\n", "model_name_or_path": "gpt-5-2025-08-07"}
