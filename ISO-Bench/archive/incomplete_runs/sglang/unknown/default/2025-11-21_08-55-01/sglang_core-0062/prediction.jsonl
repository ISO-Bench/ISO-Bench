{"instance_id": "/home/ubuntu/OmniPerf-Bench/sglan__repo-e2cdc8a", "model_patch": "diff --git a/benchmark/kernels/minmax-text-01-lighting_attention/benchmark_lighting_attention_decode.py b/benchmark/kernels/minmax-text-01-lighting_attention/benchmark_lighting_attention_decode.py\nindex 1a2036dc0..6c206dd49 100644\n--- a/benchmark/kernels/minmax-text-01-lighting_attention/benchmark_lighting_attention_decode.py\n+++ b/benchmark/kernels/minmax-text-01-lighting_attention/benchmark_lighting_attention_decode.py\n@@ -24,12 +24,17 @@ def _decode_kernel(\n     n: tl.constexpr,\n     d: tl.constexpr,\n     e: tl.constexpr,\n+    d_original: tl.constexpr,\n+    e_original: tl.constexpr,\n+    BLOCK_SIZE: tl.constexpr = 32,\n ):\n     off_bh = tl.program_id(0)\n     off_h = off_bh % h\n \n-    qk_offset = off_bh * n * d\n-    v_offset = off_bh * n * e\n+    # Offsets for flattened tensors. Q/K/V are stored using the original\n+    # inner-dimension sizes, while KV/Out use the padded/aligned sizes.\n+    qk_offset = off_bh * n * d_original\n+    v_offset = off_bh * n * e_original\n     o_offset = off_bh * n * e\n     kv_offset = off_bh * d * e\n \n@@ -39,21 +44,31 @@ def _decode_kernel(\n     d_idx = tl.arange(0, d)\n     e_idx = tl.arange(0, e)\n \n-    q = tl.load(Q + qk_offset + d_idx)\n-    k = tl.load(K + qk_offset + d_idx)\n-    v = tl.load(V + v_offset + e_idx)\n+    # Create masks for original dimensions\n+    d_mask = d_idx < d_original\n+    e_mask = e_idx < e_original\n+\n+    # Masked loads avoid the need to pad inputs in Python\n+    q = tl.load(Q + qk_offset + d_idx, mask=d_mask, other=0.0)\n+    k = tl.load(K + qk_offset + d_idx, mask=d_mask, other=0.0)\n+    v = tl.load(V + v_offset + e_idx, mask=e_mask, other=0.0)\n \n     kv = tl.load(KV + kv_offset + d_idx[:, None] * e + e_idx[None, :])\n \n     k_v_prod = k[:, None] * v[None, :]\n     kv = ratio * kv + k_v_prod\n \n+    # Store back only the valid [d_original, e_original] region\n+    kv_store_mask = (d_idx[:, None] < d_original) & (e_idx[None, :] < e_original)\n     tl.store(\n-        KV + kv_offset + d_idx[:, None] * e + e_idx[None, :], kv.to(KV.dtype.element_ty)\n+        KV + kv_offset + d_idx[:, None] * e + e_idx[None, :],\n+        kv.to(KV.dtype.element_ty),\n+        mask=kv_store_mask,\n     )\n \n     o = tl.sum(q[:, None] * kv, axis=0)\n-    tl.store(Out + o_offset + e_idx, o.to(Out.dtype.element_ty))\n+    # Only write valid output elements\n+    tl.store(Out + o_offset + e_idx, o.to(Out.dtype.element_ty), mask=e_mask)\n \n \n def lightning_attn_decode(q, k, v, kv, s):\n@@ -62,24 +77,23 @@ def lightning_attn_decode(q, k, v, kv, s):\n     e = v.shape[-1]\n     assert n == 1, \"Sequence length must be 1 in decode mode\"\n \n-    # Pad dimensions to power of 2\n-    d_padded = next_power_of_2(d)\n-    e_padded = next_power_of_2(e)\n-\n-    # Pad inputs\n-    q_padded = F.pad(q, (0, d_padded - d))\n-    k_padded = F.pad(k, (0, d_padded - d))\n-    v_padded = F.pad(v, (0, e_padded - e))\n-    kv_padded = F.pad(kv, (0, e_padded - e, 0, d_padded - d))\n-\n-    # Ensure inputs are contiguous\n-    q_padded = q_padded.contiguous()\n-    k_padded = k_padded.contiguous()\n-    v_padded = v_padded.contiguous()\n-    kv_padded = kv_padded.contiguous().to(torch.float32)\n+    # Align dimensions to a friendly block size to reduce padding overhead\n+    BLOCK_SIZE = 32\n+    d_padded = next_multiple_of(d, BLOCK_SIZE)\n+    e_padded = next_multiple_of(e, BLOCK_SIZE)\n+\n+    # Avoid padding Q/K/V by using masked loads in the Triton kernel\n+    q_padded = q.contiguous()\n+    k_padded = k.contiguous()\n+    v_padded = v.contiguous()\n+\n+    # Allocate a padded KV buffer without initializing the entire tensor\n+    # Copy only the valid region to minimize memory traffic\n+    kv_padded = torch.empty(b, h, d_padded, e_padded, dtype=torch.float32, device=kv.device)\n+    kv_padded[..., :d, :e] = kv.to(torch.float32)\n     s = s.contiguous()\n \n-    # Create output tensor (padded)\n+    # Create output tensor (padded) without zero-initialization\n     o_padded = torch.empty(b, h, n, e_padded, dtype=v.dtype, device=v.device)\n \n     # Launch kernel\n@@ -96,6 +110,8 @@ def lightning_attn_decode(q, k, v, kv, s):\n         n=n,\n         d=d_padded,\n         e=e_padded,\n+        d_original=d,\n+        e_original=e,\n     )\n \n     # Remove padding\n@@ -105,8 +121,9 @@ def lightning_attn_decode(q, k, v, kv, s):\n     return o, kv_out\n \n \n-def next_power_of_2(n):\n-    return 2 ** (int(math.ceil(math.log(n, 2))))\n+def next_multiple_of(n, k):\n+    # Return the smallest multiple of k that is >= n\n+    return ((n + k - 1) // k) * k\n \n \n class MiniMaxText01LightningAttention(nn.Module):\n", "model_name_or_path": "gpt-5-2025-08-07"}
