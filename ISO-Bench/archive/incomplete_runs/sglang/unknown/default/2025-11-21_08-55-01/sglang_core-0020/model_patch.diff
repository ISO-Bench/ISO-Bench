diff --git a/python/sglang/srt/models/deepseek_v2.py b/python/sglang/srt/models/deepseek_v2.py
index 29f18f0ef..b4fc4d7a7 100644
--- a/python/sglang/srt/models/deepseek_v2.py
+++ b/python/sglang/srt/models/deepseek_v2.py
@@ -454,6 +454,7 @@ class DeepseekV2MoE(nn.Module):
                 num_expert_group=self.num_expert_group,
                 correction_bias=self.correction_bias,
                 routed_scaling_factor=self.routed_scaling_factor,
+                num_token_non_padded=state.forward_batch.num_token_non_padded,
                 expert_location_dispatch_info=ExpertLocationDispatchInfo.init_new(
                     layer_id=self.layer_id,
                 ),
diff --git a/python/sglang/srt/two_batch_overlap.py b/python/sglang/srt/two_batch_overlap.py
index 6b0241f40..0b897fbe0 100644
--- a/python/sglang/srt/two_batch_overlap.py
+++ b/python/sglang/srt/two_batch_overlap.py
@@ -110,7 +110,12 @@ def compute_split_indices_for_cuda_graph_replay(
 
 class TboCudaGraphRunnerPlugin:
     def __init__(self):
-        pass  # TODO add logic here
+        # Persist a small device buffer used by CUDA graphs to feed
+        # per-child non-padded token counts at replay time.
+        # Using empty to avoid redundant zero-initialization overhead.
+        self._tbo_children_num_token_non_padded = torch.empty(
+            (2,), dtype=torch.int32
+        )
 
     def capture_one_batch_size(self, batch: ForwardBatch, num_tokens: int):
         if not global_server_args_dict["enable_two_batch_overlap"]:
@@ -126,13 +131,38 @@ class TboCudaGraphRunnerPlugin:
 
         TboForwardBatchPreparer.prepare(batch)
 
+        # Wire children to read num_token_non_padded from our persistent buffer
+        # and initialize values for capture.
+        assert batch.tbo_children is not None and len(batch.tbo_children) == 2
+        for i, child in enumerate(batch.tbo_children):
+            # Make each child reference a view into our buffer
+            child.num_token_non_padded = self._tbo_children_num_token_non_padded[
+                i : i + 1
+            ]
+        # Initialize capture-time values based on actual child token counts
+        self._tbo_children_num_token_non_padded[0] = (
+            batch.tbo_children[0].input_ids.shape[0]
+        )
+        self._tbo_children_num_token_non_padded[1] = (
+            batch.tbo_children[1].input_ids.shape[0]
+        )
+
     def replay_prepare(
         self, forward_mode: ForwardMode, bs: int, num_token_non_padded: int
     ):
         if not global_server_args_dict["enable_two_batch_overlap"]:
             return
 
-        pass  # TODO add logic here
+        # Compute token split for the current CUDA graph replay configuration
+        # and update the shared buffer so captured children read correct values.
+        _, tbo_split_token_index = compute_split_indices_for_cuda_graph_replay(
+            forward_mode=forward_mode, cuda_graph_num_tokens=bs
+        )
+        # Respect padding: distribute only non-padded tokens across children
+        left = min(num_token_non_padded, tbo_split_token_index)
+        right = max(num_token_non_padded - tbo_split_token_index, 0)
+        self._tbo_children_num_token_non_padded[0] = left
+        self._tbo_children_num_token_non_padded[1] = right
 
 
 class TboDPAttentionPreparer:
@@ -324,7 +354,9 @@ class TboForwardBatchPreparer:
         # TODO improve, e.g. unify w/ `init_raw`
         if global_server_args_dict["moe_dense_tp_size"] == 1:
             sum_len = end_token_index - start_token_index
-            gathered_buffer = torch.zeros(
+            # Allocate without zero-initialization; the buffer will be fully written
+            # by gather/scatter ops downstream.
+            gathered_buffer = torch.empty(
                 (sum_len, batch.gathered_buffer.shape[1]),
                 dtype=batch.gathered_buffer.dtype,
                 device=batch.gathered_buffer.device,
