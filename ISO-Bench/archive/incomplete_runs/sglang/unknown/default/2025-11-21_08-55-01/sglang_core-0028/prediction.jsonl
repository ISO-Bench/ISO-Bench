{"instance_id": "/home/ubuntu/OmniPerf-Bench/sglan__repo-b1c8d4e", "model_patch": "diff --git a/python/sglang/srt/disaggregation/mooncake/conn.py b/python/sglang/srt/disaggregation/mooncake/conn.py\nindex 8ab5066ec..4a25b79b9 100644\n--- a/python/sglang/srt/disaggregation/mooncake/conn.py\n+++ b/python/sglang/srt/disaggregation/mooncake/conn.py\n@@ -31,6 +31,7 @@ from sglang.srt.disaggregation.base.conn import (\n from sglang.srt.disaggregation.mooncake.transfer_engine import MooncakeTransferEngine\n from sglang.srt.disaggregation.utils import (\n     DisaggregationMode,\n+    FastQueue,\n     group_concurrent_contiguous,\n )\n from sglang.srt.server_args import ServerArgs\n@@ -151,7 +152,8 @@ class MooncakeKVManager(BaseKVManager):\n         self.server_socket = zmq.Context().socket(zmq.PULL)\n         self.register_buffer_to_engine()\n         if self.disaggregation_mode == DisaggregationMode.PREFILL:\n-            self.transfer_queue = queue.Queue()\n+            # Use FastQueue to reduce contention and overhead in transfer path\n+            self.transfer_queue = FastQueue()\n             self.transfer_infos: Dict[int, Dict[str, TransferInfo]] = {}\n             self.decode_kv_args_table: Dict[str, KVArgsRegisterInfo] = {}\n             self.start_prefill_thread()\ndiff --git a/python/sglang/srt/disaggregation/utils.py b/python/sglang/srt/disaggregation/utils.py\nindex 8841d5f1a..6147ac29e 100644\n--- a/python/sglang/srt/disaggregation/utils.py\n+++ b/python/sglang/srt/disaggregation/utils.py\n@@ -14,6 +14,9 @@ import torch\n import torch.distributed as dist\n \n from sglang.srt.utils import get_ip, get_local_ip_by_remote\n+import threading\n+import time\n+import queue as _queue\n \n if TYPE_CHECKING:\n     from sglang.srt.managers.schedule_batch import Req\n@@ -207,17 +210,19 @@ class MetadataBuffers:\n \n         # We transfer the metadata of first output token to decode\n         # The minimal size for RDMA is 64Bytes, so we pad it to > 64Bytes\n-        self.output_ids = torch.zeros((size, 16), dtype=torch.int32, device=\"cpu\")\n-        self.output_token_logprobs_val = torch.zeros(\n+        # Use torch.empty to avoid unnecessary zero-initialization overhead.\n+        # Callers populate only the used fields before transfer.\n+        self.output_ids = torch.empty((size, 16), dtype=torch.int32, device=\"cpu\")\n+        self.output_token_logprobs_val = torch.empty(\n             (size, 16), dtype=torch.float32, device=\"cpu\"\n         )\n-        self.output_token_logprobs_idx = torch.zeros(\n+        self.output_token_logprobs_idx = torch.empty(\n             (size, 16), dtype=torch.int32, device=\"cpu\"\n         )\n-        self.output_top_logprobs_val = torch.zeros(\n+        self.output_top_logprobs_val = torch.empty(\n             (size, max_top_logprobs_num), dtype=torch.float32, device=\"cpu\"\n         )\n-        self.output_top_logprobs_idx = torch.zeros(\n+        self.output_top_logprobs_idx = torch.empty(\n             (size, max_top_logprobs_num), dtype=torch.int32, device=\"cpu\"\n         )\n \n@@ -289,10 +294,41 @@ def group_concurrent_contiguous(\n         return [], []\n \n     brk = np.where((np.diff(src_indices) != 1) | (np.diff(dst_indices) != 1))[0] + 1\n-    src_groups = np.split(src_indices, brk)\n-    dst_groups = np.split(dst_indices, brk)\n+    # Keep groups as NumPy arrays to reduce Python list allocations and conversions\n+    src_groups = list(np.split(src_indices, brk))\n+    dst_groups = list(np.split(dst_indices, brk))\n+    return src_groups, dst_groups\n \n-    src_groups = [g.tolist() for g in src_groups]\n-    dst_groups = [g.tolist() for g in dst_groups]\n \n-    return src_groups, dst_groups\n+class FastQueue:\n+    \"\"\"\n+    A lightweight, thread-safe queue optimized for high-throughput puts/gets.\n+    Implements a minimal subset of queue.Queue with timeout support for get().\n+    \"\"\"\n+\n+    def __init__(self):\n+        self._deque = deque()\n+        self._cond = threading.Condition()\n+\n+    def put(self, item) -> None:\n+        with self._cond:\n+            self._deque.append(item)\n+            self._cond.notify()\n+\n+    def get(self, timeout: Optional[float] = None):\n+        end = None if timeout is None else (time.time() + timeout)\n+        with self._cond:\n+            while not self._deque:\n+                remaining = None if end is None else (end - time.time())\n+                if remaining is not None and remaining <= 0:\n+                    raise _queue.Empty\n+                self._cond.wait(timeout=remaining)\n+            return self._deque.popleft()\n+\n+    def empty(self) -> bool:\n+        with self._cond:\n+            return not self._deque\n+\n+    def qsize(self) -> int:\n+        with self._cond:\n+            return len(self._deque)\n", "model_name_or_path": "gpt-5-2025-08-07"}
