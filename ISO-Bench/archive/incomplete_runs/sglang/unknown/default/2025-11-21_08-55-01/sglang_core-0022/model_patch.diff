diff --git a/python/sglang/srt/models/deepseek_v2.py b/python/sglang/srt/models/deepseek_v2.py
index b299c3037..2c7bb95fa 100644
--- a/python/sglang/srt/models/deepseek_v2.py
+++ b/python/sglang/srt/models/deepseek_v2.py
@@ -569,27 +569,42 @@ class DeepseekV2AttentionMLA(nn.Module):
     def dispatch_attn_forward_method(
         self, forward_batch: ForwardBatch
     ) -> AttnForwardMethod:
+        # Micro-optimizations: avoid repeated method lookups and sum() calls.
+        fm = forward_batch.forward_mode
+        is_extend = fm.is_extend()
+        is_target_verify = fm.is_target_verify()
+        is_draft_extend = fm.is_draft_extend()
+
+        extend_prefix_lens = forward_batch.extend_prefix_lens_cpu
+        sum_extend_prefix_lens = (
+            sum(extend_prefix_lens) if extend_prefix_lens is not None else 0
+        )
+
         if self.attention_backend == "flashinfer":
             # Flashinfer MLA: Do not absorb when enabling ragged prefill
             if (
                 not self.flashinfer_mla_disable_ragged
-                and forward_batch.forward_mode.is_extend()
-                and not forward_batch.forward_mode.is_target_verify()
-                and not forward_batch.forward_mode.is_draft_extend()
-                and sum(forward_batch.extend_prefix_lens_cpu) == 0
+                and is_extend
+                and not is_target_verify
+                and not is_draft_extend
+                and sum_extend_prefix_lens == 0
             ):
                 return AttnForwardMethod.MHA
             else:
                 return AttnForwardMethod.MLA
         elif self.attention_backend == "fa3":
             # Flash Attention: Use MHA with chunked KV cache when prefilling on long sequences.
+            # Optimization: also enable chunked KV when sum of extend prefixes is 0 (pure prefill),
+            # and avoid recomputing Python-level sum repeatedly.
             if (
-                forward_batch.forward_mode.is_extend()
+                is_extend
                 and not self.disable_chunked_prefix_cache
-                and not forward_batch.forward_mode.is_target_verify()
-                and not forward_batch.forward_mode.is_draft_extend()
-                and sum(forward_batch.extend_prefix_lens_cpu)
-                >= self.chunked_prefix_cache_threshold
+                and not is_target_verify
+                and not is_draft_extend
+                and (
+                    sum_extend_prefix_lens >= self.chunked_prefix_cache_threshold
+                    or sum_extend_prefix_lens == 0
+                )
             ):
                 return AttnForwardMethod.MHA_CHUNKED_KV
             else:
@@ -597,10 +612,10 @@ class DeepseekV2AttentionMLA(nn.Module):
         else:
             # Triton: Use normal computation for prefill and use weight absorption for extend/decode
             if (
-                forward_batch.forward_mode.is_extend()
-                and not forward_batch.forward_mode.is_target_verify()
-                and not forward_batch.forward_mode.is_draft_extend()
-                and sum(forward_batch.extend_prefix_lens_cpu) == 0
+                is_extend
+                and not is_target_verify
+                and not is_draft_extend
+                and sum_extend_prefix_lens == 0
             ):
                 return AttnForwardMethod.MHA
             else:
