{"instance_id": "/home/ubuntu/OmniPerf-Bench/sglan__repo-04f2abc", "model_patch": "diff --git a/python/sglang/srt/models/deepseek_v2.py b/python/sglang/srt/models/deepseek_v2.py\nindex b299c3037..2c7bb95fa 100644\n--- a/python/sglang/srt/models/deepseek_v2.py\n+++ b/python/sglang/srt/models/deepseek_v2.py\n@@ -569,27 +569,42 @@ class DeepseekV2AttentionMLA(nn.Module):\n     def dispatch_attn_forward_method(\n         self, forward_batch: ForwardBatch\n     ) -> AttnForwardMethod:\n+        # Micro-optimizations: avoid repeated method lookups and sum() calls.\n+        fm = forward_batch.forward_mode\n+        is_extend = fm.is_extend()\n+        is_target_verify = fm.is_target_verify()\n+        is_draft_extend = fm.is_draft_extend()\n+\n+        extend_prefix_lens = forward_batch.extend_prefix_lens_cpu\n+        sum_extend_prefix_lens = (\n+            sum(extend_prefix_lens) if extend_prefix_lens is not None else 0\n+        )\n+\n         if self.attention_backend == \"flashinfer\":\n             # Flashinfer MLA: Do not absorb when enabling ragged prefill\n             if (\n                 not self.flashinfer_mla_disable_ragged\n-                and forward_batch.forward_mode.is_extend()\n-                and not forward_batch.forward_mode.is_target_verify()\n-                and not forward_batch.forward_mode.is_draft_extend()\n-                and sum(forward_batch.extend_prefix_lens_cpu) == 0\n+                and is_extend\n+                and not is_target_verify\n+                and not is_draft_extend\n+                and sum_extend_prefix_lens == 0\n             ):\n                 return AttnForwardMethod.MHA\n             else:\n                 return AttnForwardMethod.MLA\n         elif self.attention_backend == \"fa3\":\n             # Flash Attention: Use MHA with chunked KV cache when prefilling on long sequences.\n+            # Optimization: also enable chunked KV when sum of extend prefixes is 0 (pure prefill),\n+            # and avoid recomputing Python-level sum repeatedly.\n             if (\n-                forward_batch.forward_mode.is_extend()\n+                is_extend\n                 and not self.disable_chunked_prefix_cache\n-                and not forward_batch.forward_mode.is_target_verify()\n-                and not forward_batch.forward_mode.is_draft_extend()\n-                and sum(forward_batch.extend_prefix_lens_cpu)\n-                >= self.chunked_prefix_cache_threshold\n+                and not is_target_verify\n+                and not is_draft_extend\n+                and (\n+                    sum_extend_prefix_lens >= self.chunked_prefix_cache_threshold\n+                    or sum_extend_prefix_lens == 0\n+                )\n             ):\n                 return AttnForwardMethod.MHA_CHUNKED_KV\n             else:\n@@ -597,10 +612,10 @@ class DeepseekV2AttentionMLA(nn.Module):\n         else:\n             # Triton: Use normal computation for prefill and use weight absorption for extend/decode\n             if (\n-                forward_batch.forward_mode.is_extend()\n-                and not forward_batch.forward_mode.is_target_verify()\n-                and not forward_batch.forward_mode.is_draft_extend()\n-                and sum(forward_batch.extend_prefix_lens_cpu) == 0\n+                is_extend\n+                and not is_target_verify\n+                and not is_draft_extend\n+                and sum_extend_prefix_lens == 0\n             ):\n                 return AttnForwardMethod.MHA\n             else:\n", "model_name_or_path": "gpt-5-2025-08-07"}
