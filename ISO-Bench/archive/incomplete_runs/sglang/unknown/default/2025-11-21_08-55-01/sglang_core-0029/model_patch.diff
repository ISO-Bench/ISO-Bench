diff --git a/model_patch.diff b/model_patch.diff
new file mode 100644
index 000000000..e69de29bb
diff --git a/python/sglang/srt/entrypoints/http_server.py b/python/sglang/srt/entrypoints/http_server.py
index 5a97072de..273fd755d 100644
--- a/python/sglang/srt/entrypoints/http_server.py
+++ b/python/sglang/srt/entrypoints/http_server.py
@@ -720,7 +720,9 @@ def _wait_and_warmup(
 
     model_info = res.json()
 
-    # Send a warmup request
+    # Send a warmup request. When data parallelism (dp_size) > 1, send a single
+    # batched request instead of issuing dp_size individual requests. This avoids
+    # repeated request overhead during warmup and speeds up startup.
     request_name = "/generate" if model_info["is_generation"] else "/encode"
     max_new_tokens = 8 if model_info["is_generation"] else 1
     json_data = {
@@ -729,28 +731,36 @@ def _wait_and_warmup(
             "max_new_tokens": max_new_tokens,
         },
     }
+    # Prepare batched inputs for warmup according to dp_size
     if server_args.skip_tokenizer_init:
-        json_data["input_ids"] = [10, 11, 12]
+        # Use dummy token ids and replicate for each DP rank
+        json_data["input_ids"] = [[10, 11, 12] for _ in range(server_args.dp_size)]
     else:
-        json_data["text"] = "The capital city of France is"
+        # Use a short prompt and replicate for each DP rank
+        json_data["text"] = [
+            "The capital city of France is" for _ in range(server_args.dp_size)
+        ]
 
     # Debug dumping
     if server_args.debug_tensor_dump_input_file:
         json_data.pop("text", None)
-        json_data["input_ids"] = np.load(
-            server_args.debug_tensor_dump_input_file
-        ).tolist()
+        loaded = np.load(server_args.debug_tensor_dump_input_file).tolist()
+        # If the loaded data is a single sequence (1D), replicate per DP rank.
+        if isinstance(loaded, list) and (len(loaded) == 0 or isinstance(loaded[0], int)):
+            json_data["input_ids"] = [loaded for _ in range(server_args.dp_size)]
+        else:
+            json_data["input_ids"] = loaded
         json_data["sampling_params"]["max_new_tokens"] = 0
 
     try:
-        for i in range(server_args.dp_size):
-            res = requests.post(
-                url + request_name,
-                json=json_data,
-                headers=headers,
-                timeout=600,
-            )
-            assert res.status_code == 200, f"{res}"
+        # Single batched warmup request (covers dp_size replicas)
+        res = requests.post(
+            url + request_name,
+            json=json_data,
+            headers=headers,
+            timeout=600,
+        )
+        assert res.status_code == 200, f"{res}"
     except Exception:
         last_traceback = get_exception_traceback()
         if pipe_finish_writer is not None:
