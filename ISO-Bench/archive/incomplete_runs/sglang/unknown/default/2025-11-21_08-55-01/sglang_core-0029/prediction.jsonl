{"instance_id": "/home/ubuntu/OmniPerf-Bench/sglan__repo-57eec0b", "model_patch": "diff --git a/model_patch.diff b/model_patch.diff\nnew file mode 100644\nindex 000000000..e69de29bb\ndiff --git a/python/sglang/srt/entrypoints/http_server.py b/python/sglang/srt/entrypoints/http_server.py\nindex 5a97072de..273fd755d 100644\n--- a/python/sglang/srt/entrypoints/http_server.py\n+++ b/python/sglang/srt/entrypoints/http_server.py\n@@ -720,7 +720,9 @@ def _wait_and_warmup(\n \n     model_info = res.json()\n \n-    # Send a warmup request\n+    # Send a warmup request. When data parallelism (dp_size) > 1, send a single\n+    # batched request instead of issuing dp_size individual requests. This avoids\n+    # repeated request overhead during warmup and speeds up startup.\n     request_name = \"/generate\" if model_info[\"is_generation\"] else \"/encode\"\n     max_new_tokens = 8 if model_info[\"is_generation\"] else 1\n     json_data = {\n@@ -729,28 +731,36 @@ def _wait_and_warmup(\n             \"max_new_tokens\": max_new_tokens,\n         },\n     }\n+    # Prepare batched inputs for warmup according to dp_size\n     if server_args.skip_tokenizer_init:\n-        json_data[\"input_ids\"] = [10, 11, 12]\n+        # Use dummy token ids and replicate for each DP rank\n+        json_data[\"input_ids\"] = [[10, 11, 12] for _ in range(server_args.dp_size)]\n     else:\n-        json_data[\"text\"] = \"The capital city of France is\"\n+        # Use a short prompt and replicate for each DP rank\n+        json_data[\"text\"] = [\n+            \"The capital city of France is\" for _ in range(server_args.dp_size)\n+        ]\n \n     # Debug dumping\n     if server_args.debug_tensor_dump_input_file:\n         json_data.pop(\"text\", None)\n-        json_data[\"input_ids\"] = np.load(\n-            server_args.debug_tensor_dump_input_file\n-        ).tolist()\n+        loaded = np.load(server_args.debug_tensor_dump_input_file).tolist()\n+        # If the loaded data is a single sequence (1D), replicate per DP rank.\n+        if isinstance(loaded, list) and (len(loaded) == 0 or isinstance(loaded[0], int)):\n+            json_data[\"input_ids\"] = [loaded for _ in range(server_args.dp_size)]\n+        else:\n+            json_data[\"input_ids\"] = loaded\n         json_data[\"sampling_params\"][\"max_new_tokens\"] = 0\n \n     try:\n-        for i in range(server_args.dp_size):\n-            res = requests.post(\n-                url + request_name,\n-                json=json_data,\n-                headers=headers,\n-                timeout=600,\n-            )\n-            assert res.status_code == 200, f\"{res}\"\n+        # Single batched warmup request (covers dp_size replicas)\n+        res = requests.post(\n+            url + request_name,\n+            json=json_data,\n+            headers=headers,\n+            timeout=600,\n+        )\n+        assert res.status_code == 200, f\"{res}\"\n     except Exception:\n         last_traceback = get_exception_traceback()\n         if pipe_finish_writer is not None:\n", "model_name_or_path": "gpt-5-2025-08-07"}
