OpenAI Codex v0.60.1 (research preview)
--------
workdir: /home/ubuntu/OmniPerf-Bench/perf-agents-bench/.work/worktrees/sglang_core/sglang_core-0060
model: gpt-5
provider: openai
approval: never
sandbox: danger-full-access
reasoning effort: high
reasoning summaries: auto
session id: 019aa517-180c-7290-99ac-cbf980f62780
--------
user
I've uploaded a python code repository in the directory /home/ubuntu/OmniPerf-Bench/perf-agents-bench/.work/worktrees/sglang_core/sglang_core-0060.
Consider the following test script showing an example usage of the repository:

<test_script>
import torch
import time
from vllm.model_executor.layers.fused_moe import moe_align_block_size

# Benchmark the MoE align block size operation
num_tokens = 4096
num_experts = 64
topk = 2
block_size = 128

# Create input data
topk_ids = torch.randint(0, num_experts, (num_tokens * topk,), dtype=torch.int32, device='cuda')

# Time the operation
torch.cuda.synchronize()
start = time.time()

sorted_ids, expert_ids, num_tokens_post_pad = moe_align_block_size(
    topk_ids, num_experts, block_size, topk
)

torch.cuda.synchronize()
duration = time.time() - start

print(f"Duration: {duration:.4f} seconds")

</test_script>

Can you help me implement the necessary changes to the repository so that the runtime of the <test_script> is optimized?

Basic guidelines:
1. Your task is to make changes to non-test files in the /home/ubuntu/OmniPerf-Bench/perf-agents-bench/.work/worktrees/sglang_core/sglang_core-0060 directory to improve the performance of the <test_script>.
2. Make changes while ensuring the repository is functionally equivalent to the original.
3. Do not overoptimize for just the specific inputs in <test_script>. Make general performance improvements for the usage scenario shown.
4. You may need to rebuild the repo for your changes to take effect before testing. Some rebuilds may take time to run, so be patient with running them.

Follow these steps to improve performance:
1. As a first step, explore the repository structure.
2. Create a script ONLY inside /home/ubuntu/OmniPerf-Bench/perf-agents-bench/.work/worktrees/sglang_core/sglang_core-0060/.bench_scratch (e.g., /home/ubuntu/OmniPerf-Bench/perf-agents-bench/.work/worktrees/sglang_core/sglang_core-0060/.bench_scratch/test_opt.py) to reproduce and time the example, then execute it with python <filename.py> from the repo root.
3. Edit the source code of the repository to improve performance.
4. Rebuild and rerun your script to confirm that performance has improved.

Here is an example of the kind of optimizations that have been shown to improve performance in this codebase:

<example_optimization_diff>
diff --git a/benchmark/kernels/fused_moe_triton/benchmark_ep_pre_reorder_triton.py b/benchmark/kernels/fused_moe_triton/benchmark_ep_pre_reorder_triton.py
new file mode 100644
index 000000000..c62424357
--- /dev/null
+++ b/benchmark/kernels/fused_moe_triton/benchmark_ep_pre_reorder_triton.py
@@ -0,0 +1,100 @@
+import argparse
+import itertools
+
+import pandas as pd
+import torch
+import triton
+
+from sglang.srt.layers.moe.ep_moe.kernels import pre_reorder_triton_kernel
+
+
+def benchmark_pre_reorder(batch_size, topk, model_config):
+    hidden_size = model_config["hidden_size"]
+    block_size = model_config["block_size"]
+    expert_range = model_config["expert_range"]
+
+    input_ptr = torch.randn(batch_size, hidden_size, dtype=torch.float16, device="cuda")
+    gateup_input_ptr = torch.zeros(
+        batch_size * topk, hidden_size, dtype=torch.float16, device="cuda"
+    )
+    src2dst_ptr = torch.randint(
+        0, batch_size * topk, (batch_size, topk), dtype=torch.int32, device="cuda"
+    )
+    topk_ids_ptr = torch.randint(
+        expert_range[0],
</example_optimization_diff>

IMPORTANT: The above diff is an EXAMPLE of optimizations that were successful in a different context.
These changes have NOT been applied to your codebase yet.
Your task is to:
1. Understand the optimization pattern shown (e.g., torch.zeros → torch.empty)
2. Look at the CURRENT code in the target files
3. Find places where you can apply SIMILAR optimizations
4. MAKE THE CHANGES yourself using str_replace_editor

The codebase you're working with is at the BASE commit - it does NOT have these optimizations yet.
You need to IMPLEMENT similar optimizations yourself.

HERE'S WHAT YOU NEED TO DO:
1. The files CURRENTLY contain torch.zeros() calls that need optimization
2. You need to CHANGE torch.zeros to torch.empty where appropriate
3. You need to REMOVE .fill_() operations that are unnecessary
4. These are NEW changes you're making - not already in the code

START WITH THIS COMMAND to see what needs changing:
```bash
grep -n 'torch.zeros\|fill_' vllm/model_executor/layers/fused_moe/moe_align_block_size.py benchmarks/kernels/benchmark_moe_align_block_size.py
```

CRITICAL: You MUST make actual code changes. Look for patterns like:
- Analyze the target files for performance bottlenecks
- Look for unnecessary memory allocations or initializations
- Consider more efficient algorithms or data structures

Target files to optimize:
- benchmark/kernels/fused_moe_triton/benchmark_ep_pre_reorder_triton.py
- python/sglang/srt/layers/moe/ep_moe/kernels.py

IMPORTANT: You MUST make actual code changes to at least one file.
The task will fail if no files are modified.

## Constraints
- No public API breakage
- All TestPack checks must pass

## Target Files (ONLY modify these)
- `benchmark/kernels/fused_moe_triton/benchmark_ep_pre_reorder_triton.py`
- `python/sglang/srt/layers/moe/ep_moe/kernels.py`

## SPECIFIC OPTIMIZATION TARGETS:
Based on the human commit analysis, focus on these areas:
- Memory allocation patterns (torch.zeros vs torch.empty)
- Tensor initialization strategies
- Kernel parameter optimization
- Buffer reuse and caching

### Human Developer's Approach:
```
Refine pre_reorder_triton_kernel slightly to improve performance (#6627)

Co-authored-by: luoyuan.luo <luoyuan.luo@antgroup.com>
```

### Files Modified (statistics):
```
.../benchmark_ep_pre_reorder_triton.py             | 100 +++++++++++++++++++++
 python/sglang/srt/layers/moe/ep_moe/kernels.py     |  13 ++-
 2 files changed, 109 insertions(+), 4 deletions(-)
```

## IMMEDIATE ACTION REQUIREMENTS:
1. Start editing files by iteration 3
2. Create and run /home/ubuntu/OmniPerf-Bench/perf-agents-bench/.work/worktrees/sglang_core/sglang_core-0060/.bench_scratch/test_opt.py before and after edits (do not create timing scripts outside /home/ubuntu/OmniPerf-Bench/perf-agents-bench/.work/worktrees/sglang_core/sglang_core-0060/.bench_scratch)
3. Make at least 3 concrete optimizations
4. Commit changes by iteration 8
5. Use finish command by iteration 10

## TASK COMPLETION COMMAND:
When you have made optimizations:
```bash
git add -A
git reset -q .bench_scratch || true
git commit -m 'Optimize MoE align sum kernels performance'
git diff $(git merge-base HEAD origin/HEAD || git rev-parse HEAD~1) -- . ':(exclude).bench_scratch' > /home/ubuntu/OmniPerf-Bench/perf-agents-bench/.work/worktrees/sglang_core/sglang_core-0060/model_patch.diff || true
finish
```

START IMPLEMENTING IMMEDIATELY. NO MORE ANALYSIS.

mcp startup: no servers

thinking
**Exploring code impact**

I'm trying to understand how modifying the sglang code could affect the runtime of the test script, specifically regarding vllm's moe_align_block_size. It seems like the repository could be vllm-based with sglang being a fork. However, I need to confirm if sglang defines a function that vllm imports. I'll explore the repo using the developer tool guidelines and prepare to run rg to investigate further. Let's proceed!
codex
I’ll first scan the repo for the referenced files and occurrences of torch.zeros/fill_ to pinpoint optimization spots, then create a local benchmark script under .bench_scratch to measure baseline performance.
exec
bash -lc 'ls -la' in /home/ubuntu/OmniPerf-Bench/perf-agents-bench/.work/worktrees/sglang_core/sglang_core-0060 succeeded in 10ms:
total 108
drwxrwxr-x  15 ubuntu ubuntu  4096 Nov 21 06:25 .
drwxrwxr-x 142 ubuntu ubuntu  4096 Nov 21 06:25 ..
-rw-rw-r--   1 ubuntu ubuntu    35 Nov 21 06:25 .clang-format-ignore
drwxrwxr-x   2 ubuntu ubuntu  4096 Nov 21 06:25 .devcontainer
-rw-rw-r--   1 ubuntu ubuntu   341 Nov 21 06:25 .editorconfig
-rw-rw-r--   1 ubuntu ubuntu    75 Nov 21 06:25 .git
drwxrwxr-x   4 ubuntu ubuntu  4096 Nov 21 06:25 .github
-rw-rw-r--   1 ubuntu ubuntu  3695 Nov 21 06:25 .gitignore
-rw-rw-r--   1 ubuntu ubuntu     0 Nov 21 06:25 .gitmodules
-rw-rw-r--   1 ubuntu ubuntu    50 Nov 21 06:25 .isort.cfg
-rw-rw-r--   1 ubuntu ubuntu  1568 Nov 21 06:25 .pre-commit-config.yaml
drwxrwxr-x   3 ubuntu ubuntu  4096 Nov 21 06:25 3rdparty
-rw-rw-r--   1 ubuntu ubuntu 11346 Nov 21 06:25 LICENSE
-rw-rw-r--   1 ubuntu ubuntu  1759 Nov 21 06:25 Makefile
-rw-rw-r--   1 ubuntu ubuntu  6838 Nov 21 06:25 README.md
drwxrwxr-x   2 ubuntu ubuntu  4096 Nov 21 06:25 assets
drwxrwxr-x  32 ubuntu ubuntu  4096 Nov 21 06:25 benchmark
drwxrwxr-x   2 ubuntu ubuntu  4096 Nov 21 06:25 docker
drwxrwxr-x   9 ubuntu ubuntu  4096 Nov 21 06:25 docs


thinking
**Evaluating testing setup**

I need to create a .bench_scratch and a test script to replicate and time the example. The test script imports the vllm function, but I'm not sure if vllm is installed. Maybe I can pip install it since the network is allowed. However, the instructions emphasize optimizing code within the current repository, which might involve sglang repo's moe kernels. But the test script uses a function from vllm called fused_moe, which could be independent of the sglang kernels. It looks like I need to implement these changes for overall performance optimization. The final steps involve committing changes and producing a model_patch.diff, ensuring the test script is adaptable in case vllm isn't available.
thinking
**Testing script replication**

The user provided a test script, which means I should replicate its import exactly. If it fails, I’ll need to install vllm or verify the path hooking. However, I can’t modify the environment outside of what's allowed. Since the network is enabled, I could either pip install vllm or adjust sys.path to include the necessary paths if the code exists. I think my next step is to search for vllm in the repository to see if it's available.
exec
bash -lc "rg -n \"vllm\\.model_executor\\.layers\\.fused_moe|moe_align_block_size\" -S || true" in /home/ubuntu/OmniPerf-Bench/perf-agents-bench/.work/worktrees/sglang_core/sglang_core-0060 succeeded in 21ms:
sgl-kernel/tests/test_moe_align.py:7:from sgl_kernel import moe_align_block_size
sgl-kernel/tests/test_moe_align.py:15:def moe_align_block_size_stage1(
sgl-kernel/tests/test_moe_align.py:34:def moe_align_block_size_stage2(
sgl-kernel/tests/test_moe_align.py:47:def moe_align_block_size_stage3(
sgl-kernel/tests/test_moe_align.py:64:def moe_align_block_size_stage4(
sgl-kernel/tests/test_moe_align.py:93:def moe_align_block_size_triton(
sgl-kernel/tests/test_moe_align.py:109:    moe_align_block_size_stage1[grid](
sgl-kernel/tests/test_moe_align.py:116:    moe_align_block_size_stage2[grid](
sgl-kernel/tests/test_moe_align.py:120:    moe_align_block_size_stage3[(1,)](
sgl-kernel/tests/test_moe_align.py:127:    moe_align_block_size_stage4[grid](
sgl-kernel/tests/test_moe_align.py:151:def test_moe_align_block_size_compare_implementations(
sgl-kernel/tests/test_moe_align.py:189:    moe_align_block_size(
sgl-kernel/tests/test_moe_align.py:200:    moe_align_block_size_triton(
sgl-kernel/include/sgl_kernel_ops.h:187:void moe_align_block_size(
sgl-kernel/csrc/moe/moe_align_kernel.cu:43:__global__ void moe_align_block_size_kernel(
sgl-kernel/csrc/moe/moe_align_kernel.cu:102:__global__ void moe_align_block_size_small_batch_expert_kernel(
sgl-kernel/csrc/moe/moe_align_kernel.cu:160:void moe_align_block_size(
sgl-kernel/csrc/moe/moe_align_kernel.cu:178:  DISPATCH_INTEGRAL_TYPES(topk_ids.scalar_type(), "moe_align_block_size_kernel", [&] {
sgl-kernel/csrc/moe/moe_align_kernel.cu:185:      auto small_batch_expert_kernel = moe_align_block_size_small_batch_expert_kernel<scalar_t>;
sgl-kernel/csrc/moe/moe_align_kernel.cu:195:      auto align_kernel = moe_align_block_size_kernel<scalar_t>;


thinking
**Optimizing code structure**

I see that the repo includes various versions of kernel functions and a benchmark file that compares different implementations. My focus should be on optimizing the sglang kernels used in the fused moe implementation, but there's some uncertainty since the test script imports from vllm. I'm curious if the vllm setup might affect which kernels are being called. The clear instruction targets certain files for optimization, so I’ll replace specific elements accordingly and look for improvements as needed.
thinking
**Checking paths for changes**

I see there’s an initial command suggested to identify what needs changing in the code. It involves using `grep` to search for specific terms within certain files. However, I’m noting that those paths might not exist in this particular repository. Still, I should run that command to check and confirm what’s there. It’s a good first step to make sure I’m looking at the right files before proceeding with any modifications!
exec
bash -lc "grep -n 'torch.zeros\\|fill_' vllm/model_executor/layers/fused_moe/moe_align_block_size.py benchmarks/kernels/benchmark_moe_align_block_size.py || true" in /home/ubuntu/OmniPerf-Bench/perf-agents-bench/.work/worktrees/sglang_core/sglang_core-0060 succeeded in 10ms:
grep: vllm/model_executor/layers/fused_moe/moe_align_block_size.py: No such file or directory
grep: benchmarks/kernels/benchmark_moe_align_block_size.py: No such file or directory
exec
bash -lc "sed -n '1,200p' benchmark/kernels/fused_moe_triton/benchmark_ep_pre_reorder_triton.py" in /home/ubuntu/OmniPerf-Bench/perf-agents-bench/.work/worktrees/sglang_core/sglang_core-0060 exited 2 in 10ms:
sed: can't read benchmark/kernels/fused_moe_triton/benchmark_ep_pre_reorder_triton.py: No such file or directory

thinking
**Clarifying benchmark path**

I'm noticing that the benchmark file path uses the singular 'benchmark' instead of 'benchmarks.' In the repository listing, there’s a 'benchmark' directory, which seems consistent. However, there's also a 'benchmark/kernels/fused_moe_triton' that might be relevant. I should list these paths to have a clearer overview of what's available. This will help in ensuring I'm on track with what I'm supposed to be analyzing!
exec
bash -lc "ls -la benchmark/kernels || true && rg -n \"pre_reorder|ep_moe|kernels\\.py\" -S benchmark python | head -n 200" in /home/ubuntu/OmniPerf-Bench/perf-agents-bench/.work/worktrees/sglang_core/sglang_core-0060 succeeded in 22ms:
total 36
drwxrwxr-x  9 ubuntu ubuntu 4096 Nov 21 06:25 .
drwxrwxr-x 32 ubuntu ubuntu 4096 Nov 21 06:25 ..
drwxrwxr-x  2 ubuntu ubuntu 4096 Nov 21 06:25 decoding_attention_triton
drwxrwxr-x  2 ubuntu ubuntu 4096 Nov 21 06:25 deepseek
drwxrwxr-x  2 ubuntu ubuntu 4096 Nov 21 06:25 fused_moe_triton
drwxrwxr-x  2 ubuntu ubuntu 4096 Nov 21 06:25 minmax-text-01-lightning_attention
drwxrwxr-x  2 ubuntu ubuntu 4096 Nov 21 06:25 quantization
drwxrwxr-x  2 ubuntu ubuntu 4096 Nov 21 06:25 rmsnorm
drwxrwxr-x  2 ubuntu ubuntu 4096 Nov 21 06:25 scheduler_batch
python/sglang/test/runners.py:468:        enable_ep_moe: bool = False,
python/sglang/test/runners.py:509:            enable_ep_moe=enable_ep_moe,
python/sglang/test/test_block_fp8_ep.py:8:from sglang.srt.layers.moe.ep_moe.kernels import (
python/sglang/test/test_block_fp8_ep.py:11:    pre_reorder_triton_kernel,
python/sglang/test/test_block_fp8_ep.py:20:def ep_moe(
python/sglang/test/test_block_fp8_ep.py:76:    pre_reorder_triton_kernel[(hidden_states.shape[0],)](
python/sglang/test/test_block_fp8_ep.py:253:    def _w8a8_block_fp8_ep_moe(
python/sglang/test/test_block_fp8_ep.py:296:            out = ep_moe(
python/sglang/test/test_block_fp8_ep.py:312:            ref_out = ep_moe(
python/sglang/test/test_block_fp8_ep.py:334:    def test_w8a8_block_fp8_ep_moe(self):
exec
bash -lc "sed -n '1,250p' python/sglang/srt/layers/moe/ep_moe/kernels.py" in /home/ubuntu/OmniPerf-Bench/perf-agents-bench/.work/worktrees/sglang_core/sglang_core-0060 succeeded in 10ms:
import logging
from typing import List, Optional

import torch
import triton

from sglang.srt.layers.quantization.fp8_kernel import per_token_group_quant_fp8
from sglang.srt.utils import dispose_tensor, is_cuda

logger = logging.getLogger(__name__)

_is_cuda = is_cuda()
if _is_cuda:
    from sglang.srt.layers.quantization.fp8_kernel import (
        sglang_per_token_group_quant_fp8 as per_token_group_quant_fp8,
    )

    try:
        from deep_gemm import ceil_div
    except ImportError:
exec
bash -lc "sed -n '250,750p' python/sglang/srt/layers/moe/ep_moe/kernels.py" in /home/ubuntu/OmniPerf-Bench/perf-agents-bench/.work/worktrees/sglang_core/sglang_core-0060 succeeded in 10ms:
            tl.store(down_input_ptr + offset, silu_mul_output, mask=mask)


# copy from https://github.com/ModelTC/lightllm/blob/a000ab69098654df4731f5b12587dd4e7f0a4f41/lightllm/common/fused_moe/moe_silu_and_mul_mix_quant_ep.py
@triton.jit
def _silu_and_mul_post_quant_kernel(
    input_ptr,
    stride_input_0,
    stride_input_1,
    stride_input_2,
    output_ptr,
    stride_output_0,
    stride_output_1,
    stride_output_2,
    output_scale_ptr,
    stride_output_scale_0,
    stride_output_scale_1,
    stride_output_scale_2,
    masked_m_ptr,
    size_n,

thinking
**Optimizing the test script**

I’m considering that the harness might have vllm preinstalled, which is important since the user wants to optimize the script's runtime. If vllm isn't available, I could fallback on sgl_kernel's moe_align_block_size. Since external packages can't be changed, I'll create the test script to specifically import vllm. If that fails, I'll use the alternatives. It seems that "OmniPerf-Bench" likely includes vllm, so I’ll start by creating the .bench_scratch directory before implementing the script.
exec
bash -lc "mkdir -p .bench_scratch && python - << 'PY'
import torch, time
from vllm.model_executor.layers.fused_moe import moe_align_block_size
num_tokens=4096; num_experts=64; topk=2; block_size=128
x=torch.randint(0,num_experts,(num_tokens*topk,),dtype=torch.int32,device='cuda')
torch.cuda.synchronize(); t=time.time()
moe_align_block_size(x,num_experts,block_size,topk)
torch.cuda.synchronize(); print('Baseline duration:', time.time()-t)
PY" in /home/ubuntu/OmniPerf-Bench/perf-agents-bench/.work/worktrees/sglang_core/sglang_core-0060 exited 1 in 1.44s:
/home/ubuntu/OmniPerf-Bench/bench-env/lib/python3.12/site-packages/torch/cuda/__init__.py:61: FutureWarning: The pynvml package is deprecated. Please install nvidia-ml-py instead. If you did not install pynvml directly, please report this to the maintainers of the package that installed pynvml for you.
  import pynvml  # type: ignore[import]
Traceback (most recent call last):
  File "<stdin>", line 2, in <module>
ModuleNotFoundError: No module named 'vllm.model_executor'

thinking
**Examining index allocation**

I'm focusing on the idea that only index 0 should be set to zero and that other indices might need to be overwritten. My plan is to allocate an empty space first and then set the first element to zero. I suspect there might be other calls to torch.zeros, so I should search through kernels.py to identify those instances. It's essential to ensure everything lines up correctly in the code.
exec
bash -lc "rg -n \"torch\\.zeros|fill_\\(\" python/sglang/srt/layers/moe/ep_moe/kernels.py" in /home/ubuntu/OmniPerf-Bench/perf-agents-bench/.work/worktrees/sglang_core/sglang_core-0060 succeeded in 11ms:
154:    seg_indptr = torch.zeros(num_experts + 1, device=topk_ids.device, dtype=torch.int64)
682:    m_num_tiles_indptr = torch.zeros(batch_size + 1, device=a.device, dtype=torch.int64)
exec
bash -lc "rg -n \"fill_\\(\" python/sglang/srt/layers/moe/ep_moe/kernels.py || true" in /home/ubuntu/OmniPerf-Bench/perf-agents-bench/.work/worktrees/sglang_core/sglang_core-0060 succeeded in 11ms:


thinking
**Considering configuration changes**

I'm thinking about reducing default config block sizes and also looking at the example commit, "Refine pre_reorder_triton_kernel slightly." It seems like that change only applies to the kernel itself. I’ll need to implement a restructure while keeping the semantics intact. My next step is to prepare the necessary patch to reflect these changes. So there's a clear plan forming here! Let's make sure everything is set up correctly.
file update
M /home/ubuntu/OmniPerf-Bench/perf-agents-bench/.work/worktrees/sglang_core/sglang_core-0060/python/sglang/srt/layers/moe/ep_moe/kernels.py
@@ -153,3 +153,5 @@
     reorder_topk_ids, reorder_ids = torch.sort(topk_ids.view(-1), stable=True)
-    seg_indptr = torch.zeros(num_experts + 1, device=topk_ids.device, dtype=torch.int64)
+    # Only seg_indptr[0] needs to be zero-initialized; others are written by the kernel.
+    seg_indptr = torch.empty(num_experts + 1, device=topk_ids.device, dtype=torch.int64)
+    seg_indptr[0] = 0
     src2dst = torch.empty(topk_ids.numel(), device=topk_ids.device, dtype=torch.int32)
@@ -188,16 +190,21 @@
     src_ptr = input_ptr + src_idx * hidden_size
-    for idx in range(topk):
-        expert_id = tl.load(topk_ids_ptr + idx)
-        if expert_id >= start_expert_id and expert_id <= end_expert_id:
-            if a1_scales_ptr is not None:
-                scale = 1.0 / tl.load(a1_scales_ptr + expert_id - start_expert_id)
-            else:
-                scale = 1.0

-            dst_idx = tl.load(src2dst_ptr + idx)
-            dst_ptr = gateup_input_ptr + dst_idx * hidden_size
-            for start_offset in tl.range(0, hidden_size, BLOCK_SIZE):
-                offset = start_offset + tl.arange(0, BLOCK_SIZE)
-                mask = offset < hidden_size
-                in_data = tl.load(src_ptr + offset, mask=mask).to(tl.float32)
+    # Iterate over the hidden dimension first to reuse the loaded input
+    # across all top-k destinations. This reduces redundant global reads
+    # of the same source vector by a factor of topk.
+    for start_offset in tl.range(0, hidden_size, BLOCK_SIZE):
+        offset = start_offset + tl.arange(0, BLOCK_SIZE)
+        mask = offset < hidden_size
+        in_data = tl.load(src_ptr + offset, mask=mask).to(tl.float32)
+
+        for idx in range(topk):
+            expert_id = tl.load(topk_ids_ptr + idx)
+            if expert_id >= start_expert_id and expert_id <= end_expert_id:
+                if a1_scales_ptr is not None:
+                    scale = 1.0 / tl.load(a1_scales_ptr + expert_id - start_expert_id)
+                else:
+                    scale = 1.0
+
+                dst_idx = tl.load(src2dst_ptr + idx)
+                dst_ptr = gateup_input_ptr + dst_idx * hidden_size
                 out_data = (in_data * scale).to(OutDtype)
@@ -681,3 +688,6 @@

-    m_num_tiles_indptr = torch.zeros(batch_size + 1, device=a.device, dtype=torch.int64)
+    # Only the first element is required to be zero; subsequent elements
+    # are overwritten by the kernel. Avoid clearing the whole buffer.
+    m_num_tiles_indptr = torch.empty(batch_size + 1, device=a.device, dtype=torch.int64)
+    m_num_tiles_indptr[0] = 0
     compute_m_num_tiles_indptr[(1,)](
apply_patch(auto_approved=true) exited 0 in 9ms:
Success. Updated the following files:
M python/sglang/srt/layers/moe/ep_moe/kernels.py
file update:
diff --git a/python/sglang/srt/layers/moe/ep_moe/kernels.py b/python/sglang/srt/layers/moe/ep_moe/kernels.py
index 8c005527a7c58f4f4d13189a6b6ca992016b3c1f..15a07d85e655ce1b391eb0e1ed294f9aa81b6f7f
--- a/python/sglang/srt/layers/moe/ep_moe/kernels.py
+++ b/python/sglang/srt/layers/moe/ep_moe/kernels.py
@@ -151,7 +151,9 @@

 def run_moe_ep_preproess(topk_ids: torch.Tensor, num_experts: int):
     reorder_topk_ids, reorder_ids = torch.sort(topk_ids.view(-1), stable=True)
-    seg_indptr = torch.zeros(num_experts + 1, device=topk_ids.device, dtype=torch.int64)
+    # Only seg_indptr[0] needs to be zero-initialized; others are written by the kernel.
+    seg_indptr = torch.empty(num_experts + 1, device=topk_ids.device, dtype=torch.int64)
+    seg_indptr[0] = 0
     src2dst = torch.empty(topk_ids.numel(), device=topk_ids.device, dtype=torch.int32)

     compute_seg_indptr_triton_kernel[(num_experts,)](
@@ -186,20 +188,25 @@
     topk_ids_ptr = topk_ids_ptr + src_idx * topk

     src_ptr = input_ptr + src_idx * hidden_size
-    for idx in range(topk):
-        expert_id = tl.load(topk_ids_ptr + idx)
-        if expert_id >= start_expert_id and expert_id <= end_expert_id:
-            if a1_scales_ptr is not None:
-                scale = 1.0 / tl.load(a1_scales_ptr + expert_id - start_expert_id)
-            else:
-                scale = 1.0

-            dst_idx = tl.load(src2dst_ptr + idx)
-            dst_ptr = gateup_input_ptr + dst_idx * hidden_size
-            for start_offset in tl.range(0, hidden_size, BLOCK_SIZE):
-                offset = start_offset + tl.arange(0, BLOCK_SIZE)
-                mask = offset < hidden_size
-                in_data = tl.load(src_ptr + offset, mask=mask).to(tl.float32)
+    # Iterate over the hidden dimension first to reuse the loaded input
+    # across all top-k destinations. This reduces redundant global reads
+    # of the same source vector by a factor of topk.
+    for start_offset in tl.range(0, hidden_size, BLOCK_SIZE):
+        offset = start_offset + tl.arange(0, BLOCK_SIZE)
+        mask = offset < hidden_size
+        in_data = tl.load(src_ptr + offset, mask=mask).to(tl.float32)
+
+        for idx in range(topk):
+            expert_id = tl.load(topk_ids_ptr + idx)
+            if expert_id >= start_expert_id and expert_id <= end_expert_id:
+                if a1_scales_ptr is not None:
+                    scale = 1.0 / tl.load(a1_scales_ptr + expert_id - start_expert_id)
+                else:
+                    scale = 1.0
+
+                dst_idx = tl.load(src2dst_ptr + idx)
+                dst_ptr = gateup_input_ptr + dst_idx * hidden_size
                 out_data = (in_data * scale).to(OutDtype)
                 tl.store(dst_ptr + offset, out_data, mask=mask)

@@ -679,7 +686,10 @@
         "BLOCK_SIZE_K": 128,
     }

-    m_num_tiles_indptr = torch.zeros(batch_size + 1, device=a.device, dtype=torch.int64)
+    # Only the first element is required to be zero; subsequent elements
+    # are overwritten by the kernel. Avoid clearing the whole buffer.
+    m_num_tiles_indptr = torch.empty(batch_size + 1, device=a.device, dtype=torch.int64)
+    m_num_tiles_indptr[0] = 0
     compute_m_num_tiles_indptr[(1,)](
         m_num_tiles_indptr, seg_indptr, batch_size, config["BLOCK_SIZE_M"]
     )

file update:
diff --git a/python/sglang/srt/layers/moe/ep_moe/kernels.py b/python/sglang/srt/layers/moe/ep_moe/kernels.py
index 8c005527a7c58f4f4d13189a6b6ca992016b3c1f..15a07d85e655ce1b391eb0e1ed294f9aa81b6f7f
--- a/python/sglang/srt/layers/moe/ep_moe/kernels.py
+++ b/python/sglang/srt/layers/moe/ep_moe/kernels.py
@@ -151,7 +151,9 @@

 def run_moe_ep_preproess(topk_ids: torch.Tensor, num_experts: int):
     reorder_topk_ids, reorder_ids = torch.sort(topk_ids.view(-1), stable=True)
-    seg_indptr = torch.zeros(num_experts + 1, device=topk_ids.device, dtype=torch.int64)
+    # Only seg_indptr[0] needs to be zero-initialized; others are written by the kernel.
+    seg_indptr = torch.empty(num_experts + 1, device=topk_ids.device, dtype=torch.int64)
+    seg_indptr[0] = 0
     src2dst = torch.empty(topk_ids.numel(), device=topk_ids.device, dtype=torch.int32)

     compute_seg_indptr_triton_kernel[(num_experts,)](
@@ -186,20 +188,25 @@
     topk_ids_ptr = topk_ids_ptr + src_idx * topk

     src_ptr = input_ptr + src_idx * hidden_size
-    for idx in range(topk):
-        expert_id = tl.load(topk_ids_ptr + idx)
-        if expert_id >= start_expert_id and expert_id <= end_expert_id:
-            if a1_scales_ptr is not None:
-                scale = 1.0 / tl.load(a1_scales_ptr + expert_id - start_expert_id)
-            else:
-                scale = 1.0

-            dst_idx = tl.load(src2dst_ptr + idx)
-            dst_ptr = gateup_input_ptr + dst_idx * hidden_size
-            for start_offset in tl.range(0, hidden_size, BLOCK_SIZE):
-                offset = start_offset + tl.arange(0, BLOCK_SIZE)
-                mask = offset < hidden_size
-                in_data = tl.load(src_ptr + offset, mask=mask).to(tl.float32)
+    # Iterate over the hidden dimension first to reuse the loaded input
+    # across all top-k destinations. This reduces redundant global reads
+    # of the same source vector by a factor of topk.
+    for start_offset in tl.range(0, hidden_size, BLOCK_SIZE):
+        offset = start_offset + tl.arange(0, BLOCK_SIZE)
+        mask = offset < hidden_size
+        in_data = tl.load(src_ptr + offset, mask=mask).to(tl.float32)
+
+        for idx in range(topk):
+            expert_id = tl.load(topk_ids_ptr + idx)
+            if expert_id >= start_expert_id and expert_id <= end_expert_id:
+                if a1_scales_ptr is not None:
+                    scale = 1.0 / tl.load(a1_scales_ptr + expert_id - start_expert_id)
+                else:
+                    scale = 1.0
+
+                dst_idx = tl.load(src2dst_ptr + idx)
+                dst_ptr = gateup_input_ptr + dst_idx * hidden_size
                 out_data = (in_data * scale).to(OutDtype)
                 tl.store(dst_ptr + offset, out_data, mask=mask)

@@ -679,7 +686,10 @@
         "BLOCK_SIZE_K": 128,
     }

-    m_num_tiles_indptr = torch.zeros(batch_size + 1, device=a.device, dtype=torch.int64)
+    # Only the first element is required to be zero; subsequent elements
+    # are overwritten by the kernel. Avoid clearing the whole buffer.
+    m_num_tiles_indptr = torch.empty(batch_size + 1, device=a.device, dtype=torch.int64)
+    m_num_tiles_indptr[0] = 0
     compute_m_num_tiles_indptr[(1,)](
         m_num_tiles_indptr, seg_indptr, batch_size, config["BLOCK_SIZE_M"]
     )

file update
A /home/ubuntu/OmniPerf-Bench/perf-agents-bench/.work/worktrees/sglang_core/sglang_core-0060/benchmark/kernels/fused_moe_triton/benchmark_ep_pre_reorder_triton.py
import argparse
import time

import torch
import triton

from sglang.srt.layers.moe.ep_moe.kernels import pre_reorder_triton_kernel


def benchmark_pre_reorder(batch_size: int, topk: int, hidden_size: int, block_size: int):
    device = "cuda"

    # Inputs
    input_ptr = torch.randn(batch_size, hidden_size, dtype=torch.float16, device=device)

    # Use empty for outputs and index buffers to avoid unnecessary memset
    gateup_input_ptr = torch.empty(
        batch_size * topk, hidden_size, dtype=torch.float16, device=device
    )
    src2dst_ptr = torch.randint(
        0, batch_size * topk, (batch_size, topk), dtype=torch.int32, device=device
    )
    topk_ids_ptr = torch.randint(0, 1024, (batch_size, topk), dtype=torch.int32, device=device)

    # No scaling by default
    a1_scales_ptr = None
    start_expert_id = 0
    end_expert_id = 1023

    grid = (batch_size,)

    # Warmup
    for _ in range(5):
        pre_reorder_triton_kernel[grid](
            input_ptr,
            gateup_input_ptr,
            src2dst_ptr,
            topk_ids_ptr,
            a1_scales_ptr,
            start_expert_id,
            end_expert_id,
            topk,
            hidden_size,
            BLOCK_SIZE=block_size,
        )

    torch.cuda.synchronize()
    t0 = time.time()
    for _ in range(20):
        pre_reorder_triton_kernel[grid](
            input_ptr,
            gateup_input_ptr,
            src2dst_ptr,
            topk_ids_ptr,
            a1_scales_ptr,
            start_expert_id,
            end_expert_id,
            topk,
            hidden_size,
            BLOCK_SIZE=block_size,
        )
    torch.cuda.synchronize()
    return (time.time() - t0) / 20


def main():
    parser = argparse.ArgumentParser()
    parser.add_argument("--batch-size", type=int, default=4096)
    parser.add_argument("--topk", type=int, default=2)
    parser.add_argument("--hidden-size", type=int, default=4096)
    parser.add_argument("--block-size", type=int, default=128)
    args = parser.parse_args()

    dur = benchmark_pre_reorder(
        args.batch_size, args.topk, args.hidden_size, args.block_size
    )
    print(f"pre_reorder_triton_kernel avg time: {dur*1e3:.3f} ms")


if __name__ == "__main__":
    main()

apply_patch(auto_approved=true) exited 0 in 9ms:
Success. Updated the following files:
A benchmark/kernels/fused_moe_triton/benchmark_ep_pre_reorder_triton.py
file update:
diff --git a/benchmark/kernels/fused_moe_triton/benchmark_ep_pre_reorder_triton.py b/benchmark/kernels/fused_moe_triton/benchmark_ep_pre_reorder_triton.py
new file mode 100644
index 0000000000000000000000000000000000000000..75e09e3c272e3e2448f970ddbda7867bf74964a5
--- /dev/null
+++ b/benchmark/kernels/fused_moe_triton/benchmark_ep_pre_reorder_triton.py
@@ -0,0 +1,82 @@
+import argparse
+import time
+
+import torch
+import triton
+
+from sglang.srt.layers.moe.ep_moe.kernels import pre_reorder_triton_kernel
+
+
+def benchmark_pre_reorder(batch_size: int, topk: int, hidden_size: int, block_size: int):
+    device = "cuda"
+
+    # Inputs
+    input_ptr = torch.randn(batch_size, hidden_size, dtype=torch.float16, device=device)
+
+    # Use empty for outputs and index buffers to avoid unnecessary memset
+    gateup_input_ptr = torch.empty(
+        batch_size * topk, hidden_size, dtype=torch.float16, device=device
+    )
+    src2dst_ptr = torch.randint(
+        0, batch_size * topk, (batch_size, topk), dtype=torch.int32, device=device
+    )
+    topk_ids_ptr = torch.randint(0, 1024, (batch_size, topk), dtype=torch.int32, device=device)
+
+    # No scaling by default
+    a1_scales_ptr = None
+    start_expert_id = 0
+    end_expert_id = 1023
+
+    grid = (batch_size,)
+
+    # Warmup
+    for _ in range(5):
+        pre_reorder_triton_kernel[grid](
+            input_ptr,
+            gateup_input_ptr,
+            src2dst_ptr,
+            topk_ids_ptr,
+            a1_scales_ptr,
+            start_expert_id,
+            end_expert_id,
+            topk,
+            hidden_size,
+            BLOCK_SIZE=block_size,
+        )
+
+    torch.cuda.synchronize()
+    t0 = time.time()
+    for _ in range(20):
+        pre_reorder_triton_kernel[grid](
+            input_ptr,
+            gateup_input_ptr,
+            src2dst_ptr,
+            topk_ids_ptr,
+            a1_scales_ptr,
+            start_expert_id,
+            end_expert_id,
+            topk,
+            hidden_size,
+            BLOCK_SIZE=block_size,
+        )
+    torch.cuda.synchronize()
+    return (time.time() - t0) / 20
+
+
+def main():
+    parser = argparse.ArgumentParser()
+    parser.add_argument("--batch-size", type=int, default=4096)
+    parser.add_argument("--topk", type=int, default=2)
+    parser.add_argument("--hidden-size", type=int, default=4096)
+    parser.add_argument("--block-size", type=int, default=128)
+    args = parser.parse_args()
+
+    dur = benchmark_pre_reorder(
+        args.batch_size, args.topk, args.hidden_size, args.block_size
+    )
+    print(f"pre_reorder_triton_kernel avg time: {dur*1e3:.3f} ms")
+
+
+if __name__ == "__main__":
+    main()
+
diff --git a/python/sglang/srt/layers/moe/ep_moe/kernels.py b/python/sglang/srt/layers/moe/ep_moe/kernels.py
index 8c005527a7c58f4f4d13189a6b6ca992016b3c1f..15a07d85e655ce1b391eb0e1ed294f9aa81b6f7f
--- a/python/sglang/srt/layers/moe/ep_moe/kernels.py
+++ b/python/sglang/srt/layers/moe/ep_moe/kernels.py
@@ -151,7 +151,9 @@

 def run_moe_ep_preproess(topk_ids: torch.Tensor, num_experts: int):
     reorder_topk_ids, reorder_ids = torch.sort(topk_ids.view(-1), stable=True)
-    seg_indptr = torch.zeros(num_experts + 1, device=topk_ids.device, dtype=torch.int64)
+    # Only seg_indptr[0] needs to be zero-initialized; others are written by the kernel.
+    seg_indptr = torch.empty(num_experts + 1, device=topk_ids.device, dtype=torch.int64)
+    seg_indptr[0] = 0
     src2dst = torch.empty(topk_ids.numel(), device=topk_ids.device, dtype=torch.int32)

     compute_seg_indptr_triton_kernel[(num_experts,)](
@@ -186,20 +188,25 @@
     topk_ids_ptr = topk_ids_ptr + src_idx * topk

     src_ptr = input_ptr + src_idx * hidden_size
-    for idx in range(topk):
-        expert_id = tl.load(topk_ids_ptr + idx)
-        if expert_id >= start_expert_id and expert_id <= end_expert_id:
-            if a1_scales_ptr is not None:
-                scale = 1.0 / tl.load(a1_scales_ptr + expert_id - start_expert_id)
-            else:
-                scale = 1.0

-            dst_idx = tl.load(src2dst_ptr + idx)
-            dst_ptr = gateup_input_ptr + dst_idx * hidden_size
-            for start_offset in tl.range(0, hidden_size, BLOCK_SIZE):
-                offset = start_offset + tl.arange(0, BLOCK_SIZE)
-                mask = offset < hidden_size
-                in_data = tl.load(src_ptr + offset, mask=mask).to(tl.float32)
+    # Iterate over the hidden dimension first to reuse the loaded input
+    # across all top-k destinations. This reduces redundant global reads
+    # of the same source vector by a factor of topk.
+    for start_offset in tl.range(0, hidden_size, BLOCK_SIZE):
+        offset = start_offset + tl.arange(0, BLOCK_SIZE)
+        mask = offset < hidden_size
+        in_data = tl.load(src_ptr + offset, mask=mask).to(tl.float32)
+
+        for idx in range(topk):
+            expert_id = tl.load(topk_ids_ptr + idx)
+            if expert_id >= start_expert_id and expert_id <= end_expert_id:
+                if a1_scales_ptr is not None:
+                    scale = 1.0 / tl.load(a1_scales_ptr + expert_id - start_expert_id)
+                else:
+                    scale = 1.0
+
+                dst_idx = tl.load(src2dst_ptr + idx)
+                dst_ptr = gateup_input_ptr + dst_idx * hidden_size
                 out_data = (in_data * scale).to(OutDtype)
                 tl.store(dst_ptr + offset, out_data, mask=mask)

@@ -679,7 +686,10 @@
         "BLOCK_SIZE_K": 128,
     }

-    m_num_tiles_indptr = torch.zeros(batch_size + 1, device=a.device, dtype=torch.int64)
+    # Only the first element is required to be zero; subsequent elements
+    # are overwritten by the kernel. Avoid clearing the whole buffer.
+    m_num_tiles_indptr = torch.empty(batch_size + 1, device=a.device, dtype=torch.int64)
+    m_num_tiles_indptr[0] = 0
     compute_m_num_tiles_indptr[(1,)](
         m_num_tiles_indptr, seg_indptr, batch_size, config["BLOCK_SIZE_M"]
     )

file update:
diff --git a/benchmark/kernels/fused_moe_triton/benchmark_ep_pre_reorder_triton.py b/benchmark/kernels/fused_moe_triton/benchmark_ep_pre_reorder_triton.py
new file mode 100644
index 0000000000000000000000000000000000000000..75e09e3c272e3e2448f970ddbda7867bf74964a5
--- /dev/null
+++ b/benchmark/kernels/fused_moe_triton/benchmark_ep_pre_reorder_triton.py
@@ -0,0 +1,82 @@
+import argparse
+import time
+
+import torch
+import triton
+
+from sglang.srt.layers.moe.ep_moe.kernels import pre_reorder_triton_kernel
+
+
+def benchmark_pre_reorder(batch_size: int, topk: int, hidden_size: int, block_size: int):
+    device = "cuda"
+
+    # Inputs
+    input_ptr = torch.randn(batch_size, hidden_size, dtype=torch.float16, device=device)
+
+    # Use empty for outputs and index buffers to avoid unnecessary memset
+    gateup_input_ptr = torch.empty(
+        batch_size * topk, hidden_size, dtype=torch.float16, device=device
+    )
+    src2dst_ptr = torch.randint(
+        0, batch_size * topk, (batch_size, topk), dtype=torch.int32, device=device
+    )
+    topk_ids_ptr = torch.randint(0, 1024, (batch_size, topk), dtype=torch.int32, device=device)
+
+    # No scaling by default
+    a1_scales_ptr = None
+    start_expert_id = 0
+    end_expert_id = 1023
+
+    grid = (batch_size,)
+
+    # Warmup
+    for _ in range(5):
+        pre_reorder_triton_kernel[grid](
+            input_ptr,
+            gateup_input_ptr,
+            src2dst_ptr,
+            topk_ids_ptr,
+            a1_scales_ptr,
+            start_expert_id,
+            end_expert_id,
+            topk,
+            hidden_size,
+            BLOCK_SIZE=block_size,
+        )
+
+    torch.cuda.synchronize()
+    t0 = time.time()
+    for _ in range(20):
+        pre_reorder_triton_kernel[grid](
+            input_ptr,
+            gateup_input_ptr,
+            src2dst_ptr,
+            topk_ids_ptr,
+            a1_scales_ptr,
+            start_expert_id,
+            end_expert_id,
+            topk,
+            hidden_size,
+            BLOCK_SIZE=block_size,
+        )
+    torch.cuda.synchronize()
+    return (time.time() - t0) / 20
+
+
+def main():
+    parser = argparse.ArgumentParser()
+    parser.add_argument("--batch-size", type=int, default=4096)
+    parser.add_argument("--topk", type=int, default=2)
+    parser.add_argument("--hidden-size", type=int, default=4096)
+    parser.add_argument("--block-size", type=int, default=128)
+    args = parser.parse_args()
+
+    dur = benchmark_pre_reorder(
+        args.batch_size, args.topk, args.hidden_size, args.block_size
+    )
+    print(f"pre_reorder_triton_kernel avg time: {dur*1e3:.3f} ms")
+
+
+if __name__ == "__main__":
+    main()
+
diff --git a/python/sglang/srt/layers/moe/ep_moe/kernels.py b/python/sglang/srt/layers/moe/ep_moe/kernels.py
index 8c005527a7c58f4f4d13189a6b6ca992016b3c1f..15a07d85e655ce1b391eb0e1ed294f9aa81b6f7f
--- a/python/sglang/srt/layers/moe/ep_moe/kernels.py
+++ b/python/sglang/srt/layers/moe/ep_moe/kernels.py
@@ -151,7 +151,9 @@

 def run_moe_ep_preproess(topk_ids: torch.Tensor, num_experts: int):
     reorder_topk_ids, reorder_ids = torch.sort(topk_ids.view(-1), stable=True)
-    seg_indptr = torch.zeros(num_experts + 1, device=topk_ids.device, dtype=torch.int64)
+    # Only seg_indptr[0] needs to be zero-initialized; others are written by the kernel.
+    seg_indptr = torch.empty(num_experts + 1, device=topk_ids.device, dtype=torch.int64)
+    seg_indptr[0] = 0
     src2dst = torch.empty(topk_ids.numel(), device=topk_ids.device, dtype=torch.int32)

     compute_seg_indptr_triton_kernel[(num_experts,)](
@@ -186,20 +188,25 @@
     topk_ids_ptr = topk_ids_ptr + src_idx * topk

     src_ptr = input_ptr + src_idx * hidden_size
-    for idx in range(topk):
-        expert_id = tl.load(topk_ids_ptr + idx)
-        if expert_id >= start_expert_id and expert_id <= end_expert_id:
-            if a1_scales_ptr is not None:
-                scale = 1.0 / tl.load(a1_scales_ptr + expert_id - start_expert_id)
-            else:
-                scale = 1.0

-            dst_idx = tl.load(src2dst_ptr + idx)
-            dst_ptr = gateup_input_ptr + dst_idx * hidden_size
-            for start_offset in tl.range(0, hidden_size, BLOCK_SIZE):
-                offset = start_offset + tl.arange(0, BLOCK_SIZE)
-                mask = offset < hidden_size
-                in_data = tl.load(src_ptr + offset, mask=mask).to(tl.float32)
+    # Iterate over the hidden dimension first to reuse the loaded input
+    # across all top-k destinations. This reduces redundant global reads
+    # of the same source vector by a factor of topk.
+    for start_offset in tl.range(0, hidden_size, BLOCK_SIZE):
+        offset = start_offset + tl.arange(0, BLOCK_SIZE)
+        mask = offset < hidden_size
+        in_data = tl.load(src_ptr + offset, mask=mask).to(tl.float32)
+
+        for idx in range(topk):
+            expert_id = tl.load(topk_ids_ptr + idx)
+            if expert_id >= start_expert_id and expert_id <= end_expert_id:
+                if a1_scales_ptr is not None:
+                    scale = 1.0 / tl.load(a1_scales_ptr + expert_id - start_expert_id)
+                else:
+                    scale = 1.0
+
+                dst_idx = tl.load(src2dst_ptr + idx)
+                dst_ptr = gateup_input_ptr + dst_idx * hidden_size
                 out_data = (in_data * scale).to(OutDtype)
                 tl.store(dst_ptr + offset, out_data, mask=mask)

@@ -679,7 +686,10 @@
         "BLOCK_SIZE_K": 128,
     }

-    m_num_tiles_indptr = torch.zeros(batch_size + 1, device=a.device, dtype=torch.int64)
+    # Only the first element is required to be zero; subsequent elements
+    # are overwritten by the kernel. Avoid clearing the whole buffer.
+    m_num_tiles_indptr = torch.empty(batch_size + 1, device=a.device, dtype=torch.int64)
+    m_num_tiles_indptr[0] = 0
     compute_m_num_tiles_indptr[(1,)](
         m_num_tiles_indptr, seg_indptr, batch_size, config["BLOCK_SIZE_M"]
     )


thinking
**Creating the test script**

I need to create the .bench_scratch/test_opt.py file that reproduces the test. My plan is to write a script that first tries to import vllm; if that fails, it should fallback to sgl_kernel. I’ll implement it to run multiple iterations and print the time taken for each. It’s important to get this right so I can effectively measure the performance. Let’s get started on implementing this!
exec
bash -lc "cat > .bench_scratch/test_opt.py << 'PY'
import time
import torch

print('Using CUDA device:', torch.cuda.get_device_name(torch.cuda.current_device()))

num_tokens = 4096
num_experts = 64
topk = 2
block_size = 128

fn = None
provider = None
try:
    from vllm.model_executor.layers.fused_moe import moe_align_block_size as _fn
    fn = _fn
    provider = 'vllm'
except Exception as e:
    try:
        from sgl_kernel import moe_align_block_size as _fn
        fn = _fn
        provider = 'sgl_kernel'
    except Exception as e2:
        print('Neither vllm nor sgl_kernel moe_align_block_size is available:', e, e2)
        raise

# Create input data
x = torch.randint(0, num_experts, (num_tokens * topk,), dtype=torch.int32, device='cuda')

# Warmup
for _ in range(5):
    out = fn(x, num_experts, block_size, topk)

# Time the operation
torch.cuda.synchronize()
start = time.time()
for _ in range(50):
    out = fn(x, num_experts, block_size, topk)

torch.cuda.synchronize()
duration = (time.time() - start)/50.0

print(f\"Provider: {provider}\")
print(f\"Avg Duration: {duration:.6f} seconds\")
PY
python .bench_scratch/test_opt.py" in /home/ubuntu/OmniPerf-Bench/perf-agents-bench/.work/worktrees/sglang_core/sglang_core-0060 exited 1 in 1.43s:
/home/ubuntu/OmniPerf-Bench/bench-env/lib/python3.12/site-packages/torch/cuda/__init__.py:61: FutureWarning: The pynvml package is deprecated. Please install nvidia-ml-py instead. If you did not install pynvml directly, please report this to the maintainers of the package that installed pynvml for you.
  import pynvml  # type: ignore[import]
Traceback (most recent call last):
  File "/home/ubuntu/OmniPerf-Bench/perf-agents-bench/.work/worktrees/sglang_core/sglang_core-0060/.bench_scratch/test_opt.py", line 4, in <module>
    print('Using CUDA device:', torch.cuda.get_device_name(torch.cuda.current_device()))
                                                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/OmniPerf-Bench/bench-env/lib/python3.12/site-packages/torch/cuda/__init__.py", line 1026, in current_device
    _lazy_init()
  File "/home/ubuntu/OmniPerf-Bench/bench-env/lib/python3.12/site-packages/torch/cuda/__init__.py", line 372, in _lazy_init
    torch._C._cuda_init()
RuntimeError: Found no NVIDIA driver on your system. Please check that you have an NVIDIA GPU and installed a driver from http://www.nvidia.com/Download/index.aspx
file update:
diff --git a/benchmark/kernels/fused_moe_triton/benchmark_ep_pre_reorder_triton.py b/benchmark/kernels/fused_moe_triton/benchmark_ep_pre_reorder_triton.py
new file mode 100644
index 0000000000000000000000000000000000000000..75e09e3c272e3e2448f970ddbda7867bf74964a5
--- /dev/null
+++ b/benchmark/kernels/fused_moe_triton/benchmark_ep_pre_reorder_triton.py
@@ -0,0 +1,82 @@
+import argparse
+import time
+
+import torch
+import triton
+
+from sglang.srt.layers.moe.ep_moe.kernels import pre_reorder_triton_kernel
+
+
+def benchmark_pre_reorder(batch_size: int, topk: int, hidden_size: int, block_size: int):
+    device = "cuda"
+
+    # Inputs
+    input_ptr = torch.randn(batch_size, hidden_size, dtype=torch.float16, device=device)
+
+    # Use empty for outputs and index buffers to avoid unnecessary memset
+    gateup_input_ptr = torch.empty(
+        batch_size * topk, hidden_size, dtype=torch.float16, device=device
+    )
+    src2dst_ptr = torch.randint(
+        0, batch_size * topk, (batch_size, topk), dtype=torch.int32, device=device
+    )
+    topk_ids_ptr = torch.randint(0, 1024, (batch_size, topk), dtype=torch.int32, device=device)
+
+    # No scaling by default
+    a1_scales_ptr = None
+    start_expert_id = 0
+    end_expert_id = 1023
+
+    grid = (batch_size,)
+
+    # Warmup
+    for _ in range(5):
+        pre_reorder_triton_kernel[grid](
+            input_ptr,
+            gateup_input_ptr,
+            src2dst_ptr,
+            topk_ids_ptr,
+            a1_scales_ptr,
+            start_expert_id,
+            end_expert_id,
+            topk,
+            hidden_size,
+            BLOCK_SIZE=block_size,
+        )
+
+    torch.cuda.synchronize()
+    t0 = time.time()
+    for _ in range(20):
+        pre_reorder_triton_kernel[grid](
+            input_ptr,
+            gateup_input_ptr,
+            src2dst_ptr,
+            topk_ids_ptr,
+            a1_scales_ptr,
+            start_expert_id,
+            end_expert_id,
+            topk,
+            hidden_size,
+            BLOCK_SIZE=block_size,
+        )
+    torch.cuda.synchronize()
+    return (time.time() - t0) / 20
+
+
+def main():
+    parser = argparse.ArgumentParser()
+    parser.add_argument("--batch-size", type=int, default=4096)
+    parser.add_argument("--topk", type=int, default=2)
+    parser.add_argument("--hidden-size", type=int, default=4096)
+    parser.add_argument("--block-size", type=int, default=128)
+    args = parser.parse_args()
+
+    dur = benchmark_pre_reorder(
+        args.batch_size, args.topk, args.hidden_size, args.block_size
+    )
+    print(f"pre_reorder_triton_kernel avg time: {dur*1e3:.3f} ms")
+
+
+if __name__ == "__main__":
+    main()
+
diff --git a/python/sglang/srt/layers/moe/ep_moe/kernels.py b/python/sglang/srt/layers/moe/ep_moe/kernels.py
index 8c005527a7c58f4f4d13189a6b6ca992016b3c1f..15a07d85e655ce1b391eb0e1ed294f9aa81b6f7f
--- a/python/sglang/srt/layers/moe/ep_moe/kernels.py
+++ b/python/sglang/srt/layers/moe/ep_moe/kernels.py
@@ -151,7 +151,9 @@

 def run_moe_ep_preproess(topk_ids: torch.Tensor, num_experts: int):
     reorder_topk_ids, reorder_ids = torch.sort(topk_ids.view(-1), stable=True)
-    seg_indptr = torch.zeros(num_experts + 1, device=topk_ids.device, dtype=torch.int64)
+    # Only seg_indptr[0] needs to be zero-initialized; others are written by the kernel.
+    seg_indptr = torch.empty(num_experts + 1, device=topk_ids.device, dtype=torch.int64)
+    seg_indptr[0] = 0
     src2dst = torch.empty(topk_ids.numel(), device=topk_ids.device, dtype=torch.int32)

     compute_seg_indptr_triton_kernel[(num_experts,)](
@@ -186,20 +188,25 @@
     topk_ids_ptr = topk_ids_ptr + src_idx * topk

     src_ptr = input_ptr + src_idx * hidden_size
-    for idx in range(topk):
-        expert_id = tl.load(topk_ids_ptr + idx)
-        if expert_id >= start_expert_id and expert_id <= end_expert_id:
-            if a1_scales_ptr is not None:
-                scale = 1.0 / tl.load(a1_scales_ptr + expert_id - start_expert_id)
-            else:
-                scale = 1.0

-            dst_idx = tl.load(src2dst_ptr + idx)
-            dst_ptr = gateup_input_ptr + dst_idx * hidden_size
-            for start_offset in tl.range(0, hidden_size, BLOCK_SIZE):
-                offset = start_offset + tl.arange(0, BLOCK_SIZE)
-                mask = offset < hidden_size
-                in_data = tl.load(src_ptr + offset, mask=mask).to(tl.float32)
+    # Iterate over the hidden dimension first to reuse the loaded input
+    # across all top-k destinations. This reduces redundant global reads
+    # of the same source vector by a factor of topk.
+    for start_offset in tl.range(0, hidden_size, BLOCK_SIZE):
+        offset = start_offset + tl.arange(0, BLOCK_SIZE)
+        mask = offset < hidden_size
+        in_data = tl.load(src_ptr + offset, mask=mask).to(tl.float32)
+
+        for idx in range(topk):
+            expert_id = tl.load(topk_ids_ptr + idx)
+            if expert_id >= start_expert_id and expert_id <= end_expert_id:
+                if a1_scales_ptr is not None:
+                    scale = 1.0 / tl.load(a1_scales_ptr + expert_id - start_expert_id)
+                else:
+                    scale = 1.0
+
+                dst_idx = tl.load(src2dst_ptr + idx)
+                dst_ptr = gateup_input_ptr + dst_idx * hidden_size
                 out_data = (in_data * scale).to(OutDtype)
                 tl.store(dst_ptr + offset, out_data, mask=mask)

@@ -679,7 +686,10 @@
         "BLOCK_SIZE_K": 128,
     }

-    m_num_tiles_indptr = torch.zeros(batch_size + 1, device=a.device, dtype=torch.int64)
+    # Only the first element is required to be zero; subsequent elements
+    # are overwritten by the kernel. Avoid clearing the whole buffer.
+    m_num_tiles_indptr = torch.empty(batch_size + 1, device=a.device, dtype=torch.int64)
+    m_num_tiles_indptr[0] = 0
     compute_m_num_tiles_indptr[(1,)](
         m_num_tiles_indptr, seg_indptr, batch_size, config["BLOCK_SIZE_M"]
     )



thinking
**Evaluating performance testing**

It's surprising that CUDA isn't available in this environment, but I guess that's typical for an offline setup. The test script won't run because of the missing driver. Even so, the instructions say to create and run the script, so I'll complete the code modifications and commit. The creation succeeded, so if necessary, we can rebuild. Given our code is in Python and Triton, there's no build needed. However, we can't run performance tests due to the lack of GPUs, but I'll still proceed with the commit commands.
thinking
**Preparing for commit**

Next, I need to stage and commit changes, so I'll create the model_patch.diff at the specified path. It feels like a straightforward task, but I want to make sure everything is in order before I run the commit commands. It's essential for aligning with the task completion, and I’m being careful to follow instructions precisely. Let’s go ahead and execute those commit commands to finalize the process!
exec
bash -lc "git add -A
git reset -q .bench_scratch || true
git commit -m 'Optimize MoE align sum kernels performance'
git diff "'$(git merge-base HEAD origin/HEAD || git rev-parse HEAD~1) -- . '"':(exclude).bench_scratch' > /home/ubuntu/OmniPerf-Bench/perf-agents-bench/.work/worktrees/sglang_core/sglang_core-0060/model_patch.diff || true
finish" in /home/ubuntu/OmniPerf-Bench/perf-agents-bench/.work/worktrees/sglang_core/sglang_core-0060 exited 127 in 121ms:
[agent/sglang_core/c087ddd6 26f96e783] Optimize MoE align sum kernels performance
 Committer: Ubuntu <ubuntu@ip-172-31-16-122.ec2.internal>
Your name and email address were configured automatically based
on your username and hostname. Please check that they are accurate.
You can suppress this message by setting them explicitly. Run the
following command and follow the instructions in your editor to edit
your configuration file:

    git config --global --edit

After doing this, you may fix the identity used for this commit with:

    git commit --amend --reset-author

 2 files changed, 107 insertions(+), 15 deletions(-)
 create mode 100644 benchmark/kernels/fused_moe_triton/benchmark_ep_pre_reorder_triton.py
bash: line 5: finish: command not found
file update:
diff --git a/benchmark/kernels/fused_moe_triton/benchmark_ep_pre_reorder_triton.py b/benchmark/kernels/fused_moe_triton/benchmark_ep_pre_reorder_triton.py
new file mode 100644
index 0000000000000000000000000000000000000000..75e09e3c272e3e2448f970ddbda7867bf74964a5
--- /dev/null
+++ b/benchmark/kernels/fused_moe_triton/benchmark_ep_pre_reorder_triton.py
@@ -0,0 +1,82 @@
+import argparse
+import time
+
+import torch
+import triton
+
+from sglang.srt.layers.moe.ep_moe.kernels import pre_reorder_triton_kernel
+
+
+def benchmark_pre_reorder(batch_size: int, topk: int, hidden_size: int, block_size: int):
+    device = "cuda"
+
+    # Inputs
+    input_ptr = torch.randn(batch_size, hidden_size, dtype=torch.float16, device=device)
+
+    # Use empty for outputs and index buffers to avoid unnecessary memset
+    gateup_input_ptr = torch.empty(
+        batch_size * topk, hidden_size, dtype=torch.float16, device=device
+    )
+    src2dst_ptr = torch.randint(
+        0, batch_size * topk, (batch_size, topk), dtype=torch.int32, device=device
+    )
+    topk_ids_ptr = torch.randint(0, 1024, (batch_size, topk), dtype=torch.int32, device=device)
+
+    # No scaling by default
+    a1_scales_ptr = None
+    start_expert_id = 0
+    end_expert_id = 1023
+
+    grid = (batch_size,)
+
+    # Warmup
+    for _ in range(5):
+        pre_reorder_triton_kernel[grid](
+            input_ptr,
+            gateup_input_ptr,
+            src2dst_ptr,
+            topk_ids_ptr,
+            a1_scales_ptr,
+            start_expert_id,
+            end_expert_id,
+            topk,
+            hidden_size,
+            BLOCK_SIZE=block_size,
+        )
+
+    torch.cuda.synchronize()
+    t0 = time.time()
+    for _ in range(20):
+        pre_reorder_triton_kernel[grid](
+            input_ptr,
+            gateup_input_ptr,
+            src2dst_ptr,
+            topk_ids_ptr,
+            a1_scales_ptr,
+            start_expert_id,
+            end_expert_id,
+            topk,
+            hidden_size,
+            BLOCK_SIZE=block_size,
+        )
+    torch.cuda.synchronize()
+    return (time.time() - t0) / 20
+
+
+def main():
+    parser = argparse.ArgumentParser()
+    parser.add_argument("--batch-size", type=int, default=4096)
+    parser.add_argument("--topk", type=int, default=2)
+    parser.add_argument("--hidden-size", type=int, default=4096)
+    parser.add_argument("--block-size", type=int, default=128)
+    args = parser.parse_args()
+
+    dur = benchmark_pre_reorder(
+        args.batch_size, args.topk, args.hidden_size, args.block_size
+    )
+    print(f"pre_reorder_triton_kernel avg time: {dur*1e3:.3f} ms")
+
+
+if __name__ == "__main__":
+    main()
+
diff --git a/python/sglang/srt/layers/moe/ep_moe/kernels.py b/python/sglang/srt/layers/moe/ep_moe/kernels.py
index 8c005527a7c58f4f4d13189a6b6ca992016b3c1f..15a07d85e655ce1b391eb0e1ed294f9aa81b6f7f
--- a/python/sglang/srt/layers/moe/ep_moe/kernels.py
+++ b/python/sglang/srt/layers/moe/ep_moe/kernels.py
@@ -151,7 +151,9 @@

 def run_moe_ep_preproess(topk_ids: torch.Tensor, num_experts: int):
     reorder_topk_ids, reorder_ids = torch.sort(topk_ids.view(-1), stable=True)
-    seg_indptr = torch.zeros(num_experts + 1, device=topk_ids.device, dtype=torch.int64)
+    # Only seg_indptr[0] needs to be zero-initialized; others are written by the kernel.
+    seg_indptr = torch.empty(num_experts + 1, device=topk_ids.device, dtype=torch.int64)
+    seg_indptr[0] = 0
     src2dst = torch.empty(topk_ids.numel(), device=topk_ids.device, dtype=torch.int32)

     compute_seg_indptr_triton_kernel[(num_experts,)](
@@ -186,20 +188,25 @@
     topk_ids_ptr = topk_ids_ptr + src_idx * topk

     src_ptr = input_ptr + src_idx * hidden_size
-    for idx in range(topk):
-        expert_id = tl.load(topk_ids_ptr + idx)
-        if expert_id >= start_expert_id and expert_id <= end_expert_id:
-            if a1_scales_ptr is not None:
-                scale = 1.0 / tl.load(a1_scales_ptr + expert_id - start_expert_id)
-            else:
-                scale = 1.0

-            dst_idx = tl.load(src2dst_ptr + idx)
-            dst_ptr = gateup_input_ptr + dst_idx * hidden_size
-            for start_offset in tl.range(0, hidden_size, BLOCK_SIZE):
-                offset = start_offset + tl.arange(0, BLOCK_SIZE)
-                mask = offset < hidden_size
-                in_data = tl.load(src_ptr + offset, mask=mask).to(tl.float32)
+    # Iterate over the hidden dimension first to reuse the loaded input
+    # across all top-k destinations. This reduces redundant global reads
+    # of the same source vector by a factor of topk.
+    for start_offset in tl.range(0, hidden_size, BLOCK_SIZE):
+        offset = start_offset + tl.arange(0, BLOCK_SIZE)
+        mask = offset < hidden_size
+        in_data = tl.load(src_ptr + offset, mask=mask).to(tl.float32)
+
+        for idx in range(topk):
+            expert_id = tl.load(topk_ids_ptr + idx)
+            if expert_id >= start_expert_id and expert_id <= end_expert_id:
+                if a1_scales_ptr is not None:
+                    scale = 1.0 / tl.load(a1_scales_ptr + expert_id - start_expert_id)
+                else:
+                    scale = 1.0
+
+                dst_idx = tl.load(src2dst_ptr + idx)
+                dst_ptr = gateup_input_ptr + dst_idx * hidden_size
                 out_data = (in_data * scale).to(OutDtype)
                 tl.store(dst_ptr + offset, out_data, mask=mask)

@@ -679,7 +686,10 @@
         "BLOCK_SIZE_K": 128,
     }

-    m_num_tiles_indptr = torch.zeros(batch_size + 1, device=a.device, dtype=torch.int64)
+    # Only the first element is required to be zero; subsequent elements
+    # are overwritten by the kernel. Avoid clearing the whole buffer.
+    m_num_tiles_indptr = torch.empty(batch_size + 1, device=a.device, dtype=torch.int64)
+    m_num_tiles_indptr[0] = 0
     compute_m_num_tiles_indptr[(1,)](
         m_num_tiles_indptr, seg_indptr, batch_size, config["BLOCK_SIZE_M"]
     )

exec
bash -lc "nl -ba python/sglang/srt/layers/moe/ep_moe/kernels.py | sed -n '130,210p'" in /home/ubuntu/OmniPerf-Bench/perf-agents-bench/.work/worktrees/sglang_core/sglang_core-0060 succeeded in 11ms:
   130	    )
   131	    reorder_topk_ids = reorder_topk_ids[num_minus_one:]
   132	    return reorder_topk_ids, src2dst, seg_indptr
   133
   134
   135	@triton.jit
   136	def compute_seg_indptr_triton_kernel(reorder_topk_ids, seg_indptr, num_toks):
   137	    expert = tl.program_id(0)
   138	    low = 0
   139	    high = num_toks - 1
   140	    target_location = -1
   141	    while low <= high:
   142	        mid = (low + high) // 2
   143
   144	        if tl.load(reorder_topk_ids + mid) > expert:
   145	            high = mid - 1
   146	        else:
   147	            low = mid + 1
   148	            target_location = mid
   149	    tl.store(seg_indptr + expert + 1, target_location + 1)
file update:
diff --git a/benchmark/kernels/fused_moe_triton/benchmark_ep_pre_reorder_triton.py b/benchmark/kernels/fused_moe_triton/benchmark_ep_pre_reorder_triton.py
new file mode 100644
index 0000000000000000000000000000000000000000..75e09e3c272e3e2448f970ddbda7867bf74964a5
--- /dev/null
+++ b/benchmark/kernels/fused_moe_triton/benchmark_ep_pre_reorder_triton.py
@@ -0,0 +1,82 @@
+import argparse
+import time
+
+import torch
+import triton
+
+from sglang.srt.layers.moe.ep_moe.kernels import pre_reorder_triton_kernel
+
+
+def benchmark_pre_reorder(batch_size: int, topk: int, hidden_size: int, block_size: int):
+    device = "cuda"
+
+    # Inputs
+    input_ptr = torch.randn(batch_size, hidden_size, dtype=torch.float16, device=device)
+
+    # Use empty for outputs and index buffers to avoid unnecessary memset
+    gateup_input_ptr = torch.empty(
+        batch_size * topk, hidden_size, dtype=torch.float16, device=device
+    )
+    src2dst_ptr = torch.randint(
+        0, batch_size * topk, (batch_size, topk), dtype=torch.int32, device=device
+    )
+    topk_ids_ptr = torch.randint(0, 1024, (batch_size, topk), dtype=torch.int32, device=device)
+
+    # No scaling by default
+    a1_scales_ptr = None
+    start_expert_id = 0
+    end_expert_id = 1023
+
+    grid = (batch_size,)
+
+    # Warmup
+    for _ in range(5):
+        pre_reorder_triton_kernel[grid](
+            input_ptr,
+            gateup_input_ptr,
+            src2dst_ptr,
+            topk_ids_ptr,
+            a1_scales_ptr,
+            start_expert_id,
+            end_expert_id,
+            topk,
+            hidden_size,
+            BLOCK_SIZE=block_size,
+        )
+
+    torch.cuda.synchronize()
+    t0 = time.time()
+    for _ in range(20):
+        pre_reorder_triton_kernel[grid](
+            input_ptr,
+            gateup_input_ptr,
+            src2dst_ptr,
+            topk_ids_ptr,
+            a1_scales_ptr,
+            start_expert_id,
+            end_expert_id,
+            topk,
+            hidden_size,
+            BLOCK_SIZE=block_size,
+        )
+    torch.cuda.synchronize()
+    return (time.time() - t0) / 20
+
+
+def main():
+    parser = argparse.ArgumentParser()
+    parser.add_argument("--batch-size", type=int, default=4096)
+    parser.add_argument("--topk", type=int, default=2)
+    parser.add_argument("--hidden-size", type=int, default=4096)
+    parser.add_argument("--block-size", type=int, default=128)
+    args = parser.parse_args()
+
+    dur = benchmark_pre_reorder(
+        args.batch_size, args.topk, args.hidden_size, args.block_size
+    )
+    print(f"pre_reorder_triton_kernel avg time: {dur*1e3:.3f} ms")
+
+
+if __name__ == "__main__":
+    main()
+
diff --git a/python/sglang/srt/layers/moe/ep_moe/kernels.py b/python/sglang/srt/layers/moe/ep_moe/kernels.py
index 8c005527a7c58f4f4d13189a6b6ca992016b3c1f..15a07d85e655ce1b391eb0e1ed294f9aa81b6f7f
--- a/python/sglang/srt/layers/moe/ep_moe/kernels.py
+++ b/python/sglang/srt/layers/moe/ep_moe/kernels.py
@@ -151,7 +151,9 @@

 def run_moe_ep_preproess(topk_ids: torch.Tensor, num_experts: int):
     reorder_topk_ids, reorder_ids = torch.sort(topk_ids.view(-1), stable=True)
-    seg_indptr = torch.zeros(num_experts + 1, device=topk_ids.device, dtype=torch.int64)
+    # Only seg_indptr[0] needs to be zero-initialized; others are written by the kernel.
+    seg_indptr = torch.empty(num_experts + 1, device=topk_ids.device, dtype=torch.int64)
+    seg_indptr[0] = 0
     src2dst = torch.empty(topk_ids.numel(), device=topk_ids.device, dtype=torch.int32)

     compute_seg_indptr_triton_kernel[(num_experts,)](
@@ -186,20 +188,25 @@
     topk_ids_ptr = topk_ids_ptr + src_idx * topk

     src_ptr = input_ptr + src_idx * hidden_size
-    for idx in range(topk):
-        expert_id = tl.load(topk_ids_ptr + idx)
-        if expert_id >= start_expert_id and expert_id <= end_expert_id:
-            if a1_scales_ptr is not None:
-                scale = 1.0 / tl.load(a1_scales_ptr + expert_id - start_expert_id)
-            else:
-                scale = 1.0

-            dst_idx = tl.load(src2dst_ptr + idx)
-            dst_ptr = gateup_input_ptr + dst_idx * hidden_size
-            for start_offset in tl.range(0, hidden_size, BLOCK_SIZE):
-                offset = start_offset + tl.arange(0, BLOCK_SIZE)
-                mask = offset < hidden_size
-                in_data = tl.load(src_ptr + offset, mask=mask).to(tl.float32)
+    # Iterate over the hidden dimension first to reuse the loaded input
+    # across all top-k destinations. This reduces redundant global reads
+    # of the same source vector by a factor of topk.
+    for start_offset in tl.range(0, hidden_size, BLOCK_SIZE):
+        offset = start_offset + tl.arange(0, BLOCK_SIZE)
+        mask = offset < hidden_size
+        in_data = tl.load(src_ptr + offset, mask=mask).to(tl.float32)
+
+        for idx in range(topk):
+            expert_id = tl.load(topk_ids_ptr + idx)
+            if expert_id >= start_expert_id and expert_id <= end_expert_id:
+                if a1_scales_ptr is not None:
+                    scale = 1.0 / tl.load(a1_scales_ptr + expert_id - start_expert_id)
+                else:
+                    scale = 1.0
+
+                dst_idx = tl.load(src2dst_ptr + idx)
+                dst_ptr = gateup_input_ptr + dst_idx * hidden_size
                 out_data = (in_data * scale).to(OutDtype)
                 tl.store(dst_ptr + offset, out_data, mask=mask)

@@ -679,7 +686,10 @@
         "BLOCK_SIZE_K": 128,
     }

-    m_num_tiles_indptr = torch.zeros(batch_size + 1, device=a.device, dtype=torch.int64)
+    # Only the first element is required to be zero; subsequent elements
+    # are overwritten by the kernel. Avoid clearing the whole buffer.
+    m_num_tiles_indptr = torch.empty(batch_size + 1, device=a.device, dtype=torch.int64)
+    m_num_tiles_indptr[0] = 0
     compute_m_num_tiles_indptr[(1,)](
         m_num_tiles_indptr, seg_indptr, batch_size, config["BLOCK_SIZE_M"]
     )

exec
bash -lc "sed -n '210,260p' python/sglang/srt/layers/moe/ep_moe/kernels.py | nl -ba -v 210 | sed -n '1,60p'" in /home/ubuntu/OmniPerf-Bench/perf-agents-bench/.work/worktrees/sglang_core/sglang_core-0060 succeeded in 11ms:
   210	                out_data = (in_data * scale).to(OutDtype)
   211	                tl.store(dst_ptr + offset, out_data, mask=mask)
   212
   213
   214	@triton.jit
   215	def silu_and_mul_triton_kernel(
   216	    gateup_output,
   217	    down_input,
   218	    hidden_size,
   219	    reorder_topk_ids,
   220	    scales,
   221	    start_expert_id,
   222	    end_expert_id,
   223	    BLOCK_SIZE: tl.constexpr,
   224	):
   225	    InDtype = gateup_output.dtype.element_ty
   226	    OutDtype = down_input.dtype.element_ty
   227
   228	    half_hidden_size = hidden_size // 2
   229
file update:
diff --git a/benchmark/kernels/fused_moe_triton/benchmark_ep_pre_reorder_triton.py b/benchmark/kernels/fused_moe_triton/benchmark_ep_pre_reorder_triton.py
new file mode 100644
index 0000000000000000000000000000000000000000..75e09e3c272e3e2448f970ddbda7867bf74964a5
--- /dev/null
+++ b/benchmark/kernels/fused_moe_triton/benchmark_ep_pre_reorder_triton.py
@@ -0,0 +1,82 @@
+import argparse
+import time
+
+import torch
+import triton
+
+from sglang.srt.layers.moe.ep_moe.kernels import pre_reorder_triton_kernel
+
+
+def benchmark_pre_reorder(batch_size: int, topk: int, hidden_size: int, block_size: int):
+    device = "cuda"
+
+    # Inputs
+    input_ptr = torch.randn(batch_size, hidden_size, dtype=torch.float16, device=device)
+
+    # Use empty for outputs and index buffers to avoid unnecessary memset
+    gateup_input_ptr = torch.empty(
+        batch_size * topk, hidden_size, dtype=torch.float16, device=device
+    )
+    src2dst_ptr = torch.randint(
+        0, batch_size * topk, (batch_size, topk), dtype=torch.int32, device=device
+    )
+    topk_ids_ptr = torch.randint(0, 1024, (batch_size, topk), dtype=torch.int32, device=device)
+
+    # No scaling by default
+    a1_scales_ptr = None
+    start_expert_id = 0
+    end_expert_id = 1023
+
+    grid = (batch_size,)
+
+    # Warmup
+    for _ in range(5):
+        pre_reorder_triton_kernel[grid](
+            input_ptr,
+            gateup_input_ptr,
+            src2dst_ptr,
+            topk_ids_ptr,
+            a1_scales_ptr,
+            start_expert_id,
+            end_expert_id,
+            topk,
+            hidden_size,
+            BLOCK_SIZE=block_size,
+        )
+
+    torch.cuda.synchronize()
+    t0 = time.time()
+    for _ in range(20):
+        pre_reorder_triton_kernel[grid](
+            input_ptr,
+            gateup_input_ptr,
+            src2dst_ptr,
+            topk_ids_ptr,
+            a1_scales_ptr,
+            start_expert_id,
+            end_expert_id,
+            topk,
+            hidden_size,
+            BLOCK_SIZE=block_size,
+        )
+    torch.cuda.synchronize()
+    return (time.time() - t0) / 20
+
+
+def main():
+    parser = argparse.ArgumentParser()
+    parser.add_argument("--batch-size", type=int, default=4096)
+    parser.add_argument("--topk", type=int, default=2)
+    parser.add_argument("--hidden-size", type=int, default=4096)
+    parser.add_argument("--block-size", type=int, default=128)
+    args = parser.parse_args()
+
+    dur = benchmark_pre_reorder(
+        args.batch_size, args.topk, args.hidden_size, args.block_size
+    )
+    print(f"pre_reorder_triton_kernel avg time: {dur*1e3:.3f} ms")
+
+
+if __name__ == "__main__":
+    main()
+
diff --git a/python/sglang/srt/layers/moe/ep_moe/kernels.py b/python/sglang/srt/layers/moe/ep_moe/kernels.py
index 8c005527a7c58f4f4d13189a6b6ca992016b3c1f..15a07d85e655ce1b391eb0e1ed294f9aa81b6f7f
--- a/python/sglang/srt/layers/moe/ep_moe/kernels.py
+++ b/python/sglang/srt/layers/moe/ep_moe/kernels.py
@@ -151,7 +151,9 @@

 def run_moe_ep_preproess(topk_ids: torch.Tensor, num_experts: int):
     reorder_topk_ids, reorder_ids = torch.sort(topk_ids.view(-1), stable=True)
-    seg_indptr = torch.zeros(num_experts + 1, device=topk_ids.device, dtype=torch.int64)
+    # Only seg_indptr[0] needs to be zero-initialized; others are written by the kernel.
+    seg_indptr = torch.empty(num_experts + 1, device=topk_ids.device, dtype=torch.int64)
+    seg_indptr[0] = 0
     src2dst = torch.empty(topk_ids.numel(), device=topk_ids.device, dtype=torch.int32)

     compute_seg_indptr_triton_kernel[(num_experts,)](
@@ -186,20 +188,25 @@
     topk_ids_ptr = topk_ids_ptr + src_idx * topk

     src_ptr = input_ptr + src_idx * hidden_size
-    for idx in range(topk):
-        expert_id = tl.load(topk_ids_ptr + idx)
-        if expert_id >= start_expert_id and expert_id <= end_expert_id:
-            if a1_scales_ptr is not None:
-                scale = 1.0 / tl.load(a1_scales_ptr + expert_id - start_expert_id)
-            else:
-                scale = 1.0

-            dst_idx = tl.load(src2dst_ptr + idx)
-            dst_ptr = gateup_input_ptr + dst_idx * hidden_size
-            for start_offset in tl.range(0, hidden_size, BLOCK_SIZE):
-                offset = start_offset + tl.arange(0, BLOCK_SIZE)
-                mask = offset < hidden_size
-                in_data = tl.load(src_ptr + offset, mask=mask).to(tl.float32)
+    # Iterate over the hidden dimension first to reuse the loaded input
+    # across all top-k destinations. This reduces redundant global reads
+    # of the same source vector by a factor of topk.
+    for start_offset in tl.range(0, hidden_size, BLOCK_SIZE):
+        offset = start_offset + tl.arange(0, BLOCK_SIZE)
+        mask = offset < hidden_size
+        in_data = tl.load(src_ptr + offset, mask=mask).to(tl.float32)
+
+        for idx in range(topk):
+            expert_id = tl.load(topk_ids_ptr + idx)
+            if expert_id >= start_expert_id and expert_id <= end_expert_id:
+                if a1_scales_ptr is not None:
+                    scale = 1.0 / tl.load(a1_scales_ptr + expert_id - start_expert_id)
+                else:
+                    scale = 1.0
+
+                dst_idx = tl.load(src2dst_ptr + idx)
+                dst_ptr = gateup_input_ptr + dst_idx * hidden_size
                 out_data = (in_data * scale).to(OutDtype)
                 tl.store(dst_ptr + offset, out_data, mask=mask)

@@ -679,7 +686,10 @@
         "BLOCK_SIZE_K": 128,
     }

-    m_num_tiles_indptr = torch.zeros(batch_size + 1, device=a.device, dtype=torch.int64)
+    # Only the first element is required to be zero; subsequent elements
+    # are overwritten by the kernel. Avoid clearing the whole buffer.
+    m_num_tiles_indptr = torch.empty(batch_size + 1, device=a.device, dtype=torch.int64)
+    m_num_tiles_indptr[0] = 0
     compute_m_num_tiles_indptr[(1,)](
         m_num_tiles_indptr, seg_indptr, batch_size, config["BLOCK_SIZE_M"]
     )

exec
bash -lc "nl -ba python/sglang/srt/layers/moe/ep_moe/kernels.py | sed -n '660,710p'" in /home/ubuntu/OmniPerf-Bench/perf-agents-bench/.work/worktrees/sglang_core/sglang_core-0060 succeeded in 10ms:
   660	    scale_b: torch.Tensor = None,
   661	    block_shape: Optional[List[int]] = None,
   662	    c_dtype=None,
   663	):
   664	    assert weight_column_major == True  # TODO: more
   665	    if use_fp8_w8a8 and block_shape is None:
   666	        assert scale_a is not None and scale_b is not None
   667
   668	    if block_shape is not None:
   669	        a_original = a
   670
   671	        assert len(block_shape) == 2
   672	        block_n, block_k = block_shape[0], block_shape[1]
   673	        a, scale_a = per_token_group_quant_fp8(a, block_k)
   674
   675	        assert triton.cdiv(a.shape[-1], block_k) == scale_a.shape[-1]
   676	        assert triton.cdiv(b.shape[-2], block_n) == scale_b.shape[-2]
   677	        assert triton.cdiv(b.shape[-1], block_k) == scale_b.shape[-1]
   678
   679	        dispose_tensor(a_original)
file update:
diff --git a/benchmark/kernels/fused_moe_triton/benchmark_ep_pre_reorder_triton.py b/benchmark/kernels/fused_moe_triton/benchmark_ep_pre_reorder_triton.py
new file mode 100644
index 0000000000000000000000000000000000000000..75e09e3c272e3e2448f970ddbda7867bf74964a5
--- /dev/null
+++ b/benchmark/kernels/fused_moe_triton/benchmark_ep_pre_reorder_triton.py
@@ -0,0 +1,82 @@
+import argparse
+import time
+
+import torch
+import triton
+
+from sglang.srt.layers.moe.ep_moe.kernels import pre_reorder_triton_kernel
+
+
+def benchmark_pre_reorder(batch_size: int, topk: int, hidden_size: int, block_size: int):
+    device = "cuda"
+
+    # Inputs
+    input_ptr = torch.randn(batch_size, hidden_size, dtype=torch.float16, device=device)
+
+    # Use empty for outputs and index buffers to avoid unnecessary memset
+    gateup_input_ptr = torch.empty(
+        batch_size * topk, hidden_size, dtype=torch.float16, device=device
+    )
+    src2dst_ptr = torch.randint(
+        0, batch_size * topk, (batch_size, topk), dtype=torch.int32, device=device
+    )
+    topk_ids_ptr = torch.randint(0, 1024, (batch_size, topk), dtype=torch.int32, device=device)
+
+    # No scaling by default
+    a1_scales_ptr = None
+    start_expert_id = 0
+    end_expert_id = 1023
+
+    grid = (batch_size,)
+
+    # Warmup
+    for _ in range(5):
+        pre_reorder_triton_kernel[grid](
+            input_ptr,
+            gateup_input_ptr,
+            src2dst_ptr,
+            topk_ids_ptr,
+            a1_scales_ptr,
+            start_expert_id,
+            end_expert_id,
+            topk,
+            hidden_size,
+            BLOCK_SIZE=block_size,
+        )
+
+    torch.cuda.synchronize()
+    t0 = time.time()
+    for _ in range(20):
+        pre_reorder_triton_kernel[grid](
+            input_ptr,
+            gateup_input_ptr,
+            src2dst_ptr,
+            topk_ids_ptr,
+            a1_scales_ptr,
+            start_expert_id,
+            end_expert_id,
+            topk,
+            hidden_size,
+            BLOCK_SIZE=block_size,
+        )
+    torch.cuda.synchronize()
+    return (time.time() - t0) / 20
+
+
+def main():
+    parser = argparse.ArgumentParser()
+    parser.add_argument("--batch-size", type=int, default=4096)
+    parser.add_argument("--topk", type=int, default=2)
+    parser.add_argument("--hidden-size", type=int, default=4096)
+    parser.add_argument("--block-size", type=int, default=128)
+    args = parser.parse_args()
+
+    dur = benchmark_pre_reorder(
+        args.batch_size, args.topk, args.hidden_size, args.block_size
+    )
+    print(f"pre_reorder_triton_kernel avg time: {dur*1e3:.3f} ms")
+
+
+if __name__ == "__main__":
+    main()
+
diff --git a/python/sglang/srt/layers/moe/ep_moe/kernels.py b/python/sglang/srt/layers/moe/ep_moe/kernels.py
index 8c005527a7c58f4f4d13189a6b6ca992016b3c1f..15a07d85e655ce1b391eb0e1ed294f9aa81b6f7f
--- a/python/sglang/srt/layers/moe/ep_moe/kernels.py
+++ b/python/sglang/srt/layers/moe/ep_moe/kernels.py
@@ -151,7 +151,9 @@

 def run_moe_ep_preproess(topk_ids: torch.Tensor, num_experts: int):
     reorder_topk_ids, reorder_ids = torch.sort(topk_ids.view(-1), stable=True)
-    seg_indptr = torch.zeros(num_experts + 1, device=topk_ids.device, dtype=torch.int64)
+    # Only seg_indptr[0] needs to be zero-initialized; others are written by the kernel.
+    seg_indptr = torch.empty(num_experts + 1, device=topk_ids.device, dtype=torch.int64)
+    seg_indptr[0] = 0
     src2dst = torch.empty(topk_ids.numel(), device=topk_ids.device, dtype=torch.int32)

     compute_seg_indptr_triton_kernel[(num_experts,)](
@@ -186,20 +188,25 @@
     topk_ids_ptr = topk_ids_ptr + src_idx * topk

     src_ptr = input_ptr + src_idx * hidden_size
-    for idx in range(topk):
-        expert_id = tl.load(topk_ids_ptr + idx)
-        if expert_id >= start_expert_id and expert_id <= end_expert_id:
-            if a1_scales_ptr is not None:
-                scale = 1.0 / tl.load(a1_scales_ptr + expert_id - start_expert_id)
-            else:
-                scale = 1.0

-            dst_idx = tl.load(src2dst_ptr + idx)
-            dst_ptr = gateup_input_ptr + dst_idx * hidden_size
-            for start_offset in tl.range(0, hidden_size, BLOCK_SIZE):
-                offset = start_offset + tl.arange(0, BLOCK_SIZE)
-                mask = offset < hidden_size
-                in_data = tl.load(src_ptr + offset, mask=mask).to(tl.float32)
+    # Iterate over the hidden dimension first to reuse the loaded input
+    # across all top-k destinations. This reduces redundant global reads
+    # of the same source vector by a factor of topk.
+    for start_offset in tl.range(0, hidden_size, BLOCK_SIZE):
+        offset = start_offset + tl.arange(0, BLOCK_SIZE)
+        mask = offset < hidden_size
+        in_data = tl.load(src_ptr + offset, mask=mask).to(tl.float32)
+
+        for idx in range(topk):
+            expert_id = tl.load(topk_ids_ptr + idx)
+            if expert_id >= start_expert_id and expert_id <= end_expert_id:
+                if a1_scales_ptr is not None:
+                    scale = 1.0 / tl.load(a1_scales_ptr + expert_id - start_expert_id)
+                else:
+                    scale = 1.0
+
+                dst_idx = tl.load(src2dst_ptr + idx)
+                dst_ptr = gateup_input_ptr + dst_idx * hidden_size
                 out_data = (in_data * scale).to(OutDtype)
                 tl.store(dst_ptr + offset, out_data, mask=mask)

@@ -679,7 +686,10 @@
         "BLOCK_SIZE_K": 128,
     }

-    m_num_tiles_indptr = torch.zeros(batch_size + 1, device=a.device, dtype=torch.int64)
+    # Only the first element is required to be zero; subsequent elements
+    # are overwritten by the kernel. Avoid clearing the whole buffer.
+    m_num_tiles_indptr = torch.empty(batch_size + 1, device=a.device, dtype=torch.int64)
+    m_num_tiles_indptr[0] = 0
     compute_m_num_tiles_indptr[(1,)](
         m_num_tiles_indptr, seg_indptr, batch_size, config["BLOCK_SIZE_M"]
     )


thinking
**Preparing final summary**

I think I need to check for another file in the repo, specifically 'benchmark/kernels/fused_moe_triton/benchmark_vllm_vs_sglang_fused_moe_triton.py,' that I found through grep. For now, it seems best to leave it.

Next, I'll put together a final answer that summarizes everything and includes instructions to run the commit. I'll aim to keep the message concise and easy to scan with clear headers. Let's get this crafted!
codex
I’m going to add a local timing script, then optimize tensor allocation and a Triton kernel loop to reduce memory traffic.

- Created .bench_scratch/test_opt.py to benchmark moe_align_block_size with vLLM if available, or fall back to sgl_kernel.
- Optimized EP-MoE kernels by removing unnecessary zero-initializations and reducing redundant loads in the Triton pre-reorder kernel.
- Added a focused benchmark for the pre_reorder_triton kernel.

What I changed
- python/sglang/srt/layers/moe/ep_moe/kernels.py:152
  - seg_indptr: switch torch.zeros → torch.empty and only initialize seg_indptr[0] to 0, as the kernel writes the rest.
- python/sglang/srt/layers/moe/ep_moe/kernels.py:171
  - pre_reorder_triton_kernel: restructure loops to move the hidden-dimension traversal outermost and reuse the loaded source vector across all top-k destinations. This avoids reloading the same input topk times and reduces global memory traffic:
    - Previously: for each topk: for each block → load and store
    - Now: for each block: load once → for each topk: store to destinations
- python/sglang/srt/layers/moe/ep_moe/kernels.py:689
  - m_num_tiles_indptr: switch torch.zeros → torch.empty and only set the first element to 0, since the kernel overwrites the rest.
- benchmark/kernels/fused_moe_triton/benchmark_ep_pre_reorder_triton.py:1
  - New micro-benchmark to measure pre_reorder_triton_kernel, using torch.empty for outputs to avoid zero-fill overhead.

Why this speeds things up
- Memory allocation: torch.zeros clears memory; torch.empty does not. In both run_moe_ep_preproess and grouped_gemm_triton, only the first slot needs to be 0 and the rest are overwritten by Triton kernels, so we avoid unnecessary memset.
- Kernel efficiency: The revised pre_reorder_triton_kernel loads the input block once and reuses it across all top-k stores, reducing redundant global memory reads by ~topk.

Files you can review
- python/sglang/srt/layers/moe/ep_moe/kernels.py:152
- python/sglang/srt/layers/moe/ep_moe/kernels.py:171
- python/sglang/srt/layers/moe/ep_moe/kernels.py:689
- benchmark/kernels/fused_moe_triton/benchmark_ep_pre_reorder_triton.py:1

How to benchmark locally
- MoE align block size (vLLM if available, fallback to sgl_kernel):
  - python .bench_scratch/test_opt.py
  - Note: CUDA is required. The current environment lacked a driver, so runtime measurement couldn’t be executed here.
- Pre-reorder kernel microbenchmark:
  - python benchmark/kernels/fused_moe_triton/benchmark_ep_pre_reorder_triton.py --batch-size 4096 --topk 2 --hidden-size 4096 --block-size 128

What I ran
- I created and attempted to run .bench_scratch/test_opt.py to get a baseline, but the environment didn’t have a working NVIDIA driver. The script is ready for GPU.

Next steps
- If you’d like, I can also apply similar torch.zeros → torch.empty improvements in other hot paths after we profile additional kernels.
- If you can run with GPU, please execute the two benchmark scripts to validate speedups.

Repo updates
- Commit created: Optimize MoE align sum kernels performance
- model_patch.diff written to: model_patch.diff

If you want me to extend the optimizations to other kernels (e.g., additional MoE reorder/post paths), say the word and I’ll proceed.
file update:
diff --git a/benchmark/kernels/fused_moe_triton/benchmark_ep_pre_reorder_triton.py b/benchmark/kernels/fused_moe_triton/benchmark_ep_pre_reorder_triton.py