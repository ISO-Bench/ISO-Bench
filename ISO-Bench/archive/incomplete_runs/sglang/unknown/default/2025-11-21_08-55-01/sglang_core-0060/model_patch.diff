diff --git a/benchmark/kernels/fused_moe_triton/benchmark_ep_pre_reorder_triton.py b/benchmark/kernels/fused_moe_triton/benchmark_ep_pre_reorder_triton.py
new file mode 100644
index 000000000..75e09e3c2
--- /dev/null
+++ b/benchmark/kernels/fused_moe_triton/benchmark_ep_pre_reorder_triton.py
@@ -0,0 +1,82 @@
+import argparse
+import time
+
+import torch
+import triton
+
+from sglang.srt.layers.moe.ep_moe.kernels import pre_reorder_triton_kernel
+
+
+def benchmark_pre_reorder(batch_size: int, topk: int, hidden_size: int, block_size: int):
+    device = "cuda"
+
+    # Inputs
+    input_ptr = torch.randn(batch_size, hidden_size, dtype=torch.float16, device=device)
+
+    # Use empty for outputs and index buffers to avoid unnecessary memset
+    gateup_input_ptr = torch.empty(
+        batch_size * topk, hidden_size, dtype=torch.float16, device=device
+    )
+    src2dst_ptr = torch.randint(
+        0, batch_size * topk, (batch_size, topk), dtype=torch.int32, device=device
+    )
+    topk_ids_ptr = torch.randint(0, 1024, (batch_size, topk), dtype=torch.int32, device=device)
+
+    # No scaling by default
+    a1_scales_ptr = None
+    start_expert_id = 0
+    end_expert_id = 1023
+
+    grid = (batch_size,)
+
+    # Warmup
+    for _ in range(5):
+        pre_reorder_triton_kernel[grid](
+            input_ptr,
+            gateup_input_ptr,
+            src2dst_ptr,
+            topk_ids_ptr,
+            a1_scales_ptr,
+            start_expert_id,
+            end_expert_id,
+            topk,
+            hidden_size,
+            BLOCK_SIZE=block_size,
+        )
+
+    torch.cuda.synchronize()
+    t0 = time.time()
+    for _ in range(20):
+        pre_reorder_triton_kernel[grid](
+            input_ptr,
+            gateup_input_ptr,
+            src2dst_ptr,
+            topk_ids_ptr,
+            a1_scales_ptr,
+            start_expert_id,
+            end_expert_id,
+            topk,
+            hidden_size,
+            BLOCK_SIZE=block_size,
+        )
+    torch.cuda.synchronize()
+    return (time.time() - t0) / 20
+
+
+def main():
+    parser = argparse.ArgumentParser()
+    parser.add_argument("--batch-size", type=int, default=4096)
+    parser.add_argument("--topk", type=int, default=2)
+    parser.add_argument("--hidden-size", type=int, default=4096)
+    parser.add_argument("--block-size", type=int, default=128)
+    args = parser.parse_args()
+
+    dur = benchmark_pre_reorder(
+        args.batch_size, args.topk, args.hidden_size, args.block_size
+    )
+    print(f"pre_reorder_triton_kernel avg time: {dur*1e3:.3f} ms")
+
+
+if __name__ == "__main__":
+    main()
+
diff --git a/python/sglang/srt/layers/moe/ep_moe/kernels.py b/python/sglang/srt/layers/moe/ep_moe/kernels.py
index 8c005527a..15a07d85e 100644
--- a/python/sglang/srt/layers/moe/ep_moe/kernels.py
+++ b/python/sglang/srt/layers/moe/ep_moe/kernels.py
@@ -151,7 +151,9 @@ def compute_seg_indptr_triton_kernel(reorder_topk_ids, seg_indptr, num_toks):
 
 def run_moe_ep_preproess(topk_ids: torch.Tensor, num_experts: int):
     reorder_topk_ids, reorder_ids = torch.sort(topk_ids.view(-1), stable=True)
-    seg_indptr = torch.zeros(num_experts + 1, device=topk_ids.device, dtype=torch.int64)
+    # Only seg_indptr[0] needs to be zero-initialized; others are written by the kernel.
+    seg_indptr = torch.empty(num_experts + 1, device=topk_ids.device, dtype=torch.int64)
+    seg_indptr[0] = 0
     src2dst = torch.empty(topk_ids.numel(), device=topk_ids.device, dtype=torch.int32)
 
     compute_seg_indptr_triton_kernel[(num_experts,)](
@@ -186,20 +188,25 @@ def pre_reorder_triton_kernel(
     topk_ids_ptr = topk_ids_ptr + src_idx * topk
 
     src_ptr = input_ptr + src_idx * hidden_size
-    for idx in range(topk):
-        expert_id = tl.load(topk_ids_ptr + idx)
-        if expert_id >= start_expert_id and expert_id <= end_expert_id:
-            if a1_scales_ptr is not None:
-                scale = 1.0 / tl.load(a1_scales_ptr + expert_id - start_expert_id)
-            else:
-                scale = 1.0
 
-            dst_idx = tl.load(src2dst_ptr + idx)
-            dst_ptr = gateup_input_ptr + dst_idx * hidden_size
-            for start_offset in tl.range(0, hidden_size, BLOCK_SIZE):
-                offset = start_offset + tl.arange(0, BLOCK_SIZE)
-                mask = offset < hidden_size
-                in_data = tl.load(src_ptr + offset, mask=mask).to(tl.float32)
+    # Iterate over the hidden dimension first to reuse the loaded input
+    # across all top-k destinations. This reduces redundant global reads
+    # of the same source vector by a factor of topk.
+    for start_offset in tl.range(0, hidden_size, BLOCK_SIZE):
+        offset = start_offset + tl.arange(0, BLOCK_SIZE)
+        mask = offset < hidden_size
+        in_data = tl.load(src_ptr + offset, mask=mask).to(tl.float32)
+
+        for idx in range(topk):
+            expert_id = tl.load(topk_ids_ptr + idx)
+            if expert_id >= start_expert_id and expert_id <= end_expert_id:
+                if a1_scales_ptr is not None:
+                    scale = 1.0 / tl.load(a1_scales_ptr + expert_id - start_expert_id)
+                else:
+                    scale = 1.0
+
+                dst_idx = tl.load(src2dst_ptr + idx)
+                dst_ptr = gateup_input_ptr + dst_idx * hidden_size
                 out_data = (in_data * scale).to(OutDtype)
                 tl.store(dst_ptr + offset, out_data, mask=mask)
 
@@ -679,7 +686,10 @@ def grouped_gemm_triton(
         "BLOCK_SIZE_K": 128,
     }
 
-    m_num_tiles_indptr = torch.zeros(batch_size + 1, device=a.device, dtype=torch.int64)
+    # Only the first element is required to be zero; subsequent elements
+    # are overwritten by the kernel. Avoid clearing the whole buffer.
+    m_num_tiles_indptr = torch.empty(batch_size + 1, device=a.device, dtype=torch.int64)
+    m_num_tiles_indptr[0] = 0
     compute_m_num_tiles_indptr[(1,)](
         m_num_tiles_indptr, seg_indptr, batch_size, config["BLOCK_SIZE_M"]
     )
