{"instance_id": "/home/ubuntu/OmniPerf-Bench/sglan__repo-ad506a4", "model_patch": "diff --git a/python/sglang/srt/layers/attention/flashattention_backend.py b/python/sglang/srt/layers/attention/flashattention_backend.py\nindex 4e8543213..38d157a03 100644\n--- a/python/sglang/srt/layers/attention/flashattention_backend.py\n+++ b/python/sglang/srt/layers/attention/flashattention_backend.py\n@@ -354,12 +354,17 @@ class FlashAttentionBackend(AttentionBackend):\n                     metadata.cu_seqlens_q = torch.arange(\n                         0, batch_size + 1, dtype=torch.int32, device=device\n                     )\n-                    metadata.cu_seqlens_k = torch.nn.functional.pad(\n-                        torch.cumsum(\n-                            metadata.cache_seqlens_int32, dim=0, dtype=torch.int32\n-                        ),\n-                        (1, 0),\n+                    # Avoid pad allocation: write cumsum into pre-sized buffer\n+                    _tmp_cum = torch.cumsum(\n+                        metadata.cache_seqlens_int32, dim=0, dtype=torch.int32\n                     )\n+                    metadata.cu_seqlens_k = torch.empty(\n+                        _tmp_cum.numel() + 1,\n+                        dtype=torch.int32,\n+                        device=_tmp_cum.device,\n+                    )\n+                    metadata.cu_seqlens_k[0] = 0\n+                    metadata.cu_seqlens_k[1:].copy_(_tmp_cum)\n                     metadata.page_table = forward_batch.req_to_token_pool.req_to_token[\n                         forward_batch.req_pool_indices, : metadata.max_seq_len_k\n                     ]\n@@ -374,12 +379,17 @@ class FlashAttentionBackend(AttentionBackend):\n                         dtype=torch.int32,\n                         device=device,\n                     )\n-                    metadata.cu_seqlens_k = torch.nn.functional.pad(\n-                        torch.cumsum(\n-                            metadata.cache_seqlens_int32, dim=0, dtype=torch.int32\n-                        ),\n-                        (1, 0),\n+                    # Avoid extra pad: build prefix sums directly\n+                    _tmp_cum = torch.cumsum(\n+                        metadata.cache_seqlens_int32, dim=0, dtype=torch.int32\n+                    )\n+                    metadata.cu_seqlens_k = torch.empty(\n+                        _tmp_cum.numel() + 1,\n+                        dtype=torch.int32,\n+                        device=_tmp_cum.device,\n                     )\n+                    metadata.cu_seqlens_k[0] = 0\n+                    metadata.cu_seqlens_k[1:].copy_(_tmp_cum)\n                     metadata.page_table = forward_batch.req_to_token_pool.req_to_token[\n                         forward_batch.req_pool_indices, : metadata.max_seq_len_k\n                     ]\n@@ -421,9 +431,12 @@ class FlashAttentionBackend(AttentionBackend):\n                 metadata.cu_seqlens_q = torch.arange(\n                     0, batch_size + 1, dtype=torch.int32, device=device\n                 )\n-                metadata.cu_seqlens_k = torch.nn.functional.pad(\n-                    torch.cumsum(seqlens_in_batch, dim=0, dtype=torch.int32), (1, 0)\n+                _tmp_cum = torch.cumsum(seqlens_in_batch, dim=0, dtype=torch.int32)\n+                metadata.cu_seqlens_k = torch.empty(\n+                    _tmp_cum.numel() + 1, dtype=torch.int32, device=_tmp_cum.device\n                 )\n+                metadata.cu_seqlens_k[0] = 0\n+                metadata.cu_seqlens_k[1:].copy_(_tmp_cum)\n                 metadata.page_table = forward_batch.req_to_token_pool.req_to_token[\n                     forward_batch.req_pool_indices, : metadata.max_seq_len_k\n                 ]\n@@ -446,12 +459,14 @@ class FlashAttentionBackend(AttentionBackend):\n                     dtype=torch.int32,\n                     device=device,\n                 )\n-                metadata.cu_seqlens_k = torch.nn.functional.pad(\n-                    torch.cumsum(\n-                        metadata.cache_seqlens_int32, dim=0, dtype=torch.int32\n-                    ),\n-                    (1, 0),\n+                _tmp_cum = torch.cumsum(\n+                    metadata.cache_seqlens_int32, dim=0, dtype=torch.int32\n                 )\n+                metadata.cu_seqlens_k = torch.empty(\n+                    _tmp_cum.numel() + 1, dtype=torch.int32, device=_tmp_cum.device\n+                )\n+                metadata.cu_seqlens_k[0] = 0\n+                metadata.cu_seqlens_k[1:].copy_(_tmp_cum)\n                 metadata.page_table = forward_batch.req_to_token_pool.req_to_token[\n                     forward_batch.req_pool_indices, : metadata.max_seq_len_k\n                 ]\n@@ -468,12 +483,14 @@ class FlashAttentionBackend(AttentionBackend):\n                     dtype=torch.int32,\n                     device=device,\n                 )\n-                metadata.cu_seqlens_k = torch.nn.functional.pad(\n-                    torch.cumsum(\n-                        metadata.cache_seqlens_int32, dim=0, dtype=torch.int32\n-                    ),\n-                    (1, 0),\n+                _tmp_cum = torch.cumsum(\n+                    metadata.cache_seqlens_int32, dim=0, dtype=torch.int32\n                 )\n+                metadata.cu_seqlens_k = torch.empty(\n+                    _tmp_cum.numel() + 1, dtype=torch.int32, device=_tmp_cum.device\n+                )\n+                metadata.cu_seqlens_k[0] = 0\n+                metadata.cu_seqlens_k[1:].copy_(_tmp_cum)\n                 metadata.page_table = forward_batch.req_to_token_pool.req_to_token[\n                     forward_batch.req_pool_indices, : metadata.max_seq_len_k\n                 ]\n@@ -498,15 +515,13 @@ class FlashAttentionBackend(AttentionBackend):\n                 cols = offsets.expand(\n                     forward_batch.seq_lens.numel(), -1\n                 ) + forward_batch.seq_lens.unsqueeze(1)\n-                cum_len = torch.nn.functional.pad(\n-                    torch.cumsum(\n-                        (\n-                            forward_batch.seq_lens + self.speculative_num_draft_tokens\n-                        ).repeat_interleave(self.speculative_num_draft_tokens),\n-                        dim=0,\n-                    ),\n-                    (1, 0),\n-                )[:-1]\n+                _x = (\n+                    forward_batch.seq_lens + self.speculative_num_draft_tokens\n+                ).repeat_interleave(self.speculative_num_draft_tokens)\n+                _tmp_cum = torch.cumsum(_x, dim=0)\n+                cum_len = torch.empty_like(_tmp_cum)\n+                cum_len[0] = 0\n+                cum_len[1:] = _tmp_cum[:-1]\n                 mask_extraction_indices = (\n                     cols.repeat_interleave(self.speculative_num_draft_tokens, dim=0)\n                     + cum_len[:, None]\n@@ -543,12 +558,16 @@ class FlashAttentionBackend(AttentionBackend):\n                 )  # (bsz, draft_num)\n                 metadata_expand.page_table = non_masked_page_table.gather(1, sort_order)\n                 metadata_expand.cache_seqlens_int32 = mask.sum(dim=1).to(torch.int32)\n-                metadata_expand.cu_seqlens_k = torch.nn.functional.pad(\n-                    torch.cumsum(\n-                        metadata_expand.cache_seqlens_int32, dim=0, dtype=torch.int32\n-                    ),\n-                    (1, 0),\n+                _tmp_cum = torch.cumsum(\n+                    metadata_expand.cache_seqlens_int32, dim=0, dtype=torch.int32\n                 )\n+                metadata_expand.cu_seqlens_k = torch.empty(\n+                    _tmp_cum.numel() + 1,\n+                    dtype=torch.int32,\n+                    device=_tmp_cum.device,\n+                )\n+                metadata_expand.cu_seqlens_k[0] = 0\n+                metadata_expand.cu_seqlens_k[1:].copy_(_tmp_cum)\n                 metadata_expand.max_seq_len_k = (\n                     metadata_expand.cache_seqlens_int32.max().item()\n                 )\n@@ -556,9 +575,12 @@ class FlashAttentionBackend(AttentionBackend):\n         elif forward_batch.forward_mode.is_extend_or_draft_extend_or_mixed():\n             metadata.cache_seqlens_int32 = seqlens_in_batch.to(torch.int32)\n             metadata.max_seq_len_k = forward_batch.seq_lens_cpu.max().item()\n-            metadata.cu_seqlens_k = torch.nn.functional.pad(\n-                torch.cumsum(seqlens_in_batch, dim=0, dtype=torch.int32), (1, 0)\n+            _tmp_cum = torch.cumsum(seqlens_in_batch, dim=0, dtype=torch.int32)\n+            metadata.cu_seqlens_k = torch.empty(\n+                _tmp_cum.numel() + 1, dtype=torch.int32, device=_tmp_cum.device\n             )\n+            metadata.cu_seqlens_k[0] = 0\n+            metadata.cu_seqlens_k[1:].copy_(_tmp_cum)\n             metadata.page_table = forward_batch.req_to_token_pool.req_to_token[\n                 forward_batch.req_pool_indices, : metadata.max_seq_len_k\n             ]\n@@ -569,9 +591,12 @@ class FlashAttentionBackend(AttentionBackend):\n             ):\n                 extend_seq_lens = forward_batch.extend_seq_lens\n                 metadata.max_seq_len_q = max(forward_batch.extend_seq_lens_cpu)\n-                metadata.cu_seqlens_q = torch.nn.functional.pad(\n-                    torch.cumsum(extend_seq_lens, dim=0, dtype=torch.int32), (1, 0)\n+                _tmp_cum = torch.cumsum(extend_seq_lens, dim=0, dtype=torch.int32)\n+                metadata.cu_seqlens_q = torch.empty(\n+                    _tmp_cum.numel() + 1, dtype=torch.int32, device=_tmp_cum.device\n                 )\n+                metadata.cu_seqlens_q[0] = 0\n+                metadata.cu_seqlens_q[1:].copy_(_tmp_cum)\n             else:\n                 metadata.max_seq_len_q = metadata.max_seq_len_k\n                 metadata.cu_seqlens_q = metadata.cu_seqlens_k\n@@ -587,10 +612,14 @@ class FlashAttentionBackend(AttentionBackend):\n             ), \"Only encoder size 1 is supported for now\"\n \n             metadata.encoder_lens_int32 = forward_batch.encoder_lens.to(torch.int32)\n-            metadata.encoder_cu_seqlens_k = torch.nn.functional.pad(\n-                torch.cumsum(metadata.encoder_lens_int32, dim=0, dtype=torch.int32),\n-                (1, 0),\n+            _tmp_cum = torch.cumsum(\n+                metadata.encoder_lens_int32, dim=0, dtype=torch.int32\n+            )\n+            metadata.encoder_cu_seqlens_k = torch.empty(\n+                _tmp_cum.numel() + 1, dtype=torch.int32, device=_tmp_cum.device\n             )\n+            metadata.encoder_cu_seqlens_k[0] = 0\n+            metadata.encoder_cu_seqlens_k[1:].copy_(_tmp_cum)\n             metadata.encoder_max_seq_len_k = metadata.encoder_lens_int32.max().item()\n             metadata.encoder_page_table = forward_batch.req_to_token_pool.req_to_token[\n                 forward_batch.req_pool_indices, : metadata.encoder_max_seq_len_k\n@@ -1134,16 +1163,16 @@ class FlashAttentionBackend(AttentionBackend):\n             \"cu_seqlens_q\": torch.arange(\n                 0, max_bs + 1, dtype=torch.int32, device=self.device\n             ),\n-            \"cu_seqlens_k\": torch.zeros(\n+            \"cu_seqlens_k\": torch.empty(\n                 max_bs + 1, dtype=torch.int32, device=self.device\n             ),\n-            \"page_table\": torch.zeros(\n+            \"page_table\": torch.empty(\n                 max_bs,\n                 (self.max_context_len + self.page_size - 1) // self.page_size,\n                 dtype=torch.int32,\n                 device=self.device,\n             ),\n-            \"page_table_draft_decode\": torch.zeros(\n+            \"page_table_draft_decode\": torch.empty(\n                 max_bs,\n                 (self.max_context_len + self.page_size - 1) // self.page_size,\n                 dtype=torch.int32,\n@@ -1167,10 +1196,10 @@ class FlashAttentionBackend(AttentionBackend):\n                     dtype=torch.int32,\n                     device=self.device,\n                 ),\n-                \"cu_seqlens_k\": torch.zeros(\n+                \"cu_seqlens_k\": torch.empty(\n                     max_bs + 1, dtype=torch.int32, device=self.device\n                 ),\n-                \"page_table\": torch.zeros(\n+                \"page_table\": torch.empty(\n                     max_bs,\n                     self.max_context_len,\n                     dtype=torch.int32,\n@@ -1200,7 +1229,7 @@ class FlashAttentionBackend(AttentionBackend):\n                     dtype=torch.int32,\n                     device=self.device,\n                 ),\n-                \"page_table\": torch.zeros(\n+                \"page_table\": torch.empty(\n                     max_bs * self.topk,\n                     decode_length,\n                     dtype=torch.int32,\n@@ -1223,10 +1252,10 @@ class FlashAttentionBackend(AttentionBackend):\n                     dtype=torch.int32,\n                     device=self.device,\n                 ),\n-                \"cu_seqlens_k\": torch.zeros(\n+                \"cu_seqlens_k\": torch.empty(\n                     max_bs + 1, dtype=torch.int32, device=self.device\n                 ),\n-                \"page_table\": torch.zeros(\n+                \"page_table\": torch.empty(\n                     max_bs,\n                     (self.max_context_len + self.page_size - 1) // self.page_size,\n                     dtype=torch.int32,\n@@ -1249,10 +1278,10 @@ class FlashAttentionBackend(AttentionBackend):\n                     dtype=torch.int32,\n                     device=self.device,\n                 ),\n-                \"cu_seqlens_k\": torch.zeros(\n+                \"cu_seqlens_k\": torch.empty(\n                     max_bs + 1, dtype=torch.int32, device=self.device\n                 ),\n-                \"page_table\": torch.zeros(\n+                \"page_table\": torch.empty(\n                     max_bs,\n                     self.max_context_len,\n                     dtype=torch.int32,\n@@ -1266,7 +1295,7 @@ class FlashAttentionBackend(AttentionBackend):\n                     dtype=torch.int32,\n                     device=self.device,\n                 ),\n-                \"cu_seqlens_k\": torch.zeros(\n+                \"cu_seqlens_k\": torch.empty(\n                     max_bs * self.speculative_num_draft_tokens + 1,\n                     dtype=torch.int32,\n                     device=self.device,\n@@ -1277,7 +1306,7 @@ class FlashAttentionBackend(AttentionBackend):\n                     dtype=torch.int32,\n                     device=self.device,\n                 ),\n-                \"page_table\": torch.zeros(\n+                \"page_table\": torch.empty(\n                     max_bs * self.speculative_num_draft_tokens,\n                     self.speculative_num_draft_tokens,\n                     dtype=torch.int32,\n@@ -1286,16 +1315,16 @@ class FlashAttentionBackend(AttentionBackend):\n             }\n \n         self.encoder_metadata = {\n-            \"encoder_page_table\": torch.zeros(\n+            \"encoder_page_table\": torch.empty(\n                 max_bs,\n                 self.max_context_len,\n                 dtype=torch.int32,\n                 device=self.device,\n             ),\n-            \"encoder_lens_int32\": torch.zeros(\n+            \"encoder_lens_int32\": torch.empty(\n                 max_bs, dtype=torch.int32, device=self.device\n             ),\n-            \"encoder_cu_seqlens_k\": torch.zeros(\n+            \"encoder_cu_seqlens_k\": torch.empty(\n                 max_bs + 1, dtype=torch.int32, device=self.device\n             ),\n         }\n@@ -1331,12 +1360,16 @@ class FlashAttentionBackend(AttentionBackend):\n                     metadata.cu_seqlens_q = self.decode_cuda_graph_metadata[\n                         \"cu_seqlens_q\"\n                     ][: bs + 1]\n-                    metadata.cu_seqlens_k = torch.nn.functional.pad(\n-                        torch.cumsum(\n-                            metadata.cache_seqlens_int32, dim=0, dtype=torch.int32\n-                        ),\n-                        (1, 0),\n+                    _tmp_cum = torch.cumsum(\n+                        metadata.cache_seqlens_int32, dim=0, dtype=torch.int32\n                     )\n+                    metadata.cu_seqlens_k = torch.empty(\n+                        _tmp_cum.numel() + 1,\n+                        dtype=torch.int32,\n+                        device=_tmp_cum.device,\n+                    )\n+                    metadata.cu_seqlens_k[0] = 0\n+                    metadata.cu_seqlens_k[1:].copy_(_tmp_cum)\n                     metadata.page_table = self.decode_cuda_graph_metadata[\n                         \"page_table_draft_decode\"\n                     ][req_pool_indices, :]\n@@ -1390,9 +1423,12 @@ class FlashAttentionBackend(AttentionBackend):\n                 metadata.cache_seqlens_int32 = seq_lens.to(torch.int32)\n                 batch_size = len(seq_lens)\n                 device = seq_lens.device\n-                metadata.cu_seqlens_k = torch.nn.functional.pad(\n-                    torch.cumsum(seq_lens, dim=0, dtype=torch.int32), (1, 0)\n+                _tmp_cum = torch.cumsum(seq_lens, dim=0, dtype=torch.int32)\n+                metadata.cu_seqlens_k = torch.empty(\n+                    _tmp_cum.numel() + 1, dtype=torch.int32, device=_tmp_cum.device\n                 )\n+                metadata.cu_seqlens_k[0] = 0\n+                metadata.cu_seqlens_k[1:].copy_(_tmp_cum)\n                 # Precompute maximum sequence length\n                 metadata.max_seq_len_k = seq_lens.max().item()\n                 # Precompute page table\n@@ -1525,12 +1561,10 @@ class FlashAttentionBackend(AttentionBackend):\n                     metadata.max_seq_len_k = seq_lens_cpu.max().item() + (\n                         self.speculative_step_id + 1\n                     )\n-                    metadata.cu_seqlens_k.copy_(\n-                        torch.nn.functional.pad(\n-                            torch.cumsum(\n-                                metadata.cache_seqlens_int32, dim=0, dtype=torch.int32\n-                            ),\n-                            (1, 0),\n+                    metadata.cu_seqlens_k[0] = 0\n+                    metadata.cu_seqlens_k[1:].copy_(\n+                        torch.cumsum(\n+                            metadata.cache_seqlens_int32, dim=0, dtype=torch.int32\n                         )\n                     )\n \n@@ -1554,12 +1588,10 @@ class FlashAttentionBackend(AttentionBackend):\n                     # metadata.max_seq_len_q = self.topk, already set in capture\n                     metadata.max_seq_len_k = seq_lens_cpu.max().item()\n                     # metadata.cu_seqlens_q already set in capture\n-                    metadata.cu_seqlens_k.copy_(\n-                        torch.nn.functional.pad(\n-                            torch.cumsum(\n-                                metadata.cache_seqlens_int32, dim=0, dtype=torch.int32\n-                            ),\n-                            (1, 0),\n+                    metadata.cu_seqlens_k[0] = 0\n+                    metadata.cu_seqlens_k[1:].copy_(\n+                        torch.cumsum(\n+                            metadata.cache_seqlens_int32, dim=0, dtype=torch.int32\n                         )\n                     )\n \n@@ -1587,9 +1619,12 @@ class FlashAttentionBackend(AttentionBackend):\n                 metadata.max_seq_len_k = max_len\n \n                 metadata.cache_seqlens_int32 = seq_lens.to(torch.int32)\n-                metadata.cu_seqlens_k = torch.nn.functional.pad(\n-                    torch.cumsum(seq_lens, dim=0, dtype=torch.int32), (1, 0)\n+                _tmp_cum = torch.cumsum(seq_lens, dim=0, dtype=torch.int32)\n+                metadata.cu_seqlens_k = torch.empty(\n+                    _tmp_cum.numel() + 1, dtype=torch.int32, device=_tmp_cum.device\n                 )\n+                metadata.cu_seqlens_k[0] = 0\n+                metadata.cu_seqlens_k[1:].copy_(_tmp_cum)\n \n                 max_seq_pages = (\n                     metadata.max_seq_len_k + self.page_size - 1\n@@ -1615,12 +1650,10 @@ class FlashAttentionBackend(AttentionBackend):\n                 metadata.max_seq_len_k = (\n                     seq_lens_cpu.max().item() + self.speculative_num_draft_tokens\n                 )\n-                metadata.cu_seqlens_k.copy_(\n-                    torch.nn.functional.pad(\n-                        torch.cumsum(\n-                            metadata.cache_seqlens_int32, dim=0, dtype=torch.int32\n-                        ),\n-                        (1, 0),\n+                metadata.cu_seqlens_k[0] = 0\n+                metadata.cu_seqlens_k[1:].copy_(\n+                    torch.cumsum(\n+                        metadata.cache_seqlens_int32, dim=0, dtype=torch.int32\n                     )\n                 )\n                 max_seq_pages = (\n@@ -1640,12 +1673,10 @@ class FlashAttentionBackend(AttentionBackend):\n                 # metadata.max_seq_len_q = self.speculative_num_draft_tokens, already set in capture\n                 metadata.max_seq_len_k = seq_lens_cpu.max().item()\n                 # metadata.cu_seqlens_q already set in capture\n-                metadata.cu_seqlens_k.copy_(\n-                    torch.nn.functional.pad(\n-                        torch.cumsum(\n-                            metadata.cache_seqlens_int32, dim=0, dtype=torch.int32\n-                        ),\n-                        (1, 0),\n+                metadata.cu_seqlens_k[0] = 0\n+                metadata.cu_seqlens_k[1:].copy_(\n+                    torch.cumsum(\n+                        metadata.cache_seqlens_int32, dim=0, dtype=torch.int32\n                     )\n                 )\n                 page_table = self.req_to_token[\n@@ -1664,15 +1695,13 @@ class FlashAttentionBackend(AttentionBackend):\n                     0\n                 )  # shape: (1, self.speculative_num_draft_tokens)\n                 cols = offsets.expand(seq_lens.numel(), -1) + seq_lens.unsqueeze(1)\n-                cum_len = torch.nn.functional.pad(\n-                    torch.cumsum(\n-                        (\n-                            seq_lens + self.speculative_num_draft_tokens\n-                        ).repeat_interleave(self.speculative_num_draft_tokens),\n-                        dim=0,\n-                    ),\n-                    (1, 0),\n-                )[:-1]\n+                _x = (\n+                    seq_lens + self.speculative_num_draft_tokens\n+                ).repeat_interleave(self.speculative_num_draft_tokens)\n+                _tmp_cum = torch.cumsum(_x, dim=0)\n+                cum_len = torch.empty_like(_tmp_cum)\n+                cum_len[0] = 0\n+                cum_len[1:] = _tmp_cum[:-1]\n                 mask_extraction_indices = (\n                     cols.repeat_interleave(self.speculative_num_draft_tokens, dim=0)\n                     + cum_len[:, None]\n@@ -1704,14 +1733,10 @@ class FlashAttentionBackend(AttentionBackend):\n                 metadata_expand.cache_seqlens_int32.copy_(\n                     mask.sum(dim=1).to(torch.int32)\n                 )\n-                metadata_expand.cu_seqlens_k.copy_(\n-                    torch.nn.functional.pad(\n-                        torch.cumsum(\n-                            metadata_expand.cache_seqlens_int32,\n-                            dim=0,\n-                            dtype=torch.int32,\n-                        ),\n-                        (1, 0),\n+                metadata_expand.cu_seqlens_k[0] = 0\n+                metadata_expand.cu_seqlens_k[1:].copy_(\n+                    torch.cumsum(\n+                        metadata_expand.cache_seqlens_int32, dim=0, dtype=torch.int32\n                     )\n                 )\n                 metadata_expand.max_seq_len_k = (\n@@ -1722,11 +1747,9 @@ class FlashAttentionBackend(AttentionBackend):\n             # Only support encoder size 1 for now\n             metadata.encoder_max_seq_len_k = encoder_lens[0]\n             metadata.encoder_lens_int32.copy_(encoder_lens[:1])\n-            metadata.encoder_cu_seqlens_k.copy_(\n-                torch.nn.functional.pad(\n-                    torch.cumsum(metadata.encoder_lens_int32, dim=0, dtype=torch.int32),\n-                    (1, 0),\n-                )\n+            metadata.encoder_cu_seqlens_k[0] = 0\n+            metadata.encoder_cu_seqlens_k[1:].copy_(\n+                torch.cumsum(metadata.encoder_lens_int32, dim=0, dtype=torch.int32)\n             )\n \n             metadata.encoder_page_table[:, : metadata.encoder_max_seq_len_k].copy_(\n", "model_name_or_path": "gpt-5-2025-08-07"}
