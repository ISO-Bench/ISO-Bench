{"instance_id": "/home/ubuntu/OmniPerf-Bench/sglan__repo-89a5541", "model_patch": "diff --git a/python/sglang/srt/layers/moe/fused_moe_triton/fused_moe.py b/python/sglang/srt/layers/moe/fused_moe_triton/fused_moe.py\nindex c9e7547bf..ca25d026f 100644\n--- a/python/sglang/srt/layers/moe/fused_moe_triton/fused_moe.py\n+++ b/python/sglang/srt/layers/moe/fused_moe_triton/fused_moe.py\n@@ -44,6 +44,12 @@ if _is_cuda or _is_hip:\n     from sgl_kernel import moe_align_block_size as sgl_moe_align_block_size\n \n \n+# Simple per-device buffer cache to avoid repeated allocations\n+_moe_align_buf_cache: Dict[\n+    Tuple[torch.device, int], Tuple[torch.Tensor, torch.Tensor]\n+] = {}\n+\n+\n @triton.jit\n def write_zeros_to_output(\n     c_ptr,\n@@ -565,6 +571,8 @@ def moe_align_block_size_stage3(\n     num_experts: tl.constexpr,\n     block_size: tl.constexpr,\n ):\n+    # Ensure cumsum[0] is zero even if uninitialized buffer is provided\n+    tl.store(cumsum_ptr + 0, 0)\n     last_cumsum = 0\n     off_cnt = num_experts * num_experts\n     for i in range(1, num_experts + 1):\n@@ -614,10 +622,21 @@ def moe_align_block_size_triton(\n ) -> None:\n     numel = topk_ids.numel()\n     grid = (num_experts,)\n-    tokens_cnts = torch.zeros(\n-        (num_experts + 1, num_experts), dtype=torch.int32, device=topk_ids.device\n-    )\n-    cumsum = torch.zeros((num_experts + 1,), dtype=torch.int32, device=topk_ids.device)\n+    # Reuse cached buffers when shape/device match to reduce allocation overhead\n+    cache_key = (topk_ids.device, int(num_experts))\n+    tokens_cnts: torch.Tensor\n+    cumsum: torch.Tensor\n+    cached = _moe_align_buf_cache.get(cache_key)\n+    if cached is None or cached[0].shape != (num_experts + 1, num_experts):\n+        tokens_cnts = torch.empty(\n+            (num_experts + 1, num_experts), dtype=torch.int32, device=topk_ids.device\n+        )\n+        cumsum = torch.empty((num_experts + 1,), dtype=torch.int32, device=topk_ids.device)\n+        _moe_align_buf_cache[cache_key] = (tokens_cnts, cumsum)\n+    else:\n+        tokens_cnts, cumsum = cached\n+    # Zero only the rows written by stage1 (rows 1..num_experts)\n+    tokens_cnts[1:, :].zero_()\n     tokens_per_thread = ceil_div(numel, num_experts)\n \n     moe_align_block_size_stage1[grid](\n@@ -695,7 +714,6 @@ def moe_align_block_size(\n     sorted_ids = torch.empty(\n         (max_num_tokens_padded,), dtype=torch.int32, device=topk_ids.device\n     )\n-    sorted_ids.fill_(topk_ids.numel())\n     max_num_m_blocks = triton.cdiv(max_num_tokens_padded, block_size)\n     expert_ids = torch.empty(\n         (max_num_m_blocks,), dtype=torch.int32, device=topk_ids.device\n@@ -711,13 +729,13 @@ def moe_align_block_size(\n             num_tokens_post_pad,\n         )\n     else:\n-        token_cnts_buffer = torch.zeros(\n-            (num_experts + 1) * num_experts,\n-            dtype=torch.int32,\n-            device=topk_ids.device,\n+        # token_cnts_buffer is unused in native kernel; pass a minimal buffer\n+        token_cnts_buffer = torch.empty(\n+            (1,), dtype=torch.int32, device=topk_ids.device\n         )\n-        cumsum_buffer = torch.zeros(\n-            num_experts + 1, dtype=torch.int32, device=topk_ids.device\n+        # Native kernel writes cumsum completely, no need to pre-initialize\n+        cumsum_buffer = torch.empty(\n+            (num_experts + 1,), dtype=torch.int32, device=topk_ids.device\n         )\n \n         sgl_moe_align_block_size(\ndiff --git a/sgl-kernel/csrc/moe/moe_align_kernel.cu b/sgl-kernel/csrc/moe/moe_align_kernel.cu\nindex 6ffa73924..7c872a136 100644\n--- a/sgl-kernel/csrc/moe/moe_align_kernel.cu\n+++ b/sgl-kernel/csrc/moe/moe_align_kernel.cu\n@@ -91,6 +91,19 @@ __global__ void moe_align_block_size_kernel(\n \n   __syncthreads();\n \n+  // Initialize sorted_token_ids with sentinel values (numel) for all padded slots\n+  // so we can avoid an extra device-wide fill from Python.\n+  // Each thread takes care of one expert's padded span.\n+  if (threadIdx.x < num_experts) {\n+    int start = cumsum[threadIdx.x];\n+    int end = cumsum[threadIdx.x + 1];\n+    for (int i = start; i < end; ++i) {\n+      sorted_token_ids[i] = static_cast<int32_t>(numel);\n+    }\n+  }\n+\n+  __syncthreads();\n+\n   if (threadIdx.x < num_experts) {\n     for (int i = cumsum[threadIdx.x]; i < cumsum[threadIdx.x + 1]; i += block_size) {\n       expert_ids[i / block_size] = threadIdx.x;\n", "model_name_or_path": "gpt-5-2025-08-07"}
