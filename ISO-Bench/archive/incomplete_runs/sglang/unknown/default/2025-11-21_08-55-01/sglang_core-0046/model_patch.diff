diff --git a/python/sglang/srt/distributed/parallel_state.py b/python/sglang/srt/distributed/parallel_state.py
index fdde7dde8..da4c14fd0 100644
--- a/python/sglang/srt/distributed/parallel_state.py
+++ b/python/sglang/srt/distributed/parallel_state.py
@@ -246,25 +246,34 @@ class GroupCoordinator:
         self.use_xpu_communicator = use_xpu_communicator
         self.use_message_queue_broadcaster = use_message_queue_broadcaster
 
-        # lazy import to avoid documentation build error
-        from sglang.srt.distributed.device_communicators.custom_all_reduce import (
-            CustomAllreduce,
-        )
-        from sglang.srt.distributed.device_communicators.pynccl import (
-            PyNcclCommunicator,
-        )
+        # Reusable small CPU buffer to avoid frequent tiny allocations
+        # during object size send/recv in point-to-point ops.
+        self._size_tensor_cpu = torch.empty(1, dtype=torch.long, device="cpu")
+
+        # Lazy imports only when actually needed to avoid importing
+        # heavy optional backends on CPU-only or single-process setups.
 
-        self.pynccl_comm: Optional[PyNcclCommunicator] = None
+        # PyNccl communicator
+        self.pynccl_comm: Optional[Any] = None
         if use_pynccl and self.world_size > 1:
+            from sglang.srt.distributed.device_communicators.pynccl import (
+                PyNcclCommunicator,
+            )
+
             self.pynccl_comm = PyNcclCommunicator(
                 group=self.cpu_group,
                 device=self.device,
             )
 
-        self.ca_comm: Optional[CustomAllreduce] = None
+        # Custom AllReduce
+        self.ca_comm: Optional[Any] = None
         if use_custom_allreduce and self.world_size > 1:
             # Initialize a custom fast all-reduce implementation.
             try:
+                from sglang.srt.distributed.device_communicators.custom_all_reduce import (
+                    CustomAllreduce,
+                )
+
                 self.ca_comm = CustomAllreduce(
                     group=self.cpu_group,
                     device=self.device,
@@ -275,28 +284,31 @@ class GroupCoordinator:
                     "warning, specify --disable-custom-all-reduce explicitly."
                 )
 
-        from sglang.srt.distributed.device_communicators.hpu_communicator import (
-            HpuCommunicator,
-        )
-
-        self.hpu_communicator: Optional[HpuCommunicator] = None
+        # HPU communicator
+        self.hpu_communicator: Optional[Any] = None
         if use_hpu_communicator and self.world_size > 1:
-            self.hpu_communicator = HpuCommunicator(group=self.device_group)
+            from sglang.srt.distributed.device_communicators.hpu_communicator import (
+                HpuCommunicator,
+            )
 
-        from sglang.srt.distributed.device_communicators.xpu_communicator import (
-            XpuCommunicator,
-        )
+            self.hpu_communicator = HpuCommunicator(group=self.device_group)
 
-        self.xpu_communicator: Optional[XpuCommunicator] = None
+        # XPU communicator
+        self.xpu_communicator: Optional[Any] = None
         if use_xpu_communicator and self.world_size > 1:
-            self.xpu_communicator = XpuCommunicator(group=self.device_group)
+            from sglang.srt.distributed.device_communicators.xpu_communicator import (
+                XpuCommunicator,
+            )
 
-        from sglang.srt.distributed.device_communicators.shm_broadcast import (
-            MessageQueue,
-        )
+            self.xpu_communicator = XpuCommunicator(group=self.device_group)
 
-        self.mq_broadcaster: Optional[MessageQueue] = None
+        # Shared-memory broadcaster
+        self.mq_broadcaster: Optional[Any] = None
         if use_message_queue_broadcaster and self.world_size > 1:
+            from sglang.srt.distributed.device_communicators.shm_broadcast import (
+                MessageQueue,
+            )
+
             self.mq_broadcaster = MessageQueue.create_from_process_group(
                 self.cpu_group, 1 << 22, 6
             )
@@ -624,9 +636,8 @@ class GroupCoordinator:
         # Serialize object to tensor and get the size as well
         object_tensor = torch.frombuffer(pickle.dumps(obj), dtype=torch.uint8)
 
-        size_tensor = torch.tensor(
-            [object_tensor.numel()], dtype=torch.long, device="cpu"
-        )
+        size_tensor = self._size_tensor_cpu
+        size_tensor[0] = object_tensor.numel()
 
         # Send object size
 
@@ -647,7 +658,7 @@ class GroupCoordinator:
             src != self.rank_in_group
         ), "Invalid source rank. Source rank is the same as the current rank."
 
-        size_tensor = torch.empty(1, dtype=torch.long, device="cpu")
+        size_tensor = self._size_tensor_cpu
 
         # Receive object size
         rank_size = torch.distributed.recv(
@@ -1055,11 +1066,6 @@ def init_distributed_environment(
             world_size=world_size,
             rank=rank,
             timeout=timeout,
-            device_id=torch.device(
-                f"cuda:{torch.cuda.current_device()}"
-                if hasattr(torch, "cuda") and torch.cuda.is_available()
-                else None
-            ),  # Allow NCCL to eagerly init communicator
         )
 
     # set the local rank
@@ -1289,8 +1295,8 @@ def in_the_same_node_as(pg: ProcessGroup, source_rank: int = 0) -> List[bool]:
     rank = torch.distributed.get_rank(group=pg)
     world_size = torch.distributed.get_world_size(group=pg)
 
-    # local tensor in each process to store the result
-    is_in_the_same_node = torch.tensor([0] * world_size, dtype=torch.int32)
+    # local tensor in each process to store the result (avoid Python list init)
+    is_in_the_same_node = torch.zeros(world_size, dtype=torch.int32)
 
     # global ranks of the processes in the group
     ranks = torch.distributed.get_process_group_ranks(pg)
