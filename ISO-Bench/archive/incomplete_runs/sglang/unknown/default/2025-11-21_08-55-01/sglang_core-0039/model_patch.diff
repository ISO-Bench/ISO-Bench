diff --git a/python/sglang/srt/hf_transformers_utils.py b/python/sglang/srt/hf_transformers_utils.py
index 56d7c8a1f..6a2582e42 100644
--- a/python/sglang/srt/hf_transformers_utils.py
+++ b/python/sglang/srt/hf_transformers_utils.py
@@ -164,7 +164,7 @@ def get_tokenizer(
             "slowdown. Consider using a fast tokenizer instead."
         )
 
-    handle_additional_stop_token_ids(tokenizer)
+    attach_additional_stop_token_ids(tokenizer)
     return tokenizer
 
 
@@ -184,11 +184,11 @@ def get_processor(
         **kwargs,
     )
 
-    handle_additional_stop_token_ids(processor.tokenizer)
+    attach_additional_stop_token_ids(processor.tokenizer)
     return processor
 
 
-def handle_additional_stop_token_ids(tokenizer):
+def attach_additional_stop_token_ids(tokenizer):
     # Special handling for stop token <|eom_id|> generated by llama 3 tool use.
     if "<|eom_id|>" in tokenizer.get_added_vocab():
         tokenizer.additional_stop_token_ids = set(
diff --git a/python/sglang/srt/layers/sampler.py b/python/sglang/srt/layers/sampler.py
index 9ae5801cc..f43bb20f7 100644
--- a/python/sglang/srt/layers/sampler.py
+++ b/python/sglang/srt/layers/sampler.py
@@ -60,9 +60,10 @@ class Sampler(nn.Module):
 
             if global_server_args_dict["sampling_backend"] == "flashinfer":
                 max_top_k_round, batch_size = 32, probs.shape[0]
-                uniform_samples = torch.rand(
+                # Use empty + in-place uniform_ to avoid unnecessary zero-initialization
+                uniform_samples = torch.empty(
                     (max_top_k_round, batch_size), device=probs.device
-                )
+                ).uniform_()
                 if sampling_info.need_min_p_sampling:
                     probs = top_k_renorm_prob(probs, sampling_info.top_ks)
                     probs = top_p_renorm_prob(probs, sampling_info.top_ps)
diff --git a/python/sglang/srt/managers/schedule_batch.py b/python/sglang/srt/managers/schedule_batch.py
index fac008d3f..aa6701cf1 100644
--- a/python/sglang/srt/managers/schedule_batch.py
+++ b/python/sglang/srt/managers/schedule_batch.py
@@ -543,8 +543,8 @@ class ScheduleBatch:
                     or len(req.prefix_indices) >= im.num_image_tokens
                 )
 
-        self.encoder_lens = torch.tensor(self.encoder_lens_cpu, dtype=torch.int32).to(
-            self.device, non_blocking=True
+        self.encoder_lens = torch.tensor(
+            self.encoder_lens_cpu, dtype=torch.int32, device=self.device
         )
 
         # Strip encoder infos
@@ -574,12 +574,10 @@ class ScheduleBatch:
             pt += req.extend_input_len
 
         # Reassign
-        self.input_ids = torch.tensor(sum(input_ids, []), dtype=torch.int32).to(
-            self.device, non_blocking=True
-        )
-        self.seq_lens = torch.tensor(seq_lens, dtype=torch.int32).to(
-            self.device, non_blocking=True
+        self.input_ids = torch.tensor(
+            sum(input_ids, []), dtype=torch.int32, device=self.device
         )
+        self.seq_lens = torch.tensor(seq_lens, dtype=torch.int32, device=self.device)
 
         if not decoder_out_cache_loc:
             self.out_cache_loc = torch.empty(0, dtype=torch.int32).to(
@@ -589,8 +587,8 @@ class ScheduleBatch:
             self.out_cache_loc = torch.cat(decoder_out_cache_loc)
 
         if not encoder_out_cache_loc:
-            self.encoder_out_cache_loc = torch.empty(0, dtype=torch.int32).to(
-                self.device, non_blocking=True
+            self.encoder_out_cache_loc = torch.empty(
+                0, dtype=torch.int32, device=self.device
             )
         else:
             self.encoder_out_cache_loc = torch.cat(encoder_out_cache_loc)
@@ -645,15 +643,13 @@ class ScheduleBatch:
             pt += req.extend_input_len
 
         # Set fields
-        self.input_ids = torch.tensor(sum(input_ids, []), dtype=torch.int32).to(
-            self.device, non_blocking=True
-        )
-        self.req_pool_indices = torch.tensor(req_pool_indices, dtype=torch.int32).to(
-            self.device, non_blocking=True
+        self.input_ids = torch.tensor(
+            sum(input_ids, []), dtype=torch.int32, device=self.device
         )
-        self.seq_lens = torch.tensor(seq_lens, dtype=torch.int32).to(
-            self.device, non_blocking=True
+        self.req_pool_indices = torch.tensor(
+            req_pool_indices, dtype=torch.int32, device=self.device
         )
+        self.seq_lens = torch.tensor(seq_lens, dtype=torch.int32, device=self.device)
 
         self.out_cache_loc = out_cache_loc
 
diff --git a/python/sglang/srt/sampling/penaltylib/penalizers/min_new_tokens.py b/python/sglang/srt/sampling/penaltylib/penalizers/min_new_tokens.py
index c9e0f078e..dcda1320d 100644
--- a/python/sglang/srt/sampling/penaltylib/penalizers/min_new_tokens.py
+++ b/python/sglang/srt/sampling/penaltylib/penalizers/min_new_tokens.py
@@ -11,7 +11,9 @@ class BatchedMinNewTokensPenalizer(_BatchedPenalizer):
     """
 
     min_new_tokens: torch.Tensor = None
-    stop_token_penalties: torch.Tensor = None
+    # Precomputed padded stop token ids per request to avoid dense allocations
+    stop_token_ids_padded: torch.Tensor = None
+    stop_token_valid_mask: torch.Tensor = None
     len_output_tokens: torch.Tensor = None
 
     def _is_required(self) -> bool:
@@ -43,22 +45,9 @@ class BatchedMinNewTokensPenalizer(_BatchedPenalizer):
             batch_first=True,
             padding_value=self.orchestrator.vocab_size,
         )
-        self.stop_token_penalties = torch.zeros(
-            size=(self.orchestrator.batch_size(), self.orchestrator.vocab_size + 1),
-            dtype=torch.float32,
-            device=self.orchestrator.device,
-        ).scatter_add_(
-            dim=1,
-            index=padded_stop_token_ids,
-            src=torch.full_like(
-                input=padded_stop_token_ids,
-                dtype=torch.float32,
-                fill_value=float("-inf"),
-                device=self.orchestrator.device,
-            ),
-        )[
-            :, : self.orchestrator.vocab_size
-        ]
+        # Cache padded stop ids and mask for use during apply without allocating
+        self.stop_token_ids_padded = padded_stop_token_ids
+        self.stop_token_valid_mask = padded_stop_token_ids != self.orchestrator.vocab_size
 
         self.len_output_tokens = torch.zeros(
             size=(self.orchestrator.batch_size(), 1),
@@ -68,11 +57,13 @@ class BatchedMinNewTokensPenalizer(_BatchedPenalizer):
 
     def _teardown(self):
         del self.min_new_tokens
-        del self.stop_token_penalties
+        del self.stop_token_ids_padded
+        del self.stop_token_valid_mask
         del self.len_output_tokens
 
         self.min_new_tokens = None
-        self.stop_token_penalties = None
+        self.stop_token_ids_padded = None
+        self.stop_token_valid_mask = None
         self.len_output_tokens = None
 
     def _cumulate_input_tokens(self, input_ids: _TokenIDs):
@@ -82,23 +73,36 @@ class BatchedMinNewTokensPenalizer(_BatchedPenalizer):
         self.len_output_tokens += 1
 
     def _apply(self, logits: torch.Tensor) -> torch.Tensor:
-        mask = (self.len_output_tokens < self.min_new_tokens).expand_as(logits)
-        logits[mask] += self.stop_token_penalties[mask]
+        rows_to_penalize = (self.len_output_tokens < self.min_new_tokens).squeeze(1)
+        if not torch.any(rows_to_penalize):
+            return logits
+
+        rows_idx = torch.nonzero(rows_to_penalize, as_tuple=False).squeeze(1)
+        # For each selected row, set logits at stop-token indices to -inf
+        for i in rows_idx.tolist():
+            valid = self.stop_token_valid_mask[i]
+            idx = self.stop_token_ids_padded[i][valid]
+            # In-place assignment avoids allocating a dense penalty matrix
+            logits[i, idx] = float("-inf")
         return logits
 
     def _filter(
         self, indices_to_keep: typing.List[int], indices_tensor_to_keep: torch.Tensor
     ):
         self.min_new_tokens = self.min_new_tokens[indices_tensor_to_keep]
-        self.stop_token_penalties = self.stop_token_penalties[indices_tensor_to_keep]
+        self.stop_token_ids_padded = self.stop_token_ids_padded[indices_tensor_to_keep]
+        self.stop_token_valid_mask = self.stop_token_valid_mask[indices_tensor_to_keep]
         self.len_output_tokens = self.len_output_tokens[indices_tensor_to_keep]
 
     def _merge(self, their: "BatchedMinNewTokensPenalizer"):
         self.min_new_tokens = torch.cat(
             [self.min_new_tokens, their.min_new_tokens], dim=0
         )
-        self.stop_token_penalties = torch.cat(
-            [self.stop_token_penalties, their.stop_token_penalties], dim=0
+        self.stop_token_ids_padded = torch.cat(
+            [self.stop_token_ids_padded, their.stop_token_ids_padded], dim=0
+        )
+        self.stop_token_valid_mask = torch.cat(
+            [self.stop_token_valid_mask, their.stop_token_valid_mask], dim=0
         )
         self.len_output_tokens = torch.cat(
             [self.len_output_tokens, their.len_output_tokens], dim=0
diff --git a/python/sglang/srt/sampling/sampling_params.py b/python/sglang/srt/sampling/sampling_params.py
index b0863b557..36240dec1 100644
--- a/python/sglang/srt/sampling/sampling_params.py
+++ b/python/sglang/srt/sampling/sampling_params.py
@@ -63,6 +63,8 @@ class SamplingParams:
         self.n = n
         self.json_schema = json_schema
         self.no_stop_trim = no_stop_trim
+        # Internal memoization to avoid repeated updates across normalizations
+        self._added_additional_stop_token_ids = False
 
         # Process some special cases
         if self.temperature < _SAMPLING_EPS:
@@ -135,8 +137,14 @@ class SamplingParams:
             self.stop_str_max_len = stop_str_max_len
 
         # Process stop token ids
-        if tokenizer and tokenizer.additional_stop_token_ids:
+        if (
+            tokenizer
+            and hasattr(tokenizer, "additional_stop_token_ids")
+            and tokenizer.additional_stop_token_ids
+            and not self._added_additional_stop_token_ids
+        ):
             self.stop_token_ids.update(tokenizer.additional_stop_token_ids)
+            self._added_additional_stop_token_ids = True
 
     def to_srt_kwargs(self):
         return {
