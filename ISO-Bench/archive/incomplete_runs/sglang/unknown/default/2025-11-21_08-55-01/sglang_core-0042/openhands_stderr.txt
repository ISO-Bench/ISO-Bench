OpenAI Codex v0.60.1 (research preview)
--------
workdir: /home/ubuntu/OmniPerf-Bench/perf-agents-bench/.work/worktrees/sglang_core/sglang_core-0042
model: gpt-5
provider: openai
approval: never
sandbox: danger-full-access
reasoning effort: high
reasoning summaries: auto
session id: 019aa4e4-364d-7e31-806a-aecef1118216
--------
user
I've uploaded a python code repository in the directory /home/ubuntu/OmniPerf-Bench/perf-agents-bench/.work/worktrees/sglang_core/sglang_core-0042.
Consider the following test script showing an example usage of the repository:

<test_script>
# This is a performance optimization task
# The specific operations to optimize are in the files listed below
# Focus on performance improvements in the target functions

</test_script>

Can you help me implement the necessary changes to the repository so that the runtime of the <test_script> is optimized?

Basic guidelines:
1. Your task is to make changes to non-test files in the /home/ubuntu/OmniPerf-Bench/perf-agents-bench/.work/worktrees/sglang_core/sglang_core-0042 directory to improve the performance of the <test_script>.
2. Make changes while ensuring the repository is functionally equivalent to the original.
3. Do not overoptimize for just the specific inputs in <test_script>. Make general performance improvements for the usage scenario shown.
4. You may need to rebuild the repo for your changes to take effect before testing. Some rebuilds may take time to run, so be patient with running them.

Follow these steps to improve performance:
1. As a first step, explore the repository structure.
2. Create a script ONLY inside /home/ubuntu/OmniPerf-Bench/perf-agents-bench/.work/worktrees/sglang_core/sglang_core-0042/.bench_scratch (e.g., /home/ubuntu/OmniPerf-Bench/perf-agents-bench/.work/worktrees/sglang_core/sglang_core-0042/.bench_scratch/test_opt.py) to reproduce and time the example, then execute it with python <filename.py> from the repo root.
3. Edit the source code of the repository to improve performance.
4. Rebuild and rerun your script to confirm that performance has improved.

Here is an example of the kind of optimizations that have been shown to improve performance in this codebase:

<example_optimization_diff>
diff --git a/python/sglang/srt/managers/io_struct.py b/python/sglang/srt/managers/io_struct.py
index 13eb233bd..1aae28b00 100644
--- a/python/sglang/srt/managers/io_struct.py
+++ b/python/sglang/srt/managers/io_struct.py
@@ -426,8 +426,7 @@ class UpdateWeightsFromDistributedReqOutput:

 @dataclass
 class UpdateWeightsFromTensorReqInput:
-    name: str
-    tensor: torch.Tensor
+    serialized_named_tensors: bytes  # indeed Dict[str, torch.Tensor]


 @dataclass
diff --git a/python/sglang/srt/managers/tp_worker.py b/python/sglang/srt/managers/tp_worker.py
index c8e14a746..6168441d1 100644
--- a/python/sglang/srt/managers/tp_worker.py
+++ b/python/sglang/srt/managers/tp_worker.py
@@ -30,7 +30,7 @@ from sglang.srt.managers.schedule_batch import ModelWorkerBatch, global_server_a
 from sglang.srt.model_executor.forward_batch_info import ForwardBatch
 from sglang.srt.model_executor.model_runner import ModelRunner
 from sglang.srt.server_args import ServerArgs
-from sglang.srt.utils import broadcast_pyobj, set_random_seed
+from sglang.srt.utils import MultiprocessingSerializer, broadcast_pyobj, set_random_seed

 logger = logging.getLogger(__name__)

@@ -197,7 +197,7 @@ class TpModelWorker:

     def update_weights_from_tensor(self, recv_req: UpdateWeightsFromTensorReqInput):
</example_optimization_diff>

IMPORTANT: The above diff is an EXAMPLE of optimizations that were successful in a different context.
These changes have NOT been applied to your codebase yet.
Your task is to:
1. Understand the optimization pattern shown (e.g., torch.zeros → torch.empty)
2. Look at the CURRENT code in the target files
3. Find places where you can apply SIMILAR optimizations
4. MAKE THE CHANGES yourself using str_replace_editor

The codebase you're working with is at the BASE commit - it does NOT have these optimizations yet.
You need to IMPLEMENT similar optimizations yourself.

HERE'S WHAT YOU NEED TO DO:
1. The files CURRENTLY contain torch.zeros() calls that need optimization
2. You need to CHANGE torch.zeros to torch.empty where appropriate
3. You need to REMOVE .fill_() operations that are unnecessary
4. These are NEW changes you're making - not already in the code

START WITH THIS COMMAND to see what needs changing:
```bash
grep -n 'torch.zeros\|fill_' vllm/model_executor/layers/fused_moe/moe_align_block_size.py benchmarks/kernels/benchmark_moe_align_block_size.py
```

CRITICAL: You MUST make actual code changes. Look for patterns like:
- Analyze the target files for performance bottlenecks
- Look for unnecessary memory allocations or initializations
- Consider more efficient algorithms or data structures

Target files to optimize:
- python/sglang/srt/managers/io_struct.py
- python/sglang/srt/managers/tp_worker.py
- python/sglang/srt/model_executor/model_runner.py

IMPORTANT: You MUST make actual code changes to at least one file.
The task will fail if no files are modified.

## Constraints
- No public API breakage
- All TestPack checks must pass

## Target Files (ONLY modify these)
- `python/sglang/srt/managers/io_struct.py`
- `python/sglang/srt/managers/tp_worker.py`
- `python/sglang/srt/model_executor/model_runner.py`
- `python/sglang/srt/server.py`
- `python/sglang/srt/utils.py`
- `test/srt/test_update_weights_from_tensor.py`

## SPECIFIC OPTIMIZATION TARGETS:
Based on the human commit analysis, focus on these areas:
- Memory allocation patterns (torch.zeros vs torch.empty)
- Tensor initialization strategies
- Kernel parameter optimization
- Buffer reuse and caching

### Human Developer's Approach:
```
Speed up `update_weights_from_tensor` (#2695)
```

### Files Modified (statistics):
```
python/sglang/srt/managers/io_struct.py          |  3 +--
 python/sglang/srt/managers/tp_worker.py          |  4 ++--
 python/sglang/srt/model_executor/model_runner.py |  8 +++----
 python/sglang/srt/server.py                      | 11 ++++++---
 python/sglang/srt/utils.py                       | 17 ++++++++++++--
 test/srt/test_update_weights_from_tensor.py      | 30 ++++++++++++++----------
 6 files changed, 48 insertions(+), 25 deletions(-)
```

## IMMEDIATE ACTION REQUIREMENTS:
1. Start editing files by iteration 3
2. Create and run /home/ubuntu/OmniPerf-Bench/perf-agents-bench/.work/worktrees/sglang_core/sglang_core-0042/.bench_scratch/test_opt.py before and after edits (do not create timing scripts outside /home/ubuntu/OmniPerf-Bench/perf-agents-bench/.work/worktrees/sglang_core/sglang_core-0042/.bench_scratch)
3. Make at least 3 concrete optimizations
4. Commit changes by iteration 8
5. Use finish command by iteration 10

## TASK COMPLETION COMMAND:
When you have made optimizations:
```bash
git add -A
git reset -q .bench_scratch || true
git commit -m 'Optimize MoE align sum kernels performance'
git diff $(git merge-base HEAD origin/HEAD || git rev-parse HEAD~1) -- . ':(exclude).bench_scratch' > /home/ubuntu/OmniPerf-Bench/perf-agents-bench/.work/worktrees/sglang_core/sglang_core-0042/model_patch.diff || true
finish
```

START IMPLEMENTING IMMEDIATELY. NO MORE ANALYSIS.

mcp startup: no servers

thinking
**Exploring repository structure**

I need to open the repository and inspect the target files. First, I'll explore the repo structure and those files. I plan to use the grep command, although it seems less relevant right now. I also need to create the .bench_scratch/test_opt.py file for timing. The test script I have is just empty comments, so I'll create my own reproduction for the "update_weights_from_tensor" performance path. I’ll reference test/srt/test_update_weights_from_tensor.py to trigger that code path.
codex
I’m going to scan the repository and inspect the target files to identify optimization opportunities, then create a small benchmark script in .bench_scratch to measure before/after performance. Next, I’ll implement changes and rerun the benchmark.
exec
bash -lc 'ls -la' in /home/ubuntu/OmniPerf-Bench/perf-agents-bench/.work/worktrees/sglang_core/sglang_core-0042 succeeded in 10ms:
total 104
drwxrwxr-x  14 ubuntu ubuntu  4096 Nov 21 05:30 .
drwxrwxr-x 124 ubuntu ubuntu  4096 Nov 21 05:30 ..
-rw-rw-r--   1 ubuntu ubuntu   341 Nov 21 05:30 .editorconfig
-rw-rw-r--   1 ubuntu ubuntu    75 Nov 21 05:30 .git
drwxrwxr-x   4 ubuntu ubuntu  4096 Nov 21 05:30 .github
-rw-rw-r--   1 ubuntu ubuntu  3629 Nov 21 05:30 .gitignore
-rw-rw-r--   1 ubuntu ubuntu   123 Nov 21 05:30 .gitmodules
-rw-rw-r--   1 ubuntu ubuntu    50 Nov 21 05:30 .isort.cfg
-rw-rw-r--   1 ubuntu ubuntu   974 Nov 21 05:30 .pre-commit-config.yaml
drwxrwxr-x   3 ubuntu ubuntu  4096 Nov 21 05:30 3rdparty
-rw-rw-r--   1 ubuntu ubuntu 11346 Nov 21 05:30 LICENSE
-rw-rw-r--   1 ubuntu ubuntu  1720 Nov 21 05:30 Makefile
-rw-rw-r--   1 ubuntu ubuntu  5283 Nov 21 05:30 README.md
drwxrwxr-x   2 ubuntu ubuntu  4096 Nov 21 05:30 assets
drwxrwxr-x  28 ubuntu ubuntu  4096 Nov 21 05:30 benchmark
drwxrwxr-x   2 ubuntu ubuntu  4096 Nov 21 05:30 docker
drwxrwxr-x   9 ubuntu ubuntu  4096 Nov 21 05:30 docs
drwxrwxr-x   5 ubuntu ubuntu  4096 Nov 21 05:30 examples
drwxrwxr-x   3 ubuntu ubuntu  4096 Nov 21 05:30 python
exec
bash -lc "sed -n '1,220p' python/sglang/srt/managers/io_struct.py" in /home/ubuntu/OmniPerf-Bench/perf-agents-bench/.work/worktrees/sglang_core/sglang_core-0042 succeeded in 10ms:
# Copyright 2023-2024 SGLang Team
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.
# ==============================================================================
"""
The definition of objects transfered between different
processes (TokenizerManager, DetokenizerManager, Controller).
"""

import uuid
from dataclasses import dataclass
exec
bash -lc "sed -n '220,640p' python/sglang/srt/managers/io_struct.py" in /home/ubuntu/OmniPerf-Bench/perf-agents-bench/.work/worktrees/sglang_core/sglang_core-0042 succeeded in 9ms:
    # If return logprobs, the start location in the prompt for returning logprobs.
    logprob_start_len: int
    # If return logprobs, the number of top logprobs to return at each position.
    top_logprobs_num: int
    # Whether to stream output
    stream: bool

    # LoRA related
    lora_path: Optional[str] = None  # None means just use the base model
    # The input embeds
    input_embeds: Optional[Union[List[List[List[float]]], List[List[float]]]] = None

    # Session info for continual prompting
    session_params: Optional[SessionParams] = None


@dataclass
class EmbeddingReqInput:
    # The input prompt. It can be a single prompt or a batch of prompts.
    text: Optional[Union[List[str], str]] = None

thinking
**Modifying weight updates**

I see that `UpdateWeightsFromTensorReqInput` includes `name` and `torch.Tensor`. The optimization is suggesting we use serialized bytes instead, which means I’ll have to make some modifications. It looks like that’s the next step I need to focus on, particularly with `tp_worker`. There’s definitely some work to do to ensure everything aligns with the new approach!
exec
bash -lc "sed -n '1,260p' python/sglang/srt/managers/tp_worker.py" in /home/ubuntu/OmniPerf-Bench/perf-agents-bench/.work/worktrees/sglang_core/sglang_core-0042 succeeded in 10ms:
# Copyright 2023-2024 SGLang Team
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.
# ==============================================================================
"""A tensor parallel worker."""

import logging
import threading
from typing import Optional

from sglang.srt.configs.model_config import ModelConfig
exec
bash -lc "sed -n '1,260p' python/sglang/srt/model_executor/model_runner.py" in /home/ubuntu/OmniPerf-Bench/perf-agents-bench/.work/worktrees/sglang_core/sglang_core-0042 succeeded in 10ms:
# Copyright 2023-2024 SGLang Team
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.
# ==============================================================================
"""ModelRunner runs the forward passes of the models."""

import gc
import json
import logging
import time
from typing import Optional
exec
bash -lc "sed -n '260,780p' python/sglang/srt/model_executor/model_runner.py" in /home/ubuntu/OmniPerf-Bench/perf-agents-bench/.work/worktrees/sglang_core/sglang_core-0042 succeeded in 11ms:
            device_config=DeviceConfig(self.device),
        )

        # Parse other args
        self.sliding_window_size = (
            self.model.get_attention_sliding_window_size()
            if hasattr(self.model, "get_attention_sliding_window_size")
            else None
        )
        self.dtype = self.model_config.dtype

        logger.info(
            f"Load weight end. "
            f"type={type(self.model).__name__}, "
            f"dtype={self.dtype}, "
            f"avail mem={get_available_gpu_memory(self.device, self.gpu_id):.2f} GB"
        )

    def update_weights_from_disk(
        self, model_path: str, load_format: str
exec
bash -lc "sed -n '780,1460p' python/sglang/srt/model_executor/model_runner.py" in /home/ubuntu/OmniPerf-Bench/perf-agents-bench/.work/worktrees/sglang_core/sglang_core-0042 succeeded in 10ms:


thinking
**Investigating output truncation**

It seems like the ModelRunner file ended early due to truncated output, and the shell shows "Total output lines: 476." I noticed forward functions, but I'm wondering if the update_weights_from_tensor or distributed functions might be in that file too, even if they didn't appear in the sections we printed. Let's look into it further and see if we can find the missing pieces!
exec
bash -lc 'rg -n "update_weights_from_tensor|update_weights_from_distributed|get_weights_by_name|init_weights_update_group" -n python/sglang/srt -S' in /home/ubuntu/OmniPerf-Bench/perf-agents-bench/.work/worktrees/sglang_core/sglang_core-0042 succeeded in 18ms:
python/sglang/srt/managers/tokenizer_manager.py:179:        self.init_weights_update_group_communicator = _Communicator(
python/sglang/srt/managers/tokenizer_manager.py:182:        self.update_weights_from_distributed_communicator = _Communicator(
python/sglang/srt/managers/tokenizer_manager.py:185:        self.update_weights_from_tensor_communicator = _Communicator(
python/sglang/srt/managers/tokenizer_manager.py:188:        self.get_weights_by_name_communicator = _Communicator(
python/sglang/srt/managers/tokenizer_manager.py:496:    async def init_weights_update_group(
python/sglang/srt/managers/tokenizer_manager.py:505:        result = (await self.init_weights_update_group_communicator(obj))[0]
python/sglang/srt/managers/tokenizer_manager.py:508:    async def update_weights_from_distributed(
python/sglang/srt/managers/tokenizer_manager.py:521:            result = (await self.update_weights_from_distributed_communicator(obj))[0]
python/sglang/srt/managers/tokenizer_manager.py:524:    async def update_weights_from_tensor(
python/sglang/srt/managers/tokenizer_manager.py:537:            result = (await self.update_weights_from_tensor_communicator(obj))[0]
python/sglang/srt/managers/tokenizer_manager.py:540:    async def get_weights_by_name(
python/sglang/srt/managers/tokenizer_manager.py:544:        results = await self.get_weights_by_name_communicator(obj)
python/sglang/srt/managers/tokenizer_manager.py:740:                self.init_weights_update_group_communicator.handle_recv(recv_obj)
python/sglang/srt/managers/tokenizer_manager.py:745:                self.update_weights_from_distributed_communicator.handle_recv(recv_obj)
python/sglang/srt/managers/tokenizer_manager.py:750:                self.update_weights_from_tensor_communicator.handle_recv(recv_obj)
python/sglang/srt/managers/tokenizer_manager.py:752:                self.get_weights_by_name_communicator.handle_recv(recv_obj)
python/sglang/srt/managers/tp_worker.py:179:    def init_weights_update_group(self, recv_req: InitWeightsUpdateGroupReqInput):
python/sglang/srt/managers/tp_worker.py:180:        success, message = self.model_runner.init_weights_update_group(
python/sglang/srt/managers/tp_worker.py:190:    def update_weights_from_distributed(
python/sglang/srt/managers/tp_worker.py:193:        success, message = self.model_runner.update_weights_from_distributed(
exec
bash -lc "sed -n '380,520p' python/sglang/srt/model_executor/model_runner.py" in /home/ubuntu/OmniPerf-Bench/perf-agents-bench/.work/worktrees/sglang_core/sglang_core-0042 succeeded in 9ms:
        )

        try:
            self._model_update_group = init_custom_process_group(
                backend=backend,
                init_method=f"tcp://{master_address}:{master_port}",
                world_size=world_size,
                rank=rank,
                group_name=group_name,
            )
            dist.barrier(group=self._model_update_group, device_ids=[rank])
            return True, "Succeeded to initialize custom process group."
        except Exception as e:
            message = f"Failed to initialize custom process group: {e}."
            logger.error(message)
            return False, message

    def update_weights_from_distributed(self, name, dtype, shape):
        """
        Update specific parameter in the model weights online
exec
bash -lc "sed -n '1,320p' python/sglang/srt/server.py" in /home/ubuntu/OmniPerf-Bench/perf-agents-bench/.work/worktrees/sglang_core/sglang_core-0042 succeeded in 10ms:
# Copyright 2023-2024 SGLang Team
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.
# ==============================================================================
"""
The entry point of inference server.
SRT = SGLang Runtime.
"""

import asyncio
import atexit
exec
bash -lc "sed -n '320,760p' python/sglang/srt/server.py" in /home/ubuntu/OmniPerf-Bench/perf-agents-bench/.work/worktrees/sglang_core/sglang_core-0042 succeeded in 10ms:
        ret = await tokenizer_manager.generate_request(obj, request).__anext__()
        return ret
    except ValueError as e:
        return _create_error_response(e)


@app.api_route("/classify", methods=["POST", "PUT"])
@time_func_latency
async def classify_request(obj: EmbeddingReqInput, request: Request):
    """Handle a reward model request. Now the arguments and return values are the same as embedding models."""
    try:
        ret = await tokenizer_manager.generate_request(obj, request).__anext__()
        return ret
    except ValueError as e:
        return _create_error_response(e)


##### OpenAI-compatible API endpoints #####


exec
bash -lc "sed -n '760,1100p' python/sglang/srt/server.py" in /home/ubuntu/OmniPerf-Bench/perf-agents-bench/.work/worktrees/sglang_core/sglang_core-0042 succeeded in 9ms:
    async def async_generate(
        self,
        # The input prompt. It can be a single prompt or a batch of prompts.
        prompt: Optional[Union[List[str], str]] = None,
        sampling_params: Optional[Dict] = None,
        # The token ids for text; one can either specify text or input_ids.
        input_ids: Optional[Union[List[List[int]], List[int]]] = None,
        return_logprob: Optional[Union[List[bool], bool]] = False,
        logprob_start_len: Optional[Union[List[int], int]] = None,
        top_logprobs_num: Optional[Union[List[int], int]] = None,
        lora_path: Optional[List[Optional[str]]] = None,
        stream: bool = False,
    ):
        obj = GenerateReqInput(
            text=prompt,
            input_ids=input_ids,
            sampling_params=sampling_params,
            return_logprob=return_logprob,
            logprob_start_len=logprob_start_len,
            top_logprobs_num=top_logprobs_num,
exec
bash -lc "sed -n '1,280p' python/sglang/srt/utils.py" in /home/ubuntu/OmniPerf-Bench/perf-agents-bench/.work/worktrees/sglang_core/sglang_core-0042 succeeded in 10ms:
# Copyright 2023-2024 SGLang Team
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.
# ==============================================================================
"""Common utilities."""

import base64
import dataclasses
import ipaddress
import itertools
import json
exec
bash -lc "sed -n '280,720p' python/sglang/srt/utils.py" in /home/ubuntu/OmniPerf-Bench/perf-agents-bench/.work/worktrees/sglang_core/sglang_core-0042 succeeded in 10ms:

        def forward(*args, **kwargs):
            module.forward = original_forward
            device_state = {
                # here we blindly call `to(device)`
                # if the parameter is already on the device, it will be a no-op
                k: v.to(device, non_blocking=True)
                for k, v in module.state_dict().items()
            }
            output = functional_call(module, device_state, args=args, kwargs=kwargs)
            module.forward = forward
            return output

        module.forward = forward

    return module


class LayerFn(Protocol):

exec
bash -lc "sed -n '720,1240p' python/sglang/srt/utils.py" in /home/ubuntu/OmniPerf-Bench/perf-agents-bench/.work/worktrees/sglang_core/sglang_core-0042 succeeded in 10ms:
            size = len(serialized_data)
            tensor_data = torch.ByteTensor(
                np.frombuffer(serialized_data, dtype=np.uint8)
            )
            tensor_size = torch.tensor([size], dtype=torch.long)

            dist.broadcast(tensor_size, src=0, group=dist_group)
            dist.broadcast(tensor_data, src=0, group=dist_group)
        return data
    else:
        tensor_size = torch.tensor([0], dtype=torch.long)
        dist.broadcast(tensor_size, src=0, group=dist_group)
        size = tensor_size.item()

        if size == 0:
            return []

        tensor_data = torch.empty(size, dtype=torch.uint8)
        dist.broadcast(tensor_data, src=0, group=dist_group)

exec
bash -lc "sed -n '1,220p' test/srt/test_update_weights_from_tensor.py" in /home/ubuntu/OmniPerf-Bench/perf-agents-bench/.work/worktrees/sglang_core/sglang_core-0042 succeeded in 9ms:
import unittest

import torch

import sglang as sgl
from sglang.test.test_utils import DEFAULT_SMALL_MODEL_NAME_FOR_TEST


class TestReleaseGPUOccupation(unittest.TestCase):
    def test_release_and_resume_occupation(self):
        engine = sgl.Engine(model_path=DEFAULT_SMALL_MODEL_NAME_FOR_TEST)

        param_name = "model.layers.2.self_attn.k_proj.weight"

        def _check_param(expect_values):
            actual_values = torch.tensor(engine.get_weights_by_name(param_name))[0, :5]
            assert torch.allclose(
                actual_values, torch.tensor(expect_values), atol=0.001
            ), f"{actual_values=}"

exec
bash -lc "sed -n '1,260p' python/sglang/srt/managers/tokenizer_manager.py" in /home/ubuntu/OmniPerf-Bench/perf-agents-bench/.work/worktrees/sglang_core/sglang_core-0042 succeeded in 10ms:
# Copyright 2023-2024 SGLang Team
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.
# ==============================================================================
"""TokenizerManager is a process that tokenizes the text."""

import asyncio
import copy
import dataclasses
import logging
import os
exec
bash -lc "sed -n '480,820p' python/sglang/srt/managers/tokenizer_manager.py" in /home/ubuntu/OmniPerf-Bench/perf-agents-bench/.work/worktrees/sglang_core/sglang_core-0042 succeeded in 10ms:
                self.server_args.load_format = obj.load_format
                self.model_path = obj.model_path
            return result.success, result.message
        else:  # self.server_args.dp_size > 1
            self.model_update_tmp = []
            result = await self.model_update_result

            all_success = all([r.success for r in result])
            if all_success is True:
                self.server_args.model_path = obj.model_path
                self.server_args.load_format = obj.load_format
                self.model_path = obj.model_path
            all_message = [r.message for r in result]
            all_message = " | ".join(all_message)
            return all_success, all_message

    async def init_weights_update_group(
        self,
        obj: InitWeightsUpdateGroupReqInput,
        request: Optional[fastapi.Request] = None,
exec
bash -lc "sed -n '1440,1520p' python/sglang/srt/managers/scheduler.py" in /home/ubuntu/OmniPerf-Bench/perf-agents-bench/.work/worktrees/sglang_core/sglang_core-0042 succeeded in 10ms:
        for i, req in enumerate(self.waiting_queue):
            if req.rid == recv_req.rid:
                to_del = i
                break

        if to_del is not None:
            del self.waiting_queue[to_del]
            logger.debug(f"Abort queued request. {req.rid=}")
            return

        # Delete requests in the running batch
        if self.running_batch:
            for req in self.running_batch.reqs:
                if req.rid == recv_req.rid and not req.finished():
                    logger.debug(f"Abort running request. {req.rid=}")
                    req.to_abort = True
                    break

    def update_weights_from_disk(self, recv_req: UpdateWeightFromDiskReqInput):
        """In-place update of the weights from disk."""

thinking
**Updating components and logic**

I need to change UpdateWeightsFromTensorReqInput across the dataclass and update consumption in Server.Engine. The tokenization manager doesn't require changes since it passes the dataclass. The scheduler and tp_worker will update accordingly. For ModelRunner.update_weights_from_tensor, it needs to handle incoming names and tensors. If we switch to serialized bytes, we must deserialize in tp_worker before calling model_runner. I'll check out utils.py for a MultiprocessingSerializer and look for broadcast_pyobj using serialized_named_tensors bytes. Let's dive in!
exec
bash -lc 'rg -n "MultiprocessingSerializer|serializer|serialize_named_tensors|deserialize_named_tensors|named_tensors" python/sglang/srt -S' in /home/ubuntu/OmniPerf-Bench/perf-agents-bench/.work/worktrees/sglang_core/sglang_core-0042 exited 1 in 16ms:




thinking
**Revising tensor updates**

I need to change `UpdateWeightsFromTensorReqInput` to use serialized bytes for a dictionary of named tensors instead of just a name and tensor. Currently, existing flows only use one name. The `server.Engine.update_weights_from_tensor` function passes a name and tensor, but I can adjust that to represent a dictionary. This should lower overhead by creating a single large bytes object, which is much better than sending a complex dataclass with a GPU tensor, especially since pickling GPU tensors can be tricky across processes.
thinking
**Updating weight management**

In `tp_worker.update_weights_from_tensor`, I plan to import `MultiprocessingSerializer` to decode bytes into a dictionary of named tensors before calling `model_runner.load_weights`. Currently, `ModelRunner.update_weights_from_tensor` expects a name and tensor, but I could keep the old method intact while adjusting it to accept the named bytes. The process in `tp_worker` will convert to a dictionary and call `model_runner.update_weights_from_tensor` as usual. I noticed some recent changes to `model_runner.py`, possibly to improve dtype conversions or memory management. I'll also add a `MultiprocessingSerializer` utility in `utils.py` for packing and unpacking tensors.
thinking
**Adjusting tokenization and server methods**

I need to revise the tokenization manager to accept dataclasses, but since it just passes data through, no significant changes are necessary. Now, for updating server methods: the native HTTP endpoints don't include `update_weights_from_tensor`, which is only available for distributed and disk operations. The `Engine.update_weights_from_tensor` directly calls `tokenizer_manager.update_weights_from_tensor`, using the same `_Communicator` as other functions. Lastly, I noticed an instruction to optimize calls to `torch.zeros`, but I can't find that in our repo, so I’ll search for related functions in target files.
exec
bash -lc "rg -n \"torch\\.zeros|\\.fill_\\(|empty\\(\" python/sglang/srt -S" in /home/ubuntu/OmniPerf-Bench/perf-agents-bench/.work/worktrees/sglang_core/sglang_core-0042 succeeded in 18ms:
python/sglang/srt/distributed/device_communicators/shm_broadcast.py:174:                torch.frombuffer(metadata_buffer, dtype=torch.uint8).fill_(0)
python/sglang/srt/distributed/device_communicators/hpu_communicator.py:37:        output_tensor = torch.empty(
python/sglang/srt/distributed/device_communicators/xpu_communicator.py:30:        output_tensor = torch.empty(
python/sglang/srt/lora/lora_manager.py:104:        workspace_buffer = torch.empty(1 * 1024 * 1024, dtype=torch.int8, device="cuda")
python/sglang/srt/lora/lora_manager.py:203:                    torch.empty(
python/sglang/srt/lora/lora_manager.py:227:                    torch.empty(
python/sglang/srt/lora/lora_manager.py:303:        seg_indptr = torch.zeros((bs + 1,), dtype=torch.int32, device="cuda")
python/sglang/srt/lora/lora_manager.py:305:        weight_indices = torch.empty((bs,), dtype=torch.int64, device="cuda")
python/sglang/srt/distributed/device_communicators/custom_all_reduce.py:214:        self.rank_data = torch.empty(
python/sglang/srt/distributed/parallel_state.py:439:        output_tensor = torch.empty(
python/sglang/srt/distributed/parallel_state.py:582:        size_tensor = torch.empty(1, dtype=torch.long, device="cpu")
python/sglang/srt/distributed/parallel_state.py:590:        object_tensor = torch.empty(  # type: ignore[call-overload]
python/sglang/srt/distributed/parallel_state.py:662:                    tensor = torch.empty(
python/sglang/srt/distributed/parallel_state.py:771:                tensor = torch.empty(value.size, dtype=value.dtype, device=value.device)
python/sglang/srt/distributed/parallel_state.py:834:        tensor = torch.empty(size, dtype=dtype, device=self.device)
python/sglang/srt/utils.py:548:        output_tensor = torch.empty(
python/sglang/srt/utils.py:737:        tensor_data = torch.empty(size, dtype=torch.uint8)
python/sglang/srt/distributed/device_communicators/pynccl.py:112:            data = torch.zeros(1, device=device)
python/sglang/srt/managers/schedule_policy.py:110:                            prefix_ids, torch.empty(len(prefix_ids), dtype=torch.bool)
python/sglang/srt/model_executor/model_runner.py:417:            weights = torch.empty(shape, dtype=target_dtype, device=self.device)
exec
bash -lc "sed -n '1,200p' test/test_utils.py" in /home/ubuntu/OmniPerf-Bench/perf-agents-bench/.work/worktrees/sglang_core/sglang_core-0042 exited 2 in 10ms:
sed: can't read test/test_utils.py: No such file or directory
exec
bash -lc "sed -n '1,220p' test/srt/test_utils.py" in /home/ubuntu/OmniPerf-Bench/perf-agents-bench/.work/worktrees/sglang_core/sglang_core-0042 exited 2 in 10ms:
sed: can't read test/srt/test_utils.py: No such file or directory
exec
bash -lc 'rg -n "DEFAULT_SMALL_MODEL_NAME_FOR_TEST" -S' in /home/ubuntu/OmniPerf-Bench/perf-agents-bench/.work/worktrees/sglang_core/sglang_core-0042 succeeded in 19ms:
python/sglang/test/test_utils.py:31:DEFAULT_SMALL_MODEL_NAME_FOR_TEST = "meta-llama/Llama-3.2-1B-Instruct"
test/srt/test_ebnf_constrained.py:13:    DEFAULT_SMALL_MODEL_NAME_FOR_TEST,
test/srt/test_ebnf_constrained.py:21:    cls.model = DEFAULT_SMALL_MODEL_NAME_FOR_TEST
test/srt/test_session_control.py:19:    DEFAULT_SMALL_MODEL_NAME_FOR_TEST,
test/srt/test_session_control.py:33:        cls.model = DEFAULT_SMALL_MODEL_NAME_FOR_TEST
test/srt/test_srt_endpoint.py:14:    DEFAULT_SMALL_MODEL_NAME_FOR_TEST,
test/srt/test_srt_endpoint.py:24:        cls.model = DEFAULT_SMALL_MODEL_NAME_FOR_TEST
test/srt/test_openai_server.py:17:    DEFAULT_SMALL_MODEL_NAME_FOR_TEST,
test/srt/test_openai_server.py:27:        cls.model = DEFAULT_SMALL_MODEL_NAME_FOR_TEST
test/srt/test_openai_server.py:37:        cls.tokenizer = get_tokenizer(DEFAULT_SMALL_MODEL_NAME_FOR_TEST)
test/srt/test_openai_server.py:546:        cls.model = DEFAULT_SMALL_MODEL_NAME_FOR_TEST
test/srt/test_openai_server.py:560:        cls.tokenizer = get_tokenizer(DEFAULT_SMALL_MODEL_NAME_FOR_TEST)
test/srt/test_update_weights_from_distributed.py:32:    DEFAULT_SMALL_MODEL_NAME_FOR_TEST,
test/srt/test_update_weights_from_distributed.py:283:        True if model_name == DEFAULT_SMALL_MODEL_NAME_FOR_TEST else False
test/srt/test_update_weights_from_distributed.py:360:        True if model_name == DEFAULT_SMALL_MODEL_NAME_FOR_TEST else False
test/srt/test_update_weights_from_distributed.py:502:    time_limit = 3 if model_name == DEFAULT_SMALL_MODEL_NAME_FOR_TEST else 6
test/srt/test_update_weights_from_distributed.py:533:                (1, 1, DEFAULT_SMALL_MODEL_NAME_FOR_TEST, "Engine"),
test/srt/test_update_weights_from_distributed.py:537:                (1, 1, DEFAULT_SMALL_MODEL_NAME_FOR_TEST, "Engine"),
test/srt/test_update_weights_from_distributed.py:544:                        (2, 1, DEFAULT_SMALL_MODEL_NAME_FOR_TEST, "Engine"),
test/srt/test_update_weights_from_distributed.py:552:                        (2, 2, DEFAULT_SMALL_MODEL_NAME_FOR_TEST, "Engine"),
exec
bash -lc "sed -n '1,120p' python/sglang/test/test_utils.py" in /home/ubuntu/OmniPerf-Bench/perf-agents-bench/.work/worktrees/sglang_core/sglang_core-0042 succeeded in 10ms:
"""Common utilities for testing and benchmarking"""

import argparse
import asyncio
import copy
import os
import random
import subprocess
import threading
import time
from concurrent.futures import ThreadPoolExecutor
from functools import partial
from types import SimpleNamespace
from typing import Callable, List, Optional

import numpy as np
import requests
import torch
import torch.nn.functional as F








thinking
**Considering benchmark creation**

I'm thinking about creating a benchmark using DEFAULT_SMALL_MODEL_NAME_FOR_TEST "meta-llama/Llama-3.2-1B-Instruct." I want to avoid downloading a large model as it may slow things down in the environment. There's a chance this test setup could be offline or have specific environment variables. Since the user mentioned focusing on performance improvements, I might design a lightweight test for the serialization path without loading heavy models. Instead, maybe I can utilize utils MultiprocessingSerializer to measure serialization speed.
thinking
**Planning implementation steps**

The instructions say to execute the script with Python from the repo root, which is important for running it, but it seems heavy model loading isn't necessary. The main focus should be on having a script ready to measure improvements before and after runs. I notice that I need to implement code changes to speed up update_weights_from_tensor without involving heavy models.

I’ll add a class called MultiprocessingSerializer to utils.py and use `torch.save` for packing Named Tensors, ensuring to consider cross-platform serialization. I'll also move to 'cpu' to cut down on GPU overhead.
thinking
**Implementing update functions**

It seems that the update function takes a CPU tensor and then moves it onto the device using model.load_weights. The function test_update_weights_from_tensor creates a new tensor on the CPU, which is good.

I need to provide static methods for serializing and deserializing named tensors into bytes. I'll update the io_struct.UpdateWeightsFromTensorReqInput dataclass to replace the name and tensor with these serialized bytes, setting 'serialized_named_tensors: bytes.'

Next, I'll modify TpModelWorker's update_weights_from_tensor method to handle the new dataclass and serialize bytes using MultiprocessingSerializer. This will help reduce pickling overhead and allow for potential future expansions. I'll return the first failure message if any updates fail.
thinking
**Handling multiple updates**

I'll focus on handling multiple updates effectively. I need to update server.Engine.update_weights_from_tensor to use MultiprocessingSerializer for packing the {name: tensor} into bytes. Then, I'll create an UpdateWeightsFromTensorReqInput with the serialized named tensors and send it.

No updates are needed for the tokenization manager, but the tests might require some adjustments. If I keep the Engine.update_weights_from_tensor interface the same, the tests shouldn't change. However, we need to follow the project assignment to modify six specific files, even if our final diff doesn't perfectly match the expectations. I’ll ensure that the changes lead to functional improvements.
thinking
**Implementing utility and optimization updates**

I need to implement server utilities for broadcasting, specifically using MultiprocessingSerializer as seen in the example. I’ll update the ModelRunner to potentially optimize dtype conversions and ensure the weights allocation is done efficiently.

For more concrete optimizations, I plan to establish bytes serialization for updates, replace CPU allocations with `torch.frombuffer` to streamline tensor creation, and enhance memory efficiency.

I’ll also refine the broadcast methodology to avoid unnecessary numpy conversions, leveraging memoryviews for better performance while addressing serialization needs. I'll ensure to meet the assignment's requirement for three optimizations by making these enhancements.
thinking
**Addressing efficiency and serialization**

I think CPU tensors support the buffer protocol, allowing me to use `memoryview(tensor_data).tobytes()` to bypass numpy conversion. So, I want to optimize `broadcast_pyobj` to remove the numpy dependency for ByteTensor creation.

I need to ensure we make three concrete optimizations and track these changes across several files, especially regarding the use of `MultiprocessingSerializer`. In creating the performance measurement script, it'll make comparisons between pickling and `torch.save` to optimize serialization processes effectively. I also want to include static methods in the new `MultiprocessingSerializer` class in `utils.py` to streamline named tensor serialization and deserialization.
thinking
**Implementing unit tests and performance measurements**

I’ve realized that the `UpdateWeightsFromTensorReqOutput` dataclass only returns a success boolean and a message, not multiple outputs, so I'll base my adjustments on that. Next, I plan to tweak the unit test `test/srt/test_update_weights_from_tensor.py` to ensure everything still passes without major changes, even though we've altered the test file for potential improvements.

Then, I’ll create a performance measurement script in `.bench_scratch/test_opt.py`, timing serialization methods with `pickle` versus `torch.save` to bytes. I also want to measure sending serialized bytes through an intra-process queue using `mp.Pipe`. This way, I can generate quantitative results regarding the efficiencies of both methods.
codex
Next, I’ll add a benchmarking script under .bench_scratch, then implement optimizations in the specified target files: introducing a serializer to avoid pickling tensors across processes, updating the dataclass and worker to use it, and optimizing broadcast serialization to reduce NumPy copies. Finally, I’ll run the benchmark before and after changes.
exec
bash -lc "python - << 'PY'
import os, textwrap, time, pickle, torch
from io import BytesIO

bench_dir = '.bench_scratch'
os.makedirs(bench_dir, exist_ok=True)
path = os.path.join(bench_dir, 'test_opt.py')
code = r'''import time
import pickle
from io import BytesIO

import torch


def serialize_pickle(obj):
    t0 = time.time()
    b = pickle.dumps(obj)
    return b, time.time() - t0


def serialize_torchsave(named_tensors):
    t0 = time.time()
    buf = BytesIO()
    # ensure cpu and contiguous for fair compare
    named_tensors = {k: v.detach().cpu().contiguous() for k, v in named_tensors.items()}
    torch.save(named_tensors, buf)
    data = buf.getvalue()
    return data, time.time() - t0


def main():
    print('Benchmark: serialize large tensor dict (approx ~25MB)')
    t = torch.full((3072, 2048), 1.5, dtype=torch.float32)
    named = {'w': t}

    b1, t1 = serialize_pickle({'name': 'w', 'tensor': t})
    b2, t2 = serialize_torchsave(named)

    print(f'pickle.dumps size={len(b1)/1e6:.2f}MB time={t1*1000:.1f}ms')
    print(f'torch.save    size={len(b2)/1e6:.2f}MB time={t2*1000:.1f}ms')

    # optional: round-trip torch.load for correctness timing
    t0 = time.time(); _ = pickle.loads(b1); t3 = time.time()-t0
    t0 = time.time(); _ = torch.load(BytesIO(b2)); t4 = time.time()-t0
    print(f'pickle.loads  time={t3*1000:.1f}ms')
    print(f'torch.load    time={t4*1000:.1f}ms')

    # If repo provides MultiprocessingSerializer, also test it
    try:
        from sglang.srt.utils import MultiprocessingSerializer
        t0 = time.time(); b3 = MultiprocessingSerializer.dumps_named_tensors(named); t5 = time.time()-t0
        t0 = time.time(); _ = MultiprocessingSerializer.loads_named_tensors(b3); t6 = time.time()-t0
        print(f'MultiprocessingSerializer dumps size={len(b3)/1e6:.2f}MB time={t5*1000:.1f}ms')
        print(f'MultiprocessingSerializer loads time={t6*1000:.1f}ms')
    except Exception as e:
        print('MultiprocessingSerializer not available yet:', e)

if __name__ == '__main__':
    main()
'''
open(path,'w').write(code)
print(path)
PY" in /home/ubuntu/OmniPerf-Bench/perf-agents-bench/.work/worktrees/sglang_core/sglang_core-0042 succeeded in 1.43s:
/home/ubuntu/OmniPerf-Bench/bench-env/lib/python3.12/site-packages/torch/cuda/__init__.py:61: FutureWarning: The pynvml package is deprecated. Please install nvidia-ml-py instead. If you did not install pynvml directly, please report this to the maintainers of the package that installed pynvml for you.
  import pynvml  # type: ignore[import]
.bench_scratch/test_opt.py
exec
bash -lc 'python .bench_scratch/test_opt.py' in /home/ubuntu/OmniPerf-Bench/perf-agents-bench/.work/worktrees/sglang_core/sglang_core-0042 succeeded in 1.54s:
/home/ubuntu/OmniPerf-Bench/bench-env/lib/python3.12/site-packages/torch/cuda/__init__.py:61: FutureWarning: The pynvml package is deprecated. Please install nvidia-ml-py instead. If you did not install pynvml directly, please report this to the maintainers of the package that installed pynvml for you.
  import pynvml  # type: ignore[import]
Benchmark: serialize large tensor dict (approx ~25MB)
pickle.dumps size=25.17MB time=37.2ms
torch.save    size=25.17MB time=36.8ms
pickle.loads  time=14.1ms
torch.load    time=2.8ms
MultiprocessingSerializer not available yet: No module named 'sglang.srt'


thinking
**Implementing code changes**

I'm planning to implement some code changes in various target files. First, in `utils.py`, I’ll add the `MultiprocessingSerializer` class with methods for handling named tensors. Next, I’ll modify `io_struct.py` to update the `UpdateWeightsFromTensorReqInput` dataclass, changing fields to use bytes. For `tp_worker.py`, I’ll adjust imports and processing for weights. In `model_runner.py`, I’ll consider some optional changes for optimization, ensuring minimal adjustments overall.
thinking
**Making server updates**

I need to modify the `Engine.update_weights_from_tensor` method in `server.py` to incorporate `MultiprocessingSerializer` and create the `UpdateWeightsFromTensorReqInput` data class using bytes. I also have to adjust the import path since the fields in `io_struct` changed, but the FastAPI endpoints remain untouched. As for `test_update_weights_from_tensor.py`, I can keep it as is because the Engine wrapper hasn’t changed. Although there were some suggested updates to the test file, they don’t seem necessary for passing tests. Now, I'll apply the patch!
file update
M /home/ubuntu/OmniPerf-Bench/perf-agents-bench/.work/worktrees/sglang_core/sglang_core-0042/python/sglang/srt/managers/io_struct.py
@@ -428,4 +428,3 @@
 class UpdateWeightsFromTensorReqInput:
-    name: str
-    tensor: torch.Tensor
+    serialized_named_tensors: bytes  # indeed Dict[str, torch.Tensor]

apply_patch(auto_approved=true) exited 0 in 7ms:
Success. Updated the following files:
M python/sglang/srt/managers/io_struct.py
file update:
diff --git a/python/sglang/srt/managers/io_struct.py b/python/sglang/srt/managers/io_struct.py
index 13eb233bd14d88be2101b446959b0106ac8399be..1aae28b00b76697cbd2882af46d4010d600f6284
--- a/python/sglang/srt/managers/io_struct.py
+++ b/python/sglang/srt/managers/io_struct.py
@@ -426,8 +426,7 @@

 @dataclass
 class UpdateWeightsFromTensorReqInput:
-    name: str
-    tensor: torch.Tensor
+    serialized_named_tensors: bytes  # indeed Dict[str, torch.Tensor]


 @dataclass

file update:
diff --git a/python/sglang/srt/managers/io_struct.py b/python/sglang/srt/managers/io_struct.py
index 13eb233bd14d88be2101b446959b0106ac8399be..1aae28b00b76697cbd2882af46d4010d600f6284
--- a/python/sglang/srt/managers/io_struct.py
+++ b/python/sglang/srt/managers/io_struct.py
@@ -426,8 +426,7 @@

 @dataclass
 class UpdateWeightsFromTensorReqInput:
-    name: str
-    tensor: torch.Tensor
+    serialized_named_tensors: bytes  # indeed Dict[str, torch.Tensor]


 @dataclass

file update
M /home/ubuntu/OmniPerf-Bench/perf-agents-bench/.work/worktrees/sglang_core/sglang_core-0042/python/sglang/srt/managers/tp_worker.py
@@ -32,3 +32,3 @@
 from sglang.srt.server_args import ServerArgs
-from sglang.srt.utils import broadcast_pyobj, set_random_seed
+from sglang.srt.utils import MultiprocessingSerializer, broadcast_pyobj, set_random_seed

@@ -198,6 +198,15 @@
     def update_weights_from_tensor(self, recv_req: UpdateWeightsFromTensorReqInput):
-        success, message = self.model_runner.update_weights_from_tensor(
-            recv_req.name, recv_req.tensor
+        named_tensors = MultiprocessingSerializer.loads_named_tensors(
+            recv_req.serialized_named_tensors
         )
-        return success, message
+        # Support updating one or multiple tensors in a single request
+        last_success, last_message = True, "Success"
+        for name, tensor in named_tensors.items():
+            success, message = self.model_runner.update_weights_from_tensor(
+                name, tensor
+            )
+            if not success:
+                return success, message
+            last_success, last_message = success, message
+        return last_success, last_message

apply_patch(auto_approved=true) exited 0 in 9ms:
Success. Updated the following files:
M python/sglang/srt/managers/tp_worker.py
file update:
diff --git a/python/sglang/srt/managers/io_struct.py b/python/sglang/srt/managers/io_struct.py
index 13eb233bd14d88be2101b446959b0106ac8399be..1aae28b00b76697cbd2882af46d4010d600f6284
--- a/python/sglang/srt/managers/io_struct.py
+++ b/python/sglang/srt/managers/io_struct.py
@@ -426,8 +426,7 @@

 @dataclass
 class UpdateWeightsFromTensorReqInput:
-    name: str
-    tensor: torch.Tensor
+    serialized_named_tensors: bytes  # indeed Dict[str, torch.Tensor]


 @dataclass
diff --git a/python/sglang/srt/managers/tp_worker.py b/python/sglang/srt/managers/tp_worker.py
index c8e14a746c79dc4e3edb16899419fb873e9b9919..c7500f1d29252a7a0eaccd6112ccb3bee0556271
--- a/python/sglang/srt/managers/tp_worker.py
+++ b/python/sglang/srt/managers/tp_worker.py
@@ -30,7 +30,7 @@
 from sglang.srt.model_executor.forward_batch_info import ForwardBatch
 from sglang.srt.model_executor.model_runner import ModelRunner
 from sglang.srt.server_args import ServerArgs
-from sglang.srt.utils import broadcast_pyobj, set_random_seed
+from sglang.srt.utils import MultiprocessingSerializer, broadcast_pyobj, set_random_seed

 logger = logging.getLogger(__name__)

@@ -196,10 +196,19 @@
         return success, message

     def update_weights_from_tensor(self, recv_req: UpdateWeightsFromTensorReqInput):
-        success, message = self.model_runner.update_weights_from_tensor(
-            recv_req.name, recv_req.tensor
+        named_tensors = MultiprocessingSerializer.loads_named_tensors(
+            recv_req.serialized_named_tensors
         )
-        return success, message
+        # Support updating one or multiple tensors in a single request
+        last_success, last_message = True, "Success"
+        for name, tensor in named_tensors.items():
+            success, message = self.model_runner.update_weights_from_tensor(
+                name, tensor
+            )
+            if not success:
+                return success, message
+            last_success, last_message = success, message
+        return last_success, last_message

     def get_weights_by_name(self, recv_req: GetWeightsByNameReqInput):
         parameter = self.model_runner.get_weights_by_name(

file update:
diff --git a/python/sglang/srt/managers/io_struct.py b/python/sglang/srt/managers/io_struct.py
index 13eb233bd14d88be2101b446959b0106ac8399be..1aae28b00b76697cbd2882af46d4010d600f6284
--- a/python/sglang/srt/managers/io_struct.py
+++ b/python/sglang/srt/managers/io_struct.py
@@ -426,8 +426,7 @@

 @dataclass
 class UpdateWeightsFromTensorReqInput:
-    name: str
-    tensor: torch.Tensor
+    serialized_named_tensors: bytes  # indeed Dict[str, torch.Tensor]


 @dataclass
diff --git a/python/sglang/srt/managers/tp_worker.py b/python/sglang/srt/managers/tp_worker.py
index c8e14a746c79dc4e3edb16899419fb873e9b9919..c7500f1d29252a7a0eaccd6112ccb3bee0556271
--- a/python/sglang/srt/managers/tp_worker.py
+++ b/python/sglang/srt/managers/tp_worker.py
@@ -30,7 +30,7 @@
 from sglang.srt.model_executor.forward_batch_info import ForwardBatch
 from sglang.srt.model_executor.model_runner import ModelRunner
 from sglang.srt.server_args import ServerArgs
-from sglang.srt.utils import broadcast_pyobj, set_random_seed
+from sglang.srt.utils import MultiprocessingSerializer, broadcast_pyobj, set_random_seed

 logger = logging.getLogger(__name__)

@@ -196,10 +196,19 @@
         return success, message

     def update_weights_from_tensor(self, recv_req: UpdateWeightsFromTensorReqInput):
-        success, message = self.model_runner.update_weights_from_tensor(
-            recv_req.name, recv_req.tensor
+        named_tensors = MultiprocessingSerializer.loads_named_tensors(
+            recv_req.serialized_named_tensors
         )
-        return success, message
+        # Support updating one or multiple tensors in a single request
+        last_success, last_message = True, "Success"
+        for name, tensor in named_tensors.items():
+            success, message = self.model_runner.update_weights_from_tensor(
+                name, tensor
+            )
+            if not success:
+                return success, message
+            last_success, last_message = success, message
+        return last_success, last_message

     def get_weights_by_name(self, recv_req: GetWeightsByNameReqInput):
         parameter = self.model_runner.get_weights_by_name(

file update:
diff --git a/python/sglang/srt/managers/io_struct.py b/python/sglang/srt/managers/io_struct.py
index 13eb233bd14d88be2101b446959b0106ac8399be..1aae28b00b76697cbd2882af46d4010d600f6284
--- a/python/sglang/srt/managers/io_struct.py
+++ b/python/sglang/srt/managers/io_struct.py
@@ -426,8 +426,7 @@

 @dataclass
 class UpdateWeightsFromTensorReqInput:
-    name: str
-    tensor: torch.Tensor
+    serialized_named_tensors: bytes  # indeed Dict[str, torch.Tensor]


 @dataclass
diff --git a/python/sglang/srt/managers/tp_worker.py b/python/sglang/srt/managers/tp_worker.py
index c8e14a746c79dc4e3edb16899419fb873e9b9919..c7500f1d29252a7a0eaccd6112ccb3bee0556271
--- a/python/sglang/srt/managers/tp_worker.py
+++ b/python/sglang/srt/managers/tp_worker.py
@@ -30,7 +30,7 @@
 from sglang.srt.model_executor.forward_batch_info import ForwardBatch
 from sglang.srt.model_executor.model_runner import ModelRunner
 from sglang.srt.server_args import ServerArgs
-from sglang.srt.utils import broadcast_pyobj, set_random_seed
+from sglang.srt.utils import MultiprocessingSerializer, broadcast_pyobj, set_random_seed

 logger = logging.getLogger(__name__)

@@ -196,10 +196,19 @@
         return success, message

     def update_weights_from_tensor(self, recv_req: UpdateWeightsFromTensorReqInput):
-        success, message = self.model_runner.update_weights_from_tensor(
-            recv_req.name, recv_req.tensor
+        named_tensors = MultiprocessingSerializer.loads_named_tensors(
+            recv_req.serialized_named_tensors
         )
-        return success, message
+        # Support updating one or multiple tensors in a single request
+        last_success, last_message = True, "Success"
+        for name, tensor in named_tensors.items():
+            success, message = self.model_runner.update_weights_from_tensor(
+                name, tensor
+            )
+            if not success:
+                return success, message
+            last_success, last_message = success, message
+        return last_success, last_message

     def get_weights_by_name(self, recv_req: GetWeightsByNameReqInput):
         parameter = self.model_runner.get_weights_by_name(


thinking
**Reviewing patching process**

It looks like the file already imports `BytesIO`, but the patch failed because of a context mismatch. I need to find the correct section to patch `broadcast_pyobj` and then add the class in the right place. I'll look for the content related to `broadcast_pyobj` to ensure I’m patching the lines appropriately. I remember noticing lines around line 700 earlier, so I’ll open that area again.
exec
bash -lc "sed -n '680,780p' python/sglang/srt/utils.py" in /home/ubuntu/OmniPerf-Bench/perf-agents-bench/.work/worktrees/sglang_core/sglang_core-0042 succeeded in 10ms:
    """Replace a submodule in a model with a new module."""
    parent = model.get_submodule(".".join(module_name.split(".")[:-1]))
    target_name = module_name.split(".")[-1]
    setattr(parent, target_name, new_module)
    return new_module


def set_weight_attrs(
    weight: torch.Tensor,
    weight_attrs: Optional[Dict[str, Any]],
):
    """Set attributes on a weight tensor.

    This method is used to set attributes on a weight tensor. This method
    will not overwrite existing attributes.

    Args:
        weight: The weight tensor.
        weight_attrs: A dictionary of attributes to set on the weight tensor.
    """
file update:
diff --git a/python/sglang/srt/managers/io_struct.py b/python/sglang/srt/managers/io_struct.py
index 13eb233bd14d88be2101b446959b0106ac8399be..1aae28b00b76697cbd2882af46d4010d600f6284
--- a/python/sglang/srt/managers/io_struct.py
+++ b/python/sglang/srt/managers/io_struct.py
@@ -426,8 +426,7 @@

 @dataclass
 class UpdateWeightsFromTensorReqInput:
-    name: str
-    tensor: torch.Tensor
+    serialized_named_tensors: bytes  # indeed Dict[str, torch.Tensor]


 @dataclass
diff --git a/python/sglang/srt/managers/tp_worker.py b/python/sglang/srt/managers/tp_worker.py
index c8e14a746c79dc4e3edb16899419fb873e9b9919..c7500f1d29252a7a0eaccd6112ccb3bee0556271
--- a/python/sglang/srt/managers/tp_worker.py
+++ b/python/sglang/srt/managers/tp_worker.py
@@ -30,7 +30,7 @@
 from sglang.srt.model_executor.forward_batch_info import ForwardBatch
 from sglang.srt.model_executor.model_runner import ModelRunner
 from sglang.srt.server_args import ServerArgs
-from sglang.srt.utils import broadcast_pyobj, set_random_seed
+from sglang.srt.utils import MultiprocessingSerializer, broadcast_pyobj, set_random_seed

 logger = logging.getLogger(__name__)

@@ -196,10 +196,19 @@
         return success, message

     def update_weights_from_tensor(self, recv_req: UpdateWeightsFromTensorReqInput):
-        success, message = self.model_runner.update_weights_from_tensor(
-            recv_req.name, recv_req.tensor
+        named_tensors = MultiprocessingSerializer.loads_named_tensors(
+            recv_req.serialized_named_tensors
         )
-        return success, message
+        # Support updating one or multiple tensors in a single request
+        last_success, last_message = True, "Success"
+        for name, tensor in named_tensors.items():
+            success, message = self.model_runner.update_weights_from_tensor(
+                name, tensor
+            )
+            if not success:
+                return success, message
+            last_success, last_message = success, message
+        return last_success, last_message

     def get_weights_by_name(self, recv_req: GetWeightsByNameReqInput):
         parameter = self.model_runner.get_weights_by_name(

file update
M /home/ubuntu/OmniPerf-Bench/perf-agents-bench/.work/worktrees/sglang_core/sglang_core-0042/python/sglang/srt/utils.py
@@ -720,5 +720,4 @@
             size = len(serialized_data)
-            tensor_data = torch.ByteTensor(
-                np.frombuffer(serialized_data, dtype=np.uint8)
-            )
+            # Avoid intermediate NumPy copy by leveraging buffer protocol
+            tensor_data = torch.frombuffer(memoryview(serialized_data), dtype=torch.uint8).clone()
             tensor_size = torch.tensor([size], dtype=torch.long)
@@ -739,3 +738,4 @@

-        serialized_data = bytes(tensor_data.cpu().numpy())
+        # Convert tensor to bytes without routing through NumPy
+        serialized_data = memoryview(tensor_data.cpu()).tobytes()
         data = pickle.loads(serialized_data)
@@ -743,2 +743,32 @@

+class MultiprocessingSerializer:
+    """Fast serializer for cross-process transfer of named tensors.
+
+    This avoids pickling Torch tensors directly across processes and instead
+    serializes a Dict[str, torch.Tensor] to bytes via torch.save/torch.load.
+    Tensors are moved to CPU and made contiguous before serialization to
+    reduce device sync and copy overhead.
+    """
+
+    @staticmethod
+    def dumps_named_tensors(named_tensors: Dict[str, torch.Tensor]) -> bytes:
+        cpu_named: Dict[str, torch.Tensor] = {}
+        for k, v in named_tensors.items():
+            t = v.detach()
+            if t.device.type != "cpu":
+                t = t.to("cpu")
+            if not t.is_contiguous():
+                t = t.contiguous()
+            cpu_named[k] = t
+        buf = BytesIO()
+        torch.save(cpu_named, buf)
+        return buf.getvalue()
+
+    @staticmethod
+    def loads_named_tensors(data: bytes) -> Dict[str, torch.Tensor]:
+        buf = BytesIO(data)
+        named = torch.load(buf)
+        assert isinstance(named, dict), "Deserialized object must be a dict[str, Tensor]"
+        return named
+

apply_patch(auto_approved=true) exited 0 in 12ms:
Success. Updated the following files:
M python/sglang/srt/utils.py
file update:
diff --git a/python/sglang/srt/managers/io_struct.py b/python/sglang/srt/managers/io_struct.py
index 13eb233bd14d88be2101b446959b0106ac8399be..1aae28b00b76697cbd2882af46d4010d600f6284
--- a/python/sglang/srt/managers/io_struct.py
+++ b/python/sglang/srt/managers/io_struct.py
@@ -426,8 +426,7 @@

 @dataclass
 class UpdateWeightsFromTensorReqInput:
-    name: str
-    tensor: torch.Tensor
+    serialized_named_tensors: bytes  # indeed Dict[str, torch.Tensor]


 @dataclass
diff --git a/python/sglang/srt/managers/tp_worker.py b/python/sglang/srt/managers/tp_worker.py
index c8e14a746c79dc4e3edb16899419fb873e9b9919..c7500f1d29252a7a0eaccd6112ccb3bee0556271
--- a/python/sglang/srt/managers/tp_worker.py
+++ b/python/sglang/srt/managers/tp_worker.py
@@ -30,7 +30,7 @@
 from sglang.srt.model_executor.forward_batch_info import ForwardBatch
 from sglang.srt.model_executor.model_runner import ModelRunner
 from sglang.srt.server_args import ServerArgs
-from sglang.srt.utils import broadcast_pyobj, set_random_seed
+from sglang.srt.utils import MultiprocessingSerializer, broadcast_pyobj, set_random_seed

 logger = logging.getLogger(__name__)

@@ -196,10 +196,19 @@
         return success, message

     def update_weights_from_tensor(self, recv_req: UpdateWeightsFromTensorReqInput):
-        success, message = self.model_runner.update_weights_from_tensor(
-            recv_req.name, recv_req.tensor
+        named_tensors = MultiprocessingSerializer.loads_named_tensors(
+            recv_req.serialized_named_tensors
         )
-        return success, message
+        # Support updating one or multiple tensors in a single request
+        last_success, last_message = True, "Success"
+        for name, tensor in named_tensors.items():
+            success, message = self.model_runner.update_weights_from_tensor(
+                name, tensor
+            )
+            if not success:
+                return success, message
+            last_success, last_message = success, message
+        return last_success, last_message

     def get_weights_by_name(self, recv_req: GetWeightsByNameReqInput):
         parameter = self.model_runner.get_weights_by_name(
diff --git a/python/sglang/srt/utils.py b/python/sglang/srt/utils.py
index 7c3efa9a269eae7dc8bff8715239202534eb6cfe..be016b60152c195bc61d04ca14cadb5cdaa8acfa
--- a/python/sglang/srt/utils.py
+++ b/python/sglang/srt/utils.py
@@ -718,9 +718,8 @@
         else:
             serialized_data = pickle.dumps(data)
             size = len(serialized_data)
-            tensor_data = torch.ByteTensor(
-                np.frombuffer(serialized_data, dtype=np.uint8)
-            )
+            # Avoid intermediate NumPy copy by leveraging buffer protocol
+            tensor_data = torch.frombuffer(memoryview(serialized_data), dtype=torch.uint8).clone()
             tensor_size = torch.tensor([size], dtype=torch.long)

             dist.broadcast(tensor_size, src=0, group=dist_group)
@@ -737,10 +736,41 @@
         tensor_data = torch.empty(size, dtype=torch.uint8)
         dist.broadcast(tensor_data, src=0, group=dist_group)

-        serialized_data = bytes(tensor_data.cpu().numpy())
+        # Convert tensor to bytes without routing through NumPy
+        serialized_data = memoryview(tensor_data.cpu()).tobytes()
         data = pickle.loads(serialized_data)
         return data

+class MultiprocessingSerializer:
+    """Fast serializer for cross-process transfer of named tensors.
+
+    This avoids pickling Torch tensors directly across processes and instead
+    serializes a Dict[str, torch.Tensor] to bytes via torch.save/torch.load.
+    Tensors are moved to CPU and made contiguous before serialization to
+    reduce device sync and copy overhead.
+    """
+
+    @staticmethod
+    def dumps_named_tensors(named_tensors: Dict[str, torch.Tensor]) -> bytes:
+        cpu_named: Dict[str, torch.Tensor] = {}
+        for k, v in named_tensors.items():
+            t = v.detach()
+            if t.device.type != "cpu":
+                t = t.to("cpu")
+            if not t.is_contiguous():
+                t = t.contiguous()
+            cpu_named[k] = t
+        buf = BytesIO()
+        torch.save(cpu_named, buf)
+        return buf.getvalue()
+
+    @staticmethod
+    def loads_named_tensors(data: bytes) -> Dict[str, torch.Tensor]:
+        buf = BytesIO(data)
+        named = torch.load(buf)
+        assert isinstance(named, dict), "Deserialized object must be a dict[str, Tensor]"
+        return named
+

 step_counter = 0


file update:
diff --git a/python/sglang/srt/managers/io_struct.py b/python/sglang/srt/managers/io_struct.py
index 13eb233bd14d88be2101b446959b0106ac8399be..1aae28b00b76697cbd2882af46d4010d600f6284
--- a/python/sglang/srt/managers/io_struct.py
+++ b/python/sglang/srt/managers/io_struct.py
@@ -426,8 +426,7 @@

 @dataclass
 class UpdateWeightsFromTensorReqInput:
-    name: str
-    tensor: torch.Tensor
+    serialized_named_tensors: bytes  # indeed Dict[str, torch.Tensor]


 @dataclass
diff --git a/python/sglang/srt/managers/tp_worker.py b/python/sglang/srt/managers/tp_worker.py
index c8e14a746c79dc4e3edb16899419fb873e9b9919..c7500f1d29252a7a0eaccd6112ccb3bee0556271
--- a/python/sglang/srt/managers/tp_worker.py
+++ b/python/sglang/srt/managers/tp_worker.py
@@ -30,7 +30,7 @@
 from sglang.srt.model_executor.forward_batch_info import ForwardBatch
 from sglang.srt.model_executor.model_runner import ModelRunner
 from sglang.srt.server_args import ServerArgs
-from sglang.srt.utils import broadcast_pyobj, set_random_seed
+from sglang.srt.utils import MultiprocessingSerializer, broadcast_pyobj, set_random_seed

 logger = logging.getLogger(__name__)

@@ -196,10 +196,19 @@
         return success, message

     def update_weights_from_tensor(self, recv_req: UpdateWeightsFromTensorReqInput):
-        success, message = self.model_runner.update_weights_from_tensor(
-            recv_req.name, recv_req.tensor
+        named_tensors = MultiprocessingSerializer.loads_named_tensors(
+            recv_req.serialized_named_tensors
         )
-        return success, message
+        # Support updating one or multiple tensors in a single request
+        last_success, last_message = True, "Success"
+        for name, tensor in named_tensors.items():
+            success, message = self.model_runner.update_weights_from_tensor(
+                name, tensor
+            )
+            if not success:
+                return success, message
+            last_success, last_message = success, message
+        return last_success, last_message

     def get_weights_by_name(self, recv_req: GetWeightsByNameReqInput):
         parameter = self.model_runner.get_weights_by_name(
diff --git a/python/sglang/srt/utils.py b/python/sglang/srt/utils.py
index 7c3efa9a269eae7dc8bff8715239202534eb6cfe..be016b60152c195bc61d04ca14cadb5cdaa8acfa
--- a/python/sglang/srt/utils.py
+++ b/python/sglang/srt/utils.py
@@ -718,9 +718,8 @@
         else:
             serialized_data = pickle.dumps(data)
             size = len(serialized_data)
-            tensor_data = torch.ByteTensor(
-                np.frombuffer(serialized_data, dtype=np.uint8)
-            )
+            # Avoid intermediate NumPy copy by leveraging buffer protocol
+            tensor_data = torch.frombuffer(memoryview(serialized_data), dtype=torch.uint8).clone()
             tensor_size = torch.tensor([size], dtype=torch.long)

             dist.broadcast(tensor_size, src=0, group=dist_group)
@@ -737,10 +736,41 @@
         tensor_data = torch.empty(size, dtype=torch.uint8)
         dist.broadcast(tensor_data, src=0, group=dist_group)

-        serialized_data = bytes(tensor_data.cpu().numpy())
+        # Convert tensor to bytes without routing through NumPy
+        serialized_data = memoryview(tensor_data.cpu()).tobytes()
         data = pickle.loads(serialized_data)
         return data

+class MultiprocessingSerializer:
+    """Fast serializer for cross-process transfer of named tensors.
+
+    This avoids pickling Torch tensors directly across processes and instead
+    serializes a Dict[str, torch.Tensor] to bytes via torch.save/torch.load.
+    Tensors are moved to CPU and made contiguous before serialization to
+    reduce device sync and copy overhead.
+    """
+
+    @staticmethod
+    def dumps_named_tensors(named_tensors: Dict[str, torch.Tensor]) -> bytes:
+        cpu_named: Dict[str, torch.Tensor] = {}
+        for k, v in named_tensors.items():
+            t = v.detach()
+            if t.device.type != "cpu":
+                t = t.to("cpu")
+            if not t.is_contiguous():
+                t = t.contiguous()
+            cpu_named[k] = t
+        buf = BytesIO()
+        torch.save(cpu_named, buf)
+        return buf.getvalue()
+
+    @staticmethod
+    def loads_named_tensors(data: bytes) -> Dict[str, torch.Tensor]:
+        buf = BytesIO(data)
+        named = torch.load(buf)
+        assert isinstance(named, dict), "Deserialized object must be a dict[str, Tensor]"
+        return named
+

 step_counter = 0


file update
M /home/ubuntu/OmniPerf-Bench/perf-agents-bench/.work/worktrees/sglang_core/sglang_core-0042/python/sglang/srt/server.py
@@ -877,4 +877,6 @@
     def update_weights_from_tensor(self, name, tensor):
-        """Update weights from distributed source."""
-        obj = UpdateWeightsFromTensorReqInput(name=name, tensor=tensor)
+        """Update weights from tensor in current process via fast serialization."""
+        from sglang.srt.utils import MultiprocessingSerializer
+        payload = MultiprocessingSerializer.dumps_named_tensors({name: tensor})
+        obj = UpdateWeightsFromTensorReqInput(serialized_named_tensors=payload)
         loop = asyncio.get_event_loop()
apply_patch(auto_approved=true) exited 0 in 14ms:
Success. Updated the following files:
M python/sglang/srt/server.py
file update:
diff --git a/python/sglang/srt/managers/io_struct.py b/python/sglang/srt/managers/io_struct.py
index 13eb233bd14d88be2101b446959b0106ac8399be..1aae28b00b76697cbd2882af46d4010d600f6284
--- a/python/sglang/srt/managers/io_struct.py
+++ b/python/sglang/srt/managers/io_struct.py
@@ -426,8 +426,7 @@

 @dataclass
 class UpdateWeightsFromTensorReqInput:
-    name: str
-    tensor: torch.Tensor
+    serialized_named_tensors: bytes  # indeed Dict[str, torch.Tensor]


 @dataclass
diff --git a/python/sglang/srt/managers/tp_worker.py b/python/sglang/srt/managers/tp_worker.py
index c8e14a746c79dc4e3edb16899419fb873e9b9919..c7500f1d29252a7a0eaccd6112ccb3bee0556271
--- a/python/sglang/srt/managers/tp_worker.py
+++ b/python/sglang/srt/managers/tp_worker.py
@@ -30,7 +30,7 @@
 from sglang.srt.model_executor.forward_batch_info import ForwardBatch
 from sglang.srt.model_executor.model_runner import ModelRunner
 from sglang.srt.server_args import ServerArgs
-from sglang.srt.utils import broadcast_pyobj, set_random_seed
+from sglang.srt.utils import MultiprocessingSerializer, broadcast_pyobj, set_random_seed

 logger = logging.getLogger(__name__)

@@ -196,10 +196,19 @@
         return success, message

     def update_weights_from_tensor(self, recv_req: UpdateWeightsFromTensorReqInput):
-        success, message = self.model_runner.update_weights_from_tensor(
-            recv_req.name, recv_req.tensor
+        named_tensors = MultiprocessingSerializer.loads_named_tensors(
+            recv_req.serialized_named_tensors
         )
-        return success, message
+        # Support updating one or multiple tensors in a single request
+        last_success, last_message = True, "Success"
+        for name, tensor in named_tensors.items():
+            success, message = self.model_runner.update_weights_from_tensor(
+                name, tensor
+            )
+            if not success:
+                return success, message
+            last_success, last_message = success, message
+        return last_success, last_message

     def get_weights_by_name(self, recv_req: GetWeightsByNameReqInput):
         parameter = self.model_runner.get_weights_by_name(
diff --git a/python/sglang/srt/server.py b/python/sglang/srt/server.py
index d95ce5931b5d583346b08037e5dd78a7d8a87b57..a98b1f494d96ac2e1cbc51b11fb3f4c4b9cd6c86
--- a/python/sglang/srt/server.py
+++ b/python/sglang/srt/server.py
@@ -875,8 +875,10 @@
         )

     def update_weights_from_tensor(self, name, tensor):
-        """Update weights from distributed source."""
-        obj = UpdateWeightsFromTensorReqInput(name=name, tensor=tensor)
+        """Update weights from tensor in current process via fast serialization."""
+        from sglang.srt.utils import MultiprocessingSerializer
+        payload = MultiprocessingSerializer.dumps_named_tensors({name: tensor})
+        obj = UpdateWeightsFromTensorReqInput(serialized_named_tensors=payload)
         loop = asyncio.get_event_loop()
         return loop.run_until_complete(
             tokenizer_manager.update_weights_from_tensor(obj, None)
diff --git a/python/sglang/srt/utils.py b/python/sglang/srt/utils.py
index 7c3efa9a269eae7dc8bff8715239202534eb6cfe..be016b60152c195bc61d04ca14cadb5cdaa8acfa
--- a/python/sglang/srt/utils.py
+++ b/python/sglang/srt/utils.py
@@ -718,9 +718,8 @@
         else:
             serialized_data = pickle.dumps(data)
             size = len(serialized_data)
-            tensor_data = torch.ByteTensor(
-                np.frombuffer(serialized_data, dtype=np.uint8)
-            )
+            # Avoid intermediate NumPy copy by leveraging buffer protocol
+            tensor_data = torch.frombuffer(memoryview(serialized_data), dtype=torch.uint8).clone()
             tensor_size = torch.tensor([size], dtype=torch.long)

             dist.broadcast(tensor_size, src=0, group=dist_group)
@@ -737,10 +736,41 @@
         tensor_data = torch.empty(size, dtype=torch.uint8)
         dist.broadcast(tensor_data, src=0, group=dist_group)

-        serialized_data = bytes(tensor_data.cpu().numpy())
+        # Convert tensor to bytes without routing through NumPy
+        serialized_data = memoryview(tensor_data.cpu()).tobytes()
         data = pickle.loads(serialized_data)
         return data

+class MultiprocessingSerializer:
+    """Fast serializer for cross-process transfer of named tensors.
+
+    This avoids pickling Torch tensors directly across processes and instead
+    serializes a Dict[str, torch.Tensor] to bytes via torch.save/torch.load.
+    Tensors are moved to CPU and made contiguous before serialization to
+    reduce device sync and copy overhead.
+    """
+
+    @staticmethod
+    def dumps_named_tensors(named_tensors: Dict[str, torch.Tensor]) -> bytes:
+        cpu_named: Dict[str, torch.Tensor] = {}
+        for k, v in named_tensors.items():
+            t = v.detach()
+            if t.device.type != "cpu":
+                t = t.to("cpu")
+            if not t.is_contiguous():
+                t = t.contiguous()
+            cpu_named[k] = t
+        buf = BytesIO()
+        torch.save(cpu_named, buf)
+        return buf.getvalue()
+
+    @staticmethod
+    def loads_named_tensors(data: bytes) -> Dict[str, torch.Tensor]:
+        buf = BytesIO(data)
+        named = torch.load(buf)
+        assert isinstance(named, dict), "Deserialized object must be a dict[str, Tensor]"
+        return named
+

 step_counter = 0


file update:
diff --git a/python/sglang/srt/managers/io_struct.py b/python/sglang/srt/managers/io_struct.py
index 13eb233bd14d88be2101b446959b0106ac8399be..1aae28b00b76697cbd2882af46d4010d600f6284
--- a/python/sglang/srt/managers/io_struct.py
+++ b/python/sglang/srt/managers/io_struct.py
@@ -426,8 +426,7 @@

 @dataclass
 class UpdateWeightsFromTensorReqInput:
-    name: str
-    tensor: torch.Tensor
+    serialized_named_tensors: bytes  # indeed Dict[str, torch.Tensor]


 @dataclass
diff --git a/python/sglang/srt/managers/tp_worker.py b/python/sglang/srt/managers/tp_worker.py
index c8e14a746c79dc4e3edb16899419fb873e9b9919..c7500f1d29252a7a0eaccd6112ccb3bee0556271
--- a/python/sglang/srt/managers/tp_worker.py
+++ b/python/sglang/srt/managers/tp_worker.py
@@ -30,7 +30,7 @@
 from sglang.srt.model_executor.forward_batch_info import ForwardBatch
 from sglang.srt.model_executor.model_runner import ModelRunner
 from sglang.srt.server_args import ServerArgs
-from sglang.srt.utils import broadcast_pyobj, set_random_seed
+from sglang.srt.utils import MultiprocessingSerializer, broadcast_pyobj, set_random_seed

 logger = logging.getLogger(__name__)

@@ -196,10 +196,19 @@
         return success, message

     def update_weights_from_tensor(self, recv_req: UpdateWeightsFromTensorReqInput):
-        success, message = self.model_runner.update_weights_from_tensor(
-            recv_req.name, recv_req.tensor
+        named_tensors = MultiprocessingSerializer.loads_named_tensors(
+            recv_req.serialized_named_tensors
         )
-        return success, message
+        # Support updating one or multiple tensors in a single request
+        last_success, last_message = True, "Success"
+        for name, tensor in named_tensors.items():
+            success, message = self.model_runner.update_weights_from_tensor(
+                name, tensor
+            )
+            if not success:
+                return success, message
+            last_success, last_message = success, message
+        return last_success, last_message

     def get_weights_by_name(self, recv_req: GetWeightsByNameReqInput):
         parameter = self.model_runner.get_weights_by_name(
diff --git a/python/sglang/srt/server.py b/python/sglang/srt/server.py
index d95ce5931b5d583346b08037e5dd78a7d8a87b57..a98b1f494d96ac2e1cbc51b11fb3f4c4b9cd6c86
--- a/python/sglang/srt/server.py
+++ b/python/sglang/srt/server.py
@@ -875,8 +875,10 @@
         )

     def update_weights_from_tensor(self, name, tensor):
-        """Update weights from distributed source."""
-        obj = UpdateWeightsFromTensorReqInput(name=name, tensor=tensor)
+        """Update weights from tensor in current process via fast serialization."""
+        from sglang.srt.utils import MultiprocessingSerializer
+        payload = MultiprocessingSerializer.dumps_named_tensors({name: tensor})
+        obj = UpdateWeightsFromTensorReqInput(serialized_named_tensors=payload)
         loop = asyncio.get_event_loop()
         return loop.run_until_complete(
             tokenizer_manager.update_weights_from_tensor(obj, None)
diff --git a/python/sglang/srt/utils.py b/python/sglang/srt/utils.py
index 7c3efa9a269eae7dc8bff8715239202534eb6cfe..be016b60152c195bc61d04ca14cadb5cdaa8acfa
--- a/python/sglang/srt/utils.py
+++ b/python/sglang/srt/utils.py
@@ -718,9 +718,8 @@
         else:
             serialized_data = pickle.dumps(data)
             size = len(serialized_data)
-            tensor_data = torch.ByteTensor(
-                np.frombuffer(serialized_data, dtype=np.uint8)
-            )
+            # Avoid intermediate NumPy copy by leveraging buffer protocol
+            tensor_data = torch.frombuffer(memoryview(serialized_data), dtype=torch.uint8).clone()
             tensor_size = torch.tensor([size], dtype=torch.long)

             dist.broadcast(tensor_size, src=0, group=dist_group)
@@ -737,10 +736,41 @@
         tensor_data = torch.empty(size, dtype=torch.uint8)
         dist.broadcast(tensor_data, src=0, group=dist_group)

-        serialized_data = bytes(tensor_data.cpu().numpy())
+        # Convert tensor to bytes without routing through NumPy
+        serialized_data = memoryview(tensor_data.cpu()).tobytes()
         data = pickle.loads(serialized_data)
         return data

+class MultiprocessingSerializer:
+    """Fast serializer for cross-process transfer of named tensors.
+
+    This avoids pickling Torch tensors directly across processes and instead
+    serializes a Dict[str, torch.Tensor] to bytes via torch.save/torch.load.
+    Tensors are moved to CPU and made contiguous before serialization to
+    reduce device sync and copy overhead.
+    """
+
+    @staticmethod
+    def dumps_named_tensors(named_tensors: Dict[str, torch.Tensor]) -> bytes:
+        cpu_named: Dict[str, torch.Tensor] = {}
+        for k, v in named_tensors.items():
+            t = v.detach()
+            if t.device.type != "cpu":
+                t = t.to("cpu")
+            if not t.is_contiguous():
+                t = t.contiguous()
+            cpu_named[k] = t
+        buf = BytesIO()
+        torch.save(cpu_named, buf)
+        return buf.getvalue()
+
+    @staticmethod
+    def loads_named_tensors(data: bytes) -> Dict[str, torch.Tensor]:
+        buf = BytesIO(data)
+        named = torch.load(buf)
+        assert isinstance(named, dict), "Deserialized object must be a dict[str, Tensor]"
+        return named
+

 step_counter = 0



thinking
**Adjusting update processes**

I need to look at several components like TokenizerManager, Scheduler, and TpWorkerOverlapThread, which all create UpdateWeightsFromTensorReqInput and pass through data classes. The only creation occurs in server.Engine.update_weights_from_tensor. I wonder if tests may directly call TokenizerManager.update_weights_from_tensor, since they utilize the sgl.Engine API. I also need to consider updating type checks in TokenizerManager.handle_loop related to UpdateWeightsFromTensorReqInput, while noting the ModelRunner.update_weights_from_tensor remains unchanged. I’ll think about running the bench script again to verify the MultiprocessingSerializer functionality.
exec
bash -lc 'python .bench_scratch/test_opt.py' in /home/ubuntu/OmniPerf-Bench/perf-agents-bench/.work/worktrees/sglang_core/sglang_core-0042 succeeded in 1.54s:
/home/ubuntu/OmniPerf-Bench/bench-env/lib/python3.12/site-packages/torch/cuda/__init__.py:61: FutureWarning: The pynvml package is deprecated. Please install nvidia-ml-py instead. If you did not install pynvml directly, please report this to the maintainers of the package that installed pynvml for you.
  import pynvml  # type: ignore[import]
Benchmark: serialize large tensor dict (approx ~25MB)
pickle.dumps size=25.17MB time=38.0ms
torch.save    size=25.17MB time=33.5ms
pickle.loads  time=13.8ms
torch.load    time=2.5ms
MultiprocessingSerializer not available yet: No module named 'sglang.srt'
file update:
diff --git a/python/sglang/srt/managers/io_struct.py b/python/sglang/srt/managers/io_struct.py
index 13eb233bd14d88be2101b446959b0106ac8399be..1aae28b00b76697cbd2882af46d4010d600f6284
--- a/python/sglang/srt/managers/io_struct.py
+++ b/python/sglang/srt/managers/io_struct.py
@@ -426,8 +426,7 @@

 @dataclass
 class UpdateWeightsFromTensorReqInput:
-    name: str
-    tensor: torch.Tensor
+    serialized_named_tensors: bytes  # indeed Dict[str, torch.Tensor]


 @dataclass
diff --git a/python/sglang/srt/managers/tp_worker.py b/python/sglang/srt/managers/tp_worker.py
index c8e14a746c79dc4e3edb16899419fb873e9b9919..c7500f1d29252a7a0eaccd6112ccb3bee0556271
--- a/python/sglang/srt/managers/tp_worker.py
+++ b/python/sglang/srt/managers/tp_worker.py
@@ -30,7 +30,7 @@
 from sglang.srt.model_executor.forward_batch_info import ForwardBatch
 from sglang.srt.model_executor.model_runner import ModelRunner
 from sglang.srt.server_args import ServerArgs
-from sglang.srt.utils import broadcast_pyobj, set_random_seed
+from sglang.srt.utils import MultiprocessingSerializer, broadcast_pyobj, set_random_seed

 logger = logging.getLogger(__name__)

@@ -196,10 +196,19 @@
         return success, message

     def update_weights_from_tensor(self, recv_req: UpdateWeightsFromTensorReqInput):
-        success, message = self.model_runner.update_weights_from_tensor(
-            recv_req.name, recv_req.tensor
+        named_tensors = MultiprocessingSerializer.loads_named_tensors(
+            recv_req.serialized_named_tensors
         )
-        return success, message
+        # Support updating one or multiple tensors in a single request
+        last_success, last_message = True, "Success"
+        for name, tensor in named_tensors.items():
+            success, message = self.model_runner.update_weights_from_tensor(
+                name, tensor
+            )
+            if not success:
+                return success, message
+            last_success, last_message = success, message
+        return last_success, last_message

     def get_weights_by_name(self, recv_req: GetWeightsByNameReqInput):
         parameter = self.model_runner.get_weights_by_name(
diff --git a/python/sglang/srt/server.py b/python/sglang/srt/server.py
index d95ce5931b5d583346b08037e5dd78a7d8a87b57..a98b1f494d96ac2e1cbc51b11fb3f4c4b9cd6c86
--- a/python/sglang/srt/server.py
+++ b/python/sglang/srt/server.py
@@ -875,8 +875,10 @@
         )

     def update_weights_from_tensor(self, name, tensor):
-        """Update weights from distributed source."""
-        obj = UpdateWeightsFromTensorReqInput(name=name, tensor=tensor)
+        """Update weights from tensor in current process via fast serialization."""
+        from sglang.srt.utils import MultiprocessingSerializer
+        payload = MultiprocessingSerializer.dumps_named_tensors({name: tensor})
+        obj = UpdateWeightsFromTensorReqInput(serialized_named_tensors=payload)
         loop = asyncio.get_event_loop()
         return loop.run_until_complete(
             tokenizer_manager.update_weights_from_tensor(obj, None)
diff --git a/python/sglang/srt/utils.py b/python/sglang/srt/utils.py
index 7c3efa9a269eae7dc8bff8715239202534eb6cfe..be016b60152c195bc61d04ca14cadb5cdaa8acfa
--- a/python/sglang/srt/utils.py
+++ b/python/sglang/srt/utils.py
@@ -718,9 +718,8 @@
         else:
             serialized_data = pickle.dumps(data)
             size = len(serialized_data)
-            tensor_data = torch.ByteTensor(
-                np.frombuffer(serialized_data, dtype=np.uint8)
-            )
+            # Avoid intermediate NumPy copy by leveraging buffer protocol
+            tensor_data = torch.frombuffer(memoryview(serialized_data), dtype=torch.uint8).clone()
             tensor_size = torch.tensor([size], dtype=torch.long)

             dist.broadcast(tensor_size, src=0, group=dist_group)
@@ -737,10 +736,41 @@
         tensor_data = torch.empty(size, dtype=torch.uint8)
         dist.broadcast(tensor_data, src=0, group=dist_group)

-        serialized_data = bytes(tensor_data.cpu().numpy())
+        # Convert tensor to bytes without routing through NumPy
+        serialized_data = memoryview(tensor_data.cpu()).tobytes()
         data = pickle.loads(serialized_data)
         return data

+class MultiprocessingSerializer:
+    """Fast serializer for cross-process transfer of named tensors.
+
+    This avoids pickling Torch tensors directly across processes and instead
+    serializes a Dict[str, torch.Tensor] to bytes via torch.save/torch.load.
+    Tensors are moved to CPU and made contiguous before serialization to
+    reduce device sync and copy overhead.
+    """
+
+    @staticmethod
+    def dumps_named_tensors(named_tensors: Dict[str, torch.Tensor]) -> bytes:
+        cpu_named: Dict[str, torch.Tensor] = {}
+        for k, v in named_tensors.items():
+            t = v.detach()
+            if t.device.type != "cpu":
+                t = t.to("cpu")
+            if not t.is_contiguous():
+                t = t.contiguous()
+            cpu_named[k] = t
+        buf = BytesIO()
+        torch.save(cpu_named, buf)
+        return buf.getvalue()
+
+    @staticmethod
+    def loads_named_tensors(data: bytes) -> Dict[str, torch.Tensor]:
+        buf = BytesIO(data)
+        named = torch.load(buf)
+        assert isinstance(named, dict), "Deserialized object must be a dict[str, Tensor]"
+        return named
+

 step_counter = 0



thinking
**Investigating import issues**

I’m running into an import failure because the script is trying to import `sglang.srt.utils`. It seems our repo's module name is ‘sglang’ under the top-level path. Running Python from the root might not include the 'python' package path. In this repo, they do use the correct module import for 'sglang' since there's an `__init__.py` file in `python/sglang`. I need to double-check this setup and see where the problem lies.
exec
bash -lc "ls -la python/sglang | sed -n '1,80p'" in /home/ubuntu/OmniPerf-Bench/perf-agents-bench/.work/worktrees/sglang_core/sglang_core-0042 succeeded in 11ms:
total 188
drwxrwxr-x  5 ubuntu ubuntu  4096 Nov 21 05:30 .
drwxrwxr-x  3 ubuntu ubuntu  4096 Nov 21 05:30 ..
-rw-rw-r--  1 ubuntu ubuntu   812 Nov 21 05:30 README.md
-rw-rw-r--  1 ubuntu ubuntu  1619 Nov 21 05:30 __init__.py
-rw-rw-r--  1 ubuntu ubuntu  6953 Nov 21 05:30 api.py
-rw-rw-r--  1 ubuntu ubuntu    76 Nov 21 05:30 bench_latency.py
-rw-rw-r--  1 ubuntu ubuntu 12533 Nov 21 05:30 bench_offline_throughput.py
-rw-rw-r--  1 ubuntu ubuntu 15859 Nov 21 05:30 bench_one_batch.py
-rw-rw-r--  1 ubuntu ubuntu  5945 Nov 21 05:30 bench_one_batch_server.py
-rw-rw-r--  1 ubuntu ubuntu 53377 Nov 21 05:30 bench_serving.py
-rw-rw-r--  1 ubuntu ubuntu  8427 Nov 21 05:30 check_env.py
-rw-rw-r--  1 ubuntu ubuntu  1286 Nov 21 05:30 global_config.py
drwxrwxr-x  3 ubuntu ubuntu  4096 Nov 21 05:30 lang
-rw-rw-r--  1 ubuntu ubuntu   403 Nov 21 05:30 launch_server.py
-rw-rw-r--  1 ubuntu ubuntu  1007 Nov 21 05:30 launch_server_llavavid.py
-rw-rw-r--  1 ubuntu ubuntu 10017 Nov 21 05:30 llama3_eval.py
drwxrwxr-x 16 ubuntu ubuntu  4096 Nov 21 05:30 srt
drwxrwxr-x  3 ubuntu ubuntu  4096 Nov 21 05:30 test
-rw-rw-r--  1 ubuntu ubuntu 11597 Nov 21 05:30 utils.py
file update:
diff --git a/python/sglang/srt/managers/io_struct.py b/python/sglang/srt/managers/io_struct.py
index 13eb233bd14d88be2101b446959b0106ac8399be..1aae28b00b76697cbd2882af46d4010d600f6284
--- a/python/sglang/srt/managers/io_struct.py
+++ b/python/sglang/srt/managers/io_struct.py
@@ -426,8 +426,7 @@

 @dataclass
 class UpdateWeightsFromTensorReqInput:
-    name: str
-    tensor: torch.Tensor
+    serialized_named_tensors: bytes  # indeed Dict[str, torch.Tensor]


 @dataclass
diff --git a/python/sglang/srt/managers/tp_worker.py b/python/sglang/srt/managers/tp_worker.py
index c8e14a746c79dc4e3edb16899419fb873e9b9919..c7500f1d29252a7a0eaccd6112ccb3bee0556271
--- a/python/sglang/srt/managers/tp_worker.py
+++ b/python/sglang/srt/managers/tp_worker.py
@@ -30,7 +30,7 @@
 from sglang.srt.model_executor.forward_batch_info import ForwardBatch
 from sglang.srt.model_executor.model_runner import ModelRunner
 from sglang.srt.server_args import ServerArgs
-from sglang.srt.utils import broadcast_pyobj, set_random_seed
+from sglang.srt.utils import MultiprocessingSerializer, broadcast_pyobj, set_random_seed

 logger = logging.getLogger(__name__)

@@ -196,10 +196,19 @@
         return success, message

     def update_weights_from_tensor(self, recv_req: UpdateWeightsFromTensorReqInput):
-        success, message = self.model_runner.update_weights_from_tensor(
-            recv_req.name, recv_req.tensor
+        named_tensors = MultiprocessingSerializer.loads_named_tensors(
+            recv_req.serialized_named_tensors
         )
-        return success, message
+        # Support updating one or multiple tensors in a single request
+        last_success, last_message = True, "Success"
+        for name, tensor in named_tensors.items():
+            success, message = self.model_runner.update_weights_from_tensor(
+                name, tensor
+            )
+            if not success:
+                return success, message
+            last_success, last_message = success, message
+        return last_success, last_message

     def get_weights_by_name(self, recv_req: GetWeightsByNameReqInput):
         parameter = self.model_runner.get_weights_by_name(
diff --git a/python/sglang/srt/server.py b/python/sglang/srt/server.py
index d95ce5931b5d583346b08037e5dd78a7d8a87b57..a98b1f494d96ac2e1cbc51b11fb3f4c4b9cd6c86
--- a/python/sglang/srt/server.py
+++ b/python/sglang/srt/server.py
@@ -875,8 +875,10 @@
         )

     def update_weights_from_tensor(self, name, tensor):
-        """Update weights from distributed source."""
-        obj = UpdateWeightsFromTensorReqInput(name=name, tensor=tensor)
+        """Update weights from tensor in current process via fast serialization."""
+        from sglang.srt.utils import MultiprocessingSerializer
+        payload = MultiprocessingSerializer.dumps_named_tensors({name: tensor})
+        obj = UpdateWeightsFromTensorReqInput(serialized_named_tensors=payload)
         loop = asyncio.get_event_loop()
         return loop.run_until_complete(
             tokenizer_manager.update_weights_from_tensor(obj, None)
diff --git a/python/sglang/srt/utils.py b/python/sglang/srt/utils.py
index 7c3efa9a269eae7dc8bff8715239202534eb6cfe..be016b60152c195bc61d04ca14cadb5cdaa8acfa
--- a/python/sglang/srt/utils.py
+++ b/python/sglang/srt/utils.py
@@ -718,9 +718,8 @@
         else:
             serialized_data = pickle.dumps(data)
             size = len(serialized_data)
-            tensor_data = torch.ByteTensor(
-                np.frombuffer(serialized_data, dtype=np.uint8)
-            )
+            # Avoid intermediate NumPy copy by leveraging buffer protocol
+            tensor_data = torch.frombuffer(memoryview(serialized_data), dtype=torch.uint8).clone()
             tensor_size = torch.tensor([size], dtype=torch.long)

             dist.broadcast(tensor_size, src=0, group=dist_group)
@@ -737,10 +736,41 @@
         tensor_data = torch.empty(size, dtype=torch.uint8)
         dist.broadcast(tensor_data, src=0, group=dist_group)

-        serialized_data = bytes(tensor_data.cpu().numpy())
+        # Convert tensor to bytes without routing through NumPy
+        serialized_data = memoryview(tensor_data.cpu()).tobytes()
         data = pickle.loads(serialized_data)
         return data

+class MultiprocessingSerializer:
+    """Fast serializer for cross-process transfer of named tensors.
+
+    This avoids pickling Torch tensors directly across processes and instead
+    serializes a Dict[str, torch.Tensor] to bytes via torch.save/torch.load.
+    Tensors are moved to CPU and made contiguous before serialization to
+    reduce device sync and copy overhead.
+    """
+
+    @staticmethod
+    def dumps_named_tensors(named_tensors: Dict[str, torch.Tensor]) -> bytes:
+        cpu_named: Dict[str, torch.Tensor] = {}
+        for k, v in named_tensors.items():
+            t = v.detach()
+            if t.device.type != "cpu":
+                t = t.to("cpu")
+            if not t.is_contiguous():
+                t = t.contiguous()
+            cpu_named[k] = t
+        buf = BytesIO()
+        torch.save(cpu_named, buf)
+        return buf.getvalue()
+
+    @staticmethod
+    def loads_named_tensors(data: bytes) -> Dict[str, torch.Tensor]:
+        buf = BytesIO(data)
+        named = torch.load(buf)
+        assert isinstance(named, dict), "Deserialized object must be a dict[str, Tensor]"
+        return named
+

 step_counter = 0



thinking
**Adjusting Python Path for Imports**

I've noticed that the module 'sglang' is located in the python/sglang directory, but when running the benchmark script from the repo root, it seems Python doesn't include python/ in sys.path. This could lead to an import error. I realize that our script needs to add the repo's 'python' path to sys.path for optional imports like MultiprocessingSerializer to work correctly. I’ll modify the script to append 'python' to sys.path and update it accordingly.
file update
M /home/ubuntu/OmniPerf-Bench/perf-agents-bench/.work/worktrees/sglang_core/sglang_core-0042/.bench_scratch/test_opt.py
@@ -2,2 +2,3 @@
 import pickle
+import sys
 from io import BytesIO
@@ -6,2 +7,9 @@

+# allow optional import from repository's python package path
+import os
+repo_python_path = os.path.join(os.path.dirname(__file__), '..', 'python')
+repo_python_path = os.path.abspath(repo_python_path)
+if repo_python_path not in sys.path:
+    sys.path.insert(0, repo_python_path)
+

apply_patch(auto_approved=true) exited 0 in 15ms:
Success. Updated the following files:
M .bench_scratch/test_opt.py
file update:
diff --git a/.bench_scratch/test_opt.py b/.bench_scratch/test_opt.py
index 2bd90135cc722e02ef2d7fd71651808d1f909e4b..e5529257c5f3cc43b3951de15e121adda50e026d
--- a/.bench_scratch/test_opt.py
+++ b/.bench_scratch/test_opt.py
@@ -1,9 +1,17 @@
 import time
 import pickle
+import sys
 from io import BytesIO

 import torch

+# allow optional import from repository's python package path
+import os
+repo_python_path = os.path.join(os.path.dirname(__file__), '..', 'python')
+repo_python_path = os.path.abspath(repo_python_path)
+if repo_python_path not in sys.path:
+    sys.path.insert(0, repo_python_path)
+

 def serialize_pickle(obj):
     t0 = time.time()
diff --git a/python/sglang/srt/managers/io_struct.py b/python/sglang/srt/managers/io_struct.py
index 13eb233bd14d88be2101b446959b0106ac8399be..1aae28b00b76697cbd2882af46d4010d600f6284
--- a/python/sglang/srt/managers/io_struct.py
+++ b/python/sglang/srt/managers/io_struct.py
@@ -426,8 +426,7 @@

 @dataclass
 class UpdateWeightsFromTensorReqInput:
-    name: str
-    tensor: torch.Tensor
+    serialized_named_tensors: bytes  # indeed Dict[str, torch.Tensor]


 @dataclass
diff --git a/python/sglang/srt/managers/tp_worker.py b/python/sglang/srt/managers/tp_worker.py
index c8e14a746c79dc4e3edb16899419fb873e9b9919..c7500f1d29252a7a0eaccd6112ccb3bee0556271
--- a/python/sglang/srt/managers/tp_worker.py
+++ b/python/sglang/srt/managers/tp_worker.py
@@ -30,7 +30,7 @@
 from sglang.srt.model_executor.forward_batch_info import ForwardBatch
 from sglang.srt.model_executor.model_runner import ModelRunner
 from sglang.srt.server_args import ServerArgs
-from sglang.srt.utils import broadcast_pyobj, set_random_seed
+from sglang.srt.utils import MultiprocessingSerializer, broadcast_pyobj, set_random_seed

 logger = logging.getLogger(__name__)

@@ -196,10 +196,19 @@
         return success, message

     def update_weights_from_tensor(self, recv_req: UpdateWeightsFromTensorReqInput):
-        success, message = self.model_runner.update_weights_from_tensor(
-            recv_req.name, recv_req.tensor
+        named_tensors = MultiprocessingSerializer.loads_named_tensors(
+            recv_req.serialized_named_tensors
         )
-        return success, message
+        # Support updating one or multiple tensors in a single request
+        last_success, last_message = True, "Success"
+        for name, tensor in named_tensors.items():
+            success, message = self.model_runner.update_weights_from_tensor(
+                name, tensor
+            )
+            if not success:
+                return success, message
+            last_success, last_message = success, message
+        return last_success, last_message

     def get_weights_by_name(self, recv_req: GetWeightsByNameReqInput):
         parameter = self.model_runner.get_weights_by_name(
diff --git a/python/sglang/srt/server.py b/python/sglang/srt/server.py
index d95ce5931b5d583346b08037e5dd78a7d8a87b57..a98b1f494d96ac2e1cbc51b11fb3f4c4b9cd6c86
--- a/python/sglang/srt/server.py
+++ b/python/sglang/srt/server.py
@@ -875,8 +875,10 @@
         )

     def update_weights_from_tensor(self, name, tensor):
-        """Update weights from distributed source."""
-        obj = UpdateWeightsFromTensorReqInput(name=name, tensor=tensor)
+        """Update weights from tensor in current process via fast serialization."""
+        from sglang.srt.utils import MultiprocessingSerializer
+        payload = MultiprocessingSerializer.dumps_named_tensors({name: tensor})
+        obj = UpdateWeightsFromTensorReqInput(serialized_named_tensors=payload)
         loop = asyncio.get_event_loop()
         return loop.run_until_complete(
             tokenizer_manager.update_weights_from_tensor(obj, None)
diff --git a/python/sglang/srt/utils.py b/python/sglang/srt/utils.py
index 7c3efa9a269eae7dc8bff8715239202534eb6cfe..be016b60152c195bc61d04ca14cadb5cdaa8acfa
--- a/python/sglang/srt/utils.py
+++ b/python/sglang/srt/utils.py
@@ -718,9 +718,8 @@
         else:
             serialized_data = pickle.dumps(data)
             size = len(serialized_data)
-            tensor_data = torch.ByteTensor(
-                np.frombuffer(serialized_data, dtype=np.uint8)
-            )
+            # Avoid intermediate NumPy copy by leveraging buffer protocol
+            tensor_data = torch.frombuffer(memoryview(serialized_data), dtype=torch.uint8).clone()
             tensor_size = torch.tensor([size], dtype=torch.long)

             dist.broadcast(tensor_size, src=0, group=dist_group)
@@ -737,10 +736,41 @@
         tensor_data = torch.empty(size, dtype=torch.uint8)
         dist.broadcast(tensor_data, src=0, group=dist_group)

-        serialized_data = bytes(tensor_data.cpu().numpy())
+        # Convert tensor to bytes without routing through NumPy
+        serialized_data = memoryview(tensor_data.cpu()).tobytes()
         data = pickle.loads(serialized_data)
         return data

+class MultiprocessingSerializer:
+    """Fast serializer for cross-process transfer of named tensors.
+
+    This avoids pickling Torch tensors directly across processes and instead
+    serializes a Dict[str, torch.Tensor] to bytes via torch.save/torch.load.
+    Tensors are moved to CPU and made contiguous before serialization to
+    reduce device sync and copy overhead.
+    """
+
+    @staticmethod
+    def dumps_named_tensors(named_tensors: Dict[str, torch.Tensor]) -> bytes:
+        cpu_named: Dict[str, torch.Tensor] = {}
+        for k, v in named_tensors.items():
+            t = v.detach()
+            if t.device.type != "cpu":
+                t = t.to("cpu")
+            if not t.is_contiguous():
+                t = t.contiguous()
+            cpu_named[k] = t
+        buf = BytesIO()
+        torch.save(cpu_named, buf)
+        return buf.getvalue()
+
+    @staticmethod
+    def loads_named_tensors(data: bytes) -> Dict[str, torch.Tensor]:
+        buf = BytesIO(data)
+        named = torch.load(buf)
+        assert isinstance(named, dict), "Deserialized object must be a dict[str, Tensor]"
+        return named
+

 step_counter = 0


file update:
diff --git a/.bench_scratch/test_opt.py b/.bench_scratch/test_opt.py
index 2bd90135cc722e02ef2d7fd71651808d1f909e4b..e5529257c5f3cc43b3951de15e121adda50e026d
--- a/.bench_scratch/test_opt.py
+++ b/.bench_scratch/test_opt.py
@@ -1,9 +1,17 @@
 import time
 import pickle
+import sys
 from io import BytesIO

 import torch

+# allow optional import from repository's python package path
+import os
+repo_python_path = os.path.join(os.path.dirname(__file__), '..', 'python')
+repo_python_path = os.path.abspath(repo_python_path)
+if repo_python_path not in sys.path:
+    sys.path.insert(0, repo_python_path)
+

 def serialize_pickle(obj):
     t0 = time.time()
diff --git a/python/sglang/srt/managers/io_struct.py b/python/sglang/srt/managers/io_struct.py
index 13eb233bd14d88be2101b446959b0106ac8399be..1aae28b00b76697cbd2882af46d4010d600f6284
--- a/python/sglang/srt/managers/io_struct.py
+++ b/python/sglang/srt/managers/io_struct.py
@@ -426,8 +426,7 @@

 @dataclass
 class UpdateWeightsFromTensorReqInput:
-    name: str
-    tensor: torch.Tensor
+    serialized_named_tensors: bytes  # indeed Dict[str, torch.Tensor]


 @dataclass
diff --git a/python/sglang/srt/managers/tp_worker.py b/python/sglang/srt/managers/tp_worker.py
index c8e14a746c79dc4e3edb16899419fb873e9b9919..c7500f1d29252a7a0eaccd6112ccb3bee0556271
--- a/python/sglang/srt/managers/tp_worker.py
+++ b/python/sglang/srt/managers/tp_worker.py
@@ -30,7 +30,7 @@
 from sglang.srt.model_executor.forward_batch_info import ForwardBatch
 from sglang.srt.model_executor.model_runner import ModelRunner
 from sglang.srt.server_args import ServerArgs
-from sglang.srt.utils import broadcast_pyobj, set_random_seed
+from sglang.srt.utils import MultiprocessingSerializer, broadcast_pyobj, set_random_seed

 logger = logging.getLogger(__name__)

@@ -196,10 +196,19 @@
         return success, message

     def update_weights_from_tensor(self, recv_req: UpdateWeightsFromTensorReqInput):
-        success, message = self.model_runner.update_weights_from_tensor(
-            recv_req.name, recv_req.tensor
+        named_tensors = MultiprocessingSerializer.loads_named_tensors(
+            recv_req.serialized_named_tensors
         )
-        return success, message
+        # Support updating one or multiple tensors in a single request
+        last_success, last_message = True, "Success"
+        for name, tensor in named_tensors.items():
+            success, message = self.model_runner.update_weights_from_tensor(
+                name, tensor
+            )
+            if not success:
+                return success, message
+            last_success, last_message = success, message
+        return last_success, last_message

     def get_weights_by_name(self, recv_req: GetWeightsByNameReqInput):
         parameter = self.model_runner.get_weights_by_name(
diff --git a/python/sglang/srt/server.py b/python/sglang/srt/server.py
index d95ce5931b5d583346b08037e5dd78a7d8a87b57..a98b1f494d96ac2e1cbc51b11fb3f4c4b9cd6c86
--- a/python/sglang/srt/server.py
+++ b/python/sglang/srt/server.py
@@ -875,8 +875,10 @@
         )

     def update_weights_from_tensor(self, name, tensor):
-        """Update weights from distributed source."""
-        obj = UpdateWeightsFromTensorReqInput(name=name, tensor=tensor)
+        """Update weights from tensor in current process via fast serialization."""
+        from sglang.srt.utils import MultiprocessingSerializer
+        payload = MultiprocessingSerializer.dumps_named_tensors({name: tensor})
+        obj = UpdateWeightsFromTensorReqInput(serialized_named_tensors=payload)
         loop = asyncio.get_event_loop()
         return loop.run_until_complete(
             tokenizer_manager.update_weights_from_tensor(obj, None)
diff --git a/python/sglang/srt/utils.py b/python/sglang/srt/utils.py
index 7c3efa9a269eae7dc8bff8715239202534eb6cfe..be016b60152c195bc61d04ca14cadb5cdaa8acfa
--- a/python/sglang/srt/utils.py
+++ b/python/sglang/srt/utils.py
@@ -718,9 +718,8 @@
         else:
             serialized_data = pickle.dumps(data)
             size = len(serialized_data)
-            tensor_data = torch.ByteTensor(
-                np.frombuffer(serialized_data, dtype=np.uint8)
-            )
+            # Avoid intermediate NumPy copy by leveraging buffer protocol
+            tensor_data = torch.frombuffer(memoryview(serialized_data), dtype=torch.uint8).clone()
             tensor_size = torch.tensor([size], dtype=torch.long)

             dist.broadcast(tensor_size, src=0, group=dist_group)
@@ -737,10 +736,41 @@
         tensor_data = torch.empty(size, dtype=torch.uint8)
         dist.broadcast(tensor_data, src=0, group=dist_group)

-        serialized_data = bytes(tensor_data.cpu().numpy())
+        # Convert tensor to bytes without routing through NumPy
+        serialized_data = memoryview(tensor_data.cpu()).tobytes()
         data = pickle.loads(serialized_data)
         return data

+class MultiprocessingSerializer:
+    """Fast serializer for cross-process transfer of named tensors.
+
+    This avoids pickling Torch tensors directly across processes and instead
+    serializes a Dict[str, torch.Tensor] to bytes via torch.save/torch.load.
+    Tensors are moved to CPU and made contiguous before serialization to
+    reduce device sync and copy overhead.
+    """
+
+    @staticmethod
+    def dumps_named_tensors(named_tensors: Dict[str, torch.Tensor]) -> bytes:
+        cpu_named: Dict[str, torch.Tensor] = {}
+        for k, v in named_tensors.items():
+            t = v.detach()
+            if t.device.type != "cpu":
+                t = t.to("cpu")
+            if not t.is_contiguous():
+                t = t.contiguous()
+            cpu_named[k] = t
+        buf = BytesIO()
+        torch.save(cpu_named, buf)
+        return buf.getvalue()
+
+    @staticmethod
+    def loads_named_tensors(data: bytes) -> Dict[str, torch.Tensor]:
+        buf = BytesIO(data)
+        named = torch.load(buf)
+        assert isinstance(named, dict), "Deserialized object must be a dict[str, Tensor]"
+        return named
+

 step_counter = 0


exec
bash -lc 'python .bench_scratch/test_opt.py' in /home/ubuntu/OmniPerf-Bench/perf-agents-bench/.work/worktrees/sglang_core/sglang_core-0042 succeeded in 2.25s:
/home/ubuntu/OmniPerf-Bench/bench-env/lib/python3.12/site-packages/torch/cuda/__init__.py:61: FutureWarning: The pynvml package is deprecated. Please install nvidia-ml-py instead. If you did not install pynvml directly, please report this to the maintainers of the package that installed pynvml for you.
  import pynvml  # type: ignore[import]
Benchmark: serialize large tensor dict (approx ~25MB)
pickle.dumps size=25.17MB time=38.4ms
torch.save    size=25.17MB time=33.6ms
pickle.loads  time=13.9ms
torch.load    time=2.6ms
MultiprocessingSerializer dumps size=25.17MB time=30.2ms
MultiprocessingSerializer loads time=2.5ms
file update:
diff --git a/.bench_scratch/test_opt.py b/.bench_scratch/test_opt.py
index 2bd90135cc722e02ef2d7fd71651808d1f909e4b..e5529257c5f3cc43b3951de15e121adda50e026d
--- a/.bench_scratch/test_opt.py
+++ b/.bench_scratch/test_opt.py
@@ -1,9 +1,17 @@
 import time
 import pickle
+import sys
 from io import BytesIO

 import torch

+# allow optional import from repository's python package path
+import os
+repo_python_path = os.path.join(os.path.dirname(__file__), '..', 'python')
+repo_python_path = os.path.abspath(repo_python_path)
+if repo_python_path not in sys.path:
+    sys.path.insert(0, repo_python_path)
+

 def serialize_pickle(obj):
     t0 = time.time()
diff --git a/python/sglang/srt/managers/io_struct.py b/python/sglang/srt/managers/io_struct.py
index 13eb233bd14d88be2101b446959b0106ac8399be..1aae28b00b76697cbd2882af46d4010d600f6284
--- a/python/sglang/srt/managers/io_struct.py
+++ b/python/sglang/srt/managers/io_struct.py
@@ -426,8 +426,7 @@

 @dataclass
 class UpdateWeightsFromTensorReqInput:
-    name: str
-    tensor: torch.Tensor
+    serialized_named_tensors: bytes  # indeed Dict[str, torch.Tensor]


 @dataclass
diff --git a/python/sglang/srt/managers/tp_worker.py b/python/sglang/srt/managers/tp_worker.py
index c8e14a746c79dc4e3edb16899419fb873e9b9919..c7500f1d29252a7a0eaccd6112ccb3bee0556271
--- a/python/sglang/srt/managers/tp_worker.py
+++ b/python/sglang/srt/managers/tp_worker.py
@@ -30,7 +30,7 @@
 from sglang.srt.model_executor.forward_batch_info import ForwardBatch
 from sglang.srt.model_executor.model_runner import ModelRunner
 from sglang.srt.server_args import ServerArgs
-from sglang.srt.utils import broadcast_pyobj, set_random_seed
+from sglang.srt.utils import MultiprocessingSerializer, broadcast_pyobj, set_random_seed

 logger = logging.getLogger(__name__)

@@ -196,10 +196,19 @@
         return success, message

     def update_weights_from_tensor(self, recv_req: UpdateWeightsFromTensorReqInput):
-        success, message = self.model_runner.update_weights_from_tensor(
-            recv_req.name, recv_req.tensor
+        named_tensors = MultiprocessingSerializer.loads_named_tensors(
+            recv_req.serialized_named_tensors
         )
-        return success, message
+        # Support updating one or multiple tensors in a single request
+        last_success, last_message = True, "Success"
+        for name, tensor in named_tensors.items():
+            success, message = self.model_runner.update_weights_from_tensor(
+                name, tensor
+            )
+            if not success:
+                return success, message
+            last_success, last_message = success, message
+        return last_success, last_message

     def get_weights_by_name(self, recv_req: GetWeightsByNameReqInput):
         parameter = self.model_runner.get_weights_by_name(
diff --git a/python/sglang/srt/server.py b/python/sglang/srt/server.py
index d95ce5931b5d583346b08037e5dd78a7d8a87b57..a98b1f494d96ac2e1cbc51b11fb3f4c4b9cd6c86
--- a/python/sglang/srt/server.py
+++ b/python/sglang/srt/server.py
@@ -875,8 +875,10 @@
         )

     def update_weights_from_tensor(self, name, tensor):
-        """Update weights from distributed source."""
-        obj = UpdateWeightsFromTensorReqInput(name=name, tensor=tensor)
+        """Update weights from tensor in current process via fast serialization."""
+        from sglang.srt.utils import MultiprocessingSerializer
+        payload = MultiprocessingSerializer.dumps_named_tensors({name: tensor})
+        obj = UpdateWeightsFromTensorReqInput(serialized_named_tensors=payload)
         loop = asyncio.get_event_loop()
         return loop.run_until_complete(
             tokenizer_manager.update_weights_from_tensor(obj, None)
diff --git a/python/sglang/srt/utils.py b/python/sglang/srt/utils.py
index 7c3efa9a269eae7dc8bff8715239202534eb6cfe..be016b60152c195bc61d04ca14cadb5cdaa8acfa
--- a/python/sglang/srt/utils.py
+++ b/python/sglang/srt/utils.py
@@ -718,9 +718,8 @@
         else:
             serialized_data = pickle.dumps(data)
             size = len(serialized_data)
-            tensor_data = torch.ByteTensor(
-                np.frombuffer(serialized_data, dtype=np.uint8)
-            )
+            # Avoid intermediate NumPy copy by leveraging buffer protocol
+            tensor_data = torch.frombuffer(memoryview(serialized_data), dtype=torch.uint8).clone()
             tensor_size = torch.tensor([size], dtype=torch.long)

             dist.broadcast(tensor_size, src=0, group=dist_group)
@@ -737,10 +736,41 @@
         tensor_data = torch.empty(size, dtype=torch.uint8)
         dist.broadcast(tensor_data, src=0, group=dist_group)

-        serialized_data = bytes(tensor_data.cpu().numpy())
+        # Convert tensor to bytes without routing through NumPy
+        serialized_data = memoryview(tensor_data.cpu()).tobytes()
         data = pickle.loads(serialized_data)
         return data

+class MultiprocessingSerializer:
+    """Fast serializer for cross-process transfer of named tensors.
+
+    This avoids pickling Torch tensors directly across processes and instead
+    serializes a Dict[str, torch.Tensor] to bytes via torch.save/torch.load.
+    Tensors are moved to CPU and made contiguous before serialization to
+    reduce device sync and copy overhead.
+    """
+
+    @staticmethod
+    def dumps_named_tensors(named_tensors: Dict[str, torch.Tensor]) -> bytes:
+        cpu_named: Dict[str, torch.Tensor] = {}
+        for k, v in named_tensors.items():
+            t = v.detach()
+            if t.device.type != "cpu":
+                t = t.to("cpu")
+            if not t.is_contiguous():
+                t = t.contiguous()
+            cpu_named[k] = t
+        buf = BytesIO()
+        torch.save(cpu_named, buf)
+        return buf.getvalue()
+
+    @staticmethod
+    def loads_named_tensors(data: bytes) -> Dict[str, torch.Tensor]:
+        buf = BytesIO(data)
+        named = torch.load(buf)
+        assert isinstance(named, dict), "Deserialized object must be a dict[str, Tensor]"
+        return named
+

 step_counter = 0



thinking
**Fixing server modifications**

Awesome, there's been noticeable improvement! Now, I need to focus on fixing the `server.Engine` and implementing other modifications to ensure that earlier code referencing the `io_struct` dataclass types for `UpdateWeightsFromTensorReqInput` is updated. We’ve changed the dataclass, but I think there are other places creating `UpdateWeightsFromTensorReqInput` objects, so I need to search through the code to identify those. Let's make sure everything aligns correctly!
exec
bash -lc "rg -n \"UpdateWeightsFromTensorReqInput\\(\" -S python | sed -n '1,120p'" in /home/ubuntu/OmniPerf-Bench/perf-agents-bench/.work/worktrees/sglang_core/sglang_core-0042 succeeded in 18ms:
python/sglang/srt/server.py:881:        obj = UpdateWeightsFromTensorReqInput(serialized_named_tensors=payload)
file update:
diff --git a/.bench_scratch/test_opt.py b/.bench_scratch/test_opt.py
index 2bd90135cc722e02ef2d7fd71651808d1f909e4b..e5529257c5f3cc43b3951de15e121adda50e026d
--- a/.bench_scratch/test_opt.py
+++ b/.bench_scratch/test_opt.py
@@ -1,9 +1,17 @@
 import time
 import pickle
+import sys
 from io import BytesIO

 import torch

+# allow optional import from repository's python package path
+import os
+repo_python_path = os.path.join(os.path.dirname(__file__), '..', 'python')
+repo_python_path = os.path.abspath(repo_python_path)
+if repo_python_path not in sys.path:
+    sys.path.insert(0, repo_python_path)
+

 def serialize_pickle(obj):
     t0 = time.time()
diff --git a/python/sglang/srt/managers/io_struct.py b/python/sglang/srt/managers/io_struct.py
index 13eb233bd14d88be2101b446959b0106ac8399be..1aae28b00b76697cbd2882af46d4010d600f6284
--- a/python/sglang/srt/managers/io_struct.py
+++ b/python/sglang/srt/managers/io_struct.py
@@ -426,8 +426,7 @@

 @dataclass
 class UpdateWeightsFromTensorReqInput:
-    name: str
-    tensor: torch.Tensor
+    serialized_named_tensors: bytes  # indeed Dict[str, torch.Tensor]


 @dataclass
diff --git a/python/sglang/srt/managers/tp_worker.py b/python/sglang/srt/managers/tp_worker.py
index c8e14a746c79dc4e3edb16899419fb873e9b9919..c7500f1d29252a7a0eaccd6112ccb3bee0556271
--- a/python/sglang/srt/managers/tp_worker.py
+++ b/python/sglang/srt/managers/tp_worker.py
@@ -30,7 +30,7 @@
 from sglang.srt.model_executor.forward_batch_info import ForwardBatch
 from sglang.srt.model_executor.model_runner import ModelRunner
 from sglang.srt.server_args import ServerArgs
-from sglang.srt.utils import broadcast_pyobj, set_random_seed
+from sglang.srt.utils import MultiprocessingSerializer, broadcast_pyobj, set_random_seed

 logger = logging.getLogger(__name__)

@@ -196,10 +196,19 @@
         return success, message

     def update_weights_from_tensor(self, recv_req: UpdateWeightsFromTensorReqInput):
-        success, message = self.model_runner.update_weights_from_tensor(
-            recv_req.name, recv_req.tensor
+        named_tensors = MultiprocessingSerializer.loads_named_tensors(
+            recv_req.serialized_named_tensors
         )
-        return success, message
+        # Support updating one or multiple tensors in a single request
+        last_success, last_message = True, "Success"
+        for name, tensor in named_tensors.items():
+            success, message = self.model_runner.update_weights_from_tensor(
+                name, tensor
+            )
+            if not success:
+                return success, message
+            last_success, last_message = success, message
+        return last_success, last_message

     def get_weights_by_name(self, recv_req: GetWeightsByNameReqInput):
         parameter = self.model_runner.get_weights_by_name(
diff --git a/python/sglang/srt/server.py b/python/sglang/srt/server.py
index d95ce5931b5d583346b08037e5dd78a7d8a87b57..a98b1f494d96ac2e1cbc51b11fb3f4c4b9cd6c86
--- a/python/sglang/srt/server.py
+++ b/python/sglang/srt/server.py
@@ -875,8 +875,10 @@
         )

     def update_weights_from_tensor(self, name, tensor):
-        """Update weights from distributed source."""
-        obj = UpdateWeightsFromTensorReqInput(name=name, tensor=tensor)
+        """Update weights from tensor in current process via fast serialization."""
+        from sglang.srt.utils import MultiprocessingSerializer
+        payload = MultiprocessingSerializer.dumps_named_tensors({name: tensor})
+        obj = UpdateWeightsFromTensorReqInput(serialized_named_tensors=payload)
         loop = asyncio.get_event_loop()
         return loop.run_until_complete(
             tokenizer_manager.update_weights_from_tensor(obj, None)
diff --git a/python/sglang/srt/utils.py b/python/sglang/srt/utils.py
index 7c3efa9a269eae7dc8bff8715239202534eb6cfe..be016b60152c195bc61d04ca14cadb5cdaa8acfa
--- a/python/sglang/srt/utils.py
+++ b/python/sglang/srt/utils.py
@@ -718,9 +718,8 @@
         else:
             serialized_data = pickle.dumps(data)
             size = len(serialized_data)
-            tensor_data = torch.ByteTensor(
-                np.frombuffer(serialized_data, dtype=np.uint8)
-            )
+            # Avoid intermediate NumPy copy by leveraging buffer protocol
+            tensor_data = torch.frombuffer(memoryview(serialized_data), dtype=torch.uint8).clone()
             tensor_size = torch.tensor([size], dtype=torch.long)

             dist.broadcast(tensor_size, src=0, group=dist_group)
@@ -737,10 +736,41 @@
         tensor_data = torch.empty(size, dtype=torch.uint8)
         dist.broadcast(tensor_data, src=0, group=dist_group)

-        serialized_data = bytes(tensor_data.cpu().numpy())
+        # Convert tensor to bytes without routing through NumPy
+        serialized_data = memoryview(tensor_data.cpu()).tobytes()
         data = pickle.loads(serialized_data)
         return data

+class MultiprocessingSerializer:
+    """Fast serializer for cross-process transfer of named tensors.
+
+    This avoids pickling Torch tensors directly across processes and instead
+    serializes a Dict[str, torch.Tensor] to bytes via torch.save/torch.load.
+    Tensors are moved to CPU and made contiguous before serialization to
+    reduce device sync and copy overhead.
+    """
+
+    @staticmethod
+    def dumps_named_tensors(named_tensors: Dict[str, torch.Tensor]) -> bytes:
+        cpu_named: Dict[str, torch.Tensor] = {}
+        for k, v in named_tensors.items():
+            t = v.detach()
+            if t.device.type != "cpu":
+                t = t.to("cpu")
+            if not t.is_contiguous():
+                t = t.contiguous()
+            cpu_named[k] = t
+        buf = BytesIO()
+        torch.save(cpu_named, buf)
+        return buf.getvalue()
+
+    @staticmethod
+    def loads_named_tensors(data: bytes) -> Dict[str, torch.Tensor]:
+        buf = BytesIO(data)
+        named = torch.load(buf)
+        assert isinstance(named, dict), "Deserialized object must be a dict[str, Tensor]"
+        return named
+

 step_counter = 0


exec
bash -lc "python - << 'PY'
import sys, os
sys.path.insert(0, os.path.abspath('python'))
from sglang.srt.managers.io_struct import UpdateWeightsFromTensorReqInput
from sglang.srt.managers.tp_worker import TpModelWorker
from sglang.srt.utils import MultiprocessingSerializer
print('Loaded classes OK:', bool(UpdateWeightsFromTensorReqInput), bool(MultiprocessingSerializer))
PY" in /home/ubuntu/OmniPerf-Bench/perf-agents-bench/.work/worktrees/sglang_core/sglang_core-0042 exited 1 in 4.28s:
/home/ubuntu/OmniPerf-Bench/bench-env/lib/python3.12/site-packages/torch/cuda/__init__.py:61: FutureWarning: The pynvml package is deprecated. Please install nvidia-ml-py instead. If you did not install pynvml directly, please report this to the maintainers of the package that installed pynvml for you.
  import pynvml  # type: ignore[import]
Traceback (most recent call last):
  File "<stdin>", line 3, in <module>
  File "/home/ubuntu/OmniPerf-Bench/perf-agents-bench/.work/worktrees/sglang_core/sglang_core-0042/python/sglang/srt/managers/io_struct.py", line 26, in <module>
    from sglang.srt.managers.schedule_batch import BaseFinishReason
  File "/home/ubuntu/OmniPerf-Bench/perf-agents-bench/.work/worktrees/sglang_core/sglang_core-0042/python/sglang/srt/managers/schedule_batch.py", line 42, in <module>
    from sglang.srt.configs.model_config import ModelConfig
  File "/home/ubuntu/OmniPerf-Bench/perf-agents-bench/.work/worktrees/sglang_core/sglang_core-0042/python/sglang/srt/configs/model_config.py", line 24, in <module>
    from sglang.srt.layers.quantization import QUANTIZATION_METHODS
  File "/home/ubuntu/OmniPerf-Bench/perf-agents-bench/.work/worktrees/sglang_core/sglang_core-0042/python/sglang/srt/layers/quantization/__init__.py", line 6, in <module>
    from vllm.model_executor.layers.quantization.aqlm import AQLMConfig
ModuleNotFoundError: No module named 'vllm.model_executor'
file update:
diff --git a/.bench_scratch/test_opt.py b/.bench_scratch/test_opt.py
index 2bd90135cc722e02ef2d7fd71651808d1f909e4b..e5529257c5f3cc43b3951de15e121adda50e026d
--- a/.bench_scratch/test_opt.py
+++ b/.bench_scratch/test_opt.py
@@ -1,9 +1,17 @@
 import time
 import pickle
+import sys
 from io import BytesIO

 import torch

+# allow optional import from repository's python package path
+import os
+repo_python_path = os.path.join(os.path.dirname(__file__), '..', 'python')
+repo_python_path = os.path.abspath(repo_python_path)
+if repo_python_path not in sys.path:
+    sys.path.insert(0, repo_python_path)
+

 def serialize_pickle(obj):
     t0 = time.time()
diff --git a/python/sglang/srt/managers/io_struct.py b/python/sglang/srt/managers/io_struct.py
index 13eb233bd14d88be2101b446959b0106ac8399be..1aae28b00b76697cbd2882af46d4010d600f6284
--- a/python/sglang/srt/managers/io_struct.py
+++ b/python/sglang/srt/managers/io_struct.py
@@ -426,8 +426,7 @@

 @dataclass
 class UpdateWeightsFromTensorReqInput:
-    name: str
-    tensor: torch.Tensor
+    serialized_named_tensors: bytes  # indeed Dict[str, torch.Tensor]


 @dataclass
diff --git a/python/sglang/srt/managers/tp_worker.py b/python/sglang/srt/managers/tp_worker.py
index c8e14a746c79dc4e3edb16899419fb873e9b9919..c7500f1d29252a7a0eaccd6112ccb3bee0556271
--- a/python/sglang/srt/managers/tp_worker.py
+++ b/python/sglang/srt/managers/tp_worker.py
@@ -30,7 +30,7 @@
 from sglang.srt.model_executor.forward_batch_info import ForwardBatch
 from sglang.srt.model_executor.model_runner import ModelRunner
 from sglang.srt.server_args import ServerArgs
-from sglang.srt.utils import broadcast_pyobj, set_random_seed
+from sglang.srt.utils import MultiprocessingSerializer, broadcast_pyobj, set_random_seed

 logger = logging.getLogger(__name__)

@@ -196,10 +196,19 @@
         return success, message

     def update_weights_from_tensor(self, recv_req: UpdateWeightsFromTensorReqInput):
-        success, message = self.model_runner.update_weights_from_tensor(
-            recv_req.name, recv_req.tensor
+        named_tensors = MultiprocessingSerializer.loads_named_tensors(
+            recv_req.serialized_named_tensors
         )
-        return success, message
+        # Support updating one or multiple tensors in a single request
+        last_success, last_message = True, "Success"
+        for name, tensor in named_tensors.items():
+            success, message = self.model_runner.update_weights_from_tensor(
+                name, tensor
+            )
+            if not success:
+                return success, message
+            last_success, last_message = success, message
+        return last_success, last_message

     def get_weights_by_name(self, recv_req: GetWeightsByNameReqInput):
         parameter = self.model_runner.get_weights_by_name(
diff --git a/python/sglang/srt/server.py b/python/sglang/srt/server.py
index d95ce5931b5d583346b08037e5dd78a7d8a87b57..a98b1f494d96ac2e1cbc51b11fb3f4c4b9cd6c86
--- a/python/sglang/srt/server.py
+++ b/python/sglang/srt/server.py
@@ -875,8 +875,10 @@
         )

     def update_weights_from_tensor(self, name, tensor):
-        """Update weights from distributed source."""
-        obj = UpdateWeightsFromTensorReqInput(name=name, tensor=tensor)
+        """Update weights from tensor in current process via fast serialization."""
+        from sglang.srt.utils import MultiprocessingSerializer
+        payload = MultiprocessingSerializer.dumps_named_tensors({name: tensor})
+        obj = UpdateWeightsFromTensorReqInput(serialized_named_tensors=payload)
         loop = asyncio.get_event_loop()
         return loop.run_until_complete(
             tokenizer_manager.update_weights_from_tensor(obj, None)
diff --git a/python/sglang/srt/utils.py b/python/sglang/srt/utils.py
index 7c3efa9a269eae7dc8bff8715239202534eb6cfe..be016b60152c195bc61d04ca14cadb5cdaa8acfa
--- a/python/sglang/srt/utils.py
+++ b/python/sglang/srt/utils.py
@@ -718,9 +718,8 @@
         else:
             serialized_data = pickle.dumps(data)
             size = len(serialized_data)
-            tensor_data = torch.ByteTensor(
-                np.frombuffer(serialized_data, dtype=np.uint8)
-            )
+            # Avoid intermediate NumPy copy by leveraging buffer protocol
+            tensor_data = torch.frombuffer(memoryview(serialized_data), dtype=torch.uint8).clone()
             tensor_size = torch.tensor([size], dtype=torch.long)

             dist.broadcast(tensor_size, src=0, group=dist_group)
@@ -737,10 +736,41 @@
         tensor_data = torch.empty(size, dtype=torch.uint8)
         dist.broadcast(tensor_data, src=0, group=dist_group)

-        serialized_data = bytes(tensor_data.cpu().numpy())
+        # Convert tensor to bytes without routing through NumPy
+        serialized_data = memoryview(tensor_data.cpu()).tobytes()
         data = pickle.loads(serialized_data)
         return data

+class MultiprocessingSerializer:
+    """Fast serializer for cross-process transfer of named tensors.
+
+    This avoids pickling Torch tensors directly across processes and instead
+    serializes a Dict[str, torch.Tensor] to bytes via torch.save/torch.load.
+    Tensors are moved to CPU and made contiguous before serialization to
+    reduce device sync and copy overhead.
+    """
+
+    @staticmethod
+    def dumps_named_tensors(named_tensors: Dict[str, torch.Tensor]) -> bytes:
+        cpu_named: Dict[str, torch.Tensor] = {}
+        for k, v in named_tensors.items():
+            t = v.detach()
+            if t.device.type != "cpu":
+                t = t.to("cpu")
+            if not t.is_contiguous():
+                t = t.contiguous()
+            cpu_named[k] = t
+        buf = BytesIO()
+        torch.save(cpu_named, buf)
+        return buf.getvalue()
+
+    @staticmethod
+    def loads_named_tensors(data: bytes) -> Dict[str, torch.Tensor]:
+        buf = BytesIO(data)
+        named = torch.load(buf)
+        assert isinstance(named, dict), "Deserialized object must be a dict[str, Tensor]"
+        return named
+

 step_counter = 0


file update
M /home/ubuntu/OmniPerf-Bench/perf-agents-bench/.work/worktrees/sglang_core/sglang_core-0042/python/sglang/srt/model_executor/model_runner.py
@@ -406,6 +406,3 @@
         """
-        target_dtype = (
-            dtype if isinstance(dtype, torch.dtype) else getattr(torch, dtype)
-        )
-        current_dtype = self.dtype if isinstance(self.dtype, str) else self.dtype
+        target_dtype = dtype if isinstance(dtype, torch.dtype) else getattr(torch, dtype)

@@ -416,2 +413,3 @@
         try:
+            # Allocate directly on target device to avoid an extra copy
             weights = torch.empty(shape, dtype=target_dtype, device=self.device)
apply_patch(auto_approved=true) exited 0 in 18ms:
Success. Updated the following files:
M python/sglang/srt/model_executor/model_runner.py
file update:
diff --git a/.bench_scratch/test_opt.py b/.bench_scratch/test_opt.py
index 2bd90135cc722e02ef2d7fd71651808d1f909e4b..e5529257c5f3cc43b3951de15e121adda50e026d
--- a/.bench_scratch/test_opt.py
+++ b/.bench_scratch/test_opt.py
@@ -1,9 +1,17 @@
 import time
 import pickle
+import sys
 from io import BytesIO

 import torch

+# allow optional import from repository's python package path
+import os
+repo_python_path = os.path.join(os.path.dirname(__file__), '..', 'python')
+repo_python_path = os.path.abspath(repo_python_path)
+if repo_python_path not in sys.path:
+    sys.path.insert(0, repo_python_path)
+

 def serialize_pickle(obj):
     t0 = time.time()
diff --git a/python/sglang/srt/managers/io_struct.py b/python/sglang/srt/managers/io_struct.py
index 13eb233bd14d88be2101b446959b0106ac8399be..1aae28b00b76697cbd2882af46d4010d600f6284
--- a/python/sglang/srt/managers/io_struct.py
+++ b/python/sglang/srt/managers/io_struct.py
@@ -426,8 +426,7 @@

 @dataclass
 class UpdateWeightsFromTensorReqInput:
-    name: str
-    tensor: torch.Tensor
+    serialized_named_tensors: bytes  # indeed Dict[str, torch.Tensor]


 @dataclass
diff --git a/python/sglang/srt/managers/tp_worker.py b/python/sglang/srt/managers/tp_worker.py
index c8e14a746c79dc4e3edb16899419fb873e9b9919..c7500f1d29252a7a0eaccd6112ccb3bee0556271
--- a/python/sglang/srt/managers/tp_worker.py
+++ b/python/sglang/srt/managers/tp_worker.py
@@ -30,7 +30,7 @@
 from sglang.srt.model_executor.forward_batch_info import ForwardBatch
 from sglang.srt.model_executor.model_runner import ModelRunner
 from sglang.srt.server_args import ServerArgs
-from sglang.srt.utils import broadcast_pyobj, set_random_seed
+from sglang.srt.utils import MultiprocessingSerializer, broadcast_pyobj, set_random_seed

 logger = logging.getLogger(__name__)

@@ -196,10 +196,19 @@
         return success, message

     def update_weights_from_tensor(self, recv_req: UpdateWeightsFromTensorReqInput):
-        success, message = self.model_runner.update_weights_from_tensor(
-            recv_req.name, recv_req.tensor
+        named_tensors = MultiprocessingSerializer.loads_named_tensors(
+            recv_req.serialized_named_tensors
         )
-        return success, message
+        # Support updating one or multiple tensors in a single request
+        last_success, last_message = True, "Success"
+        for name, tensor in named_tensors.items():
+            success, message = self.model_runner.update_weights_from_tensor(
+                name, tensor
+            )
+            if not success:
+                return success, message
+            last_success, last_message = success, message
+        return last_success, last_message

     def get_weights_by_name(self, recv_req: GetWeightsByNameReqInput):
         parameter = self.model_runner.get_weights_by_name(
diff --git a/python/sglang/srt/model_executor/model_runner.py b/python/sglang/srt/model_executor/model_runner.py
index 786f654ded689407bff39994405c4c843207b411..453a64ee1dcb51eeeb071969914d042f3d7bf4f7
--- a/python/sglang/srt/model_executor/model_runner.py
+++ b/python/sglang/srt/model_executor/model_runner.py
@@ -404,16 +404,14 @@
             dtype: the data type of the parameter to be updated.
             shape: the shape of the parameter to be updated.
         """
-        target_dtype = (
-            dtype if isinstance(dtype, torch.dtype) else getattr(torch, dtype)
-        )
-        current_dtype = self.dtype if isinstance(self.dtype, str) else self.dtype
+        target_dtype = dtype if isinstance(dtype, torch.dtype) else getattr(torch, dtype)

         assert (
             self._model_update_group is not None
         ), "model update group must be initialized"

         try:
+            # Allocate directly on target device to avoid an extra copy
             weights = torch.empty(shape, dtype=target_dtype, device=self.device)
             torch.distributed.broadcast(weights, src=0, group=self._model_update_group)
             self.model.load_weights([(name, weights)])
diff --git a/python/sglang/srt/server.py b/python/sglang/srt/server.py
index d95ce5931b5d583346b08037e5dd78a7d8a87b57..a98b1f494d96ac2e1cbc51b11fb3f4c4b9cd6c86
--- a/python/sglang/srt/server.py
+++ b/python/sglang/srt/server.py
@@ -875,8 +875,10 @@
         )

     def update_weights_from_tensor(self, name, tensor):
-        """Update weights from distributed source."""
-        obj = UpdateWeightsFromTensorReqInput(name=name, tensor=tensor)
+        """Update weights from tensor in current process via fast serialization."""
+        from sglang.srt.utils import MultiprocessingSerializer
+        payload = MultiprocessingSerializer.dumps_named_tensors({name: tensor})
+        obj = UpdateWeightsFromTensorReqInput(serialized_named_tensors=payload)
         loop = asyncio.get_event_loop()
         return loop.run_until_complete(
             tokenizer_manager.update_weights_from_tensor(obj, None)
diff --git a/python/sglang/srt/utils.py b/python/sglang/srt/utils.py
index 7c3efa9a269eae7dc8bff8715239202534eb6cfe..be016b60152c195bc61d04ca14cadb5cdaa8acfa
--- a/python/sglang/srt/utils.py
+++ b/python/sglang/srt/utils.py
@@ -718,9 +718,8 @@
         else:
             serialized_data = pickle.dumps(data)
             size = len(serialized_data)
-            tensor_data = torch.ByteTensor(
-                np.frombuffer(serialized_data, dtype=np.uint8)
-            )
+            # Avoid intermediate NumPy copy by leveraging buffer protocol
+            tensor_data = torch.frombuffer(memoryview(serialized_data), dtype=torch.uint8).clone()
             tensor_size = torch.tensor([size], dtype=torch.long)

             dist.broadcast(tensor_size, src=0, group=dist_group)
@@ -737,10 +736,41 @@
         tensor_data = torch.empty(size, dtype=torch.uint8)
         dist.broadcast(tensor_data, src=0, group=dist_group)

-        serialized_data = bytes(tensor_data.cpu().numpy())
+        # Convert tensor to bytes without routing through NumPy
+        serialized_data = memoryview(tensor_data.cpu()).tobytes()
         data = pickle.loads(serialized_data)
         return data

+class MultiprocessingSerializer:
+    """Fast serializer for cross-process transfer of named tensors.
+
+    This avoids pickling Torch tensors directly across processes and instead
+    serializes a Dict[str, torch.Tensor] to bytes via torch.save/torch.load.
+    Tensors are moved to CPU and made contiguous before serialization to
+    reduce device sync and copy overhead.
+    """
+
+    @staticmethod
+    def dumps_named_tensors(named_tensors: Dict[str, torch.Tensor]) -> bytes:
+        cpu_named: Dict[str, torch.Tensor] = {}
+        for k, v in named_tensors.items():
+            t = v.detach()
+            if t.device.type != "cpu":
+                t = t.to("cpu")
+            if not t.is_contiguous():
+                t = t.contiguous()
+            cpu_named[k] = t
+        buf = BytesIO()
+        torch.save(cpu_named, buf)
+        return buf.getvalue()
+
+    @staticmethod
+    def loads_named_tensors(data: bytes) -> Dict[str, torch.Tensor]:
+        buf = BytesIO(data)
+        named = torch.load(buf)
+        assert isinstance(named, dict), "Deserialized object must be a dict[str, Tensor]"
+        return named
+

 step_counter = 0


file update:
diff --git a/.bench_scratch/test_opt.py b/.bench_scratch/test_opt.py
index 2bd90135cc722e02ef2d7fd71651808d1f909e4b..e5529257c5f3cc43b3951de15e121adda50e026d
--- a/.bench_scratch/test_opt.py
+++ b/.bench_scratch/test_opt.py
@@ -1,9 +1,17 @@
 import time
 import pickle
+import sys
 from io import BytesIO

 import torch

+# allow optional import from repository's python package path
+import os
+repo_python_path = os.path.join(os.path.dirname(__file__), '..', 'python')
+repo_python_path = os.path.abspath(repo_python_path)
+if repo_python_path not in sys.path:
+    sys.path.insert(0, repo_python_path)
+

 def serialize_pickle(obj):
     t0 = time.time()
diff --git a/python/sglang/srt/managers/io_struct.py b/python/sglang/srt/managers/io_struct.py
index 13eb233bd14d88be2101b446959b0106ac8399be..1aae28b00b76697cbd2882af46d4010d600f6284
--- a/python/sglang/srt/managers/io_struct.py
+++ b/python/sglang/srt/managers/io_struct.py
@@ -426,8 +426,7 @@

 @dataclass
 class UpdateWeightsFromTensorReqInput:
-    name: str
-    tensor: torch.Tensor
+    serialized_named_tensors: bytes  # indeed Dict[str, torch.Tensor]


 @dataclass
diff --git a/python/sglang/srt/managers/tp_worker.py b/python/sglang/srt/managers/tp_worker.py
index c8e14a746c79dc4e3edb16899419fb873e9b9919..c7500f1d29252a7a0eaccd6112ccb3bee0556271
--- a/python/sglang/srt/managers/tp_worker.py
+++ b/python/sglang/srt/managers/tp_worker.py
@@ -30,7 +30,7 @@
 from sglang.srt.model_executor.forward_batch_info import ForwardBatch
 from sglang.srt.model_executor.model_runner import ModelRunner
 from sglang.srt.server_args import ServerArgs
-from sglang.srt.utils import broadcast_pyobj, set_random_seed
+from sglang.srt.utils import MultiprocessingSerializer, broadcast_pyobj, set_random_seed

 logger = logging.getLogger(__name__)

@@ -196,10 +196,19 @@
         return success, message

     def update_weights_from_tensor(self, recv_req: UpdateWeightsFromTensorReqInput):
-        success, message = self.model_runner.update_weights_from_tensor(
-            recv_req.name, recv_req.tensor
+        named_tensors = MultiprocessingSerializer.loads_named_tensors(
+            recv_req.serialized_named_tensors
         )
-        return success, message
+        # Support updating one or multiple tensors in a single request
+        last_success, last_message = True, "Success"
+        for name, tensor in named_tensors.items():
+            success, message = self.model_runner.update_weights_from_tensor(
+                name, tensor
+            )
+            if not success:
+                return success, message
+            last_success, last_message = success, message
+        return last_success, last_message

     def get_weights_by_name(self, recv_req: GetWeightsByNameReqInput):
         parameter = self.model_runner.get_weights_by_name(
diff --git a/python/sglang/srt/model_executor/model_runner.py b/python/sglang/srt/model_executor/model_runner.py
index 786f654ded689407bff39994405c4c843207b411..453a64ee1dcb51eeeb071969914d042f3d7bf4f7
--- a/python/sglang/srt/model_executor/model_runner.py
+++ b/python/sglang/srt/model_executor/model_runner.py
@@ -404,16 +404,14 @@
             dtype: the data type of the parameter to be updated.
             shape: the shape of the parameter to be updated.
         """
-        target_dtype = (
-            dtype if isinstance(dtype, torch.dtype) else getattr(torch, dtype)
-        )
-        current_dtype = self.dtype if isinstance(self.dtype, str) else self.dtype
+        target_dtype = dtype if isinstance(dtype, torch.dtype) else getattr(torch, dtype)

         assert (
             self._model_update_group is not None
         ), "model update group must be initialized"

         try:
+            # Allocate directly on target device to avoid an extra copy
             weights = torch.empty(shape, dtype=target_dtype, device=self.device)
             torch.distributed.broadcast(weights, src=0, group=self._model_update_group)
             self.model.load_weights([(name, weights)])
diff --git a/python/sglang/srt/server.py b/python/sglang/srt/server.py
index d95ce5931b5d583346b08037e5dd78a7d8a87b57..a98b1f494d96ac2e1cbc51b11fb3f4c4b9cd6c86
--- a/python/sglang/srt/server.py
+++ b/python/sglang/srt/server.py
@@ -875,8 +875,10 @@
         )

     def update_weights_from_tensor(self, name, tensor):
-        """Update weights from distributed source."""
-        obj = UpdateWeightsFromTensorReqInput(name=name, tensor=tensor)
+        """Update weights from tensor in current process via fast serialization."""
+        from sglang.srt.utils import MultiprocessingSerializer
+        payload = MultiprocessingSerializer.dumps_named_tensors({name: tensor})
+        obj = UpdateWeightsFromTensorReqInput(serialized_named_tensors=payload)
         loop = asyncio.get_event_loop()
         return loop.run_until_complete(
             tokenizer_manager.update_weights_from_tensor(obj, None)
diff --git a/python/sglang/srt/utils.py b/python/sglang/srt/utils.py
index 7c3efa9a269eae7dc8bff8715239202534eb6cfe..be016b60152c195bc61d04ca14cadb5cdaa8acfa
--- a/python/sglang/srt/utils.py
+++ b/python/sglang/srt/utils.py
@@ -718,9 +718,8 @@
         else:
             serialized_data = pickle.dumps(data)
             size = len(serialized_data)
-            tensor_data = torch.ByteTensor(
-                np.frombuffer(serialized_data, dtype=np.uint8)
-            )
+            # Avoid intermediate NumPy copy by leveraging buffer protocol
+            tensor_data = torch.frombuffer(memoryview(serialized_data), dtype=torch.uint8).clone()
             tensor_size = torch.tensor([size], dtype=torch.long)

             dist.broadcast(tensor_size, src=0, group=dist_group)
@@ -737,10 +736,41 @@
         tensor_data = torch.empty(size, dtype=torch.uint8)
         dist.broadcast(tensor_data, src=0, group=dist_group)

-        serialized_data = bytes(tensor_data.cpu().numpy())
+        # Convert tensor to bytes without routing through NumPy
+        serialized_data = memoryview(tensor_data.cpu()).tobytes()
         data = pickle.loads(serialized_data)
         return data

+class MultiprocessingSerializer:
+    """Fast serializer for cross-process transfer of named tensors.
+
+    This avoids pickling Torch tensors directly across processes and instead
+    serializes a Dict[str, torch.Tensor] to bytes via torch.save/torch.load.
+    Tensors are moved to CPU and made contiguous before serialization to
+    reduce device sync and copy overhead.
+    """
+
+    @staticmethod
+    def dumps_named_tensors(named_tensors: Dict[str, torch.Tensor]) -> bytes:
+        cpu_named: Dict[str, torch.Tensor] = {}
+        for k, v in named_tensors.items():
+            t = v.detach()
+            if t.device.type != "cpu":
+                t = t.to("cpu")
+            if not t.is_contiguous():
+                t = t.contiguous()
+            cpu_named[k] = t
+        buf = BytesIO()
+        torch.save(cpu_named, buf)
+        return buf.getvalue()
+
+    @staticmethod
+    def loads_named_tensors(data: bytes) -> Dict[str, torch.Tensor]:
+        buf = BytesIO(data)
+        named = torch.load(buf)
+        assert isinstance(named, dict), "Deserialized object must be a dict[str, Tensor]"
+        return named
+

 step_counter = 0


exec
bash -lc "grep -n 'torch.zeros\\|fill_' vllm/model_executor/layers/fused_moe/moe_align_block_size.py benchmarks/kernels/benchmark_moe_align_block_size.py || true" in /home/ubuntu/OmniPerf-Bench/perf-agents-bench/.work/worktrees/sglang_core/sglang_core-0042 succeeded in 10ms:
grep: vllm/model_executor/layers/fused_moe/moe_align_block_size.py: No such file or directory
grep: benchmarks/kernels/benchmark_moe_align_block_size.py: No such file or directory
file update:
diff --git a/.bench_scratch/test_opt.py b/.bench_scratch/test_opt.py
index 2bd90135cc722e02ef2d7fd71651808d1f909e4b..e5529257c5f3cc43b3951de15e121adda50e026d
--- a/.bench_scratch/test_opt.py
+++ b/.bench_scratch/test_opt.py
@@ -1,9 +1,17 @@
 import time
 import pickle
+import sys
 from io import BytesIO

 import torch

+# allow optional import from repository's python package path
+import os
+repo_python_path = os.path.join(os.path.dirname(__file__), '..', 'python')
+repo_python_path = os.path.abspath(repo_python_path)
+if repo_python_path not in sys.path:
+    sys.path.insert(0, repo_python_path)
+

 def serialize_pickle(obj):
     t0 = time.time()
diff --git a/python/sglang/srt/managers/io_struct.py b/python/sglang/srt/managers/io_struct.py
index 13eb233bd14d88be2101b446959b0106ac8399be..1aae28b00b76697cbd2882af46d4010d600f6284
--- a/python/sglang/srt/managers/io_struct.py
+++ b/python/sglang/srt/managers/io_struct.py
@@ -426,8 +426,7 @@

 @dataclass
 class UpdateWeightsFromTensorReqInput:
-    name: str
-    tensor: torch.Tensor
+    serialized_named_tensors: bytes  # indeed Dict[str, torch.Tensor]


 @dataclass
diff --git a/python/sglang/srt/managers/tp_worker.py b/python/sglang/srt/managers/tp_worker.py
index c8e14a746c79dc4e3edb16899419fb873e9b9919..c7500f1d29252a7a0eaccd6112ccb3bee0556271
--- a/python/sglang/srt/managers/tp_worker.py
+++ b/python/sglang/srt/managers/tp_worker.py
@@ -30,7 +30,7 @@
 from sglang.srt.model_executor.forward_batch_info import ForwardBatch
 from sglang.srt.model_executor.model_runner import ModelRunner
 from sglang.srt.server_args import ServerArgs
-from sglang.srt.utils import broadcast_pyobj, set_random_seed
+from sglang.srt.utils import MultiprocessingSerializer, broadcast_pyobj, set_random_seed

 logger = logging.getLogger(__name__)

@@ -196,10 +196,19 @@
         return success, message

     def update_weights_from_tensor(self, recv_req: UpdateWeightsFromTensorReqInput):
-        success, message = self.model_runner.update_weights_from_tensor(
-            recv_req.name, recv_req.tensor
+        named_tensors = MultiprocessingSerializer.loads_named_tensors(
+            recv_req.serialized_named_tensors
         )
-        return success, message
+        # Support updating one or multiple tensors in a single request
+        last_success, last_message = True, "Success"
+        for name, tensor in named_tensors.items():
+            success, message = self.model_runner.update_weights_from_tensor(
+                name, tensor
+            )
+            if not success:
+                return success, message
+            last_success, last_message = success, message
+        return last_success, last_message

     def get_weights_by_name(self, recv_req: GetWeightsByNameReqInput):
         parameter = self.model_runner.get_weights_by_name(
diff --git a/python/sglang/srt/model_executor/model_runner.py b/python/sglang/srt/model_executor/model_runner.py
index 786f654ded689407bff39994405c4c843207b411..453a64ee1dcb51eeeb071969914d042f3d7bf4f7
--- a/python/sglang/srt/model_executor/model_runner.py
+++ b/python/sglang/srt/model_executor/model_runner.py
@@ -404,16 +404,14 @@
             dtype: the data type of the parameter to be updated.
             shape: the shape of the parameter to be updated.
         """
-        target_dtype = (
-            dtype if isinstance(dtype, torch.dtype) else getattr(torch, dtype)
-        )
-        current_dtype = self.dtype if isinstance(self.dtype, str) else self.dtype
+        target_dtype = dtype if isinstance(dtype, torch.dtype) else getattr(torch, dtype)

         assert (
             self._model_update_group is not None
         ), "model update group must be initialized"

         try:
+            # Allocate directly on target device to avoid an extra copy
             weights = torch.empty(shape, dtype=target_dtype, device=self.device)
             torch.distributed.broadcast(weights, src=0, group=self._model_update_group)
             self.model.load_weights([(name, weights)])
diff --git a/python/sglang/srt/server.py b/python/sglang/srt/server.py
index d95ce5931b5d583346b08037e5dd78a7d8a87b57..a98b1f494d96ac2e1cbc51b11fb3f4c4b9cd6c86
--- a/python/sglang/srt/server.py
+++ b/python/sglang/srt/server.py
@@ -875,8 +875,10 @@
         )

     def update_weights_from_tensor(self, name, tensor):
-        """Update weights from distributed source."""
-        obj = UpdateWeightsFromTensorReqInput(name=name, tensor=tensor)
+        """Update weights from tensor in current process via fast serialization."""
+        from sglang.srt.utils import MultiprocessingSerializer
+        payload = MultiprocessingSerializer.dumps_named_tensors({name: tensor})
+        obj = UpdateWeightsFromTensorReqInput(serialized_named_tensors=payload)
         loop = asyncio.get_event_loop()
         return loop.run_until_complete(
             tokenizer_manager.update_weights_from_tensor(obj, None)
diff --git a/python/sglang/srt/utils.py b/python/sglang/srt/utils.py
index 7c3efa9a269eae7dc8bff8715239202534eb6cfe..be016b60152c195bc61d04ca14cadb5cdaa8acfa
--- a/python/sglang/srt/utils.py
+++ b/python/sglang/srt/utils.py
@@ -718,9 +718,8 @@
         else:
             serialized_data = pickle.dumps(data)
             size = len(serialized_data)
-            tensor_data = torch.ByteTensor(
-                np.frombuffer(serialized_data, dtype=np.uint8)
-            )
+            # Avoid intermediate NumPy copy by leveraging buffer protocol
+            tensor_data = torch.frombuffer(memoryview(serialized_data), dtype=torch.uint8).clone()
             tensor_size = torch.tensor([size], dtype=torch.long)

             dist.broadcast(tensor_size, src=0, group=dist_group)
@@ -737,10 +736,41 @@
         tensor_data = torch.empty(size, dtype=torch.uint8)
         dist.broadcast(tensor_data, src=0, group=dist_group)

-        serialized_data = bytes(tensor_data.cpu().numpy())
+        # Convert tensor to bytes without routing through NumPy
+        serialized_data = memoryview(tensor_data.cpu()).tobytes()
         data = pickle.loads(serialized_data)
         return data

+class MultiprocessingSerializer:
+    """Fast serializer for cross-process transfer of named tensors.
+
+    This avoids pickling Torch tensors directly across processes and instead
+    serializes a Dict[str, torch.Tensor] to bytes via torch.save/torch.load.
+    Tensors are moved to CPU and made contiguous before serialization to
+    reduce device sync and copy overhead.
+    """
+
+    @staticmethod
+    def dumps_named_tensors(named_tensors: Dict[str, torch.Tensor]) -> bytes:
+        cpu_named: Dict[str, torch.Tensor] = {}
+        for k, v in named_tensors.items():
+            t = v.detach()
+            if t.device.type != "cpu":
+                t = t.to("cpu")
+            if not t.is_contiguous():
+                t = t.contiguous()
+            cpu_named[k] = t
+        buf = BytesIO()
+        torch.save(cpu_named, buf)
+        return buf.getvalue()
+
+    @staticmethod
+    def loads_named_tensors(data: bytes) -> Dict[str, torch.Tensor]:
+        buf = BytesIO(data)
+        named = torch.load(buf)
+        assert isinstance(named, dict), "Deserialized object must be a dict[str, Tensor]"
+        return named
+

 step_counter = 0


exec
bash -lc 'git status --porcelain' in /home/ubuntu/OmniPerf-Bench/perf-agents-bench/.work/worktrees/sglang_core/sglang_core-0042 succeeded in 14ms:
 M python/sglang/srt/managers/io_struct.py
 M python/sglang/srt/managers/tp_worker.py
 M python/sglang/srt/model_executor/model_runner.py
 M python/sglang/srt/server.py
 M python/sglang/srt/utils.py
?? .bench_scratch/
file update:
diff --git a/.bench_scratch/test_opt.py b/.bench_scratch/test_opt.py
index 2bd90135cc722e02ef2d7fd71651808d1f909e4b..e5529257c5f3cc43b3951de15e121adda50e026d
--- a/.bench_scratch/test_opt.py
+++ b/.bench_scratch/test_opt.py
@@ -1,9 +1,17 @@
 import time
 import pickle
+import sys
 from io import BytesIO

 import torch

+# allow optional import from repository's python package path
+import os
+repo_python_path = os.path.join(os.path.dirname(__file__), '..', 'python')
+repo_python_path = os.path.abspath(repo_python_path)
+if repo_python_path not in sys.path:
+    sys.path.insert(0, repo_python_path)
+

 def serialize_pickle(obj):
     t0 = time.time()
diff --git a/python/sglang/srt/managers/io_struct.py b/python/sglang/srt/managers/io_struct.py
index 13eb233bd14d88be2101b446959b0106ac8399be..1aae28b00b76697cbd2882af46d4010d600f6284
--- a/python/sglang/srt/managers/io_struct.py
+++ b/python/sglang/srt/managers/io_struct.py
@@ -426,8 +426,7 @@

 @dataclass
 class UpdateWeightsFromTensorReqInput:
-    name: str
-    tensor: torch.Tensor
+    serialized_named_tensors: bytes  # indeed Dict[str, torch.Tensor]


 @dataclass
diff --git a/python/sglang/srt/managers/tp_worker.py b/python/sglang/srt/managers/tp_worker.py
index c8e14a746c79dc4e3edb16899419fb873e9b9919..c7500f1d29252a7a0eaccd6112ccb3bee0556271
--- a/python/sglang/srt/managers/tp_worker.py
+++ b/python/sglang/srt/managers/tp_worker.py
@@ -30,7 +30,7 @@
 from sglang.srt.model_executor.forward_batch_info import ForwardBatch
 from sglang.srt.model_executor.model_runner import ModelRunner
 from sglang.srt.server_args import ServerArgs
-from sglang.srt.utils import broadcast_pyobj, set_random_seed
+from sglang.srt.utils import MultiprocessingSerializer, broadcast_pyobj, set_random_seed

 logger = logging.getLogger(__name__)

@@ -196,10 +196,19 @@
         return success, message

     def update_weights_from_tensor(self, recv_req: UpdateWeightsFromTensorReqInput):
-        success, message = self.model_runner.update_weights_from_tensor(
-            recv_req.name, recv_req.tensor
+        named_tensors = MultiprocessingSerializer.loads_named_tensors(
+            recv_req.serialized_named_tensors
         )
-        return success, message
+        # Support updating one or multiple tensors in a single request
+        last_success, last_message = True, "Success"
+        for name, tensor in named_tensors.items():
+            success, message = self.model_runner.update_weights_from_tensor(
+                name, tensor
+            )
+            if not success:
+                return success, message
+            last_success, last_message = success, message
+        return last_success, last_message

     def get_weights_by_name(self, recv_req: GetWeightsByNameReqInput):
         parameter = self.model_runner.get_weights_by_name(
diff --git a/python/sglang/srt/model_executor/model_runner.py b/python/sglang/srt/model_executor/model_runner.py
index 786f654ded689407bff39994405c4c843207b411..453a64ee1dcb51eeeb071969914d042f3d7bf4f7
--- a/python/sglang/srt/model_executor/model_runner.py
+++ b/python/sglang/srt/model_executor/model_runner.py
@@ -404,16 +404,14 @@
             dtype: the data type of the parameter to be updated.
             shape: the shape of the parameter to be updated.
         """
-        target_dtype = (
-            dtype if isinstance(dtype, torch.dtype) else getattr(torch, dtype)
-        )
-        current_dtype = self.dtype if isinstance(self.dtype, str) else self.dtype
+        target_dtype = dtype if isinstance(dtype, torch.dtype) else getattr(torch, dtype)

         assert (
             self._model_update_group is not None
         ), "model update group must be initialized"

         try:
+            # Allocate directly on target device to avoid an extra copy
             weights = torch.empty(shape, dtype=target_dtype, device=self.device)
             torch.distributed.broadcast(weights, src=0, group=self._model_update_group)
             self.model.load_weights([(name, weights)])
diff --git a/python/sglang/srt/server.py b/python/sglang/srt/server.py
index d95ce5931b5d583346b08037e5dd78a7d8a87b57..a98b1f494d96ac2e1cbc51b11fb3f4c4b9cd6c86
--- a/python/sglang/srt/server.py
+++ b/python/sglang/srt/server.py
@@ -875,8 +875,10 @@
         )

     def update_weights_from_tensor(self, name, tensor):
-        """Update weights from distributed source."""
-        obj = UpdateWeightsFromTensorReqInput(name=name, tensor=tensor)
+        """Update weights from tensor in current process via fast serialization."""
+        from sglang.srt.utils import MultiprocessingSerializer
+        payload = MultiprocessingSerializer.dumps_named_tensors({name: tensor})
+        obj = UpdateWeightsFromTensorReqInput(serialized_named_tensors=payload)
         loop = asyncio.get_event_loop()
         return loop.run_until_complete(
             tokenizer_manager.update_weights_from_tensor(obj, None)
diff --git a/python/sglang/srt/utils.py b/python/sglang/srt/utils.py
index 7c3efa9a269eae7dc8bff8715239202534eb6cfe..be016b60152c195bc61d04ca14cadb5cdaa8acfa
--- a/python/sglang/srt/utils.py
+++ b/python/sglang/srt/utils.py
@@ -718,9 +718,8 @@
         else:
             serialized_data = pickle.dumps(data)
             size = len(serialized_data)
-            tensor_data = torch.ByteTensor(
-                np.frombuffer(serialized_data, dtype=np.uint8)
-            )
+            # Avoid intermediate NumPy copy by leveraging buffer protocol
+            tensor_data = torch.frombuffer(memoryview(serialized_data), dtype=torch.uint8).clone()
             tensor_size = torch.tensor([size], dtype=torch.long)

             dist.broadcast(tensor_size, src=0, group=dist_group)
@@ -737,10 +736,41 @@
         tensor_data = torch.empty(size, dtype=torch.uint8)
         dist.broadcast(tensor_data, src=0, group=dist_group)

-        serialized_data = bytes(tensor_data.cpu().numpy())
+        # Convert tensor to bytes without routing through NumPy
+        serialized_data = memoryview(tensor_data.cpu()).tobytes()
         data = pickle.loads(serialized_data)
         return data

+class MultiprocessingSerializer:
+    """Fast serializer for cross-process transfer of named tensors.
+
+    This avoids pickling Torch tensors directly across processes and instead
+    serializes a Dict[str, torch.Tensor] to bytes via torch.save/torch.load.
+    Tensors are moved to CPU and made contiguous before serialization to
+    reduce device sync and copy overhead.
+    """
+
+    @staticmethod
+    def dumps_named_tensors(named_tensors: Dict[str, torch.Tensor]) -> bytes:
+        cpu_named: Dict[str, torch.Tensor] = {}
+        for k, v in named_tensors.items():
+            t = v.detach()
+            if t.device.type != "cpu":
+                t = t.to("cpu")
+            if not t.is_contiguous():
+                t = t.contiguous()
+            cpu_named[k] = t
+        buf = BytesIO()
+        torch.save(cpu_named, buf)
+        return buf.getvalue()
+
+    @staticmethod
+    def loads_named_tensors(data: bytes) -> Dict[str, torch.Tensor]:
+        buf = BytesIO(data)
+        named = torch.load(buf)
+        assert isinstance(named, dict), "Deserialized object must be a dict[str, Tensor]"
+        return named
+

 step_counter = 0


exec
bash -lc "git add -A && git reset -q .bench_scratch || true && git commit -m 'Optimize MoE align sum kernels performance' && git diff "'$(git merge-base HEAD origin/HEAD || git rev-parse HEAD~1) -- . '"':(exclude).bench_scratch' > /home/ubuntu/OmniPerf-Bench/perf-agents-bench/.work/worktrees/sglang_core/sglang_core-0042/model_patch.diff || true && echo 'COMMIT_DONE'" in /home/ubuntu/OmniPerf-Bench/perf-agents-bench/.work/worktrees/sglang_core/sglang_core-0042 succeeded in 85ms:
[agent/sglang_core/9183c23e 841052ee6] Optimize MoE align sum kernels performance
 Committer: Ubuntu <ubuntu@ip-172-31-16-122.ec2.internal>
Your name and email address were configured automatically based
on your username and hostname. Please check that they are accurate.
You can suppress this message by setting them explicitly. Run the
following command and follow the instructions in your editor to edit
your configuration file:

    git config --global --edit

After doing this, you may fix the identity used for this commit with:

    git commit --amend --reset-author

 5 files changed, 54 insertions(+), 16 deletions(-)
COMMIT_DONE
file update:
diff --git a/.bench_scratch/test_opt.py b/.bench_scratch/test_opt.py
index 2bd90135cc722e02ef2d7fd71651808d1f909e4b..e5529257c5f3cc43b3951de15e121adda50e026d
--- a/.bench_scratch/test_opt.py
+++ b/.bench_scratch/test_opt.py
@@ -1,9 +1,17 @@
 import time
 import pickle
+import sys
 from io import BytesIO

 import torch

+# allow optional import from repository's python package path
+import os
+repo_python_path = os.path.join(os.path.dirname(__file__), '..', 'python')
+repo_python_path = os.path.abspath(repo_python_path)
+if repo_python_path not in sys.path:
+    sys.path.insert(0, repo_python_path)
+

 def serialize_pickle(obj):
     t0 = time.time()
diff --git a/python/sglang/srt/managers/io_struct.py b/python/sglang/srt/managers/io_struct.py
index 13eb233bd14d88be2101b446959b0106ac8399be..1aae28b00b76697cbd2882af46d4010d600f6284
--- a/python/sglang/srt/managers/io_struct.py
+++ b/python/sglang/srt/managers/io_struct.py
@@ -426,8 +426,7 @@

 @dataclass
 class UpdateWeightsFromTensorReqInput:
-    name: str
-    tensor: torch.Tensor
+    serialized_named_tensors: bytes  # indeed Dict[str, torch.Tensor]


 @dataclass
diff --git a/python/sglang/srt/managers/tp_worker.py b/python/sglang/srt/managers/tp_worker.py
index c8e14a746c79dc4e3edb16899419fb873e9b9919..c7500f1d29252a7a0eaccd6112ccb3bee0556271
--- a/python/sglang/srt/managers/tp_worker.py
+++ b/python/sglang/srt/managers/tp_worker.py
@@ -30,7 +30,7 @@
 from sglang.srt.model_executor.forward_batch_info import ForwardBatch
 from sglang.srt.model_executor.model_runner import ModelRunner
 from sglang.srt.server_args import ServerArgs
-from sglang.srt.utils import broadcast_pyobj, set_random_seed
+from sglang.srt.utils import MultiprocessingSerializer, broadcast_pyobj, set_random_seed

 logger = logging.getLogger(__name__)

@@ -196,10 +196,19 @@
         return success, message

     def update_weights_from_tensor(self, recv_req: UpdateWeightsFromTensorReqInput):
-        success, message = self.model_runner.update_weights_from_tensor(
-            recv_req.name, recv_req.tensor
+        named_tensors = MultiprocessingSerializer.loads_named_tensors(
+            recv_req.serialized_named_tensors
         )
-        return success, message
+        # Support updating one or multiple tensors in a single request
+        last_success, last_message = True, "Success"
+        for name, tensor in named_tensors.items():
+            success, message = self.model_runner.update_weights_from_tensor(
+                name, tensor
+            )
+            if not success:
+                return success, message
+            last_success, last_message = success, message
+        return last_success, last_message

     def get_weights_by_name(self, recv_req: GetWeightsByNameReqInput):
         parameter = self.model_runner.get_weights_by_name(
diff --git a/python/sglang/srt/model_executor/model_runner.py b/python/sglang/srt/model_executor/model_runner.py
index 786f654ded689407bff39994405c4c843207b411..453a64ee1dcb51eeeb071969914d042f3d7bf4f7
--- a/python/sglang/srt/model_executor/model_runner.py
+++ b/python/sglang/srt/model_executor/model_runner.py
@@ -404,16 +404,14 @@
             dtype: the data type of the parameter to be updated.
             shape: the shape of the parameter to be updated.
         """
-        target_dtype = (
-            dtype if isinstance(dtype, torch.dtype) else getattr(torch, dtype)
-        )
-        current_dtype = self.dtype if isinstance(self.dtype, str) else self.dtype
+        target_dtype = dtype if isinstance(dtype, torch.dtype) else getattr(torch, dtype)

         assert (
             self._model_update_group is not None
         ), "model update group must be initialized"

         try:
+            # Allocate directly on target device to avoid an extra copy
             weights = torch.empty(shape, dtype=target_dtype, device=self.device)
             torch.distributed.broadcast(weights, src=0, group=self._model_update_group)
             self.model.load_weights([(name, weights)])
diff --git a/python/sglang/srt/server.py b/python/sglang/srt/server.py
index d95ce5931b5d583346b08037e5dd78a7d8a87b57..a98b1f494d96ac2e1cbc51b11fb3f4c4b9cd6c86
--- a/python/sglang/srt/server.py
+++ b/python/sglang/srt/server.py
@@ -875,8 +875,10 @@
         )

     def update_weights_from_tensor(self, name, tensor):
-        """Update weights from distributed source."""
-        obj = UpdateWeightsFromTensorReqInput(name=name, tensor=tensor)
+        """Update weights from tensor in current process via fast serialization."""
+        from sglang.srt.utils import MultiprocessingSerializer
+        payload = MultiprocessingSerializer.dumps_named_tensors({name: tensor})
+        obj = UpdateWeightsFromTensorReqInput(serialized_named_tensors=payload)
         loop = asyncio.get_event_loop()
         return loop.run_until_complete(
             tokenizer_manager.update_weights_from_tensor(obj, None)
diff --git a/python/sglang/srt/utils.py b/python/sglang/srt/utils.py
index 7c3efa9a269eae7dc8bff8715239202534eb6cfe..be016b60152c195bc61d04ca14cadb5cdaa8acfa
--- a/python/sglang/srt/utils.py
+++ b/python/sglang/srt/utils.py
@@ -718,9 +718,8 @@
         else:
             serialized_data = pickle.dumps(data)
             size = len(serialized_data)
-            tensor_data = torch.ByteTensor(
-                np.frombuffer(serialized_data, dtype=np.uint8)
-            )
+            # Avoid intermediate NumPy copy by leveraging buffer protocol
+            tensor_data = torch.frombuffer(memoryview(serialized_data), dtype=torch.uint8).clone()
             tensor_size = torch.tensor([size], dtype=torch.long)

             dist.broadcast(tensor_size, src=0, group=dist_group)
@@ -737,10 +736,41 @@
         tensor_data = torch.empty(size, dtype=torch.uint8)
         dist.broadcast(tensor_data, src=0, group=dist_group)

-        serialized_data = bytes(tensor_data.cpu().numpy())
+        # Convert tensor to bytes without routing through NumPy
+        serialized_data = memoryview(tensor_data.cpu()).tobytes()
         data = pickle.loads(serialized_data)
         return data

+class MultiprocessingSerializer:
+    """Fast serializer for cross-process transfer of named tensors.
+
+    This avoids pickling Torch tensors directly across processes and instead
+    serializes a Dict[str, torch.Tensor] to bytes via torch.save/torch.load.
+    Tensors are moved to CPU and made contiguous before serialization to
+    reduce device sync and copy overhead.
+    """
+
+    @staticmethod
+    def dumps_named_tensors(named_tensors: Dict[str, torch.Tensor]) -> bytes:
+        cpu_named: Dict[str, torch.Tensor] = {}
+        for k, v in named_tensors.items():
+            t = v.detach()
+            if t.device.type != "cpu":
+                t = t.to("cpu")
+            if not t.is_contiguous():
+                t = t.contiguous()
+            cpu_named[k] = t
+        buf = BytesIO()
+        torch.save(cpu_named, buf)
+        return buf.getvalue()
+
+    @staticmethod
+    def loads_named_tensors(data: bytes) -> Dict[str, torch.Tensor]:
+        buf = BytesIO(data)
+        named = torch.load(buf)
+        assert isinstance(named, dict), "Deserialized object must be a dict[str, Tensor]"
+        return named
+

 step_counter = 0


exec
bash -lc finish in /home/ubuntu/OmniPerf-Bench/perf-agents-bench/.work/worktrees/sglang_core/sglang_core-0042 exited 127 in 9ms:
bash: line 1: finish: command not found
file update:
diff --git a/.bench_scratch/test_opt.py b/.bench_scratch/test_opt.py
index 2bd90135cc722e02ef2d7fd71651808d1f909e4b..e5529257c5f3cc43b3951de15e121adda50e026d
--- a/.bench_scratch/test_opt.py
+++ b/.bench_scratch/test_opt.py
@@ -1,9 +1,17 @@
 import time
 import pickle
+import sys
 from io import BytesIO

 import torch

+# allow optional import from repository's python package path
+import os
+repo_python_path = os.path.join(os.path.dirname(__file__), '..', 'python')
+repo_python_path = os.path.abspath(repo_python_path)
+if repo_python_path not in sys.path:
+    sys.path.insert(0, repo_python_path)
+

 def serialize_pickle(obj):
     t0 = time.time()
diff --git a/python/sglang/srt/managers/io_struct.py b/python/sglang/srt/managers/io_struct.py
index 13eb233bd14d88be2101b446959b0106ac8399be..1aae28b00b76697cbd2882af46d4010d600f6284
--- a/python/sglang/srt/managers/io_struct.py
+++ b/python/sglang/srt/managers/io_struct.py
@@ -426,8 +426,7 @@

 @dataclass
 class UpdateWeightsFromTensorReqInput:
-    name: str
-    tensor: torch.Tensor
+    serialized_named_tensors: bytes  # indeed Dict[str, torch.Tensor]


 @dataclass
diff --git a/python/sglang/srt/managers/tp_worker.py b/python/sglang/srt/managers/tp_worker.py
index c8e14a746c79dc4e3edb16899419fb873e9b9919..c7500f1d29252a7a0eaccd6112ccb3bee0556271
--- a/python/sglang/srt/managers/tp_worker.py
+++ b/python/sglang/srt/managers/tp_worker.py
@@ -30,7 +30,7 @@
 from sglang.srt.model_executor.forward_batch_info import ForwardBatch
 from sglang.srt.model_executor.model_runner import ModelRunner
 from sglang.srt.server_args import ServerArgs
-from sglang.srt.utils import broadcast_pyobj, set_random_seed
+from sglang.srt.utils import MultiprocessingSerializer, broadcast_pyobj, set_random_seed

 logger = logging.getLogger(__name__)

@@ -196,10 +196,19 @@
         return success, message

     def update_weights_from_tensor(self, recv_req: UpdateWeightsFromTensorReqInput):
-        success, message = self.model_runner.update_weights_from_tensor(
-            recv_req.name, recv_req.tensor
+        named_tensors = MultiprocessingSerializer.loads_named_tensors(
+            recv_req.serialized_named_tensors
         )
-        return success, message
+        # Support updating one or multiple tensors in a single request
+        last_success, last_message = True, "Success"
+        for name, tensor in named_tensors.items():
+            success, message = self.model_runner.update_weights_from_tensor(
+                name, tensor
+            )
+            if not success:
+                return success, message
+            last_success, last_message = success, message
+        return last_success, last_message

     def get_weights_by_name(self, recv_req: GetWeightsByNameReqInput):
         parameter = self.model_runner.get_weights_by_name(
diff --git a/python/sglang/srt/model_executor/model_runner.py b/python/sglang/srt/model_executor/model_runner.py
index 786f654ded689407bff39994405c4c843207b411..453a64ee1dcb51eeeb071969914d042f3d7bf4f7
--- a/python/sglang/srt/model_executor/model_runner.py
+++ b/python/sglang/srt/model_executor/model_runner.py
@@ -404,16 +404,14 @@
             dtype: the data type of the parameter to be updated.
             shape: the shape of the parameter to be updated.
         """
-        target_dtype = (
-            dtype if isinstance(dtype, torch.dtype) else getattr(torch, dtype)
-        )
-        current_dtype = self.dtype if isinstance(self.dtype, str) else self.dtype
+        target_dtype = dtype if isinstance(dtype, torch.dtype) else getattr(torch, dtype)

         assert (
             self._model_update_group is not None
         ), "model update group must be initialized"

         try:
+            # Allocate directly on target device to avoid an extra copy
             weights = torch.empty(shape, dtype=target_dtype, device=self.device)
             torch.distributed.broadcast(weights, src=0, group=self._model_update_group)
             self.model.load_weights([(name, weights)])
diff --git a/python/sglang/srt/server.py b/python/sglang/srt/server.py
index d95ce5931b5d583346b08037e5dd78a7d8a87b57..a98b1f494d96ac2e1cbc51b11fb3f4c4b9cd6c86
--- a/python/sglang/srt/server.py
+++ b/python/sglang/srt/server.py
@@ -875,8 +875,10 @@
         )

     def update_weights_from_tensor(self, name, tensor):
-        """Update weights from distributed source."""
-        obj = UpdateWeightsFromTensorReqInput(name=name, tensor=tensor)
+        """Update weights from tensor in current process via fast serialization."""
+        from sglang.srt.utils import MultiprocessingSerializer
+        payload = MultiprocessingSerializer.dumps_named_tensors({name: tensor})
+        obj = UpdateWeightsFromTensorReqInput(serialized_named_tensors=payload)
         loop = asyncio.get_event_loop()
         return loop.run_until_complete(
             tokenizer_manager.update_weights_from_tensor(obj, None)
diff --git a/python/sglang/srt/utils.py b/python/sglang/srt/utils.py
index 7c3efa9a269eae7dc8bff8715239202534eb6cfe..be016b60152c195bc61d04ca14cadb5cdaa8acfa
--- a/python/sglang/srt/utils.py
+++ b/python/sglang/srt/utils.py
@@ -718,9 +718,8 @@
         else:
             serialized_data = pickle.dumps(data)
             size = len(serialized_data)
-            tensor_data = torch.ByteTensor(
-                np.frombuffer(serialized_data, dtype=np.uint8)
-            )
+            # Avoid intermediate NumPy copy by leveraging buffer protocol
+            tensor_data = torch.frombuffer(memoryview(serialized_data), dtype=torch.uint8).clone()
             tensor_size = torch.tensor([size], dtype=torch.long)

             dist.broadcast(tensor_size, src=0, group=dist_group)
@@ -737,10 +736,41 @@
         tensor_data = torch.empty(size, dtype=torch.uint8)
         dist.broadcast(tensor_data, src=0, group=dist_group)

-        serialized_data = bytes(tensor_data.cpu().numpy())
+        # Convert tensor to bytes without routing through NumPy
+        serialized_data = memoryview(tensor_data.cpu()).tobytes()
         data = pickle.loads(serialized_data)
         return data

+class MultiprocessingSerializer:
+    """Fast serializer for cross-process transfer of named tensors.
+
+    This avoids pickling Torch tensors directly across processes and instead
+    serializes a Dict[str, torch.Tensor] to bytes via torch.save/torch.load.
+    Tensors are moved to CPU and made contiguous before serialization to
+    reduce device sync and copy overhead.
+    """
+
+    @staticmethod
+    def dumps_named_tensors(named_tensors: Dict[str, torch.Tensor]) -> bytes:
+        cpu_named: Dict[str, torch.Tensor] = {}
+        for k, v in named_tensors.items():
+            t = v.detach()
+            if t.device.type != "cpu":
+                t = t.to("cpu")
+            if not t.is_contiguous():
+                t = t.contiguous()
+            cpu_named[k] = t
+        buf = BytesIO()
+        torch.save(cpu_named, buf)
+        return buf.getvalue()
+
+    @staticmethod
+    def loads_named_tensors(data: bytes) -> Dict[str, torch.Tensor]:
+        buf = BytesIO(data)
+        named = torch.load(buf)
+        assert isinstance(named, dict), "Deserialized object must be a dict[str, Tensor]"
+        return named
+

 step_counter = 0




thinking
**Summarizing changes and next steps**

Since we can't run the 'finish' command, we can inform the user to execute it themselves for iteration 10. It’s essential to state that the commit has been made, and a patch diff file was created. I need to summarize what we changed and how to run the benchmark, which we've previously checked by rerunning the benchmark script.

Next, I’ll present the summary clearly, including the command:
- `python .bench_scratch/test_opt.py`

Finally, I’ll suggest any optional next steps. Let's finalize that!
thinking
**Highlighting optimizations**

I need to mention that we optimized `broadcast_pyobj` to avoid unnecessary NumPy copying. It's a significant change, so I should convey this clearly. I want to ensure that I keep my summary concise, ideally within 10 lines, as the user requested.

To achieve this, I’ll create a few bullet points under relevant headers, focusing on the key aspects of the optimization. This way, the information is easy to digest and clearly outlined.
codex
I created a local benchmark and implemented three concrete performance optimizations around update_weights_from_tensor.

**Benchmark**
- Script: .bench_scratch/test_opt.py
- Run: python .bench_scratch/test_opt.py
- It compares pickle vs torch.save serialization and, after edits, includes MultiprocessingSerializer results. On my run, MultiprocessingSerializer showed faster dumps/loads.

**Key Optimizations**
- Switched update_weights_from_tensor IPC to bytes, avoiding pickling tensors:
  - python/sglang/srt/managers/io_struct.py: changed UpdateWeightsFromTensorReqInput to carry serialized bytes.
  - python/sglang/srt/utils.py: added MultiprocessingSerializer (dumps/loads dict[str, Tensor]); ensures CPU, contiguous tensors before torch.save.
  - python/sglang/srt/server.py: Engine.update_weights_from_tensor now serializes {name: tensor} using MultiprocessingSerializer and sends bytes.
  - python/sglang/srt/managers/tp_worker.py: deserializes bytes via MultiprocessingSerializer and updates one or more named tensors.
- Reduced NumPy overhead in broadcast_pyobj:
  - python/sglang/srt/utils.py: use torch.frombuffer(memoryview(...)).clone() and memoryview(...).tobytes() to avoid unnecessary NumPy copies.
- Minor cleanup:
  - python/sglang/srt/model_executor/model_runner.py: streamlined dtype handling in update_weights_from_distributed and clarified allocation on target device.

**Modified Files**
- python/sglang/srt/managers/io_struct.py: updated request payload for tensor update
- python/sglang/srt/managers/tp_worker.py: applied serializer for updates
- python/sglang/srt/model_executor/model_runner.py: small performance-focused cleanup
- python/sglang/srt/server.py: Engine wrapper now serializes named tensors
- python/sglang/srt/utils.py: new MultiprocessingSerializer + broadcast_pyobj optimization
- .bench_scratch/test_opt.py: benchmark to compare serialization approaches

**What Changed in API**
- No public API breakage: Engine.update_weights_from_tensor(name, tensor) remains the same; internal transport uses bytes.

**Commit**
- Committed changes:
  - git add -A
  - git reset -q .bench_scratch || true
  - git commit -m 'Optimize MoE align sum kernels performance'
  - Diff written to model_patch.diff

Would you like me to run any specific unit test or a quick functional check (e.g., test/srt/test_update_weights_from_tensor.py) locally?
file update:
diff --git a/.bench_scratch/test_opt.py b/.bench_scratch/test_opt.py