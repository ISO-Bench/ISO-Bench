{"instance_id": "/home/ubuntu/OmniPerf-Bench/sglan__repo-0f75b90", "model_patch": "diff --git a/python/sglang/srt/layers/attention/flashattention_backend.py b/python/sglang/srt/layers/attention/flashattention_backend.py\nindex f200a367b..3afd1f1b1 100644\n--- a/python/sglang/srt/layers/attention/flashattention_backend.py\n+++ b/python/sglang/srt/layers/attention/flashattention_backend.py\n@@ -1130,21 +1130,24 @@ class FlashAttentionBackend(AttentionBackend):\n         to avoid memory allocations.\n         \"\"\"\n         # This is being used by normal decode and draft decode when topk == 1\n+        # Prefer torch.empty for buffers that will be fully written later.\n+        cu_seqlens_k_buf = torch.empty(\n+            max_bs + 1, dtype=torch.int32, device=self.device\n+        )\n+        cu_seqlens_k_buf[0] = 0  # cumsum pad leading zero\n         self.decode_cuda_graph_metadata = {\n-            \"cache_seqlens\": torch.zeros(max_bs, dtype=torch.int32, device=self.device),\n+            \"cache_seqlens\": torch.empty(max_bs, dtype=torch.int32, device=self.device),\n             \"cu_seqlens_q\": torch.arange(\n                 0, max_bs + 1, dtype=torch.int32, device=self.device\n             ),\n-            \"cu_seqlens_k\": torch.zeros(\n-                max_bs + 1, dtype=torch.int32, device=self.device\n-            ),\n-            \"page_table\": torch.zeros(\n+            \"cu_seqlens_k\": cu_seqlens_k_buf,\n+            \"page_table\": torch.empty(\n                 max_bs,\n                 (self.max_context_len + self.page_size - 1) // self.page_size,\n                 dtype=torch.int32,\n                 device=self.device,\n             ),\n-            \"page_table_draft_decode\": torch.zeros(\n+            \"page_table_draft_decode\": torch.empty(\n                 max_bs,\n                 (self.max_context_len + self.page_size - 1) // self.page_size,\n                 dtype=torch.int32,\n@@ -1165,19 +1168,20 @@ class FlashAttentionBackend(AttentionBackend):\n             max_virtual_batches = max_bs * (\n                 (max_seq_len + attn_chunk_size - 1) // attn_chunk_size\n             )\n-            max_blocks_per_seq = (max_seq_len + attn_chunk_size - 1) // attn_chunk_size\n             max_pages_per_block = (attn_chunk_size + page_size - 1) // page_size\n \n             self.decode_cuda_graph_local_attn_metadata = {\n-                \"local_query_start_loc\": torch.zeros(\n+                # These buffers are populated before replay; avoid zero-fill overhead.\n+                \"local_query_start_loc\": torch.empty(\n                     max_virtual_batches + 1, dtype=torch.int32, device=self.device\n                 ),\n-                \"local_seqused_k\": torch.zeros(\n+                \"local_seqused_k\": torch.empty(\n                     max_virtual_batches, dtype=torch.int32, device=self.device\n                 ),\n-                \"local_block_table\": torch.zeros(\n+                # Only pages per local block are needed per virtual batch.\n+                \"local_block_table\": torch.empty(\n                     max_virtual_batches,\n-                    max_blocks_per_seq * max_pages_per_block,\n+                    max_pages_per_block,\n                     dtype=torch.int32,\n                     device=self.device,\n                 ),\n@@ -1185,8 +1189,12 @@ class FlashAttentionBackend(AttentionBackend):\n \n         # This is used by draft decode's first half of metadata when topk > 1\n         if self.topk > 1:\n+            cu_seqlens_k_topk = torch.empty(\n+                max_bs + 1, dtype=torch.int32, device=self.device\n+            )\n+            cu_seqlens_k_topk[0] = 0\n             self.draft_decode_metadata_topk_normal = {\n-                \"cache_seqlens\": torch.zeros(\n+                \"cache_seqlens\": torch.empty(\n                     max_bs, dtype=torch.int32, device=self.device\n                 ),\n                 \"cu_seqlens_q\": torch.arange(\n@@ -1196,10 +1204,8 @@ class FlashAttentionBackend(AttentionBackend):\n                     dtype=torch.int32,\n                     device=self.device,\n                 ),\n-                \"cu_seqlens_k\": torch.zeros(\n-                    max_bs + 1, dtype=torch.int32, device=self.device\n-                ),\n-                \"page_table\": torch.zeros(\n+                \"cu_seqlens_k\": cu_seqlens_k_topk,\n+                \"page_table\": torch.empty(\n                     max_bs,\n                     self.max_context_len,\n                     dtype=torch.int32,\n@@ -1242,7 +1248,7 @@ class FlashAttentionBackend(AttentionBackend):\n             and self.speculative_num_draft_tokens > 0\n         ):\n             self.target_verify_metadata = {\n-                \"cache_seqlens\": torch.zeros(\n+                \"cache_seqlens\": torch.empty(\n                     max_bs, dtype=torch.int32, device=self.device\n                 ),\n                 \"cu_seqlens_q\": torch.arange(\n@@ -1252,10 +1258,12 @@ class FlashAttentionBackend(AttentionBackend):\n                     dtype=torch.int32,\n                     device=self.device,\n                 ),\n-                \"cu_seqlens_k\": torch.zeros(\n-                    max_bs + 1, dtype=torch.int32, device=self.device\n-                ),\n-                \"page_table\": torch.zeros(\n+                \"cu_seqlens_k\": (lambda n=max_bs + 1: (lambda t: t)(\n+                    (lambda t: (t.__setitem__(0, 0) or t))(\n+                        torch.empty(n, dtype=torch.int32, device=self.device)\n+                    )\n+                ))(),\n+                \"page_table\": torch.empty(\n                     max_bs,\n                     (self.max_context_len + self.page_size - 1) // self.page_size,\n                     dtype=torch.int32,\n@@ -1267,8 +1275,12 @@ class FlashAttentionBackend(AttentionBackend):\n             }\n \n         if self.topk > 1:\n+            cu_seqlens_k_tv = torch.empty(\n+                max_bs + 1, dtype=torch.int32, device=self.device\n+            )\n+            cu_seqlens_k_tv[0] = 0\n             self.target_verify_metadata_topk_normal = {\n-                \"cache_seqlens\": torch.zeros(\n+                \"cache_seqlens\": torch.empty(\n                     max_bs, dtype=torch.int32, device=self.device\n                 ),\n                 \"cu_seqlens_q\": torch.arange(\n@@ -1278,10 +1290,8 @@ class FlashAttentionBackend(AttentionBackend):\n                     dtype=torch.int32,\n                     device=self.device,\n                 ),\n-                \"cu_seqlens_k\": torch.zeros(\n-                    max_bs + 1, dtype=torch.int32, device=self.device\n-                ),\n-                \"page_table\": torch.zeros(\n+                \"cu_seqlens_k\": cu_seqlens_k_tv,\n+                \"page_table\": torch.empty(\n                     max_bs,\n                     self.max_context_len,\n                     dtype=torch.int32,\n@@ -1290,23 +1300,23 @@ class FlashAttentionBackend(AttentionBackend):\n             }\n \n             self.target_verify_metadata_topk_expand = {\n-                \"cache_seqlens\": torch.zeros(\n+                \"cache_seqlens\": torch.empty(\n                     max_bs * self.speculative_num_draft_tokens,\n                     dtype=torch.int32,\n                     device=self.device,\n                 ),\n-                \"cu_seqlens_k\": torch.zeros(\n-                    max_bs * self.speculative_num_draft_tokens + 1,\n-                    dtype=torch.int32,\n-                    device=self.device,\n-                ),\n+                \"cu_seqlens_k\": (lambda n=max_bs * self.speculative_num_draft_tokens + 1: (lambda t: t)(\n+                    (lambda t: (t.__setitem__(0, 0) or t))(\n+                        torch.empty(n, dtype=torch.int32, device=self.device)\n+                    )\n+                ))(),\n                 \"cu_seqlens_q\": torch.arange(\n                     0,\n                     max_bs * self.speculative_num_draft_tokens + 1,\n                     dtype=torch.int32,\n                     device=self.device,\n                 ),\n-                \"page_table\": torch.zeros(\n+                \"page_table\": torch.empty(\n                     max_bs * self.speculative_num_draft_tokens,\n                     self.speculative_num_draft_tokens,\n                     dtype=torch.int32,\n@@ -1315,18 +1325,20 @@ class FlashAttentionBackend(AttentionBackend):\n             }\n \n         self.encoder_metadata = {\n-            \"encoder_page_table\": torch.zeros(\n+            \"encoder_page_table\": torch.empty(\n                 max_bs,\n                 self.max_context_len,\n                 dtype=torch.int32,\n                 device=self.device,\n             ),\n-            \"encoder_lens_int32\": torch.zeros(\n+            \"encoder_lens_int32\": torch.empty(\n                 max_bs, dtype=torch.int32, device=self.device\n             ),\n-            \"encoder_cu_seqlens_k\": torch.zeros(\n-                max_bs + 1, dtype=torch.int32, device=self.device\n-            ),\n+            \"encoder_cu_seqlens_k\": (lambda n=max_bs + 1: (lambda t: t)(\n+                (lambda t: (t.__setitem__(0, 0) or t))(\n+                    torch.empty(n, dtype=torch.int32, device=self.device)\n+                )\n+            ))(),\n         }\n \n     def init_forward_metadata_capture_cuda_graph(\n@@ -1867,14 +1879,10 @@ class FlashAttentionBackend(AttentionBackend):\n         k_len = seqlens_k_local.shape[0]\n         b0, b1 = block_table_local.shape\n \n-        # In-place updates into preallocated tensors and zero out the unused space\n+        # In-place updates into preallocated tensors; only the valid ranges are read by kernels.\n         local_q_buf[:q_len].copy_(cu_seqlens_q_local)\n-        local_q_buf[q_len:].fill_(0)\n         local_k_buf[:k_len].copy_(seqlens_k_local)\n-        local_k_buf[k_len:].fill_(0)\n         local_block_buf[:b0, :b1].copy_(block_table_local)\n-        local_block_buf[b0:, :].fill_(0)\n-        local_block_buf[:b0, b1:].fill_(0)\n \n         if metadata.local_attn_metadata is not None:\n             lam = metadata.local_attn_metadata\n", "model_name_or_path": "gpt-5-2025-08-07"}
