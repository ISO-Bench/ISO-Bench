{"instance_id": "/home/ubuntu/OmniPerf-Bench/sglan__repo-d5c097a", "model_patch": "diff --git a/python/sglang/srt/layers/moe/fused_moe_triton/configs/README b/python/sglang/srt/layers/moe/fused_moe_triton/configs/README\nindex 4aa527f27..d5260da32 100644\n--- a/python/sglang/srt/layers/moe/fused_moe_triton/configs/README\n+++ b/python/sglang/srt/layers/moe/fused_moe_triton/configs/README\n@@ -3,6 +3,8 @@ For different settings of\n - E (number of experts)\n - N (intermediate size)\n - device_name (torch.cuda.get_device_name())\n+- dtype: The data type used by the fused MoE kernel for computation. Supported types include fp8_w8a8, int8_w8a8, int8_w8a16, int4_w4a16, etc. This determines the precision and quantization scheme for both weights and activations.\n+- block_shape: The block quantization shape introduced starting from DeepSeek V3/R1 models. This parameter defines the granularity for block-wise quantization, typically specified as [block_n, block_k] where block_n and block_k represent the block dimensions. For example, DeepSeek V3 commonly uses [128, 128] block shapes for efficient block-wise FP8 quantization.\n the JSON file contains a mapping from M (batch size) to the chosen configuration.\n \n The example configurations provided are for the Mixtral model for TP2 on H100\ndiff --git a/python/sglang/srt/layers/moe/fused_moe_triton/configs/triton_3_1_0/E=1,N=14336,device_name=NVIDIA_A100-SXM4-80GB,dtype=int8_w8a16.json b/python/sglang/srt/layers/moe/fused_moe_triton/configs/triton_3_1_0/E=1,N=14336,device_name=NVIDIA_A100-SXM4-80GB,dtype=int8_w8a16.json\nnew file mode 100644\nindex 000000000..2dd17d929\n--- /dev/null\n+++ b/python/sglang/srt/layers/moe/fused_moe_triton/configs/triton_3_1_0/E=1,N=14336,device_name=NVIDIA_A100-SXM4-80GB,dtype=int8_w8a16.json\n@@ -0,0 +1,147 @@\n+{\n+  \"1\": {\n+    \"BLOCK_SIZE_M\": 16,\n+    \"BLOCK_SIZE_N\": 32,\n+    \"BLOCK_SIZE_K\": 128,\n+    \"GROUP_SIZE_M\": 1,\n+    \"num_warps\": 4,\n+    \"num_stages\": 4\n+  },\n+  \"2\": {\n+    \"BLOCK_SIZE_M\": 16,\n+    \"BLOCK_SIZE_N\": 32,\n+    \"BLOCK_SIZE_K\": 128,\n+    \"GROUP_SIZE_M\": 64,\n+    \"num_warps\": 4,\n+    \"num_stages\": 3\n+  },\n+  \"4\": {\n+    \"BLOCK_SIZE_M\": 16,\n+    \"BLOCK_SIZE_N\": 32,\n+    \"BLOCK_SIZE_K\": 128,\n+    \"GROUP_SIZE_M\": 1,\n+    \"num_warps\": 4,\n+    \"num_stages\": 4\n+  },\n+  \"8\": {\n+    \"BLOCK_SIZE_M\": 16,\n+    \"BLOCK_SIZE_N\": 32,\n+    \"BLOCK_SIZE_K\": 256,\n+    \"GROUP_SIZE_M\": 32,\n+    \"num_warps\": 4,\n+    \"num_stages\": 3\n+  },\n+  \"16\": {\n+    \"BLOCK_SIZE_M\": 16,\n+    \"BLOCK_SIZE_N\": 32,\n+    \"BLOCK_SIZE_K\": 128,\n+    \"GROUP_SIZE_M\": 1,\n+    \"num_warps\": 4,\n+    \"num_stages\": 4\n+  },\n+  \"24\": {\n+    \"BLOCK_SIZE_M\": 16,\n+    \"BLOCK_SIZE_N\": 64,\n+    \"BLOCK_SIZE_K\": 64,\n+    \"GROUP_SIZE_M\": 16,\n+    \"num_warps\": 4,\n+    \"num_stages\": 5\n+  },\n+  \"32\": {\n+    \"BLOCK_SIZE_M\": 16,\n+    \"BLOCK_SIZE_N\": 32,\n+    \"BLOCK_SIZE_K\": 256,\n+    \"GROUP_SIZE_M\": 1,\n+    \"num_warps\": 4,\n+    \"num_stages\": 2\n+  },\n+  \"48\": {\n+    \"BLOCK_SIZE_M\": 64,\n+    \"BLOCK_SIZE_N\": 64,\n+    \"BLOCK_SIZE_K\": 64,\n+    \"GROUP_SIZE_M\": 1,\n+    \"num_warps\": 4,\n+    \"num_stages\": 3\n+  },\n+  \"64\": {\n+    \"BLOCK_SIZE_M\": 64,\n+    \"BLOCK_SIZE_N\": 64,\n+    \"BLOCK_SIZE_K\": 64,\n+    \"GROUP_SIZE_M\": 1,\n+    \"num_warps\": 4,\n+    \"num_stages\": 3\n+  },\n+  \"96\": {\n+    \"BLOCK_SIZE_M\": 32,\n+    \"BLOCK_SIZE_N\": 128,\n+    \"BLOCK_SIZE_K\": 128,\n+    \"GROUP_SIZE_M\": 1,\n+    \"num_warps\": 4,\n+    \"num_stages\": 3\n+  },\n+  \"128\": {\n+    \"BLOCK_SIZE_M\": 64,\n+    \"BLOCK_SIZE_N\": 64,\n+    \"BLOCK_SIZE_K\": 64,\n+    \"GROUP_SIZE_M\": 16,\n+    \"num_warps\": 4,\n+    \"num_stages\": 3\n+  },\n+  \"256\": {\n+    \"BLOCK_SIZE_M\": 64,\n+    \"BLOCK_SIZE_N\": 64,\n+    \"BLOCK_SIZE_K\": 64,\n+    \"GROUP_SIZE_M\": 32,\n+    \"num_warps\": 4,\n+    \"num_stages\": 4\n+  },\n+  \"512\": {\n+    \"BLOCK_SIZE_M\": 64,\n+    \"BLOCK_SIZE_N\": 256,\n+    \"BLOCK_SIZE_K\": 64,\n+    \"GROUP_SIZE_M\": 32,\n+    \"num_warps\": 4,\n+    \"num_stages\": 4\n+  },\n+  \"1024\": {\n+    \"BLOCK_SIZE_M\": 64,\n+    \"BLOCK_SIZE_N\": 256,\n+    \"BLOCK_SIZE_K\": 64,\n+    \"GROUP_SIZE_M\": 64,\n+    \"num_warps\": 4,\n+    \"num_stages\": 4\n+  },\n+  \"1536\": {\n+    \"BLOCK_SIZE_M\": 64,\n+    \"BLOCK_SIZE_N\": 256,\n+    \"BLOCK_SIZE_K\": 64,\n+    \"GROUP_SIZE_M\": 64,\n+    \"num_warps\": 4,\n+    \"num_stages\": 4\n+  },\n+  \"2048\": {\n+    \"BLOCK_SIZE_M\": 64,\n+    \"BLOCK_SIZE_N\": 256,\n+    \"BLOCK_SIZE_K\": 64,\n+    \"GROUP_SIZE_M\": 32,\n+    \"num_warps\": 4,\n+    \"num_stages\": 4\n+  },\n+  \"3072\": {\n+    \"BLOCK_SIZE_M\": 64,\n+    \"BLOCK_SIZE_N\": 256,\n+    \"BLOCK_SIZE_K\": 64,\n+    \"GROUP_SIZE_M\": 32,\n+    \"num_warps\": 4,\n+    \"num_stages\": 4\n+  },\n+  \"4096\": {\n+    \"BLOCK_SIZE_M\": 64,\n+    \"BLOCK_SIZE_N\": 256,\n+    \"BLOCK_SIZE_K\": 64,\n+    \"GROUP_SIZE_M\": 16,\n+    \"num_warps\": 4,\n+    \"num_stages\": 4\n+  }\n+}\n+\ndiff --git a/python/sglang/srt/layers/moe/fused_moe_triton/configs/triton_3_1_0/E=1,N=14336,device_name=NVIDIA_A100-SXM4-80GB.json b/python/sglang/srt/layers/moe/fused_moe_triton/configs/triton_3_1_0/E=1,N=14336,device_name=NVIDIA_A100-SXM4-80GB.json\nnew file mode 100644\nindex 000000000..0a685439b\n--- /dev/null\n+++ b/python/sglang/srt/layers/moe/fused_moe_triton/configs/triton_3_1_0/E=1,N=14336,device_name=NVIDIA_A100-SXM4-80GB.json\n@@ -0,0 +1,147 @@\n+{\n+  \"1\": {\n+    \"BLOCK_SIZE_M\": 16,\n+    \"BLOCK_SIZE_N\": 64,\n+    \"BLOCK_SIZE_K\": 256,\n+    \"GROUP_SIZE_M\": 16,\n+    \"num_warps\": 4,\n+    \"num_stages\": 4\n+  },\n+  \"2\": {\n+    \"BLOCK_SIZE_M\": 16,\n+    \"BLOCK_SIZE_N\": 64,\n+    \"BLOCK_SIZE_K\": 256,\n+    \"GROUP_SIZE_M\": 32,\n+    \"num_warps\": 4,\n+    \"num_stages\": 4\n+  },\n+  \"4\": {\n+    \"BLOCK_SIZE_M\": 16,\n+    \"BLOCK_SIZE_N\": 64,\n+    \"BLOCK_SIZE_K\": 256,\n+    \"GROUP_SIZE_M\": 16,\n+    \"num_warps\": 4,\n+    \"num_stages\": 4\n+  },\n+  \"8\": {\n+    \"BLOCK_SIZE_M\": 16,\n+    \"BLOCK_SIZE_N\": 64,\n+    \"BLOCK_SIZE_K\": 256,\n+    \"GROUP_SIZE_M\": 32,\n+    \"num_warps\": 4,\n+    \"num_stages\": 4\n+  },\n+  \"16\": {\n+    \"BLOCK_SIZE_M\": 16,\n+    \"BLOCK_SIZE_N\": 64,\n+    \"BLOCK_SIZE_K\": 256,\n+    \"GROUP_SIZE_M\": 64,\n+    \"num_warps\": 4,\n+    \"num_stages\": 4\n+  },\n+  \"24\": {\n+    \"BLOCK_SIZE_M\": 32,\n+    \"BLOCK_SIZE_N\": 64,\n+    \"BLOCK_SIZE_K\": 128,\n+    \"GROUP_SIZE_M\": 16,\n+    \"num_warps\": 4,\n+    \"num_stages\": 5\n+  },\n+  \"32\": {\n+    \"BLOCK_SIZE_M\": 32,\n+    \"BLOCK_SIZE_N\": 64,\n+    \"BLOCK_SIZE_K\": 128,\n+    \"GROUP_SIZE_M\": 1,\n+    \"num_warps\": 4,\n+    \"num_stages\": 5\n+  },\n+  \"48\": {\n+    \"BLOCK_SIZE_M\": 32,\n+    \"BLOCK_SIZE_N\": 64,\n+    \"BLOCK_SIZE_K\": 64,\n+    \"GROUP_SIZE_M\": 16,\n+    \"num_warps\": 4,\n+    \"num_stages\": 5\n+  },\n+  \"64\": {\n+    \"BLOCK_SIZE_M\": 64,\n+    \"BLOCK_SIZE_N\": 64,\n+    \"BLOCK_SIZE_K\": 128,\n+    \"GROUP_SIZE_M\": 1,\n+    \"num_warps\": 4,\n+    \"num_stages\": 4\n+  },\n+  \"96\": {\n+    \"BLOCK_SIZE_M\": 32,\n+    \"BLOCK_SIZE_N\": 256,\n+    \"BLOCK_SIZE_K\": 64,\n+    \"GROUP_SIZE_M\": 16,\n+    \"num_warps\": 4,\n+    \"num_stages\": 5\n+  },\n+  \"128\": {\n+    \"BLOCK_SIZE_M\": 64,\n+    \"BLOCK_SIZE_N\": 128,\n+    \"BLOCK_SIZE_K\": 64,\n+    \"GROUP_SIZE_M\": 64,\n+    \"num_warps\": 4,\n+    \"num_stages\": 4\n+  },\n+  \"256\": {\n+    \"BLOCK_SIZE_M\": 128,\n+    \"BLOCK_SIZE_N\": 128,\n+    \"BLOCK_SIZE_K\": 64,\n+    \"GROUP_SIZE_M\": 16,\n+    \"num_warps\": 8,\n+    \"num_stages\": 4\n+  },\n+  \"512\": {\n+    \"BLOCK_SIZE_M\": 128,\n+    \"BLOCK_SIZE_N\": 128,\n+    \"BLOCK_SIZE_K\": 64,\n+    \"GROUP_SIZE_M\": 64,\n+    \"num_warps\": 4,\n+    \"num_stages\": 3\n+  },\n+  \"1024\": {\n+    \"BLOCK_SIZE_M\": 128,\n+    \"BLOCK_SIZE_N\": 128,\n+    \"BLOCK_SIZE_K\": 64,\n+    \"GROUP_SIZE_M\": 64,\n+    \"num_warps\": 4,\n+    \"num_stages\": 3\n+  },\n+  \"1536\": {\n+    \"BLOCK_SIZE_M\": 128,\n+    \"BLOCK_SIZE_N\": 128,\n+    \"BLOCK_SIZE_K\": 64,\n+    \"GROUP_SIZE_M\": 16,\n+    \"num_warps\": 4,\n+    \"num_stages\": 3\n+  },\n+  \"2048\": {\n+    \"BLOCK_SIZE_M\": 128,\n+    \"BLOCK_SIZE_N\": 128,\n+    \"BLOCK_SIZE_K\": 64,\n+    \"GROUP_SIZE_M\": 32,\n+    \"num_warps\": 4,\n+    \"num_stages\": 3\n+  },\n+  \"3072\": {\n+    \"BLOCK_SIZE_M\": 128,\n+    \"BLOCK_SIZE_N\": 128,\n+    \"BLOCK_SIZE_K\": 64,\n+    \"GROUP_SIZE_M\": 16,\n+    \"num_warps\": 4,\n+    \"num_stages\": 3\n+  },\n+  \"4096\": {\n+    \"BLOCK_SIZE_M\": 128,\n+    \"BLOCK_SIZE_N\": 128,\n+    \"BLOCK_SIZE_K\": 64,\n+    \"GROUP_SIZE_M\": 16,\n+    \"num_warps\": 4,\n+    \"num_stages\": 3\n+  }\n+}\n+\ndiff --git a/python/sglang/srt/layers/moe/fused_moe_triton/configs/triton_3_1_0/E=1,N=1792,device_name=NVIDIA_A100-SXM4-80GB,dtype=int8_w8a16.json b/python/sglang/srt/layers/moe/fused_moe_triton/configs/triton_3_1_0/E=1,N=1792,device_name=NVIDIA_A100-SXM4-80GB,dtype=int8_w8a16.json\nnew file mode 100644\nindex 000000000..5d59ed250\n--- /dev/null\n+++ b/python/sglang/srt/layers/moe/fused_moe_triton/configs/triton_3_1_0/E=1,N=1792,device_name=NVIDIA_A100-SXM4-80GB,dtype=int8_w8a16.json\n@@ -0,0 +1,203 @@\n+{\n+  \"1\": {\n+    \"BLOCK_SIZE_M\": 16,\n+    \"BLOCK_SIZE_N\": 32,\n+    \"BLOCK_SIZE_K\": 256,\n+    \"GROUP_SIZE_M\": 32,\n+    \"num_warps\": 4,\n+    \"num_stages\": 3\n+  },\n+  \"2\": {\n+    \"BLOCK_SIZE_M\": 16,\n+    \"BLOCK_SIZE_N\": 32,\n+    \"BLOCK_SIZE_K\": 256,\n+    \"GROUP_SIZE_M\": 16,\n+    \"num_warps\": 4,\n+    \"num_stages\": 3\n+  },\n+  \"4\": {\n+    \"BLOCK_SIZE_M\": 16,\n+    \"BLOCK_SIZE_N\": 32,\n+    \"BLOCK_SIZE_K\": 256,\n+    \"GROUP_SIZE_M\": 32,\n+    \"num_warps\": 4,\n+    \"num_stages\": 3\n+  },\n+  \"8\": {\n+    \"BLOCK_SIZE_M\": 16,\n+    \"BLOCK_SIZE_N\": 32,\n+    \"BLOCK_SIZE_K\": 256,\n+    \"GROUP_SIZE_M\": 64,\n+    \"num_warps\": 4,\n+    \"num_stages\": 3\n+  },\n+  \"16\": {\n+    \"BLOCK_SIZE_M\": 16,\n+    \"BLOCK_SIZE_N\": 32,\n+    \"BLOCK_SIZE_K\": 256,\n+    \"GROUP_SIZE_M\": 32,\n+    \"num_warps\": 4,\n+    \"num_stages\": 3\n+  },\n+  \"24\": {\n+    \"BLOCK_SIZE_M\": 16,\n+    \"BLOCK_SIZE_N\": 32,\n+    \"BLOCK_SIZE_K\": 256,\n+    \"GROUP_SIZE_M\": 1,\n+    \"num_warps\": 4,\n+    \"num_stages\": 3\n+  },\n+  \"32\": {\n+    \"BLOCK_SIZE_M\": 16,\n+    \"BLOCK_SIZE_N\": 32,\n+    \"BLOCK_SIZE_K\": 256,\n+    \"GROUP_SIZE_M\": 1,\n+    \"num_warps\": 4,\n+    \"num_stages\": 3\n+  },\n+  \"48\": {\n+    \"BLOCK_SIZE_M\": 16,\n+    \"BLOCK_SIZE_N\": 128,\n+    \"BLOCK_SIZE_K\": 128,\n+    \"GROUP_SIZE_M\": 1,\n+    \"num_warps\": 8,\n+    \"num_stages\": 3\n+  },\n+  \"64\": {\n+    \"BLOCK_SIZE_M\": 64,\n+    \"BLOCK_SIZE_N\": 64,\n+    \"BLOCK_SIZE_K\": 64,\n+    \"GROUP_SIZE_M\": 1,\n+    \"num_warps\": 4,\n+    \"num_stages\": 3\n+  },\n+  \"96\": {\n+    \"BLOCK_SIZE_M\": 32,\n+    \"BLOCK_SIZE_N\": 128,\n+    \"BLOCK_SIZE_K\": 128,\n+    \"GROUP_SIZE_M\": 1,\n+    \"num_warps\": 4,\n+    \"num_stages\": 3\n+  },\n+  \"128\": {\n+    \"BLOCK_SIZE_M\": 64,\n+    \"BLOCK_SIZE_N\": 128,\n+    \"BLOCK_SIZE_K\": 128,\n+    \"GROUP_SIZE_M\": 1,\n+    \"num_warps\": 8,\n+    \"num_stages\": 3\n+  },\n+  \"256\": {\n+    \"BLOCK_SIZE_M\": 64,\n+    \"BLOCK_SIZE_N\": 64,\n+    \"BLOCK_SIZE_K\": 64,\n+    \"GROUP_SIZE_M\": 64,\n+    \"num_warps\": 4,\n+    \"num_stages\": 3\n+  },\n+  \"512\": {\n+    \"BLOCK_SIZE_M\": 64,\n+    \"BLOCK_SIZE_N\": 64,\n+    \"BLOCK_SIZE_K\": 64,\n+    \"GROUP_SIZE_M\": 1,\n+    \"num_warps\": 4,\n+    \"num_stages\": 4\n+  },\n+  \"1024\": {\n+    \"BLOCK_SIZE_M\": 64,\n+    \"BLOCK_SIZE_N\": 64,\n+    \"BLOCK_SIZE_K\": 64,\n+    \"GROUP_SIZE_M\": 1,\n+    \"num_warps\": 4,\n+    \"num_stages\": 3\n+  },\n+  \"1536\": {\n+    \"BLOCK_SIZE_M\": 64,\n+    \"BLOCK_SIZE_N\": 256,\n+    \"BLOCK_SIZE_K\": 64,\n+    \"GROUP_SIZE_M\": 1,\n+    \"num_warps\": 4,\n+    \"num_stages\": 4\n+  },\n+  \"2048\": {\n+    \"BLOCK_SIZE_M\": 64,\n+    \"BLOCK_SIZE_N\": 256,\n+    \"BLOCK_SIZE_K\": 64,\n+    \"GROUP_SIZE_M\": 32,\n+    \"num_warps\": 4,\n+    \"num_stages\": 4\n+  },\n+  \"3072\": {\n+    \"BLOCK_SIZE_M\": 64,\n+    \"BLOCK_SIZE_N\": 256,\n+    \"BLOCK_SIZE_K\": 64,\n+    \"GROUP_SIZE_M\": 64,\n+    \"num_warps\": 4,\n+    \"num_stages\": 4\n+  },\n+  \"4096\": {\n+    \"BLOCK_SIZE_M\": 64,\n+    \"BLOCK_SIZE_N\": 256,\n+    \"BLOCK_SIZE_K\": 64,\n+    \"GROUP_SIZE_M\": 64,\n+    \"num_warps\": 4,\n+    \"num_stages\": 4\n+  },\n+  \"5120\": {\n+    \"BLOCK_SIZE_M\": 64,\n+    \"BLOCK_SIZE_N\": 256,\n+    \"BLOCK_SIZE_K\": 64,\n+    \"GROUP_SIZE_M\": 16,\n+    \"num_warps\": 4,\n+    \"num_stages\": 4\n+  },\n+  \"9216\": {\n+    \"BLOCK_SIZE_M\": 64,\n+    \"BLOCK_SIZE_N\": 256,\n+    \"BLOCK_SIZE_K\": 64,\n+    \"GROUP_SIZE_M\": 32,\n+    \"num_warps\": 4,\n+    \"num_stages\": 4\n+  },\n+  \"13312\": {\n+    \"BLOCK_SIZE_M\": 64,\n+    \"BLOCK_SIZE_N\": 256,\n+    \"BLOCK_SIZE_K\": 64,\n+    \"GROUP_SIZE_M\": 16,\n+    \"num_warps\": 4,\n+    \"num_stages\": 4\n+  },\n+  \"17408\": {\n+    \"BLOCK_SIZE_M\": 64,\n+    \"BLOCK_SIZE_N\": 256,\n+    \"BLOCK_SIZE_K\": 64,\n+    \"GROUP_SIZE_M\": 32,\n+    \"num_warps\": 4,\n+    \"num_stages\": 4\n+  },\n+  \"25600\": {\n+    \"BLOCK_SIZE_M\": 64,\n+    \"BLOCK_SIZE_N\": 256,\n+    \"BLOCK_SIZE_K\": 64,\n+    \"GROUP_SIZE_M\": 16,\n+    \"num_warps\": 4,\n+    \"num_stages\": 4\n+  },\n+  \"33792\": {\n+    \"BLOCK_SIZE_M\": 64,\n+    \"BLOCK_SIZE_N\": 256,\n+    \"BLOCK_SIZE_K\": 64,\n+    \"GROUP_SIZE_M\": 16,\n+    \"num_warps\": 4,\n+    \"num_stages\": 4\n+  },\n+  \"41984\": {\n+    \"BLOCK_SIZE_M\": 64,\n+    \"BLOCK_SIZE_N\": 256,\n+    \"BLOCK_SIZE_K\": 64,\n+    \"GROUP_SIZE_M\": 16,\n+    \"num_warps\": 4,\n+    \"num_stages\": 4\n+  }\n+}\n+\ndiff --git a/python/sglang/srt/layers/moe/fused_moe_triton/configs/triton_3_1_0/E=1,N=1792,device_name=NVIDIA_A100-SXM4-80GB.json b/python/sglang/srt/layers/moe/fused_moe_triton/configs/triton_3_1_0/E=1,N=1792,device_name=NVIDIA_A100-SXM4-80GB.json\nnew file mode 100644\nindex 000000000..60ae89b03\n--- /dev/null\n+++ b/python/sglang/srt/layers/moe/fused_moe_triton/configs/triton_3_1_0/E=1,N=1792,device_name=NVIDIA_A100-SXM4-80GB.json\n@@ -0,0 +1,163 @@\n+{\n+  \"1\": {\n+    \"BLOCK_SIZE_M\": 16,\n+    \"BLOCK_SIZE_N\": 64,\n+    \"BLOCK_SIZE_K\": 128,\n+    \"GROUP_SIZE_M\": 16,\n+    \"num_warps\": 4,\n+    \"num_stages\": 5\n+  },\n+  \"2\": {\n+    \"BLOCK_SIZE_M\": 16,\n+    \"BLOCK_SIZE_N\": 64,\n+    \"BLOCK_SIZE_K\": 128,\n+    \"GROUP_SIZE_M\": 1,\n+    \"num_warps\": 4,\n+    \"num_stages\": 5\n+  },\n+  \"4\": {\n+    \"BLOCK_SIZE_M\": 16,\n+    \"BLOCK_SIZE_N\": 64,\n+    \"BLOCK_SIZE_K\": 128,\n+    \"GROUP_SIZE_M\": 1,\n+    \"num_warps\": 4,\n+    \"num_stages\": 5\n+  },\n+  \"8\": {\n+    \"BLOCK_SIZE_M\": 16,\n+    \"BLOCK_SIZE_N\": 64,\n+    \"BLOCK_SIZE_K\": 128,\n+    \"GROUP_SIZE_M\": 1,\n+    \"num_warps\": 4,\n+    \"num_stages\": 5\n+  },\n+  \"16\": {\n+    \"BLOCK_SIZE_M\": 16,\n+    \"BLOCK_SIZE_N\": 64,\n+    \"BLOCK_SIZE_K\": 128,\n+    \"GROUP_SIZE_M\": 1,\n+    \"num_warps\": 4,\n+    \"num_stages\": 5\n+  },\n+  \"24\": {\n+    \"BLOCK_SIZE_M\": 32,\n+    \"BLOCK_SIZE_N\": 64,\n+    \"BLOCK_SIZE_K\": 128,\n+    \"GROUP_SIZE_M\": 1,\n+    \"num_warps\": 4,\n+    \"num_stages\": 5\n+  },\n+  \"32\": {\n+    \"BLOCK_SIZE_M\": 32,\n+    \"BLOCK_SIZE_N\": 64,\n+    \"BLOCK_SIZE_K\": 128,\n+    \"GROUP_SIZE_M\": 1,\n+    \"num_warps\": 4,\n+    \"num_stages\": 5\n+  },\n+  \"48\": {\n+    \"BLOCK_SIZE_M\": 64,\n+    \"BLOCK_SIZE_N\": 64,\n+    \"BLOCK_SIZE_K\": 64,\n+    \"GROUP_SIZE_M\": 1,\n+    \"num_warps\": 4,\n+    \"num_stages\": 5\n+  },\n+  \"64\": {\n+    \"BLOCK_SIZE_M\": 64,\n+    \"BLOCK_SIZE_N\": 64,\n+    \"BLOCK_SIZE_K\": 64,\n+    \"GROUP_SIZE_M\": 1,\n+    \"num_warps\": 4,\n+    \"num_stages\": 5\n+  },\n+  \"96\": {\n+    \"BLOCK_SIZE_M\": 32,\n+    \"BLOCK_SIZE_N\": 128,\n+    \"BLOCK_SIZE_K\": 64,\n+    \"GROUP_SIZE_M\": 1,\n+    \"num_warps\": 4,\n+    \"num_stages\": 5\n+  },\n+  \"128\": {\n+    \"BLOCK_SIZE_M\": 64,\n+    \"BLOCK_SIZE_N\": 128,\n+    \"BLOCK_SIZE_K\": 64,\n+    \"GROUP_SIZE_M\": 1,\n+    \"num_warps\": 4,\n+    \"num_stages\": 4\n+  },\n+  \"256\": {\n+    \"BLOCK_SIZE_M\": 128,\n+    \"BLOCK_SIZE_N\": 128,\n+    \"BLOCK_SIZE_K\": 64,\n+    \"GROUP_SIZE_M\": 1,\n+    \"num_warps\": 8,\n+    \"num_stages\": 3\n+  },\n+  \"512\": {\n+    \"BLOCK_SIZE_M\": 128,\n+    \"BLOCK_SIZE_N\": 256,\n+    \"BLOCK_SIZE_K\": 64,\n+    \"GROUP_SIZE_M\": 1,\n+    \"num_warps\": 8,\n+    \"num_stages\": 3\n+  },\n+  \"1024\": {\n+    \"BLOCK_SIZE_M\": 64,\n+    \"BLOCK_SIZE_N\": 128,\n+    \"BLOCK_SIZE_K\": 64,\n+    \"GROUP_SIZE_M\": 1,\n+    \"num_warps\": 4,\n+    \"num_stages\": 4\n+  },\n+  \"1536\": {\n+    \"BLOCK_SIZE_M\": 128,\n+    \"BLOCK_SIZE_N\": 128,\n+    \"BLOCK_SIZE_K\": 64,\n+    \"GROUP_SIZE_M\": 16,\n+    \"num_warps\": 4,\n+    \"num_stages\": 3\n+  },\n+  \"2048\": {\n+    \"BLOCK_SIZE_M\": 128,\n+    \"BLOCK_SIZE_N\": 128,\n+    \"BLOCK_SIZE_K\": 64,\n+    \"GROUP_SIZE_M\": 64,\n+    \"num_warps\": 4,\n+    \"num_stages\": 3\n+  },\n+  \"3072\": {\n+    \"BLOCK_SIZE_M\": 128,\n+    \"BLOCK_SIZE_N\": 128,\n+    \"BLOCK_SIZE_K\": 64,\n+    \"GROUP_SIZE_M\": 64,\n+    \"num_warps\": 4,\n+    \"num_stages\": 3\n+  },\n+  \"4096\": {\n+    \"BLOCK_SIZE_M\": 128,\n+    \"BLOCK_SIZE_N\": 128,\n+    \"BLOCK_SIZE_K\": 64,\n+    \"GROUP_SIZE_M\": 16,\n+    \"num_warps\": 4,\n+    \"num_stages\": 3\n+  },\n+  \"5120\": {\n+    \"BLOCK_SIZE_M\": 128,\n+    \"BLOCK_SIZE_N\": 128,\n+    \"BLOCK_SIZE_K\": 64,\n+    \"GROUP_SIZE_M\": 16,\n+    \"num_warps\": 4,\n+    \"num_stages\": 3\n+  },\n+  \"9216\": {\n+    \"BLOCK_SIZE_M\": 128,\n+    \"BLOCK_SIZE_N\": 128,\n+    \"BLOCK_SIZE_K\": 64,\n+    \"GROUP_SIZE_M\": 16,\n+    \"num_warps\": 4,\n+    \"num_stages\": 3\n+  }\n+}\n+\ndiff --git a/python/sglang/srt/layers/moe/fused_moe_triton/configs/triton_3_1_0/E=1,N=3072,device_name=NVIDIA_A100-SXM4-80GB,dtype=int8_w8a16.json b/python/sglang/srt/layers/moe/fused_moe_triton/configs/triton_3_1_0/E=1,N=3072,device_name=NVIDIA_A100-SXM4-80GB,dtype=int8_w8a16.json\nnew file mode 100644\nindex 000000000..04b654fea\n--- /dev/null\n+++ b/python/sglang/srt/layers/moe/fused_moe_triton/configs/triton_3_1_0/E=1,N=3072,device_name=NVIDIA_A100-SXM4-80GB,dtype=int8_w8a16.json\n@@ -0,0 +1,203 @@\n+{\n+  \"1\": {\n+    \"BLOCK_SIZE_M\": 16,\n+    \"BLOCK_SIZE_N\": 32,\n+    \"BLOCK_SIZE_K\": 256,\n+    \"GROUP_SIZE_M\": 32,\n+    \"num_warps\": 4,\n+    \"num_stages\": 3\n+  },\n+  \"2\": {\n+    \"BLOCK_SIZE_M\": 16,\n+    \"BLOCK_SIZE_N\": 32,\n+    \"BLOCK_SIZE_K\": 256,\n+    \"GROUP_SIZE_M\": 1,\n+    \"num_warps\": 4,\n+    \"num_stages\": 4\n+  },\n+  \"4\": {\n+    \"BLOCK_SIZE_M\": 16,\n+    \"BLOCK_SIZE_N\": 32,\n+    \"BLOCK_SIZE_K\": 256,\n+    \"GROUP_SIZE_M\": 1,\n+    \"num_warps\": 4,\n+    \"num_stages\": 4\n+  },\n+  \"8\": {\n+    \"BLOCK_SIZE_M\": 16,\n+    \"BLOCK_SIZE_N\": 32,\n+    \"BLOCK_SIZE_K\": 256,\n+    \"GROUP_SIZE_M\": 1,\n+    \"num_warps\": 4,\n+    \"num_stages\": 4\n+  },\n+  \"16\": {\n+    \"BLOCK_SIZE_M\": 16,\n+    \"BLOCK_SIZE_N\": 32,\n+    \"BLOCK_SIZE_K\": 256,\n+    \"GROUP_SIZE_M\": 1,\n+    \"num_warps\": 4,\n+    \"num_stages\": 4\n+  },\n+  \"24\": {\n+    \"BLOCK_SIZE_M\": 32,\n+    \"BLOCK_SIZE_N\": 32,\n+    \"BLOCK_SIZE_K\": 128,\n+    \"GROUP_SIZE_M\": 1,\n+    \"num_warps\": 4,\n+    \"num_stages\": 5\n+  },\n+  \"32\": {\n+    \"BLOCK_SIZE_M\": 16,\n+    \"BLOCK_SIZE_N\": 64,\n+    \"BLOCK_SIZE_K\": 128,\n+    \"GROUP_SIZE_M\": 1,\n+    \"num_warps\": 4,\n+    \"num_stages\": 3\n+  },\n+  \"48\": {\n+    \"BLOCK_SIZE_M\": 32,\n+    \"BLOCK_SIZE_N\": 128,\n+    \"BLOCK_SIZE_K\": 128,\n+    \"GROUP_SIZE_M\": 1,\n+    \"num_warps\": 4,\n+    \"num_stages\": 3\n+  },\n+  \"64\": {\n+    \"BLOCK_SIZE_M\": 32,\n+    \"BLOCK_SIZE_N\": 128,\n+    \"BLOCK_SIZE_K\": 128,\n+    \"GROUP_SIZE_M\": 1,\n+    \"num_warps\": 4,\n+    \"num_stages\": 3\n+  },\n+  \"96\": {\n+    \"BLOCK_SIZE_M\": 32,\n+    \"BLOCK_SIZE_N\": 128,\n+    \"BLOCK_SIZE_K\": 128,\n+    \"GROUP_SIZE_M\": 1,\n+    \"num_warps\": 4,\n+    \"num_stages\": 3\n+  },\n+  \"128\": {\n+    \"BLOCK_SIZE_M\": 128,\n+    \"BLOCK_SIZE_N\": 32,\n+    \"BLOCK_SIZE_K\": 128,\n+    \"GROUP_SIZE_M\": 1,\n+    \"num_warps\": 4,\n+    \"num_stages\": 3\n+  },\n+  \"256\": {\n+    \"BLOCK_SIZE_M\": 64,\n+    \"BLOCK_SIZE_N\": 64,\n+    \"BLOCK_SIZE_K\": 64,\n+    \"GROUP_SIZE_M\": 16,\n+    \"num_warps\": 4,\n+    \"num_stages\": 3\n+  },\n+  \"512\": {\n+    \"BLOCK_SIZE_M\": 64,\n+    \"BLOCK_SIZE_N\": 256,\n+    \"BLOCK_SIZE_K\": 64,\n+    \"GROUP_SIZE_M\": 16,\n+    \"num_warps\": 4,\n+    \"num_stages\": 4\n+  },\n+  \"1024\": {\n+    \"BLOCK_SIZE_M\": 64,\n+    \"BLOCK_SIZE_N\": 256,\n+    \"BLOCK_SIZE_K\": 64,\n+    \"GROUP_SIZE_M\": 16,\n+    \"num_warps\": 4,\n+    \"num_stages\": 3\n+  },\n+  \"1536\": {\n+    \"BLOCK_SIZE_M\": 64,\n+    \"BLOCK_SIZE_N\": 256,\n+    \"BLOCK_SIZE_K\": 64,\n+    \"GROUP_SIZE_M\": 64,\n+    \"num_warps\": 4,\n+    \"num_stages\": 4\n+  },\n+  \"2048\": {\n+    \"BLOCK_SIZE_M\": 64,\n+    \"BLOCK_SIZE_N\": 256,\n+    \"BLOCK_SIZE_K\": 64,\n+    \"GROUP_SIZE_M\": 64,\n+    \"num_warps\": 4,\n+    \"num_stages\": 4\n+  },\n+  \"3072\": {\n+    \"BLOCK_SIZE_M\": 64,\n+    \"BLOCK_SIZE_N\": 256,\n+    \"BLOCK_SIZE_K\": 64,\n+    \"GROUP_SIZE_M\": 32,\n+    \"num_warps\": 4,\n+    \"num_stages\": 4\n+  },\n+  \"4096\": {\n+    \"BLOCK_SIZE_M\": 64,\n+    \"BLOCK_SIZE_N\": 256,\n+    \"BLOCK_SIZE_K\": 64,\n+    \"GROUP_SIZE_M\": 16,\n+    \"num_warps\": 4,\n+    \"num_stages\": 4\n+  },\n+  \"5120\": {\n+    \"BLOCK_SIZE_M\": 64,\n+    \"BLOCK_SIZE_N\": 256,\n+    \"BLOCK_SIZE_K\": 64,\n+    \"GROUP_SIZE_M\": 16,\n+    \"num_warps\": 4,\n+    \"num_stages\": 4\n+  },\n+  \"9216\": {\n+    \"BLOCK_SIZE_M\": 64,\n+    \"BLOCK_SIZE_N\": 256,\n+    \"BLOCK_SIZE_K\": 64,\n+    \"GROUP_SIZE_M\": 32,\n+    \"num_warps\": 4,\n+    \"num_stages\": 3\n+  },\n+  \"13312\": {\n+    \"BLOCK_SIZE_M\": 64,\n+    \"BLOCK_SIZE_N\": 256,\n+    \"BLOCK_SIZE_K\": 64,\n+    \"GROUP_SIZE_M\": 32,\n+    \"num_warps\": 4,\n+    \"num_stages\": 4\n+  },\n+  \"17408\": {\n+    \"BLOCK_SIZE_M\": 64,\n+    \"BLOCK_SIZE_N\": 256,\n+    \"BLOCK_SIZE_K\": 64,\n+    \"GROUP_SIZE_M\": 16,\n+    \"num_warps\": 4,\n+    \"num_stages\": 4\n+  },\n+  \"25600\": {\n+    \"BLOCK_SIZE_M\": 64,\n+    \"BLOCK_SIZE_N\": 256,\n+    \"BLOCK_SIZE_K\": 64,\n+    \"GROUP_SIZE_M\": 16,\n+    \"num_warps\": 4,\n+    \"num_stages\": 4\n+  },\n+  \"33792\": {\n+    \"BLOCK_SIZE_M\": 64,\n+    \"BLOCK_SIZE_N\": 256,\n+    \"BLOCK_SIZE_K\": 64,\n+    \"GROUP_SIZE_M\": 16,\n+    \"num_warps\": 4,\n+    \"num_stages\": 4\n+  },\n+  \"41984\": {\n+    \"BLOCK_SIZE_M\": 64,\n+    \"BLOCK_SIZE_N\": 256,\n+    \"BLOCK_SIZE_K\": 64,\n+    \"GROUP_SIZE_M\": 32,\n+    \"num_warps\": 4,\n+    \"num_stages\": 4\n+  }\n+}\n+\ndiff --git a/python/sglang/srt/layers/moe/fused_moe_triton/configs/triton_3_1_0/E=1,N=3072,device_name=NVIDIA_H100_80GB_HBM3,dtype=int8_w8a16.json b/python/sglang/srt/layers/moe/fused_moe_triton/configs/triton_3_1_0/E=1,N=3072,device_name=NVIDIA_H100_80GB_HBM3,dtype=int8_w8a16.json\nnew file mode 100644\nindex 000000000..48f56942f\n--- /dev/null\n+++ b/python/sglang/srt/layers/moe/fused_moe_triton/configs/triton_3_1_0/E=1,N=3072,device_name=NVIDIA_H100_80GB_HBM3,dtype=int8_w8a16.json\n@@ -0,0 +1,123 @@\n+{\n+  \"1\": {\n+    \"BLOCK_SIZE_M\": 16,\n+    \"BLOCK_SIZE_N\": 32,\n+    \"BLOCK_SIZE_K\": 256,\n+    \"GROUP_SIZE_M\": 32,\n+    \"num_warps\": 4,\n+    \"num_stages\": 3\n+  },\n+  \"2\": {\n+    \"BLOCK_SIZE_M\": 16,\n+    \"BLOCK_SIZE_N\": 64,\n+    \"BLOCK_SIZE_K\": 128,\n+    \"GROUP_SIZE_M\": 1,\n+    \"num_warps\": 8,\n+    \"num_stages\": 5\n+  },\n+  \"4\": {\n+    \"BLOCK_SIZE_M\": 16,\n+    \"BLOCK_SIZE_N\": 64,\n+    \"BLOCK_SIZE_K\": 128,\n+    \"GROUP_SIZE_M\": 1,\n+    \"num_warps\": 8,\n+    \"num_stages\": 5\n+  },\n+  \"8\": {\n+    \"BLOCK_SIZE_M\": 16,\n+    \"BLOCK_SIZE_N\": 64,\n+    \"BLOCK_SIZE_K\": 128,\n+    \"GROUP_SIZE_M\": 1,\n+    \"num_warps\": 8,\n+    \"num_stages\": 4\n+  },\n+  \"16\": {\n+    \"BLOCK_SIZE_M\": 16,\n+    \"BLOCK_SIZE_N\": 64,\n+    \"BLOCK_SIZE_K\": 128,\n+    \"GROUP_SIZE_M\": 1,\n+    \"num_warps\": 8,\n+    \"num_stages\": 5\n+  },\n+  \"24\": {\n+    \"BLOCK_SIZE_M\": 32,\n+    \"BLOCK_SIZE_N\": 64,\n+    \"BLOCK_SIZE_K\": 256,\n+    \"GROUP_SIZE_M\": 1,\n+    \"num_warps\": 8,\n+    \"num_stages\": 3\n+  },\n+  \"32\": {\n+    \"BLOCK_SIZE_M\": 16,\n+    \"BLOCK_SIZE_N\": 32,\n+    \"BLOCK_SIZE_K\": 256,\n+    \"GROUP_SIZE_M\": 1,\n+    \"num_warps\": 4,\n+    \"num_stages\": 5\n+  },\n+  \"48\": {\n+    \"BLOCK_SIZE_M\": 64,\n+    \"BLOCK_SIZE_N\": 32,\n+    \"BLOCK_SIZE_K\": 256,\n+    \"GROUP_SIZE_M\": 1,\n+    \"num_warps\": 4,\n+    \"num_stages\": 3\n+  },\n+  \"64\": {\n+    \"BLOCK_SIZE_M\": 64,\n+    \"BLOCK_SIZE_N\": 32,\n+    \"BLOCK_SIZE_K\": 256,\n+    \"GROUP_SIZE_M\": 1,\n+    \"num_warps\": 4,\n+    \"num_stages\": 3\n+  },\n+  \"96\": {\n+    \"BLOCK_SIZE_M\": 64,\n+    \"BLOCK_SIZE_N\": 64,\n+    \"BLOCK_SIZE_K\": 256,\n+    \"GROUP_SIZE_M\": 1,\n+    \"num_warps\": 4,\n+    \"num_stages\": 2\n+  },\n+  \"128\": {\n+    \"BLOCK_SIZE_M\": 64,\n+    \"BLOCK_SIZE_N\": 64,\n+    \"BLOCK_SIZE_K\": 256,\n+    \"GROUP_SIZE_M\": 1,\n+    \"num_warps\": 4,\n+    \"num_stages\": 2\n+  },\n+  \"256\": {\n+    \"BLOCK_SIZE_M\": 256,\n+    \"BLOCK_SIZE_N\": 64,\n+    \"BLOCK_SIZE_K\": 128,\n+    \"GROUP_SIZE_M\": 64,\n+    \"num_warps\": 8,\n+    \"num_stages\": 3\n+  },\n+  \"512\": {\n+    \"BLOCK_SIZE_M\": 256,\n+    \"BLOCK_SIZE_N\": 128,\n+    \"BLOCK_SIZE_K\": 128,\n+    \"GROUP_SIZE_M\": 1,\n+    \"num_warps\": 8,\n+    \"num_stages\": 3\n+  },\n+  \"1024\": {\n+    \"BLOCK_SIZE_M\": 256,\n+    \"BLOCK_SIZE_N\": 128,\n+    \"BLOCK_SIZE_K\": 128,\n+    \"GROUP_SIZE_M\": 1,\n+    \"num_warps\": 8,\n+    \"num_stages\": 3\n+  },\n+  \"1536\": {\n+    \"BLOCK_SIZE_M\": 256,\n+    \"BLOCK_SIZE_N\": 128,\n+    \"BLOCK_SIZE_K\": 128,\n+    \"GROUP_SIZE_M\": 1,\n+    \"num_warps\": 8,\n+    \"num_stages\": 3\n+  }\n+}\n+\ndiff --git a/python/sglang/srt/layers/moe/fused_moe_triton/configs/triton_3_1_0/E=1,N=3072,device_name=NVIDIA_H100_80GB_HBM3.json b/python/sglang/srt/layers/moe/fused_moe_triton/configs/triton_3_1_0/E=1,N=3072,device_name=NVIDIA_H100_80GB_HBM3.json\nnew file mode 100644\nindex 000000000..a84c43779\n--- /dev/null\n+++ b/python/sglang/srt/layers/moe/fused_moe_triton/configs/triton_3_1_0/E=1,N=3072,device_name=NVIDIA_H100_80GB_HBM3.json\n@@ -0,0 +1,123 @@\n+{\n+  \"1\": {\n+    \"BLOCK_SIZE_M\": 16,\n+    \"BLOCK_SIZE_N\": 64,\n+    \"BLOCK_SIZE_K\": 256,\n+    \"GROUP_SIZE_M\": 1,\n+    \"num_warps\": 4,\n+    \"num_stages\": 4\n+  },\n+  \"2\": {\n+    \"BLOCK_SIZE_M\": 16,\n+    \"BLOCK_SIZE_N\": 64,\n+    \"BLOCK_SIZE_K\": 256,\n+    \"GROUP_SIZE_M\": 1,\n+    \"num_warps\": 4,\n+    \"num_stages\": 3\n+  },\n+  \"4\": {\n+    \"BLOCK_SIZE_M\": 16,\n+    \"BLOCK_SIZE_N\": 64,\n+    \"BLOCK_SIZE_K\": 256,\n+    \"GROUP_SIZE_M\": 1,\n+    \"num_warps\": 4,\n+    \"num_stages\": 3\n+  },\n+  \"8\": {\n+    \"BLOCK_SIZE_M\": 16,\n+    \"BLOCK_SIZE_N\": 64,\n+    \"BLOCK_SIZE_K\": 256,\n+    \"GROUP_SIZE_M\": 1,\n+    \"num_warps\": 4,\n+    \"num_stages\": 3\n+  },\n+  \"16\": {\n+    \"BLOCK_SIZE_M\": 16,\n+    \"BLOCK_SIZE_N\": 64,\n+    \"BLOCK_SIZE_K\": 256,\n+    \"GROUP_SIZE_M\": 1,\n+    \"num_warps\": 4,\n+    \"num_stages\": 5\n+  },\n+  \"24\": {\n+    \"BLOCK_SIZE_M\": 32,\n+    \"BLOCK_SIZE_N\": 64,\n+    \"BLOCK_SIZE_K\": 256,\n+    \"GROUP_SIZE_M\": 1,\n+    \"num_warps\": 8,\n+    \"num_stages\": 4\n+  },\n+  \"32\": {\n+    \"BLOCK_SIZE_M\": 64,\n+    \"BLOCK_SIZE_N\": 64,\n+    \"BLOCK_SIZE_K\": 128,\n+    \"GROUP_SIZE_M\": 1,\n+    \"num_warps\": 4,\n+    \"num_stages\": 5\n+  },\n+  \"48\": {\n+    \"BLOCK_SIZE_M\": 64,\n+    \"BLOCK_SIZE_N\": 64,\n+    \"BLOCK_SIZE_K\": 128,\n+    \"GROUP_SIZE_M\": 1,\n+    \"num_warps\": 4,\n+    \"num_stages\": 5\n+  },\n+  \"64\": {\n+    \"BLOCK_SIZE_M\": 64,\n+    \"BLOCK_SIZE_N\": 64,\n+    \"BLOCK_SIZE_K\": 128,\n+    \"GROUP_SIZE_M\": 1,\n+    \"num_warps\": 4,\n+    \"num_stages\": 5\n+  },\n+  \"96\": {\n+    \"BLOCK_SIZE_M\": 64,\n+    \"BLOCK_SIZE_N\": 128,\n+    \"BLOCK_SIZE_K\": 128,\n+    \"GROUP_SIZE_M\": 16,\n+    \"num_warps\": 4,\n+    \"num_stages\": 4\n+  },\n+  \"128\": {\n+    \"BLOCK_SIZE_M\": 64,\n+    \"BLOCK_SIZE_N\": 128,\n+    \"BLOCK_SIZE_K\": 128,\n+    \"GROUP_SIZE_M\": 16,\n+    \"num_warps\": 4,\n+    \"num_stages\": 4\n+  },\n+  \"256\": {\n+    \"BLOCK_SIZE_M\": 128,\n+    \"BLOCK_SIZE_N\": 128,\n+    \"BLOCK_SIZE_K\": 64,\n+    \"GROUP_SIZE_M\": 64,\n+    \"num_warps\": 8,\n+    \"num_stages\": 5\n+  },\n+  \"512\": {\n+    \"BLOCK_SIZE_M\": 128,\n+    \"BLOCK_SIZE_N\": 256,\n+    \"BLOCK_SIZE_K\": 64,\n+    \"GROUP_SIZE_M\": 16,\n+    \"num_warps\": 8,\n+    \"num_stages\": 4\n+  },\n+  \"1024\": {\n+    \"BLOCK_SIZE_M\": 128,\n+    \"BLOCK_SIZE_N\": 256,\n+    \"BLOCK_SIZE_K\": 64,\n+    \"GROUP_SIZE_M\": 32,\n+    \"num_warps\": 8,\n+    \"num_stages\": 4\n+  },\n+  \"1536\": {\n+    \"BLOCK_SIZE_M\": 128,\n+    \"BLOCK_SIZE_N\": 256,\n+    \"BLOCK_SIZE_K\": 64,\n+    \"GROUP_SIZE_M\": 32,\n+    \"num_warps\": 8,\n+    \"num_stages\": 4\n+  }\n+}\n+\ndiff --git a/python/sglang/srt/layers/moe/fused_moe_triton/configs/triton_3_1_0/E=1,N=3584,device_name=NVIDIA_A100-SXM4-80GB,dtype=int8_w8a16.json b/python/sglang/srt/layers/moe/fused_moe_triton/configs/triton_3_1_0/E=1,N=3584,device_name=NVIDIA_A100-SXM4-80GB,dtype=int8_w8a16.json\nnew file mode 100644\nindex 000000000..ad1317105\n--- /dev/null\n+++ b/python/sglang/srt/layers/moe/fused_moe_triton/configs/triton_3_1_0/E=1,N=3584,device_name=NVIDIA_A100-SXM4-80GB,dtype=int8_w8a16.json\n@@ -0,0 +1,147 @@\n+{\n+  \"1\": {\n+    \"BLOCK_SIZE_M\": 16,\n+    \"BLOCK_SIZE_N\": 32,\n+    \"BLOCK_SIZE_K\": 256,\n+    \"GROUP_SIZE_M\": 32,\n+    \"num_warps\": 4,\n+    \"num_stages\": 3\n+  },\n+  \"2\": {\n+    \"BLOCK_SIZE_M\": 16,\n+    \"BLOCK_SIZE_N\": 32,\n+    \"BLOCK_SIZE_K\": 256,\n+    \"GROUP_SIZE_M\": 1,\n+    \"num_warps\": 4,\n+    \"num_stages\": 3\n+  },\n+  \"4\": {\n+    \"BLOCK_SIZE_M\": 16,\n+    \"BLOCK_SIZE_N\": 32,\n+    \"BLOCK_SIZE_K\": 256,\n+    \"GROUP_SIZE_M\": 1,\n+    \"num_warps\": 4,\n+    \"num_stages\": 3\n+  },\n+  \"8\": {\n+    \"BLOCK_SIZE_M\": 16,\n+    \"BLOCK_SIZE_N\": 32,\n+    \"BLOCK_SIZE_K\": 256,\n+    \"GROUP_SIZE_M\": 1,\n+    \"num_warps\": 4,\n+    \"num_stages\": 3\n+  },\n+  \"16\": {\n+    \"BLOCK_SIZE_M\": 16,\n+    \"BLOCK_SIZE_N\": 32,\n+    \"BLOCK_SIZE_K\": 256,\n+    \"GROUP_SIZE_M\": 1,\n+    \"num_warps\": 4,\n+    \"num_stages\": 3\n+  },\n+  \"24\": {\n+    \"BLOCK_SIZE_M\": 32,\n+    \"BLOCK_SIZE_N\": 32,\n+    \"BLOCK_SIZE_K\": 128,\n+    \"GROUP_SIZE_M\": 16,\n+    \"num_warps\": 4,\n+    \"num_stages\": 5\n+  },\n+  \"32\": {\n+    \"BLOCK_SIZE_M\": 16,\n+    \"BLOCK_SIZE_N\": 64,\n+    \"BLOCK_SIZE_K\": 128,\n+    \"GROUP_SIZE_M\": 1,\n+    \"num_warps\": 4,\n+    \"num_stages\": 3\n+  },\n+  \"48\": {\n+    \"BLOCK_SIZE_M\": 32,\n+    \"BLOCK_SIZE_N\": 128,\n+    \"BLOCK_SIZE_K\": 128,\n+    \"GROUP_SIZE_M\": 1,\n+    \"num_warps\": 4,\n+    \"num_stages\": 3\n+  },\n+  \"64\": {\n+    \"BLOCK_SIZE_M\": 32,\n+    \"BLOCK_SIZE_N\": 128,\n+    \"BLOCK_SIZE_K\": 128,\n+    \"GROUP_SIZE_M\": 1,\n+    \"num_warps\": 4,\n+    \"num_stages\": 3\n+  },\n+  \"96\": {\n+    \"BLOCK_SIZE_M\": 32,\n+    \"BLOCK_SIZE_N\": 128,\n+    \"BLOCK_SIZE_K\": 128,\n+    \"GROUP_SIZE_M\": 1,\n+    \"num_warps\": 4,\n+    \"num_stages\": 3\n+  },\n+  \"128\": {\n+    \"BLOCK_SIZE_M\": 128,\n+    \"BLOCK_SIZE_N\": 32,\n+    \"BLOCK_SIZE_K\": 128,\n+    \"GROUP_SIZE_M\": 1,\n+    \"num_warps\": 4,\n+    \"num_stages\": 3\n+  },\n+  \"256\": {\n+    \"BLOCK_SIZE_M\": 64,\n+    \"BLOCK_SIZE_N\": 64,\n+    \"BLOCK_SIZE_K\": 64,\n+    \"GROUP_SIZE_M\": 16,\n+    \"num_warps\": 4,\n+    \"num_stages\": 3\n+  },\n+  \"512\": {\n+    \"BLOCK_SIZE_M\": 64,\n+    \"BLOCK_SIZE_N\": 256,\n+    \"BLOCK_SIZE_K\": 64,\n+    \"GROUP_SIZE_M\": 16,\n+    \"num_warps\": 4,\n+    \"num_stages\": 4\n+  },\n+  \"1024\": {\n+    \"BLOCK_SIZE_M\": 64,\n+    \"BLOCK_SIZE_N\": 256,\n+    \"BLOCK_SIZE_K\": 64,\n+    \"GROUP_SIZE_M\": 16,\n+    \"num_warps\": 4,\n+    \"num_stages\": 3\n+  },\n+  \"1536\": {\n+    \"BLOCK_SIZE_M\": 64,\n+    \"BLOCK_SIZE_N\": 256,\n+    \"BLOCK_SIZE_K\": 64,\n+    \"GROUP_SIZE_M\": 64,\n+    \"num_warps\": 4,\n+    \"num_stages\": 4\n+  },\n+  \"2048\": {\n+    \"BLOCK_SIZE_M\": 64,\n+    \"BLOCK_SIZE_N\": 256,\n+    \"BLOCK_SIZE_K\": 64,\n+    \"GROUP_SIZE_M\": 64,\n+    \"num_warps\": 4,\n+    \"num_stages\": 4\n+  },\n+  \"3072\": {\n+    \"BLOCK_SIZE_M\": 64,\n+    \"BLOCK_SIZE_N\": 256,\n+    \"BLOCK_SIZE_K\": 64,\n+    \"GROUP_SIZE_M\": 32,\n+    \"num_warps\": 4,\n+    \"num_stages\": 4\n+  },\n+  \"4096\": {\n+    \"BLOCK_SIZE_M\": 64,\n+    \"BLOCK_SIZE_N\": 256,\n+    \"BLOCK_SIZE_K\": 64,\n+    \"GROUP_SIZE_M\": 16,\n+    \"num_warps\": 4,\n+    \"num_stages\": 4\n+  }\n+}\n+\ndiff --git a/python/sglang/srt/layers/moe/fused_moe_triton/configs/triton_3_1_0/E=1,N=3584,device_name=NVIDIA_A100-SXM4-80GB.json b/python/sglang/srt/layers/moe/fused_moe_triton/configs/triton_3_1_0/E=1,N=3584,device_name=NVIDIA_A100-SXM4-80GB.json\nnew file mode 100644\nindex 000000000..a84c43779\n--- /dev/null\n+++ b/python/sglang/srt/layers/moe/fused_moe_triton/configs/triton_3_1_0/E=1,N=3584,device_name=NVIDIA_A100-SXM4-80GB.json\n@@ -0,0 +1,123 @@\n+{\n+  \"1\": {\n+    \"BLOCK_SIZE_M\": 16,\n+    \"BLOCK_SIZE_N\": 64,\n+    \"BLOCK_SIZE_K\": 256,\n+    \"GROUP_SIZE_M\": 1,\n+    \"num_warps\": 4,\n+    \"num_stages\": 4\n+  },\n+  \"2\": {\n+    \"BLOCK_SIZE_M\": 16,\n+    \"BLOCK_SIZE_N\": 64,\n+    \"BLOCK_SIZE_K\": 256,\n+    \"GROUP_SIZE_M\": 1,\n+    \"num_warps\": 4,\n+    \"num_stages\": 3\n+  },\n+  \"4\": {\n+    \"BLOCK_SIZE_M\": 16,\n+    \"BLOCK_SIZE_N\": 64,\n+    \"BLOCK_SIZE_K\": 256,\n+    \"GROUP_SIZE_M\": 1,\n+    \"num_warps\": 4,\n+    \"num_stages\": 3\n+  },\n+  \"8\": {\n+    \"BLOCK_SIZE_M\": 16,\n+    \"BLOCK_SIZE_N\": 64,\n+    \"BLOCK_SIZE_K\": 256,\n+    \"GROUP_SIZE_M\": 1,\n+    \"num_warps\": 4,\n+    \"num_stages\": 3\n+  },\n+  \"16\": {\n+    \"BLOCK_SIZE_M\": 16,\n+    \"BLOCK_SIZE_N\": 64,\n+    \"BLOCK_SIZE_K\": 256,\n+    \"GROUP_SIZE_M\": 1,\n+    \"num_warps\": 4,\n+    \"num_stages\": 5\n+  },\n+  \"24\": {\n+    \"BLOCK_SIZE_M\": 32,\n+    \"BLOCK_SIZE_N\": 64,\n+    \"BLOCK_SIZE_K\": 256,\n+    \"GROUP_SIZE_M\": 1,\n+    \"num_warps\": 8,\n+    \"num_stages\": 4\n+  },\n+  \"32\": {\n+    \"BLOCK_SIZE_M\": 64,\n+    \"BLOCK_SIZE_N\": 64,\n+    \"BLOCK_SIZE_K\": 128,\n+    \"GROUP_SIZE_M\": 1,\n+    \"num_warps\": 4,\n+    \"num_stages\": 5\n+  },\n+  \"48\": {\n+    \"BLOCK_SIZE_M\": 64,\n+    \"BLOCK_SIZE_N\": 64,\n+    \"BLOCK_SIZE_K\": 128,\n+    \"GROUP_SIZE_M\": 1,\n+    \"num_warps\": 4,\n+    \"num_stages\": 5\n+  },\n+  \"64\": {\n+    \"BLOCK_SIZE_M\": 64,\n+    \"BLOCK_SIZE_N\": 64,\n+    \"BLOCK_SIZE_K\": 128,\n+    \"GROUP_SIZE_M\": 1,\n+    \"num_warps\": 4,\n+    \"num_stages\": 5\n+  },\n+  \"96\": {\n+    \"BLOCK_SIZE_M\": 64,\n+    \"BLOCK_SIZE_N\": 128,\n+    \"BLOCK_SIZE_K\": 128,\n+    \"GROUP_SIZE_M\": 16,\n+    \"num_warps\": 4,\n+    \"num_stages\": 4\n+  },\n+  \"128\": {\n+    \"BLOCK_SIZE_M\": 64,\n+    \"BLOCK_SIZE_N\": 128,\n+    \"BLOCK_SIZE_K\": 128,\n+    \"GROUP_SIZE_M\": 16,\n+    \"num_warps\": 4,\n+    \"num_stages\": 4\n+  },\n+  \"256\": {\n+    \"BLOCK_SIZE_M\": 128,\n+    \"BLOCK_SIZE_N\": 128,\n+    \"BLOCK_SIZE_K\": 64,\n+    \"GROUP_SIZE_M\": 64,\n+    \"num_warps\": 8,\n+    \"num_stages\": 5\n+  },\n+  \"512\": {\n+    \"BLOCK_SIZE_M\": 128,\n+    \"BLOCK_SIZE_N\": 256,\n+    \"BLOCK_SIZE_K\": 64,\n+    \"GROUP_SIZE_M\": 16,\n+    \"num_warps\": 8,\n+    \"num_stages\": 4\n+  },\n+  \"1024\": {\n+    \"BLOCK_SIZE_M\": 128,\n+    \"BLOCK_SIZE_N\": 256,\n+    \"BLOCK_SIZE_K\": 64,\n+    \"GROUP_SIZE_M\": 32,\n+    \"num_warps\": 8,\n+    \"num_stages\": 4\n+  },\n+  \"1536\": {\n+    \"BLOCK_SIZE_M\": 128,\n+    \"BLOCK_SIZE_N\": 256,\n+    \"BLOCK_SIZE_K\": 64,\n+    \"GROUP_SIZE_M\": 32,\n+    \"num_warps\": 8,\n+    \"num_stages\": 4\n+  }\n+}\n+\ndiff --git a/python/sglang/srt/layers/moe/fused_moe_triton/configs/triton_3_1_0/E=1,N=7168,device_name=NVIDIA_A100-SXM4-80GB,dtype=int8_w8a16.json b/python/sglang/srt/layers/moe/fused_moe_triton/configs/triton_3_1_0/E=1,N=7168,device_name=NVIDIA_A100-SXM4-80GB,dtype=int8_w8a16.json\nnew file mode 100644\nindex 000000000..569dc1e38\n--- /dev/null\n+++ b/python/sglang/srt/layers/moe/fused_moe_triton/configs/triton_3_1_0/E=1,N=7168,device_name=NVIDIA_A100-SXM4-80GB,dtype=int8_w8a16.json\n@@ -0,0 +1,147 @@\n+{\n+  \"1\": {\n+    \"BLOCK_SIZE_M\": 16,\n+    \"BLOCK_SIZE_N\": 32,\n+    \"BLOCK_SIZE_K\": 128,\n+    \"GROUP_SIZE_M\": 32,\n+    \"num_warps\": 4,\n+    \"num_stages\": 4\n+  },\n+  \"2\": {\n+    \"BLOCK_SIZE_M\": 16,\n+    \"BLOCK_SIZE_N\": 32,\n+    \"BLOCK_SIZE_K\": 256,\n+    \"GROUP_SIZE_M\": 32,\n+    \"num_warps\": 4,\n+    \"num_stages\": 3\n+  },\n+  \"4\": {\n+    \"BLOCK_SIZE_M\": 16,\n+    \"BLOCK_SIZE_N\": 32,\n+    \"BLOCK_SIZE_K\": 256,\n+    \"GROUP_SIZE_M\": 1,\n+    \"num_warps\": 4,\n+    \"num_stages\": 3\n+  },\n+  \"8\": {\n+    \"BLOCK_SIZE_M\": 16,\n+    \"BLOCK_SIZE_N\": 32,\n+    \"BLOCK_SIZE_K\": 128,\n+    \"GROUP_SIZE_M\": 64,\n+    \"num_warps\": 4,\n+    \"num_stages\": 3\n+  },\n+  \"16\": {\n+    \"BLOCK_SIZE_M\": 16,\n+    \"BLOCK_SIZE_N\": 32,\n+    \"BLOCK_SIZE_K\": 128,\n+    \"GROUP_SIZE_M\": 32,\n+    \"num_warps\": 4,\n+    \"num_stages\": 4\n+  },\n+  \"24\": {\n+    \"BLOCK_SIZE_M\": 16,\n+    \"BLOCK_SIZE_N\": 64,\n+    \"BLOCK_SIZE_K\": 64,\n+    \"GROUP_SIZE_M\": 1,\n+    \"num_warps\": 4,\n+    \"num_stages\": 5\n+  },\n+  \"32\": {\n+    \"BLOCK_SIZE_M\": 16,\n+    \"BLOCK_SIZE_N\": 64,\n+    \"BLOCK_SIZE_K\": 128,\n+    \"GROUP_SIZE_M\": 16,\n+    \"num_warps\": 4,\n+    \"num_stages\": 2\n+  },\n+  \"48\": {\n+    \"BLOCK_SIZE_M\": 64,\n+    \"BLOCK_SIZE_N\": 64,\n+    \"BLOCK_SIZE_K\": 128,\n+    \"GROUP_SIZE_M\": 16,\n+    \"num_warps\": 8,\n+    \"num_stages\": 3\n+  },\n+  \"64\": {\n+    \"BLOCK_SIZE_M\": 64,\n+    \"BLOCK_SIZE_N\": 64,\n+    \"BLOCK_SIZE_K\": 128,\n+    \"GROUP_SIZE_M\": 16,\n+    \"num_warps\": 8,\n+    \"num_stages\": 3\n+  },\n+  \"96\": {\n+    \"BLOCK_SIZE_M\": 32,\n+    \"BLOCK_SIZE_N\": 128,\n+    \"BLOCK_SIZE_K\": 64,\n+    \"GROUP_SIZE_M\": 16,\n+    \"num_warps\": 4,\n+    \"num_stages\": 3\n+  },\n+  \"128\": {\n+    \"BLOCK_SIZE_M\": 64,\n+    \"BLOCK_SIZE_N\": 128,\n+    \"BLOCK_SIZE_K\": 64,\n+    \"GROUP_SIZE_M\": 1,\n+    \"num_warps\": 4,\n+    \"num_stages\": 3\n+  },\n+  \"256\": {\n+    \"BLOCK_SIZE_M\": 64,\n+    \"BLOCK_SIZE_N\": 64,\n+    \"BLOCK_SIZE_K\": 64,\n+    \"GROUP_SIZE_M\": 16,\n+    \"num_warps\": 4,\n+    \"num_stages\": 3\n+  },\n+  \"512\": {\n+    \"BLOCK_SIZE_M\": 64,\n+    \"BLOCK_SIZE_N\": 256,\n+    \"BLOCK_SIZE_K\": 64,\n+    \"GROUP_SIZE_M\": 16,\n+    \"num_warps\": 4,\n+    \"num_stages\": 4\n+  },\n+  \"1024\": {\n+    \"BLOCK_SIZE_M\": 64,\n+    \"BLOCK_SIZE_N\": 256,\n+    \"BLOCK_SIZE_K\": 64,\n+    \"GROUP_SIZE_M\": 16,\n+    \"num_warps\": 4,\n+    \"num_stages\": 3\n+  },\n+  \"1536\": {\n+    \"BLOCK_SIZE_M\": 64,\n+    \"BLOCK_SIZE_N\": 256,\n+    \"BLOCK_SIZE_K\": 64,\n+    \"GROUP_SIZE_M\": 64,\n+    \"num_warps\": 4,\n+    \"num_stages\": 4\n+  },\n+  \"2048\": {\n+    \"BLOCK_SIZE_M\": 64,\n+    \"BLOCK_SIZE_N\": 256,\n+    \"BLOCK_SIZE_K\": 64,\n+    \"GROUP_SIZE_M\": 64,\n+    \"num_warps\": 4,\n+    \"num_stages\": 4\n+  },\n+  \"3072\": {\n+    \"BLOCK_SIZE_M\": 64,\n+    \"BLOCK_SIZE_N\": 256,\n+    \"BLOCK_SIZE_K\": 64,\n+    \"GROUP_SIZE_M\": 16,\n+    \"num_warps\": 4,\n+    \"num_stages\": 4\n+  },\n+  \"4096\": {\n+    \"BLOCK_SIZE_M\": 64,\n+    \"BLOCK_SIZE_N\": 256,\n+    \"BLOCK_SIZE_K\": 64,\n+    \"GROUP_SIZE_M\": 16,\n+    \"num_warps\": 4,\n+    \"num_stages\": 4\n+  }\n+}\n+\ndiff --git a/python/sglang/srt/layers/moe/fused_moe_triton/configs/triton_3_1_0/E=1,N=7168,device_name=NVIDIA_A100-SXM4-80GB.json b/python/sglang/srt/layers/moe/fused_moe_triton/configs/triton_3_1_0/E=1,N=7168,device_name=NVIDIA_A100-SXM4-80GB.json\nnew file mode 100644\nindex 000000000..0a685439b\n--- /dev/null\n+++ b/python/sglang/srt/layers/moe/fused_moe_triton/configs/triton_3_1_0/E=1,N=7168,device_name=NVIDIA_A100-SXM4-80GB.json\n@@ -0,0 +1,147 @@\n+{\n+  \"1\": {\n+    \"BLOCK_SIZE_M\": 16,\n+    \"BLOCK_SIZE_N\": 64,\n+    \"BLOCK_SIZE_K\": 256,\n+    \"GROUP_SIZE_M\": 16,\n+    \"num_warps\": 4,\n+    \"num_stages\": 4\n+  },\n+  \"2\": {\n+    \"BLOCK_SIZE_M\": 16,\n+    \"BLOCK_SIZE_N\": 64,\n+    \"BLOCK_SIZE_K\": 256,\n+    \"GROUP_SIZE_M\": 32,\n+    \"num_warps\": 4,\n+    \"num_stages\": 4\n+  },\n+  \"4\": {\n+    \"BLOCK_SIZE_M\": 16,\n+    \"BLOCK_SIZE_N\": 64,\n+    \"BLOCK_SIZE_K\": 256,\n+    \"GROUP_SIZE_M\": 16,\n+    \"num_warps\": 4,\n+    \"num_stages\": 4\n+  },\n+  \"8\": {\n+    \"BLOCK_SIZE_M\": 16,\n+    \"BLOCK_SIZE_N\": 64,\n+    \"BLOCK_SIZE_K\": 256,\n+    \"GROUP_SIZE_M\": 32,\n+    \"num_warps\": 4,\n+    \"num_stages\": 4\n+  },\n+  \"16\": {\n+    \"BLOCK_SIZE_M\": 16,\n+    \"BLOCK_SIZE_N\": 64,\n+    \"BLOCK_SIZE_K\": 256,\n+    \"GROUP_SIZE_M\": 64,\n+    \"num_warps\": 4,\n+    \"num_stages\": 4\n+  },\n+  \"24\": {\n+    \"BLOCK_SIZE_M\": 32,\n+    \"BLOCK_SIZE_N\": 64,\n+    \"BLOCK_SIZE_K\": 128,\n+    \"GROUP_SIZE_M\": 16,\n+    \"num_warps\": 4,\n+    \"num_stages\": 5\n+  },\n+  \"32\": {\n+    \"BLOCK_SIZE_M\": 32,\n+    \"BLOCK_SIZE_N\": 64,\n+    \"BLOCK_SIZE_K\": 128,\n+    \"GROUP_SIZE_M\": 1,\n+    \"num_warps\": 4,\n+    \"num_stages\": 5\n+  },\n+  \"48\": {\n+    \"BLOCK_SIZE_M\": 32,\n+    \"BLOCK_SIZE_N\": 64,\n+    \"BLOCK_SIZE_K\": 64,\n+    \"GROUP_SIZE_M\": 16,\n+    \"num_warps\": 4,\n+    \"num_stages\": 5\n+  },\n+  \"64\": {\n+    \"BLOCK_SIZE_M\": 64,\n+    \"BLOCK_SIZE_N\": 64,\n+    \"BLOCK_SIZE_K\": 128,\n+    \"GROUP_SIZE_M\": 1,\n+    \"num_warps\": 4,\n+    \"num_stages\": 4\n+  },\n+  \"96\": {\n+    \"BLOCK_SIZE_M\": 32,\n+    \"BLOCK_SIZE_N\": 256,\n+    \"BLOCK_SIZE_K\": 64,\n+    \"GROUP_SIZE_M\": 16,\n+    \"num_warps\": 4,\n+    \"num_stages\": 5\n+  },\n+  \"128\": {\n+    \"BLOCK_SIZE_M\": 64,\n+    \"BLOCK_SIZE_N\": 128,\n+    \"BLOCK_SIZE_K\": 64,\n+    \"GROUP_SIZE_M\": 64,\n+    \"num_warps\": 4,\n+    \"num_stages\": 4\n+  },\n+  \"256\": {\n+    \"BLOCK_SIZE_M\": 128,\n+    \"BLOCK_SIZE_N\": 128,\n+    \"BLOCK_SIZE_K\": 64,\n+    \"GROUP_SIZE_M\": 16,\n+    \"num_warps\": 8,\n+    \"num_stages\": 4\n+  },\n+  \"512\": {\n+    \"BLOCK_SIZE_M\": 128,\n+    \"BLOCK_SIZE_N\": 128,\n+    \"BLOCK_SIZE_K\": 64,\n+    \"GROUP_SIZE_M\": 64,\n+    \"num_warps\": 4,\n+    \"num_stages\": 3\n+  },\n+  \"1024\": {\n+    \"BLOCK_SIZE_M\": 128,\n+    \"BLOCK_SIZE_N\": 128,\n+    \"BLOCK_SIZE_K\": 64,\n+    \"GROUP_SIZE_M\": 64,\n+    \"num_warps\": 4,\n+    \"num_stages\": 3\n+  },\n+  \"1536\": {\n+    \"BLOCK_SIZE_M\": 128,\n+    \"BLOCK_SIZE_N\": 128,\n+    \"BLOCK_SIZE_K\": 64,\n+    \"GROUP_SIZE_M\": 16,\n+    \"num_warps\": 4,\n+    \"num_stages\": 3\n+  },\n+  \"2048\": {\n+    \"BLOCK_SIZE_M\": 128,\n+    \"BLOCK_SIZE_N\": 128,\n+    \"BLOCK_SIZE_K\": 64,\n+    \"GROUP_SIZE_M\": 32,\n+    \"num_warps\": 4,\n+    \"num_stages\": 3\n+  },\n+  \"3072\": {\n+    \"BLOCK_SIZE_M\": 128,\n+    \"BLOCK_SIZE_N\": 128,\n+    \"BLOCK_SIZE_K\": 64,\n+    \"GROUP_SIZE_M\": 16,\n+    \"num_warps\": 4,\n+    \"num_stages\": 3\n+  },\n+  \"4096\": {\n+    \"BLOCK_SIZE_M\": 128,\n+    \"BLOCK_SIZE_N\": 128,\n+    \"BLOCK_SIZE_K\": 64,\n+    \"GROUP_SIZE_M\": 16,\n+    \"num_warps\": 4,\n+    \"num_stages\": 3\n+  }\n+}\n+\n", "model_name_or_path": "gpt-5-2025-08-07"}
