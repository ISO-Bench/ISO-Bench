diff --git a/examples/usage/json_logprobs.py b/examples/usage/json_logprobs.py
new file mode 100644
index 000000000..5312fa4ee
--- /dev/null
+++ b/examples/usage/json_logprobs.py
@@ -0,0 +1,59 @@
+"""
+Example: Requesting top-k logprobs for regex-constrained JSON output via HTTP.
+
+Usage:
+  python -m sglang.launch_server --model-path meta-llama/Llama-2-7b-chat-hf --port 30000
+  python examples/usage/json_logprobs.py
+"""
+import json
+from concurrent.futures import ThreadPoolExecutor
+
+from sglang.utils import http_request
+
+# Reuse the regex from the JSON decode example
+try:
+    from examples.usage.json_decode import character_regex  # type: ignore
+except Exception:
+    # Fallback minimal regex
+    character_regex = r"\{\n\s*\"name\": \"[\\w\\d\\s]{1,16}\"\n\}"
+
+
+character_names = ["Hermione Granger", "Ron Weasley", "Harry Potter"]
+
+base_url = "http://localhost:30000"
+
+prompt = (
+    "is a character in Harry Potter. Please fill in the following information about this character.\n"
+)
+
+
+def api_request(name):
+    data = {
+        "model": "",
+        "prompt": name + " " + prompt,
+        "temperature": 0,
+        "max_tokens": 128,
+        "regex": character_regex,
+        "logprobs": 3,
+    }
+    r = http_request(base_url + "/generate/", json=data)
+    assert r.status_code == 200, f"HTTP {r.status_code}"
+    out = r.json()
+    print(f"\n=== {name} ===\n{text_with_logprobs(out)}\n")
+
+
+def text_with_logprobs(resp):
+    # Pretty print the text and a few logprobs entries
+    text = resp.get("text", "")
+    lps = resp.get("logprobs", {})
+    tops = lps.get("top_logprobs", [])
+    san = []
+    for i, step in enumerate(tops[:5]):
+        san.append(f"step[{i}]: " + ", ".join([f"{tok}:{p:.2f}" for tok, p in step]))
+    return text + "\n" + "\n".join(san)
+
+
+if __name__ == "__main__":
+    with ThreadPoolExecutor(len(character_names)) as ex:
+        list(ex.map(api_request, character_names))
+
diff --git a/python/sglang/srt/layers/logits_processor.py b/python/sglang/srt/layers/logits_processor.py
index 53d6620e9..4ca64a077 100644
--- a/python/sglang/srt/layers/logits_processor.py
+++ b/python/sglang/srt/layers/logits_processor.py
@@ -38,13 +38,23 @@ class LogitsProcessor(nn.Module):
 
     def _get_top_logprobs(self, all_logprobs, input_metadata: InputMetadata):
         if input_metadata.forward_mode == ForwardMode.DECODE:
+            # Batched top-k to reduce per-row kernel launches.
+            top_ks = input_metadata.top_logprobs_nums
+            max_k = max(top_ks) if len(top_ks) > 0 else 0
             decode_top_logprobs = []
-            for i in range(all_logprobs.shape[0]):
-                k = input_metadata.top_logprobs_nums[i]
-                t = all_logprobs[i].topk(k)
-                v_cpu = t.values.tolist()
-                p_cpu = t.indices.tolist()
-                decode_top_logprobs.append(list(zip(v_cpu, p_cpu)))
+            if max_k == 0:
+                decode_top_logprobs = [[] for _ in range(all_logprobs.shape[0])]
+            else:
+                t = all_logprobs.topk(max_k, dim=-1)
+                vs = t.values
+                ps = t.indices
+                for i, k in enumerate(top_ks):
+                    if k == 0:
+                        decode_top_logprobs.append([])
+                        continue
+                    v_cpu = vs[i, :k].tolist()
+                    p_cpu = ps[i, :k].tolist()
+                    decode_top_logprobs.append(list(zip(v_cpu, p_cpu)))
             return None, decode_top_logprobs
         else:
             prefill_top_logprobs, decode_top_logprobs = [], []
diff --git a/python/sglang/srt/managers/router/infer_batch.py b/python/sglang/srt/managers/router/infer_batch.py
index dbe94371b..680df6934 100644
--- a/python/sglang/srt/managers/router/infer_batch.py
+++ b/python/sglang/srt/managers/router/infer_batch.py
@@ -304,6 +304,8 @@ class Batch:
             dtype=torch.float,
             device=device,
         ).view(-1, 1)
+        # Precompute inverse temperatures to replace expensive division
+        self.inv_temperatures = self.temperatures.reciprocal()
         self.top_ps = torch.tensor(
             [r.sampling_params.top_p for r in reqs], dtype=torch.float, device=device
         ).view(-1, 1)
@@ -507,13 +509,19 @@ class Batch:
     def sample(self, logits: torch.Tensor):
         # Post process logits
         logits = logits.contiguous()
-        logits.div_(self.temperatures)
+        # Use multiplication with reciprocal instead of division for speed
+        logits.mul_(self.inv_temperatures)
         if self.logit_bias is not None:
             logits.add_(self.logit_bias)
 
         has_regex = any(req.regex_fsm is not None for req in self.reqs)
         if has_regex:
-            allowed_mask = torch.empty_like(logits[0], dtype=torch.bool)
+            # Reuse a persistent mask buffer to avoid reallocations
+            buf = getattr(self, "_allowed_mask_buf", None)
+            if buf is None or buf.shape != logits[0].shape or buf.device != logits.device:
+                buf = torch.empty_like(logits[0], dtype=torch.bool)
+                self._allowed_mask_buf = buf
+            allowed_mask = buf
             for i, req in enumerate(self.reqs):
                 if req.regex_fsm is not None:
                     allowed_mask.zero_()
diff --git a/python/sglang/srt/managers/router/model_rpc.py b/python/sglang/srt/managers/router/model_rpc.py
index 6abb20b25..1a1a3f95f 100644
--- a/python/sglang/srt/managers/router/model_rpc.py
+++ b/python/sglang/srt/managers/router/model_rpc.py
@@ -100,8 +100,11 @@ class ModelRpcServer:
                 else server_args.max_prefill_num_token
             ),
         )
+        # Keep integer-token logit bias on GPU to avoid repeated host->device copies
         self.int_token_logit_bias = torch.tensor(
-            get_int_token_logit_bias(self.tokenizer, self.model_config.vocab_size)
+            get_int_token_logit_bias(self.tokenizer, self.model_config.vocab_size),
+            dtype=torch.float32,
+            device="cuda",
         )
         set_random_seed(server_args.random_seed)
 
