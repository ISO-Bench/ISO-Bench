{"instance_id": "/home/ubuntu/OmniPerf-Bench/sglan__repo-36fc926", "model_patch": "diff --git a/python/sglang/srt/layers/moe/ep_moe/layer.py b/python/sglang/srt/layers/moe/ep_moe/layer.py\nindex 66fbb36ea..ccee17478 100644\n--- a/python/sglang/srt/layers/moe/ep_moe/layer.py\n+++ b/python/sglang/srt/layers/moe/ep_moe/layer.py\n@@ -502,8 +502,8 @@ class DeepEPMoE(EPMoE):\n                 dtype=hidden_states_fp8.dtype,\n             ),\n             (\n-                # TODO check whether need `zeros`\n-                torch.zeros(\n+                # Allocate without initialization; ep_scatter writes full buffer\n+                torch.empty(\n                     (ceil_div(K // 128, 4), all_tokens),\n                     device=hidden_states_fp8.device,\n                     dtype=torch.int,\ndiff --git a/python/sglang/srt/layers/moe/fused_moe_triton/layer.py b/python/sglang/srt/layers/moe/fused_moe_triton/layer.py\nindex d0a9ed132..efe99e6bf 100644\n--- a/python/sglang/srt/layers/moe/fused_moe_triton/layer.py\n+++ b/python/sglang/srt/layers/moe/fused_moe_triton/layer.py\n@@ -127,9 +127,6 @@ class FusedMoE(torch.nn.Module):\n             self.expert_map_cpu = torch.full(\n                 (self.num_experts,), -1, dtype=torch.int32, device=\"cpu\"\n             )\n-            self.expert_map_cpu = torch.full(\n-                (self.num_experts,), -1, dtype=torch.int32, device=\"cpu\"\n-            )\n             # Create a expert map for the local experts\n             self.expert_map_cpu[\n                 self.moe_ep_rank\ndiff --git a/python/sglang/srt/managers/schedule_batch.py b/python/sglang/srt/managers/schedule_batch.py\nindex 03faea684..d98b6a145 100644\n--- a/python/sglang/srt/managers/schedule_batch.py\n+++ b/python/sglang/srt/managers/schedule_batch.py\n@@ -1099,14 +1099,16 @@ class ScheduleBatch(ScheduleBatchDisaggregationDecodeMixin):\n         )\n \n         if not decoder_out_cache_loc:\n-            self.out_cache_loc = torch.zeros(0, dtype=torch.int64).to(\n+            # Avoid unnecessary zero-initialization for empty tensors\n+            self.out_cache_loc = torch.empty(0, dtype=torch.int64).to(\n                 self.device, non_blocking=True\n             )\n         else:\n             self.out_cache_loc = torch.cat(decoder_out_cache_loc)\n \n         if not encoder_out_cache_loc:\n-            self.encoder_out_cache_loc = torch.zeros(0, dtype=torch.int64).to(\n+            # Avoid unnecessary zero-initialization for empty tensors\n+            self.encoder_out_cache_loc = torch.empty(0, dtype=torch.int64).to(\n                 self.device, non_blocking=True\n             )\n         else:\ndiff --git a/python/sglang/srt/models/deepseek_v2.py b/python/sglang/srt/models/deepseek_v2.py\nindex b5b13d9ac..16e715ab9 100644\n--- a/python/sglang/srt/models/deepseek_v2.py\n+++ b/python/sglang/srt/models/deepseek_v2.py\n@@ -596,8 +596,9 @@ class DeepseekV2MoE(nn.Module):\n                 ),\n             )\n         else:\n-            topk_idx = torch.full(\n-                (0, self.top_k), -1, dtype=torch.int, device=hidden_states.device\n+            # Use empty tensors for zero-token case to avoid unnecessary fills\n+            topk_idx = torch.empty(\n+                (0, self.top_k), dtype=torch.int, device=hidden_states.device\n             )\n             topk_weights = torch.empty(\n                 (0, self.top_k), dtype=torch.float32, device=hidden_states.device\n@@ -660,8 +661,9 @@ class DeepseekV2MoE(nn.Module):\n                     ),\n                 )\n         else:\n-            state.topk_idx_local = torch.full(\n-                (0, self.top_k), -1, dtype=torch.int, device=hidden_states.device\n+            # Use empty tensors for zero-token case to avoid unnecessary fills\n+            state.topk_idx_local = torch.empty(\n+                (0, self.top_k), dtype=torch.int, device=hidden_states.device\n             )\n             state.topk_weights_local = torch.empty(\n                 (0, self.top_k), dtype=torch.float32, device=hidden_states.device\n", "model_name_or_path": "gpt-5-2025-08-07"}
