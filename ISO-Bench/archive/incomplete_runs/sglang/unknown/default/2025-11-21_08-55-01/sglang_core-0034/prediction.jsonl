{"instance_id": "/home/ubuntu/OmniPerf-Bench/sglan__repo-8609e63", "model_patch": "diff --git a/python/sglang/srt/managers/scheduler.py b/python/sglang/srt/managers/scheduler.py\nindex 8253a303b..d5ca35a68 100644\n--- a/python/sglang/srt/managers/scheduler.py\n+++ b/python/sglang/srt/managers/scheduler.py\n@@ -509,6 +509,20 @@ class Scheduler(\n         )\n         self.init_disaggregation()\n \n+        # Cached small tensors to reduce per-step allocations\n+        self._coord_spec_local_info: Optional[torch.Tensor] = None  # shape: (1,)\n+        self._coord_spec_global_info: Optional[torch.Tensor] = None  # shape: (dp, attn_tp, 1)\n+        self._dp_world_local_info6: Optional[torch.Tensor] = None  # shape: (6,)\n+        self._dp_world_global_info6: Optional[torch.Tensor] = None  # shape: (dp, attn_tp, 6)\n+        self._tp_ready_timeout_info: Optional[torch.Tensor] = None  # shape: (2,)\n+\n+    def _get_or_alloc_tensor(self, attr_name: str, shape: Tuple[int, ...], dtype: torch.dtype) -> torch.Tensor:\n+        t = getattr(self, attr_name, None)\n+        if t is None or t.dtype != dtype or tuple(t.shape) != tuple(shape):\n+            t = torch.empty(shape, dtype=dtype)\n+            setattr(self, attr_name, t)\n+        return t\n+\n     def maybe_sleep_on_idle(self):\n         if self.idle_sleeper is not None:\n             self.idle_sleeper.maybe_sleep()\n@@ -1401,26 +1415,26 @@ class Scheduler(\n \n     def coordinate_spec_dp_attn_batch(self, new_batch: Optional[ScheduleBatch]):\n         \"\"\"Coordinate the DP attention batch.\"\"\"\n+        # If we already have a new batch locally, skip coordination.\n+        if new_batch is not None:\n+            return True\n \n-        local_info = torch.tensor(\n-            [\n-                (new_batch is not None),\n-            ],\n-            dtype=torch.int64,\n-        )\n-        global_info = torch.empty(\n+        # Reuse cached buffers to avoid frequent allocations.\n+        local_info = self._get_or_alloc_tensor(\"_coord_spec_local_info\", (1,), torch.int64)\n+        local_info[0] = 0\n+\n+        global_info = self._get_or_alloc_tensor(\n+            \"_coord_spec_global_info\",\n             (self.server_args.dp_size, self.attn_tp_size, 1),\n-            dtype=torch.int64,\n+            torch.int64,\n         )\n         torch.distributed.all_gather_into_tensor(\n             global_info.flatten(),\n             local_info,\n             group=self.tp_cpu_group,\n         )\n-        any_new_batch = any(\n-            global_info[:, 0, 0].tolist()\n-        )  # Any DP worker has forward batch\n-        return any_new_batch\n+        # Any DP worker has forward batch\n+        return bool(global_info[:, 0, 0].any().item())\n \n     def get_next_batch_to_run(self) -> Optional[ScheduleBatch]:\n         # Merge the prefill batch into the running batch\n@@ -1842,32 +1856,31 @@ class Scheduler(\n \n         tbo_preparer = TboDPAttentionPreparer()\n \n-        local_info = torch.tensor(\n-            [\n-                num_tokens,\n-                can_cuda_graph,\n-                num_tokens_for_logprob,\n-                is_extend_in_batch,\n-                *tbo_preparer.prepare_all_gather(\n-                    local_batch,\n-                    deepep_mode,\n-                    enable_deepep_moe,\n-                    enable_two_batch_overlap,\n-                ),\n-            ],\n-            dtype=torch.int64,\n-        )\n-        global_info = torch.empty(\n-            (dp_size, attn_tp_size, 6),\n-            dtype=torch.int64,\n+        # Reuse small local/global buffers to reduce allocations.\n+        local_info = self._get_or_alloc_tensor(\"_dp_world_local_info6\", (6,), torch.int64)\n+        # Compute two-batch-overlap gather info once.\n+        tbo_0, tbo_1 = tbo_preparer.prepare_all_gather(\n+            local_batch,\n+            deepep_mode,\n+            enable_deepep_moe,\n+            enable_two_batch_overlap,\n         )\n+        local_info[0] = int(num_tokens)\n+        local_info[1] = int(can_cuda_graph)\n+        local_info[2] = int(num_tokens_for_logprob)\n+        local_info[3] = int(is_extend_in_batch)\n+        local_info[4] = int(tbo_0)\n+        local_info[5] = int(tbo_1)\n+\n+        global_info = self._get_or_alloc_tensor(\"_dp_world_global_info6\", (dp_size, attn_tp_size, 6), torch.int64)\n         torch.distributed.all_gather_into_tensor(\n             global_info.flatten(),\n             local_info,\n             group=tp_cpu_group,\n         )\n         global_num_tokens = global_info[:, 0, 0].tolist()\n-        can_cuda_graph = min(global_info[:, 0, 1].tolist())\n+        # Use tensor min to avoid Python list conversion overhead\n+        can_cuda_graph = int(global_info[:, 0, 1].min().item())\n         global_num_tokens_for_logprob = global_info[:, 0, 2].tolist()\n         is_extend_in_batch = global_info[:, 0, 3].tolist()\n \n@@ -1947,7 +1960,9 @@ class Scheduler(\n \n         if tp_size > 1:\n             # Sync across TP ranks to make sure they have the same number of ready requests\n-            tensor = torch.tensor([num_ready_reqs, num_timeout_reqs], dtype=torch.int32)\n+            tensor = self._get_or_alloc_tensor(\"_tp_ready_timeout_info\", (2,), torch.int32)\n+            tensor[0] = int(num_ready_reqs)\n+            tensor[1] = int(num_timeout_reqs)\n             torch.distributed.all_reduce(\n                 tensor, op=torch.distributed.ReduceOp.MAX, group=tp_group\n             )\n", "model_name_or_path": "gpt-5-2025-08-07"}
