{"instance_id": "/home/ubuntu/OmniPerf-Bench/sglan__repo-8879944", "model_patch": "diff --git a/python/sglang/srt/models/llama4.py b/python/sglang/srt/models/llama4.py\nindex d933f27ae..789985c9d 100644\n--- a/python/sglang/srt/models/llama4.py\n+++ b/python/sglang/srt/models/llama4.py\n@@ -243,6 +243,16 @@ class Llama4Attention(nn.Module):\n \n         return attn_scale.unsqueeze(-1)\n \n+    @torch.compile(dynamic=True, backend=get_compiler_backend())\n+    def _mul_attn_scale(self, positions: torch.Tensor, q: torch.Tensor) -> torch.Tensor:\n+        \"\"\"Fuse attention-scale computation and multiplication.\n+\n+        This reduces intermediate allocations and lets the compiler\n+        generate a fused kernel for the temperature tuning path.\n+        \"\"\"\n+        attn_scale = self._get_attn_scale(positions)\n+        return (q * attn_scale).to(q.dtype)\n+\n     def forward(\n         self,\n         positions: torch.Tensor,\n@@ -250,10 +260,16 @@ class Llama4Attention(nn.Module):\n         forward_batch: ForwardBatch,\n     ) -> torch.Tensor:\n         qkv, _ = self.qkv_proj(hidden_states)\n-        q, k, v = qkv.split([self.q_size, self.kv_size, self.kv_size], dim=-1)\n+        # Split once to get (q+k) and v to reduce overhead from 3-way split.\n+        qk, v = qkv.split([self.q_size + self.kv_size, self.kv_size], dim=-1)\n \n         if self.rotary_emb is not None:\n-            q, k = self.rotary_emb(positions, q, k)\n+            # Defer splitting q/k until needed to minimize ops and let\n+            # rotary embedding operate on views.\n+            q_view, k_view = qk.split([self.q_size, self.kv_size], dim=-1)\n+            q, k = self.rotary_emb(positions, q_view, k_view)\n+        else:\n+            q, k = qk.split([self.q_size, self.kv_size], dim=-1)\n \n         if self.qk_norm is not None:\n             # TODO: support float\n@@ -269,8 +285,8 @@ class Llama4Attention(nn.Module):\n         # while working at very long context\n         # https://arxiv.org/abs/2501.19399\n         if self.attn_temperature_tuning and not self.use_rope:\n-            attn_scale = self._get_attn_scale(positions)\n-            q = (q * attn_scale).to(q.dtype)\n+            # Use compiled fused path to compute and apply the scale.\n+            q = self._mul_attn_scale(positions, q)\n \n         attn_output = self.attn(q, k, v, forward_batch)\n         output, _ = self.o_proj(attn_output)\n", "model_name_or_path": "gpt-5-2025-08-07"}
