{"instance_id": "/home/ubuntu/OmniPerf-Bench/sglan__repo-4c60523", "model_patch": "diff --git a/python/sglang/srt/mem_cache/allocator.py b/python/sglang/srt/mem_cache/allocator.py\nindex 7dd488e9c..f039740bf 100644\n--- a/python/sglang/srt/mem_cache/allocator.py\n+++ b/python/sglang/srt/mem_cache/allocator.py\n@@ -51,6 +51,8 @@ class BaseTokenToKVPoolAllocator(abc.ABC):\n         self._kvcache = kvcache\n \n         self.free_pages = None\n+        # Staging buffer for freed pages to avoid frequent cat() on the hot path\n+        self.release_pages = None\n         self.is_not_in_free_group = True\n         self.free_group = []\n \n@@ -58,7 +60,10 @@ class BaseTokenToKVPoolAllocator(abc.ABC):\n         return \"\"\n \n     def available_size(self):\n-        return len(self.free_pages) * self.page_size\n+        # Count both immediate free pages and staged release pages\n+        n_free = len(self.free_pages) if self.free_pages is not None else 0\n+        n_rel = len(self.release_pages) if self.release_pages is not None else 0\n+        return (n_free + n_rel) * self.page_size\n \n     def get_kvcache(self):\n         return self._kvcache\n@@ -77,6 +82,23 @@ class BaseTokenToKVPoolAllocator(abc.ABC):\n         self.is_not_in_free_group = True\n         if self.free_group:\n             self.free(torch.cat(self.free_group))\n+            self.free_group = []\n+\n+    def _merge_release_pages(self, *, sort: bool = False):\n+        \"\"\"Merge staged release_pages into free_pages.\n+        Optionally sort the merged pages for better locality.\n+        \"\"\"\n+        if self.release_pages is None or len(self.release_pages) == 0:\n+            return\n+        if self.free_pages is None or len(self.free_pages) == 0:\n+            merged = self.release_pages\n+        else:\n+            merged = torch.cat((self.free_pages, self.release_pages))\n+        if sort:\n+            merged, _ = torch.sort(merged)\n+        self.free_pages = merged\n+        # reset release_pages buffer\n+        self.release_pages = self.release_pages[:0]\n \n     def get_cpu_copy(self, *args, **kwargs):\n         # FIXME: reuse the get_cpu_copy after paged allocator is implemented\n@@ -117,16 +139,26 @@ class TokenToKVPoolAllocator(BaseTokenToKVPoolAllocator):\n         self.free_pages = torch.arange(\n             1, self.size + 1, dtype=torch.int64, device=self.device\n         )\n+        self.release_pages = torch.empty(0, dtype=torch.int64, device=self.device)\n         self.is_not_in_free_group = True\n         self.free_group = []\n \n     def available_size(self):\n-        # To avoid minor \"len(free_pages) * 1\" overhead\n-        return len(self.free_pages)\n+        # Fast path for token granularity (page_size==1)\n+        n_free = len(self.free_pages)\n+        n_rel = len(self.release_pages) if self.release_pages is not None else 0\n+        return n_free + n_rel\n \n     def alloc(self, need_size: int):\n         if need_size > len(self.free_pages):\n-            return None\n+            # Try to reclaim staged pages lazily\n+            total = len(self.free_pages) + (\n+                len(self.release_pages) if self.release_pages is not None else 0\n+            )\n+            if need_size <= total:\n+                self._merge_release_pages(sort=True)\n+            else:\n+                return None\n \n         select_index = self.free_pages[:need_size]\n         self.free_pages = self.free_pages[need_size:]\n@@ -137,7 +169,11 @@ class TokenToKVPoolAllocator(BaseTokenToKVPoolAllocator):\n             return\n \n         if self.is_not_in_free_group:\n-            self.free_pages = torch.cat((self.free_pages, free_index))\n+            # Stage freed indices to avoid frequent reallocations\n+            if self.release_pages is None or len(self.release_pages) == 0:\n+                self.release_pages = free_index\n+            else:\n+                self.release_pages = torch.cat((self.release_pages, free_index))\n         else:\n             self.free_group.append(free_index)\n \n@@ -255,7 +291,7 @@ class SWATokenToKVPoolAllocator(BaseTokenToKVPoolAllocator):\n     def clear(self):\n         self.swa_attn_allocator.clear()\n         self.full_attn_allocator.clear()\n-        self.full_to_swa_index_mapping.fill_(0)\n+        self.full_to_swa_index_mapping.zero_()\n         self.is_in_free_group = False\n         self.free_group = []\n \n@@ -411,6 +447,8 @@ class PagedTokenToKVPoolAllocator(BaseTokenToKVPoolAllocator):\n         self.num_pages = size // page_size\n         self.debug_mode = get_bool_env_var(\"SGLANG_DEBUG_MEMORY_POOL\")\n         self.ret_values = torch.empty((), dtype=torch.int64, device=self.device)\n+        # cached page offsets to avoid repeated arange allocations\n+        self._page_offset = torch.arange(self.page_size, device=self.device)\n         self.clear()\n \n     def alloc(self, need_size: int):\n@@ -422,15 +460,19 @@ class PagedTokenToKVPoolAllocator(BaseTokenToKVPoolAllocator):\n \n         num_pages = need_size // self.page_size\n         if num_pages > len(self.free_pages):\n-            return None\n+            # Lazy merge staged pages (sorted for better locality)\n+            total = len(self.free_pages) + (\n+                len(self.release_pages) if self.release_pages is not None else 0\n+            )\n+            if num_pages <= total:\n+                self._merge_release_pages(sort=True)\n+            else:\n+                return None\n \n         out_pages = self.free_pages[:num_pages]\n         self.free_pages = self.free_pages[num_pages:]\n \n-        out_indices = (\n-            out_pages[:, None] * self.page_size\n-            + torch.arange(self.page_size, device=self.device)\n-        ).reshape(-1)\n+        out_indices = (out_pages[:, None] * self.page_size + self._page_offset).reshape(-1)\n \n         return out_indices\n \n@@ -450,6 +492,8 @@ class PagedTokenToKVPoolAllocator(BaseTokenToKVPoolAllocator):\n         out_indices = torch.empty(\n             (extend_num_tokens,), dtype=torch.int64, device=self.device\n         )\n+        # Maximize available pages before kernel reads from free_pages\n+        self._merge_release_pages()\n         alloc_extend_kernel[(bs,)](\n             prefix_lens,\n             seq_lens,\n@@ -468,7 +512,13 @@ class PagedTokenToKVPoolAllocator(BaseTokenToKVPoolAllocator):\n         merged_value = self.ret_values.item()\n         num_new_pages = merged_value >> 32\n         if num_new_pages > len(self.free_pages):\n-            return None\n+            # Try merging staged pages if available\n+            total = len(self.free_pages) + (\n+                len(self.release_pages) if self.release_pages is not None else 0\n+            )\n+            if num_new_pages > total:\n+                return None\n+            self._merge_release_pages(sort=False)\n \n         self.free_pages = self.free_pages[num_new_pages:]\n         return out_indices\n@@ -485,6 +535,8 @@ class PagedTokenToKVPoolAllocator(BaseTokenToKVPoolAllocator):\n \n         bs = len(seq_lens)\n         out_indices = torch.empty((bs,), dtype=torch.int64, device=self.device)\n+        # Maximize available pages before kernel reads from free_pages\n+        self._merge_release_pages()\n         alloc_decode_kernel[(bs,)](\n             seq_lens,\n             last_loc,\n@@ -500,7 +552,12 @@ class PagedTokenToKVPoolAllocator(BaseTokenToKVPoolAllocator):\n \n         num_new_pages = self.ret_values.item()\n         if num_new_pages > len(self.free_pages):\n-            return None\n+            total = len(self.free_pages) + (\n+                len(self.release_pages) if self.release_pages is not None else 0\n+            )\n+            if num_new_pages > total:\n+                return None\n+            self._merge_release_pages(sort=False)\n \n         self.free_pages = self.free_pages[num_new_pages:]\n         return out_indices\n@@ -511,18 +568,27 @@ class PagedTokenToKVPoolAllocator(BaseTokenToKVPoolAllocator):\n \n         if self.is_not_in_free_group:\n             free_page_indices = torch.unique(free_index // self.page_size)\n-            self.free_pages = torch.cat((free_page_indices, self.free_pages))\n+            if self.release_pages is None or len(self.release_pages) == 0:\n+                self.release_pages = free_page_indices\n+            else:\n+                self.release_pages = torch.cat((self.release_pages, free_page_indices))\n         else:\n             self.free_group.append(free_index)\n \n         if self.debug_mode:\n-            assert len(torch.unique(self.free_pages)) == len(self.free_pages)\n+            combined = (\n+                self.free_pages\n+                if self.release_pages is None or len(self.release_pages) == 0\n+                else torch.cat((self.free_pages, self.release_pages))\n+            )\n+            assert len(torch.unique(combined)) == len(combined)\n \n     def clear(self):\n         # The padded slot 0 is used for writing dummy outputs from padded tokens.\n         self.free_pages = torch.arange(\n             1, self.num_pages + 1, dtype=torch.int64, device=self.device\n         )\n+        self.release_pages = torch.empty(0, dtype=torch.int64, device=self.device)\n         self.is_not_in_free_group = True\n         self.free_group = []\n \n@@ -692,3 +758,4 @@ class AscendPagedTokenToKVPoolAllocator(PagedTokenToKVPoolAllocator):\n     def clear(self):\n         super().clear()\n         self.free_pages = self.free_pages.to(torch.int32)\n+        self.release_pages = self.release_pages.to(torch.int32)\n", "model_name_or_path": "gpt-5-2025-08-07"}
