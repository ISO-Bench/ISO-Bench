diff --git a/python/sglang/srt/mem_cache/allocator.py b/python/sglang/srt/mem_cache/allocator.py
index 7dd488e9c..f039740bf 100644
--- a/python/sglang/srt/mem_cache/allocator.py
+++ b/python/sglang/srt/mem_cache/allocator.py
@@ -51,6 +51,8 @@ class BaseTokenToKVPoolAllocator(abc.ABC):
         self._kvcache = kvcache
 
         self.free_pages = None
+        # Staging buffer for freed pages to avoid frequent cat() on the hot path
+        self.release_pages = None
         self.is_not_in_free_group = True
         self.free_group = []
 
@@ -58,7 +60,10 @@ class BaseTokenToKVPoolAllocator(abc.ABC):
         return ""
 
     def available_size(self):
-        return len(self.free_pages) * self.page_size
+        # Count both immediate free pages and staged release pages
+        n_free = len(self.free_pages) if self.free_pages is not None else 0
+        n_rel = len(self.release_pages) if self.release_pages is not None else 0
+        return (n_free + n_rel) * self.page_size
 
     def get_kvcache(self):
         return self._kvcache
@@ -77,6 +82,23 @@ class BaseTokenToKVPoolAllocator(abc.ABC):
         self.is_not_in_free_group = True
         if self.free_group:
             self.free(torch.cat(self.free_group))
+            self.free_group = []
+
+    def _merge_release_pages(self, *, sort: bool = False):
+        """Merge staged release_pages into free_pages.
+        Optionally sort the merged pages for better locality.
+        """
+        if self.release_pages is None or len(self.release_pages) == 0:
+            return
+        if self.free_pages is None or len(self.free_pages) == 0:
+            merged = self.release_pages
+        else:
+            merged = torch.cat((self.free_pages, self.release_pages))
+        if sort:
+            merged, _ = torch.sort(merged)
+        self.free_pages = merged
+        # reset release_pages buffer
+        self.release_pages = self.release_pages[:0]
 
     def get_cpu_copy(self, *args, **kwargs):
         # FIXME: reuse the get_cpu_copy after paged allocator is implemented
@@ -117,16 +139,26 @@ class TokenToKVPoolAllocator(BaseTokenToKVPoolAllocator):
         self.free_pages = torch.arange(
             1, self.size + 1, dtype=torch.int64, device=self.device
         )
+        self.release_pages = torch.empty(0, dtype=torch.int64, device=self.device)
         self.is_not_in_free_group = True
         self.free_group = []
 
     def available_size(self):
-        # To avoid minor "len(free_pages) * 1" overhead
-        return len(self.free_pages)
+        # Fast path for token granularity (page_size==1)
+        n_free = len(self.free_pages)
+        n_rel = len(self.release_pages) if self.release_pages is not None else 0
+        return n_free + n_rel
 
     def alloc(self, need_size: int):
         if need_size > len(self.free_pages):
-            return None
+            # Try to reclaim staged pages lazily
+            total = len(self.free_pages) + (
+                len(self.release_pages) if self.release_pages is not None else 0
+            )
+            if need_size <= total:
+                self._merge_release_pages(sort=True)
+            else:
+                return None
 
         select_index = self.free_pages[:need_size]
         self.free_pages = self.free_pages[need_size:]
@@ -137,7 +169,11 @@ class TokenToKVPoolAllocator(BaseTokenToKVPoolAllocator):
             return
 
         if self.is_not_in_free_group:
-            self.free_pages = torch.cat((self.free_pages, free_index))
+            # Stage freed indices to avoid frequent reallocations
+            if self.release_pages is None or len(self.release_pages) == 0:
+                self.release_pages = free_index
+            else:
+                self.release_pages = torch.cat((self.release_pages, free_index))
         else:
             self.free_group.append(free_index)
 
@@ -255,7 +291,7 @@ class SWATokenToKVPoolAllocator(BaseTokenToKVPoolAllocator):
     def clear(self):
         self.swa_attn_allocator.clear()
         self.full_attn_allocator.clear()
-        self.full_to_swa_index_mapping.fill_(0)
+        self.full_to_swa_index_mapping.zero_()
         self.is_in_free_group = False
         self.free_group = []
 
@@ -411,6 +447,8 @@ class PagedTokenToKVPoolAllocator(BaseTokenToKVPoolAllocator):
         self.num_pages = size // page_size
         self.debug_mode = get_bool_env_var("SGLANG_DEBUG_MEMORY_POOL")
         self.ret_values = torch.empty((), dtype=torch.int64, device=self.device)
+        # cached page offsets to avoid repeated arange allocations
+        self._page_offset = torch.arange(self.page_size, device=self.device)
         self.clear()
 
     def alloc(self, need_size: int):
@@ -422,15 +460,19 @@ class PagedTokenToKVPoolAllocator(BaseTokenToKVPoolAllocator):
 
         num_pages = need_size // self.page_size
         if num_pages > len(self.free_pages):
-            return None
+            # Lazy merge staged pages (sorted for better locality)
+            total = len(self.free_pages) + (
+                len(self.release_pages) if self.release_pages is not None else 0
+            )
+            if num_pages <= total:
+                self._merge_release_pages(sort=True)
+            else:
+                return None
 
         out_pages = self.free_pages[:num_pages]
         self.free_pages = self.free_pages[num_pages:]
 
-        out_indices = (
-            out_pages[:, None] * self.page_size
-            + torch.arange(self.page_size, device=self.device)
-        ).reshape(-1)
+        out_indices = (out_pages[:, None] * self.page_size + self._page_offset).reshape(-1)
 
         return out_indices
 
@@ -450,6 +492,8 @@ class PagedTokenToKVPoolAllocator(BaseTokenToKVPoolAllocator):
         out_indices = torch.empty(
             (extend_num_tokens,), dtype=torch.int64, device=self.device
         )
+        # Maximize available pages before kernel reads from free_pages
+        self._merge_release_pages()
         alloc_extend_kernel[(bs,)](
             prefix_lens,
             seq_lens,
@@ -468,7 +512,13 @@ class PagedTokenToKVPoolAllocator(BaseTokenToKVPoolAllocator):
         merged_value = self.ret_values.item()
         num_new_pages = merged_value >> 32
         if num_new_pages > len(self.free_pages):
-            return None
+            # Try merging staged pages if available
+            total = len(self.free_pages) + (
+                len(self.release_pages) if self.release_pages is not None else 0
+            )
+            if num_new_pages > total:
+                return None
+            self._merge_release_pages(sort=False)
 
         self.free_pages = self.free_pages[num_new_pages:]
         return out_indices
@@ -485,6 +535,8 @@ class PagedTokenToKVPoolAllocator(BaseTokenToKVPoolAllocator):
 
         bs = len(seq_lens)
         out_indices = torch.empty((bs,), dtype=torch.int64, device=self.device)
+        # Maximize available pages before kernel reads from free_pages
+        self._merge_release_pages()
         alloc_decode_kernel[(bs,)](
             seq_lens,
             last_loc,
@@ -500,7 +552,12 @@ class PagedTokenToKVPoolAllocator(BaseTokenToKVPoolAllocator):
 
         num_new_pages = self.ret_values.item()
         if num_new_pages > len(self.free_pages):
-            return None
+            total = len(self.free_pages) + (
+                len(self.release_pages) if self.release_pages is not None else 0
+            )
+            if num_new_pages > total:
+                return None
+            self._merge_release_pages(sort=False)
 
         self.free_pages = self.free_pages[num_new_pages:]
         return out_indices
@@ -511,18 +568,27 @@ class PagedTokenToKVPoolAllocator(BaseTokenToKVPoolAllocator):
 
         if self.is_not_in_free_group:
             free_page_indices = torch.unique(free_index // self.page_size)
-            self.free_pages = torch.cat((free_page_indices, self.free_pages))
+            if self.release_pages is None or len(self.release_pages) == 0:
+                self.release_pages = free_page_indices
+            else:
+                self.release_pages = torch.cat((self.release_pages, free_page_indices))
         else:
             self.free_group.append(free_index)
 
         if self.debug_mode:
-            assert len(torch.unique(self.free_pages)) == len(self.free_pages)
+            combined = (
+                self.free_pages
+                if self.release_pages is None or len(self.release_pages) == 0
+                else torch.cat((self.free_pages, self.release_pages))
+            )
+            assert len(torch.unique(combined)) == len(combined)
 
     def clear(self):
         # The padded slot 0 is used for writing dummy outputs from padded tokens.
         self.free_pages = torch.arange(
             1, self.num_pages + 1, dtype=torch.int64, device=self.device
         )
+        self.release_pages = torch.empty(0, dtype=torch.int64, device=self.device)
         self.is_not_in_free_group = True
         self.free_group = []
 
@@ -692,3 +758,4 @@ class AscendPagedTokenToKVPoolAllocator(PagedTokenToKVPoolAllocator):
     def clear(self):
         super().clear()
         self.free_pages = self.free_pages.to(torch.int32)
+        self.release_pages = self.release_pages.to(torch.int32)
