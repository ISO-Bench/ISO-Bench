diff --git a/python/sglang/srt/layers/quantization/fp8_utils.py b/python/sglang/srt/layers/quantization/fp8_utils.py
index 0e1640fcf..2f9dbe0cb 100644
--- a/python/sglang/srt/layers/quantization/fp8_utils.py
+++ b/python/sglang/srt/layers/quantization/fp8_utils.py
@@ -332,22 +332,16 @@ def block_quant_to_tensor_quant(
     assert n_tiles == x_s.shape[0]
     assert k_tiles == x_s.shape[1]
 
-    x_dq_block = x_q_block.to(torch.float32)
-
-    x_dq_block_tiles = [
-        [
-            x_dq_block[
-                j * block_n : min((j + 1) * block_n, n),
-                i * block_k : min((i + 1) * block_k, k),
-            ]
-            for i in range(k_tiles)
-        ]
-        for j in range(n_tiles)
-    ]
-
-    for i in range(k_tiles):
-        for j in range(n_tiles):
-            x_dq_block_tiles[j][i][:, :] = x_dq_block_tiles[j][i] * x_s[j][i]
+    # Vectorized dequantization: expand block scales to elementwise scales
+    # Shape: (n, k)
+    scale_expanded = (
+        x_s.repeat_interleave(block_n, dim=0)
+        .repeat_interleave(block_k, dim=1)
+        [:n, :k]
+    )
+
+    # Dequantize in one shot
+    x_dq_block = x_q_block.to(torch.float32) * scale_expanded
 
     x_q_tensor, scale = (
         scaled_fp8_quant(x_dq_block)
@@ -375,21 +369,14 @@ def block_quant_dequant(
     assert n_tiles == x_s.shape[0]
     assert k_tiles == x_s.shape[1]
 
-    x_dq_block = torch.empty_like(x_q_block, dtype=dtype)
-
-    for j in range(n_tiles):
-        for i in range(k_tiles):
-            x_q_block_tile = x_q_block[
-                j * block_n : min((j + 1) * block_n, n),
-                i * block_k : min((i + 1) * block_k, k),
-            ]
-            x_dq_block_tile = x_dq_block[
-                j * block_n : min((j + 1) * block_n, n),
-                i * block_k : min((i + 1) * block_k, k),
-            ]
-            x_dq_block_tile[:, :] = x_q_block_tile.to(torch.float32) * x_s[j][i]
-
-    return x_dq_block
+    # Expand per-block scales to elementwise scales and dequantize vectorially
+    scale_expanded = (
+        x_s.repeat_interleave(block_n, dim=0)
+        .repeat_interleave(block_k, dim=1)
+        [:n, :k]
+    )
+    x_dq = x_q_block.to(torch.float32) * scale_expanded
+    return x_dq.to(dtype)
 
 
 def channel_quant_to_tensor_quant(
