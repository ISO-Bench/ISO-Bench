OpenAI Codex v0.60.1 (research preview)
--------
workdir: /home/ubuntu/OmniPerf-Bench/perf-agents-bench/.work/worktrees/sglang_core/sglang_core-0037
model: gpt-5
provider: openai
approval: never
sandbox: danger-full-access
reasoning effort: high
reasoning summaries: auto
session id: 019aa4d8-6b8e-7522-aeea-f8cb99b56c25
--------
user
I've uploaded a python code repository in the directory /home/ubuntu/OmniPerf-Bench/perf-agents-bench/.work/worktrees/sglang_core/sglang_core-0037.
Consider the following test script showing an example usage of the repository:

<test_script>
# This is a performance optimization task
# The specific operations to optimize are in the files listed below
# Focus on performance improvements in the target functions

</test_script>

Can you help me implement the necessary changes to the repository so that the runtime of the <test_script> is optimized?

Basic guidelines:
1. Your task is to make changes to non-test files in the /home/ubuntu/OmniPerf-Bench/perf-agents-bench/.work/worktrees/sglang_core/sglang_core-0037 directory to improve the performance of the <test_script>.
2. Make changes while ensuring the repository is functionally equivalent to the original.
3. Do not overoptimize for just the specific inputs in <test_script>. Make general performance improvements for the usage scenario shown.
4. You may need to rebuild the repo for your changes to take effect before testing. Some rebuilds may take time to run, so be patient with running them.

Follow these steps to improve performance:
1. As a first step, explore the repository structure.
2. Create a script ONLY inside /home/ubuntu/OmniPerf-Bench/perf-agents-bench/.work/worktrees/sglang_core/sglang_core-0037/.bench_scratch (e.g., /home/ubuntu/OmniPerf-Bench/perf-agents-bench/.work/worktrees/sglang_core/sglang_core-0037/.bench_scratch/test_opt.py) to reproduce and time the example, then execute it with python <filename.py> from the repo root.
3. Edit the source code of the repository to improve performance.
4. Rebuild and rerun your script to confirm that performance has improved.

Here is an example of the kind of optimizations that have been shown to improve performance in this codebase:

<example_optimization_diff>
diff --git a/python/sglang/srt/models/llama4.py b/python/sglang/srt/models/llama4.py
index 4e4ba9a1e..8015c18a0 100644
--- a/python/sglang/srt/models/llama4.py
+++ b/python/sglang/srt/models/llama4.py
@@ -48,7 +48,7 @@ from sglang.srt.layers.vocab_parallel_embedding import VocabParallelEmbedding
 from sglang.srt.managers.schedule_batch import global_server_args_dict
 from sglang.srt.model_executor.forward_batch_info import ForwardBatch
 from sglang.srt.models.llama import LlamaForCausalLM, LlamaMLP
-from sglang.srt.utils import add_prefix, get_compiler_backend, make_layers
+from sglang.srt.utils import add_prefix, fast_topk, get_compiler_backend, make_layers

 logger = logging.getLogger(__name__)

@@ -63,7 +63,7 @@ class Llama4MoE(nn.Module):
         topk: int,
         renormalize: bool,
     ) -> Tuple[torch.Tensor, torch.Tensor]:
-        router_scores_aK, router_indices_aK = torch.topk(gating_output, topk, dim=-1)
+        router_scores_aK, router_indices_aK = fast_topk(gating_output, topk, dim=-1)
         router_scores_aK = torch.sigmoid(router_scores_aK.float()).to(
             hidden_states.dtype
         )
diff --git a/python/sglang/srt/speculative/eagle_utils.py b/python/sglang/srt/speculative/eagle_utils.py
index 19fa1807c..10c9e54c2 100644
--- a/python/sglang/srt/speculative/eagle_utils.py
+++ b/python/sglang/srt/speculative/eagle_utils.py
@@ -19,7 +19,7 @@ from sglang.srt.managers.schedule_batch import (
 from sglang.srt.mem_cache.memory_pool import TokenToKVPoolAllocator
 from sglang.srt.model_executor.forward_batch_info import CaptureHiddenMode
 from sglang.srt.speculative.build_eagle_tree import build_tree_kernel_efficient
</example_optimization_diff>

IMPORTANT: The above diff is an EXAMPLE of optimizations that were successful in a different context.
These changes have NOT been applied to your codebase yet.
Your task is to:
1. Understand the optimization pattern shown (e.g., torch.zeros → torch.empty)
2. Look at the CURRENT code in the target files
3. Find places where you can apply SIMILAR optimizations
4. MAKE THE CHANGES yourself using str_replace_editor

The codebase you're working with is at the BASE commit - it does NOT have these optimizations yet.
You need to IMPLEMENT similar optimizations yourself.

HERE'S WHAT YOU NEED TO DO:
1. The files CURRENTLY contain torch.zeros() calls that need optimization
2. You need to CHANGE torch.zeros to torch.empty where appropriate
3. You need to REMOVE .fill_() operations that are unnecessary
4. These are NEW changes you're making - not already in the code

START WITH THIS COMMAND to see what needs changing:
```bash
grep -n 'torch.zeros\|fill_' vllm/model_executor/layers/fused_moe/moe_align_block_size.py benchmarks/kernels/benchmark_moe_align_block_size.py
```

CRITICAL: You MUST make actual code changes. Look for patterns like:
- Analyze the target files for performance bottlenecks
- Look for unnecessary memory allocations or initializations
- Consider more efficient algorithms or data structures

Target files to optimize:
- python/sglang/srt/models/llama4.py
- python/sglang/srt/speculative/eagle_utils.py
- python/sglang/srt/speculative/eagle_worker.py

IMPORTANT: You MUST make actual code changes to at least one file.
The task will fail if no files are modified.

## Constraints
- No public API breakage
- All TestPack checks must pass

## Target Files (ONLY modify these)
- `python/sglang/srt/models/llama4.py`
- `python/sglang/srt/speculative/eagle_utils.py`
- `python/sglang/srt/speculative/eagle_worker.py`
- `python/sglang/srt/utils.py`

## SPECIFIC OPTIMIZATION TARGETS:
Based on the human commit analysis, focus on these areas:
- Memory allocation patterns (torch.zeros vs torch.empty)
- Tensor initialization strategies
- Kernel parameter optimization
- Buffer reuse and caching

### Human Developer's Approach:
```
Optimize topk operation in llama4 (#5128)
```

### Files Modified (statistics):
```
python/sglang/srt/models/llama4.py            |  4 ++--
 python/sglang/srt/speculative/eagle_utils.py  | 12 +-----------
 python/sglang/srt/speculative/eagle_worker.py |  8 ++++++--
 python/sglang/srt/utils.py                    |  9 +++++++++
 4 files changed, 18 insertions(+), 15 deletions(-)
```

## IMMEDIATE ACTION REQUIREMENTS:
1. Start editing files by iteration 3
2. Create and run /home/ubuntu/OmniPerf-Bench/perf-agents-bench/.work/worktrees/sglang_core/sglang_core-0037/.bench_scratch/test_opt.py before and after edits (do not create timing scripts outside /home/ubuntu/OmniPerf-Bench/perf-agents-bench/.work/worktrees/sglang_core/sglang_core-0037/.bench_scratch)
3. Make at least 3 concrete optimizations
4. Commit changes by iteration 8
5. Use finish command by iteration 10

## TASK COMPLETION COMMAND:
When you have made optimizations:
```bash
git add -A
git reset -q .bench_scratch || true
git commit -m 'Optimize MoE align sum kernels performance'
git diff $(git merge-base HEAD origin/HEAD || git rev-parse HEAD~1) -- . ':(exclude).bench_scratch' > /home/ubuntu/OmniPerf-Bench/perf-agents-bench/.work/worktrees/sglang_core/sglang_core-0037/model_patch.diff || true
finish
```

START IMPLEMENTING IMMEDIATELY. NO MORE ANALYSIS.

mcp startup: no servers

thinking
**Assessing test implementation**

I'm thinking about crafting `test_opt` to import files and use functions to exercise `topk` and maybe some Eagle utils. I need to check the file contents for any necessary modifications, so I'll use the shell to list and open the files. They instruct us to run a starting command with grep that includes paths like `vllm/...`, but those might not exist in this repository. I wonder if the "start command" comes from another codebase. Nonetheless, our target files are different, so we'll stick to the overall guidelines. I'll remember to create preamble messages before tool calls.
codex
I’m going to scan the repo structure and open the target Python files to identify spots for top-k and allocation optimizations, then create a quick timing script under .bench_scratch to compare before/after.
exec
bash -lc 'ls -la' in /home/ubuntu/OmniPerf-Bench/perf-agents-bench/.work/worktrees/sglang_core/sglang_core-0037 succeeded in 10ms:
total 112
drwxrwxr-x  15 ubuntu ubuntu  4096 Nov 21 05:17 .
drwxrwxr-x 119 ubuntu ubuntu  4096 Nov 21 05:17 ..
-rw-rw-r--   1 ubuntu ubuntu    35 Nov 21 05:17 .clang-format-ignore
drwxrwxr-x   2 ubuntu ubuntu  4096 Nov 21 05:17 .devcontainer
-rw-rw-r--   1 ubuntu ubuntu   341 Nov 21 05:17 .editorconfig
-rw-rw-r--   1 ubuntu ubuntu    75 Nov 21 05:17 .git
drwxrwxr-x   4 ubuntu ubuntu  4096 Nov 21 05:17 .github
-rw-rw-r--   1 ubuntu ubuntu  3650 Nov 21 05:17 .gitignore
-rw-rw-r--   1 ubuntu ubuntu   158 Nov 21 05:17 .gitmodules
-rw-rw-r--   1 ubuntu ubuntu    50 Nov 21 05:17 .isort.cfg
-rw-rw-r--   1 ubuntu ubuntu  1369 Nov 21 05:17 .pre-commit-config.yaml
drwxrwxr-x   3 ubuntu ubuntu  4096 Nov 21 05:17 3rdparty
-rw-rw-r--   1 ubuntu ubuntu 11346 Nov 21 05:17 LICENSE
-rw-rw-r--   1 ubuntu ubuntu  1720 Nov 21 05:17 Makefile
-rw-rw-r--   1 ubuntu ubuntu  6516 Nov 21 05:17 README.md
drwxrwxr-x   2 ubuntu ubuntu  4096 Nov 21 05:17 assets
drwxrwxr-x  31 ubuntu ubuntu  4096 Nov 21 05:17 benchmark
drwxrwxr-x   2 ubuntu ubuntu  4096 Nov 21 05:17 docker
drwxrwxr-x   9 ubuntu ubuntu  4096 Nov 21 05:17 docs
exec
bash -lc "sed -n '1,200p' python/sglang/srt/models/llama4.py" in /home/ubuntu/OmniPerf-Bench/perf-agents-bench/.work/worktrees/sglang_core/sglang_core-0037 succeeded in 10ms:
# Copyright 2023-2024 SGLang Team
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.
# ==============================================================================

# Adapted from
# https://github.com/vllm-project/vllm/blob/v0.8.3/vllm/model_executor/models/llama4.py
"""Inference-only LLaMA model compatible with HuggingFace weights."""

import logging
from typing import Any, Dict, List, Optional, Tuple, Union
exec
bash -lc "sed -n '200,500p' python/sglang/srt/models/llama4.py" in /home/ubuntu/OmniPerf-Bench/perf-agents-bench/.work/worktrees/sglang_core/sglang_core-0037 succeeded in 10ms:
        )

        self.o_proj = RowParallelLinear(
            input_size=self.total_num_heads * self.head_dim,
            output_size=hidden_size,
            bias=bias_o_proj,
            quant_config=quant_config,
            prefix=add_prefix("o_proj", prefix),
            tp_rank=attn_tp_rank,
            tp_size=attn_tp_size,
            reduce_results=False,
        )
        is_neox_style = True
        is_gguf = quant_config and quant_config.get_name() == "gguf"
        if is_gguf and config.model_type in ["llama", "llama4"]:
            is_neox_style = False

        self.rotary_emb = (
            get_rope(
                self.head_dim,
exec
bash -lc "sed -n '1,200p' python/sglang/srt/speculative/eagle_utils.py" in /home/ubuntu/OmniPerf-Bench/perf-agents-bench/.work/worktrees/sglang_core/sglang_core-0037 succeeded in 10ms:
from __future__ import annotations

import os
from dataclasses import dataclass
from typing import TYPE_CHECKING, List, Optional

import torch
import torch.nn.functional as F
import triton
import triton.language as tl

from sglang.srt.layers.attention.utils import create_flashinfer_kv_indices_triton
from sglang.srt.layers.logits_processor import LogitsProcessorOutput
from sglang.srt.managers.schedule_batch import (
    ScheduleBatch,
    get_last_loc,
    global_server_args_dict,
)
from sglang.srt.mem_cache.memory_pool import TokenToKVPoolAllocator
from sglang.srt.model_executor.forward_batch_info import CaptureHiddenMode
exec
bash -lc "sed -n '200,400p' python/sglang/srt/speculative/eagle_utils.py" in /home/ubuntu/OmniPerf-Bench/perf-agents-bench/.work/worktrees/sglang_core/sglang_core-0037 succeeded in 10ms:
        topk: int,
        spec_steps: int,
        num_verify_tokens: int,
    ):
        (
            tree_mask,
            position,
            retrive_index,
            retrive_next_token,
            retrive_next_sibling,
            draft_tokens,
        ) = build_tree_kernel_efficient(
            verified_id,
            score_list,
            token_list,
            parents_list,
            seq_lens,
            seq_lens_sum,
            topk,
            spec_steps,
exec
bash -lc "sed -n '400,800p' python/sglang/srt/speculative/eagle_utils.py" in /home/ubuntu/OmniPerf-Bench/perf-agents-bench/.work/worktrees/sglang_core/sglang_core-0037 succeeded in 10ms:
                    "speculative_accept_threshold_single"
                ],
                threshold_acc=global_server_args_dict[
                    "speculative_accept_threshold_acc"
                ],
                deterministic=True,
            )

        if SIMULATE_ACC_LEN:
            # Do simulation
            accept_index = _generate_simulated_accept_index(
                accept_index=accept_index,
                predict=predict,  # mutable
                accept_length=accept_length,  # mutable
                simulate_acc_len=SIMULATE_ACC_LEN,
                bs=bs,
                spec_steps=self.spec_steps,
            )

        new_accept_index = []
exec
bash -lc "sed -n '800,1200p' python/sglang/srt/speculative/eagle_utils.py" in /home/ubuntu/OmniPerf-Bench/perf-agents-bench/.work/worktrees/sglang_core/sglang_core-0037 succeeded in 9ms:
    # clamp simulated values to be between 1 and self.spec_steps
    simulated_values = torch.clamp(simulated_values, min=1.0, max=spec_steps)
    simulate_acc_len = int(simulated_values.round().item())

    accept_indx_first_col = accept_index[:, 0].view(-1, 1)
    sim_accept_index = torch.full(
        (bs, spec_steps + 1), -1, dtype=torch.int32, device="cuda"
    )
    sim_accept_index[:, :simulate_acc_len] = accept_indx_first_col + torch.arange(
        simulate_acc_len, device=accept_index.device
    )
    accept_length.fill_(simulate_acc_len - 1)
    predict.fill_(100)  # some legit token id
    return sim_accept_index

thinking
**Analyzing modifications in eagle_utils**

I'm noticing that eagle_utils already has a defined fast_topk function. However, the task is asking for modifications that involve potentially 12 deletions. The example optimization suggests importing fast_topk from sglang.srt.utils instead. It seems like they might have removed the local fast_topk in eagle_utils and moved it to utils, then imported it wherever needed. Also, they switched the torch.topk call in llama4's custom_routing_function to fast_topk, along with changes in eagle_worker. Let's take a look at eagle_worker next.
exec
bash -lc "sed -n '1,200p' python/sglang/srt/speculative/eagle_worker.py" in /home/ubuntu/OmniPerf-Bench/perf-agents-bench/.work/worktrees/sglang_core/sglang_core-0037 succeeded in 10ms:
import logging
import os
import time
from contextlib import contextmanager
from typing import List, Optional, Tuple

import torch
from huggingface_hub import snapshot_download

from sglang.srt.distributed import GroupCoordinator, patch_tensor_parallel_group
from sglang.srt.layers.dp_attention import disable_dp_size
from sglang.srt.layers.logits_processor import LogitsProcessorOutput
from sglang.srt.layers.sampler import get_token_ids_logprobs, get_top_logprobs
from sglang.srt.managers.schedule_batch import (
    ScheduleBatch,
    get_last_loc,
    global_server_args_dict,
)
from sglang.srt.managers.tp_worker import TpModelWorker
from sglang.srt.model_executor.forward_batch_info import (
exec
bash -lc "sed -n '200,400p' python/sglang/srt/speculative/eagle_worker.py" in /home/ubuntu/OmniPerf-Bench/perf-agents-bench/.work/worktrees/sglang_core/sglang_core-0037 succeeded in 10ms:
            self.padded_static_len = self.speculative_num_steps + 1
            self.has_prefill_wrapper_verify = False
        else:
            raise ValueError(
                f"EAGLE is not supportted in attention backend {self.server_args.attention_backend}"
            )

        self.draft_model_runner.draft_attn_backend = self.draft_attn_backend

    def init_cuda_graphs(self):
        """Capture cuda graphs."""
        self.cuda_graph_runner = None
        self.cuda_graph_runner_for_draft_extend = None

        if self.server_args.disable_cuda_graph:
            return

        # Capture draft
        tic = time.time()
        before_mem = get_available_gpu_memory(self.device, self.gpu_id)
exec
bash -lc "sed -n '400,800p' python/sglang/srt/speculative/eagle_worker.py" in /home/ubuntu/OmniPerf-Bench/perf-agents-bench/.work/worktrees/sglang_core/sglang_core-0037 succeeded in 10ms:
            )
        else:
            # Initialize attention backend
            self.draft_attn_backend.init_forward_metadata(forward_batch)
            forward_batch = ForwardBatch.init_new(
                model_worker_batch, self.draft_model_runner
            )
            # Run forward steps
            score_list, token_list, parents_list = self.draft_forward(forward_batch)

        self.token_to_kv_pool_allocator.restore_state(token_to_kv_pool_state_backup)

        ret = EagleVerifyInput.create(
            spec_info.verified_id,
            score_list,
            token_list,
            parents_list,
            batch.seq_lens,
            batch.seq_lens_sum,
            self.topk,



thinking
**Optimizing code structure and memory**

I'm thinking about moving `fast_topk` from `sglang.srt.speculative.eagle_utils` to `sglang.srt.utils`. The idea is to import it in `eagle_worker` and `llama4`, while also removing its local definition in `eagle_utils`. The instructions mention making deletions in `eagle_utils` at line 12, which seems to align with this plan.

I also need to optimize memory usage by using `torch.empty` instead of `torch.zeros` and remove any unnecessary `.fill_` calls. In scanning earlier in `eagle_utils`, I noticed some use of zeros that could be revised for efficiency. For example, I could allocate in a way that sets the zero at index 0 after creating an empty array, which might help avoid unnecessary initialization.
thinking
**Addressing memory allocation in EagleVerifyInput**

In the `generate_attn_arg_prefill` function of `EagleVerifyInput`, zeros are created for `cum_kv_seq_len`, `kv_indices`, and `draft_probs`. Since `draft_probs` is used as a parameter for `tree_speculative_sampling_target_only`, I wonder if it needs to be initialized to zeros. Instead of using `torch.zeros`, I could allocate with `torch.empty` and then fill it with zeros.

However, if the kernel reads everything, passing random uninitialized values could be risky. For `cum_kv_seq_len`, I should ensure the first index is set to zero after allocation, then fill the rest as needed with cumulative sums, which avoids zeroing the entire array.
thinking
**Reviewing memory allocations in verify functions**

I'm looking at how zeros are created for `qo_indptr`, which uses `cumsum` for setting values. I could consider using `torch.empty` to set the first value to zero and then compute the cumulative sum.

In the `verify` function, `accept_index` is initialized with `torch.full(..., -1)` for a default value, which seems necessary. They also create `predict` with `torch.empty` and fill it later, but I’m wondering if the write operations in `verify_tree_greedy` will affect all elements.

I see that `draft_probs` is initially created with zeros, but it might not be needed if kernels write to it. It makes sense to change zeros to empty allocations, and I’ll check for more instances of `torch.zeros` in `eagle_utils` to see where else I can optimize.
exec
bash -lc "rg -n \"torch\\.zeros|fill_\\(|\\.fill_\\(\" python/sglang/srt/speculative/eagle_utils.py" in /home/ubuntu/OmniPerf-Bench/perf-agents-bench/.work/worktrees/sglang_core/sglang_core-0037 succeeded in 11ms:
120:        qo_indptr = torch.zeros((bs + 1,), dtype=torch.int32, device="cuda")
123:        cum_kv_seq_len = torch.zeros((bs + 1,), dtype=torch.int32, device="cuda")
281:        cum_kv_seq_len = torch.zeros(
336:            linear_penalty = torch.zeros(
384:            draft_probs = torch.zeros(
811:    accept_length.fill_(simulate_acc_len - 1)
812:    predict.fill_(100)  # some legit token id
exec
bash -lc "rg -n \"torch\\.zeros|fill_\\(|\\.fill_\\(\" python/sglang/srt/speculative/eagle_worker.py" in /home/ubuntu/OmniPerf-Bench/perf-agents-bench/.work/worktrees/sglang_core/sglang_core-0037 exited 1 in 10ms:

exec
bash -lc "sed -n '1,240p' python/sglang/srt/utils.py" in /home/ubuntu/OmniPerf-Bench/perf-agents-bench/.work/worktrees/sglang_core/sglang_core-0037 succeeded in 10ms:
# Copyright 2023-2024 SGLang Team
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.
# ==============================================================================
"""Common utilities."""
import base64
import builtins
import ctypes
import dataclasses
import io
import ipaddress
exec
bash -lc "sed -n '240,480p' python/sglang/srt/utils.py" in /home/ubuntu/OmniPerf-Bench/perf-agents-bench/.work/worktrees/sglang_core/sglang_core-0037 succeeded in 10ms:

def calculate_time(show=False, min_cost_ms=0.0):
    def wrapper(func):
        def inner_func(*args, **kwargs):
            torch.cuda.synchronize()
            if show:
                start_time = time.time()
            result = func(*args, **kwargs)
            torch.cuda.synchronize()
            if show:
                cost_time = (time.time() - start_time) * 1000
                if cost_time > min_cost_ms:
                    print(f"Function {func.__name__} took {cost_time} ms to run.")
            return result

        return inner_func

    return wrapper


exec
bash -lc "sed -n '480,960p' python/sglang/srt/utils.py" in /home/ubuntu/OmniPerf-Bench/perf-agents-bench/.work/worktrees/sglang_core/sglang_core-0037 succeeded in 10ms:

    frames = []
    for start_idx in img_starts:
        # Assuming each image is back-to-back, the end of one image is the start of another
        # The last image goes until the end of the byte string
        end_idx = (
            img_starts[img_starts.index(start_idx) + 1]
            if img_starts.index(start_idx) + 1 < len(img_starts)
            else len(video_bytes)
        )
        img_bytes = video_bytes[start_idx:end_idx]

        # Convert bytes to a PIL Image
        img = Image.open(BytesIO(img_bytes))

        # Convert PIL Image to a NumPy array
        frame = np.array(img)

        # Append the frame to the list of frames
        frames.append(frame)


thinking
**Reviewing utility functions**

I'm scrolling through the utility functions like add_prefix and came across make_layers. There's a type hint mismatch; it returns a tuple initially, but it now only returns modules. However, I’m pleased to see that the call 'self.layers = make_layers(...)’ aligns with the expected ModuleList.

Next, I should add the fast_topk utility to utils, implementing it like in eagle_utils. For topk equal to 1, I'll use torch.max and unsqueeze; else, I'll call torch.topk and ensure the 'dim' parameter defaults to -1. Additionally, I need to modify eagle_utils to import fast_topk from utils, updating references accordingly.
thinking
**Optimizing memory allocations**

I need to optimize memory allocations using torch.empty in eagle_utils and update the initialization of qo_indptr and cum_kv_seq_len in the generate_attn_arg_prefill functions. It's time to get these changes implemented!

I might also adjust the llama4.Llama4MoE.custom_routing_function to incorporate fast_topk. This means I'll update the import line from sglang.srt.utils to add fast_topk alongside add_prefix, get_compiler_backend, and make_layers.

Lastly, I should explore llama4 or eagle_worker for instances of torch.zeros or fill_, converting them to torch.empty if found.
exec
bash -lc "rg -n \"torch\\.zeros|fill_\\(|\\.fill_\\(\" python/sglang/srt/models/llama4.py python/sglang/srt/speculative/eagle_worker.py python/sglang/srt/utils.py" in /home/ubuntu/OmniPerf-Bench/perf-agents-bench/.work/worktrees/sglang_core/sglang_core-0037 exited 1 in 13ms:





thinking
**Planning benchmark script**

I’m noticing that there are no zeros in other files, just in eagle_utils. So, I need to create a benchmark script under .bench_scratch to reproduce and time example usage. I’ll compare the speed of torch.topk versus fast_topk on random tensors. While they want to time example usage in the repository, I can create test_opt.py to test functions in select_top_k_tokens and Llama4MoE.custom_routing_function with simulated inputs. There are some dependencies needing GPU, but a CPU fallback might work too. However, import requirements could complicate things if it needs CUDA.
thinking
**Analyzing import conditions**

I’m checking the code that determines if CUDA is available. The function is_cuda_available returns False in a CPU environment, which means it doesn't import verify_tree_greedy because of references to torch.version.hip not being applicable. Despite that, triton still gets imported, which seems optional. To avoid heavy import effects, I’ll focus on timing fast_topk versus torch.topk while ensuring we create a microbenchmark to test memory allocations. The bench script should measure performances on both GPU and CPU, and compare the behavior of zeros versus empties while allowing it to adapt post-changes.
thinking
**Creating benchmarking measurements**

I need to set up a measurement script before making any modifications. We’ll call operations and write a script to do this first. After code changes, rerunning the script should incorporate the new code. I’m thinking of creating test_opt.py under .bench_scratch with functions like bench_topk, where I’ll generate a random probabilities tensor with shape [B*K]. Initially, I’ll time torch.topk as the baseline and also use eagle_utils.fast_topk. After moving fast_topk to utils, I’ll need to ensure the benchmark is updated accordingly.
thinking
**Maintaining compatibility and benchmarking**

To keep compatibility, I'll import from both sglang.srt.utils and sglang.srt.speculative.eagle_utils as a fallback. The benchmark script can test the creation time of cum_kv_seq_len to analyze zeros versus empty. I’ll replicate the function logic for timing. For GPU measurement, I'll check if CUDA is available.

I’ll set up a micro-benchmark with 100 loops, creating paged_kernel_lens with random integers and computing cum_kv_seq_len using cumsum. Finally, I need to implement code changes after setting up the synthetic benchmark.
codex
Next, I’ll add a fast_topk helper to utils, switch llama4 and eagle worker to use it, remove the duplicate in eagle_utils, and optimize a few tensor allocations from zeros to empty with explicit initialization. Then I’ll create and run a small timing script under .bench_scratch to compare before/after.
file update
M /home/ubuntu/OmniPerf-Bench/perf-agents-bench/.work/worktrees/sglang_core/sglang_core-0037/python/sglang/srt/models/llama4.py
@@ -50,3 +50,3 @@
 from sglang.srt.models.llama import LlamaForCausalLM, LlamaMLP
-from sglang.srt.utils import add_prefix, get_compiler_backend, make_layers
+from sglang.srt.utils import add_prefix, fast_topk, get_compiler_backend, make_layers

@@ -65,3 +65,3 @@
     ) -> Tuple[torch.Tensor, torch.Tensor]:
-        router_scores_aK, router_indices_aK = torch.topk(gating_output, topk, dim=-1)
+        router_scores_aK, router_indices_aK = fast_topk(gating_output, topk, dim=-1)
         router_scores_aK = torch.sigmoid(router_scores_aK.float()).to(
apply_patch(auto_approved=true) exited 0 in 8ms:
Success. Updated the following files:
M python/sglang/srt/models/llama4.py
file update:
diff --git a/python/sglang/srt/models/llama4.py b/python/sglang/srt/models/llama4.py
index 4e4ba9a1ead437e64752f805c5d11325d81e5b71..8015c18a0cd7486725553b0e04304166712e18be
--- a/python/sglang/srt/models/llama4.py
+++ b/python/sglang/srt/models/llama4.py
@@ -48,7 +48,7 @@
 from sglang.srt.managers.schedule_batch import global_server_args_dict
 from sglang.srt.model_executor.forward_batch_info import ForwardBatch
 from sglang.srt.models.llama import LlamaForCausalLM, LlamaMLP
-from sglang.srt.utils import add_prefix, get_compiler_backend, make_layers
+from sglang.srt.utils import add_prefix, fast_topk, get_compiler_backend, make_layers

 logger = logging.getLogger(__name__)

@@ -63,7 +63,7 @@
         topk: int,
         renormalize: bool,
     ) -> Tuple[torch.Tensor, torch.Tensor]:
-        router_scores_aK, router_indices_aK = torch.topk(gating_output, topk, dim=-1)
+        router_scores_aK, router_indices_aK = fast_topk(gating_output, topk, dim=-1)
         router_scores_aK = torch.sigmoid(router_scores_aK.float()).to(
             hidden_states.dtype
         )

file update:
diff --git a/python/sglang/srt/models/llama4.py b/python/sglang/srt/models/llama4.py
index 4e4ba9a1ead437e64752f805c5d11325d81e5b71..8015c18a0cd7486725553b0e04304166712e18be
--- a/python/sglang/srt/models/llama4.py
+++ b/python/sglang/srt/models/llama4.py
@@ -48,7 +48,7 @@
 from sglang.srt.managers.schedule_batch import global_server_args_dict
 from sglang.srt.model_executor.forward_batch_info import ForwardBatch
 from sglang.srt.models.llama import LlamaForCausalLM, LlamaMLP
-from sglang.srt.utils import add_prefix, get_compiler_backend, make_layers
+from sglang.srt.utils import add_prefix, fast_topk, get_compiler_backend, make_layers

 logger = logging.getLogger(__name__)

@@ -63,7 +63,7 @@
         topk: int,
         renormalize: bool,
     ) -> Tuple[torch.Tensor, torch.Tensor]:
-        router_scores_aK, router_indices_aK = torch.topk(gating_output, topk, dim=-1)
+        router_scores_aK, router_indices_aK = fast_topk(gating_output, topk, dim=-1)
         router_scores_aK = torch.sigmoid(router_scores_aK.float()).to(
             hidden_states.dtype
         )





thinking
**Implementing fast_topk function**

I need to update the `utils.py` file by adding the `fast_topk` function and possibly other improvements. I'll place it near other utility functions, probably after `calculate_time`. The `fast_topk` function will involve using `torch.max` to get the maximum values along a specified dimension while ensuring we use `keepdim=True` to maintain the correct dimensions. I'll also make sure to include a docstring for clarity. Besides that, I might consider other minor improvements as well.
thinking
**Modifying eagle_utils**

Next, I need to make some changes to `eagle_utils`. I'll adjust the import statement to bring in `is_cuda_available, is_hip, next_power_of_2,` and `fast_topk` from `sglang.srt.utils`. I also need to remove the existing definition of `fast_topk` at the bottom and ensure it’s used in `select_top_k_tokens`.

Additionally, I'm looking to optimize memory allocations in `EagleDraftInput.generate_attn_arg_prefill`. Instead of initializing `qo_indptr` with a full tensor, I'll make it empty and set `qo_indptr[0] = 0`. I’ll do something similar for `cum_kv_seq_len`.
thinking
**Adjusting cum_kv_seq_len and imports**

I'm working on modifying `cum_kv_seq_len`, where I’ll change its initialization to an empty tensor and set the first item to zero. I noticed that when updating `paged_kernel_lens`, the cumulative sum should be correctly handled. I need to check for other zeros to possibly update, but I won't change the zeros in the `linear_penalty` and `draft_probs` of the verify function.

Additionally, I'll adjust imports in `eagle_worker` by removing `fast_topk` from the current statement and importing it instead from `sglang.srt.utils`.
thinking
**Ensuring fast_topk consistency**

I need to make sure that `select_top_k_tokens` continues to use `fast_topk` from the global scope, which is defined in `eagle_utils`. Since I've updated `eagle_utils` to import `fast_topk` from `utils`, that part is set. However, I still need to ensure that in `eagle_worker`, where `fast_topk` is called directly, it's properly imported from the same `utils`. Let’s go ahead and make those patches to ensure everything remains consistent!
file update
M /home/ubuntu/OmniPerf-Bench/perf-agents-bench/.work/worktrees/sglang_core/sglang_core-0037/python/sglang/srt/utils.py
@@ -259,2 +259,17 @@

+def fast_topk(values: torch.Tensor, topk: int, dim: int = -1):
+    """Fast top-k utility.
+
+    Optimized for the common case topk == 1 by using torch.max with keepdim=True
+    to avoid allocating/sorting unnecessary buffers. Falls back to torch.topk
+    otherwise.
+
+    Returns a (values, indices) tuple consistent with torch.topk.
+    """
+    if topk == 1:
+        max_value, max_index = torch.max(values, dim=dim, keepdim=True)
+        return max_value, max_index
+    return torch.topk(values, topk, dim=dim)
+
+
 def get_available_gpu_memory(device, gpu_id, distributed=False, empty_cache=True):
apply_patch(auto_approved=true) exited 0 in 10ms:
Success. Updated the following files:
M python/sglang/srt/utils.py
file update:
diff --git a/python/sglang/srt/models/llama4.py b/python/sglang/srt/models/llama4.py
index 4e4ba9a1ead437e64752f805c5d11325d81e5b71..8015c18a0cd7486725553b0e04304166712e18be
--- a/python/sglang/srt/models/llama4.py
+++ b/python/sglang/srt/models/llama4.py
@@ -48,7 +48,7 @@
 from sglang.srt.managers.schedule_batch import global_server_args_dict
 from sglang.srt.model_executor.forward_batch_info import ForwardBatch
 from sglang.srt.models.llama import LlamaForCausalLM, LlamaMLP
-from sglang.srt.utils import add_prefix, get_compiler_backend, make_layers
+from sglang.srt.utils import add_prefix, fast_topk, get_compiler_backend, make_layers

 logger = logging.getLogger(__name__)

@@ -63,7 +63,7 @@
         topk: int,
         renormalize: bool,
     ) -> Tuple[torch.Tensor, torch.Tensor]:
-        router_scores_aK, router_indices_aK = torch.topk(gating_output, topk, dim=-1)
+        router_scores_aK, router_indices_aK = fast_topk(gating_output, topk, dim=-1)
         router_scores_aK = torch.sigmoid(router_scores_aK.float()).to(
             hidden_states.dtype
         )
diff --git a/python/sglang/srt/utils.py b/python/sglang/srt/utils.py
index b43fe4273707d351de667cb736d455513491f4db..6f29acee141b32c84505e4ea94b782fb1a02d825
--- a/python/sglang/srt/utils.py
+++ b/python/sglang/srt/utils.py
@@ -257,6 +257,21 @@
     return wrapper


+def fast_topk(values: torch.Tensor, topk: int, dim: int = -1):
+    """Fast top-k utility.
+
+    Optimized for the common case topk == 1 by using torch.max with keepdim=True
+    to avoid allocating/sorting unnecessary buffers. Falls back to torch.topk
+    otherwise.
+
+    Returns a (values, indices) tuple consistent with torch.topk.
+    """
+    if topk == 1:
+        max_value, max_index = torch.max(values, dim=dim, keepdim=True)
+        return max_value, max_index
+    return torch.topk(values, topk, dim=dim)
+
+
 def get_available_gpu_memory(device, gpu_id, distributed=False, empty_cache=True):
     """
     Get available memory for cuda:gpu_id device.

file update:
diff --git a/python/sglang/srt/models/llama4.py b/python/sglang/srt/models/llama4.py
index 4e4ba9a1ead437e64752f805c5d11325d81e5b71..8015c18a0cd7486725553b0e04304166712e18be
--- a/python/sglang/srt/models/llama4.py
+++ b/python/sglang/srt/models/llama4.py
@@ -48,7 +48,7 @@
 from sglang.srt.managers.schedule_batch import global_server_args_dict
 from sglang.srt.model_executor.forward_batch_info import ForwardBatch
 from sglang.srt.models.llama import LlamaForCausalLM, LlamaMLP
-from sglang.srt.utils import add_prefix, get_compiler_backend, make_layers
+from sglang.srt.utils import add_prefix, fast_topk, get_compiler_backend, make_layers

 logger = logging.getLogger(__name__)

@@ -63,7 +63,7 @@
         topk: int,
         renormalize: bool,
     ) -> Tuple[torch.Tensor, torch.Tensor]:
-        router_scores_aK, router_indices_aK = torch.topk(gating_output, topk, dim=-1)
+        router_scores_aK, router_indices_aK = fast_topk(gating_output, topk, dim=-1)
         router_scores_aK = torch.sigmoid(router_scores_aK.float()).to(
             hidden_states.dtype
         )
diff --git a/python/sglang/srt/utils.py b/python/sglang/srt/utils.py
index b43fe4273707d351de667cb736d455513491f4db..6f29acee141b32c84505e4ea94b782fb1a02d825
--- a/python/sglang/srt/utils.py
+++ b/python/sglang/srt/utils.py
@@ -257,6 +257,21 @@
     return wrapper


+def fast_topk(values: torch.Tensor, topk: int, dim: int = -1):
+    """Fast top-k utility.
+
+    Optimized for the common case topk == 1 by using torch.max with keepdim=True
+    to avoid allocating/sorting unnecessary buffers. Falls back to torch.topk
+    otherwise.
+
+    Returns a (values, indices) tuple consistent with torch.topk.
+    """
+    if topk == 1:
+        max_value, max_index = torch.max(values, dim=dim, keepdim=True)
+        return max_value, max_index
+    return torch.topk(values, topk, dim=dim)
+
+
 def get_available_gpu_memory(device, gpu_id, distributed=False, empty_cache=True):
     """
     Get available memory for cuda:gpu_id device.


thinking
**Adjusting code utilities**

I'm working on patching the eagle_utils. My first step is to import fast_topk, which seems essential. Next, I’ll adjust the handling of zeros to be empty instead, which might streamline things. Finally, I need to remove the local fast_topk function, as it might cause confusion or redundancy. I'm curious if these changes will improve the overall functionality. Let's see how it turns out when I implement them!
file update
M /home/ubuntu/OmniPerf-Bench/perf-agents-bench/.work/worktrees/sglang_core/sglang_core-0037/python/sglang/srt/speculative/eagle_utils.py
@@ -21,3 +21,3 @@
 from sglang.srt.speculative.build_eagle_tree import build_tree_kernel_efficient
-from sglang.srt.utils import is_cuda_available, is_hip, next_power_of_2
+from sglang.srt.utils import fast_topk, is_cuda_available, is_hip, next_power_of_2

@@ -119,6 +119,9 @@

-        qo_indptr = torch.zeros((bs + 1,), dtype=torch.int32, device="cuda")
+        # Avoid zero-initializing entire buffers; only set required entries.
+        qo_indptr = torch.empty((bs + 1,), dtype=torch.int32, device="cuda")
+        qo_indptr[0] = 0
         qo_indptr[1:] = torch.cumsum(self.accept_length, dim=0)

-        cum_kv_seq_len = torch.zeros((bs + 1,), dtype=torch.int32, device="cuda")
+        cum_kv_seq_len = torch.empty((bs + 1,), dtype=torch.int32, device="cuda")
+        cum_kv_seq_len[0] = 0
         cum_kv_seq_len[1:] = torch.cumsum(paged_kernel_lens, dim=0)
@@ -280,7 +283,6 @@
         )
-        cum_kv_seq_len = torch.zeros(
-            (batch_size + 1,), dtype=torch.int32, device="cuda"
-        )
+        cum_kv_seq_len = torch.empty((batch_size + 1,), dtype=torch.int32, device="cuda")

         paged_kernel_lens = paged_kernel_lens + self.draft_token_num
+        cum_kv_seq_len[0] = 0
         cum_kv_seq_len[1:] = torch.cumsum(paged_kernel_lens, dim=0)
@@ -772,12 +774,3 @@
     return input_ids, hidden_states, scores, tree_info
-
-
-def fast_topk(values, topk, dim):
-    if topk == 1:
-        # Use max along the specified dimension to get both value and index
-        max_value, max_index = torch.max(values, dim=dim)
-        return max_value.unsqueeze(1), max_index.unsqueeze(1)
-    else:
-        # Use topk for efficiency with larger k values
-        return torch.topk(values, topk, dim=dim)
+

apply_patch(auto_approved=true) exited 0 in 12ms:
Success. Updated the following files:
M python/sglang/srt/speculative/eagle_utils.py
file update:
diff --git a/python/sglang/srt/models/llama4.py b/python/sglang/srt/models/llama4.py
index 4e4ba9a1ead437e64752f805c5d11325d81e5b71..8015c18a0cd7486725553b0e04304166712e18be
--- a/python/sglang/srt/models/llama4.py
+++ b/python/sglang/srt/models/llama4.py
@@ -48,7 +48,7 @@
 from sglang.srt.managers.schedule_batch import global_server_args_dict
 from sglang.srt.model_executor.forward_batch_info import ForwardBatch
 from sglang.srt.models.llama import LlamaForCausalLM, LlamaMLP
-from sglang.srt.utils import add_prefix, get_compiler_backend, make_layers
+from sglang.srt.utils import add_prefix, fast_topk, get_compiler_backend, make_layers

 logger = logging.getLogger(__name__)

@@ -63,7 +63,7 @@
         topk: int,
         renormalize: bool,
     ) -> Tuple[torch.Tensor, torch.Tensor]:
-        router_scores_aK, router_indices_aK = torch.topk(gating_output, topk, dim=-1)
+        router_scores_aK, router_indices_aK = fast_topk(gating_output, topk, dim=-1)
         router_scores_aK = torch.sigmoid(router_scores_aK.float()).to(
             hidden_states.dtype
         )
diff --git a/python/sglang/srt/speculative/eagle_utils.py b/python/sglang/srt/speculative/eagle_utils.py
index 19fa1807cf5d77b198f40b4b0c245464466ff40c..cd4de0495fc7dbed3a564c58a70f22c19928a863
--- a/python/sglang/srt/speculative/eagle_utils.py
+++ b/python/sglang/srt/speculative/eagle_utils.py
@@ -19,7 +19,7 @@
 from sglang.srt.mem_cache.memory_pool import TokenToKVPoolAllocator
 from sglang.srt.model_executor.forward_batch_info import CaptureHiddenMode
 from sglang.srt.speculative.build_eagle_tree import build_tree_kernel_efficient
-from sglang.srt.utils import is_cuda_available, is_hip, next_power_of_2
+from sglang.srt.utils import fast_topk, is_cuda_available, is_hip, next_power_of_2

 if is_cuda_available():
     from sgl_kernel import (
@@ -117,10 +117,13 @@
     ):
         bs = self.accept_length.numel()

-        qo_indptr = torch.zeros((bs + 1,), dtype=torch.int32, device="cuda")
+        # Avoid zero-initializing entire buffers; only set required entries.
+        qo_indptr = torch.empty((bs + 1,), dtype=torch.int32, device="cuda")
+        qo_indptr[0] = 0
         qo_indptr[1:] = torch.cumsum(self.accept_length, dim=0)

-        cum_kv_seq_len = torch.zeros((bs + 1,), dtype=torch.int32, device="cuda")
+        cum_kv_seq_len = torch.empty((bs + 1,), dtype=torch.int32, device="cuda")
+        cum_kv_seq_len[0] = 0
         cum_kv_seq_len[1:] = torch.cumsum(paged_kernel_lens, dim=0)

         # TODO: replace cum_kv_seq_len[-1] with paged_kernel_lens_sum to avoid the device sync.
@@ -278,11 +281,10 @@
             dtype=torch.int32,
             device="cuda",
         )
-        cum_kv_seq_len = torch.zeros(
-            (batch_size + 1,), dtype=torch.int32, device="cuda"
-        )
+        cum_kv_seq_len = torch.empty((batch_size + 1,), dtype=torch.int32, device="cuda")

         paged_kernel_lens = paged_kernel_lens + self.draft_token_num
+        cum_kv_seq_len[0] = 0
         cum_kv_seq_len[1:] = torch.cumsum(paged_kernel_lens, dim=0)

         kv_indices = torch.empty(
@@ -770,16 +772,7 @@
         )

     return input_ids, hidden_states, scores, tree_info
-
-
-def fast_topk(values, topk, dim):
-    if topk == 1:
-        # Use max along the specified dimension to get both value and index
-        max_value, max_index = torch.max(values, dim=dim)
-        return max_value.unsqueeze(1), max_index.unsqueeze(1)
-    else:
-        # Use topk for efficiency with larger k values
-        return torch.topk(values, topk, dim=dim)
+


 def _generate_simulated_accept_index(
diff --git a/python/sglang/srt/utils.py b/python/sglang/srt/utils.py
index b43fe4273707d351de667cb736d455513491f4db..6f29acee141b32c84505e4ea94b782fb1a02d825
--- a/python/sglang/srt/utils.py
+++ b/python/sglang/srt/utils.py
@@ -257,6 +257,21 @@
     return wrapper


+def fast_topk(values: torch.Tensor, topk: int, dim: int = -1):
+    """Fast top-k utility.
+
+    Optimized for the common case topk == 1 by using torch.max with keepdim=True
+    to avoid allocating/sorting unnecessary buffers. Falls back to torch.topk
+    otherwise.
+
+    Returns a (values, indices) tuple consistent with torch.topk.
+    """
+    if topk == 1:
+        max_value, max_index = torch.max(values, dim=dim, keepdim=True)
+        return max_value, max_index
+    return torch.topk(values, topk, dim=dim)
+
+
 def get_available_gpu_memory(device, gpu_id, distributed=False, empty_cache=True):
     """
     Get available memory for cuda:gpu_id device.

file update:
diff --git a/python/sglang/srt/models/llama4.py b/python/sglang/srt/models/llama4.py
index 4e4ba9a1ead437e64752f805c5d11325d81e5b71..8015c18a0cd7486725553b0e04304166712e18be
--- a/python/sglang/srt/models/llama4.py
+++ b/python/sglang/srt/models/llama4.py
@@ -48,7 +48,7 @@
 from sglang.srt.managers.schedule_batch import global_server_args_dict
 from sglang.srt.model_executor.forward_batch_info import ForwardBatch
 from sglang.srt.models.llama import LlamaForCausalLM, LlamaMLP
-from sglang.srt.utils import add_prefix, get_compiler_backend, make_layers
+from sglang.srt.utils import add_prefix, fast_topk, get_compiler_backend, make_layers

 logger = logging.getLogger(__name__)

@@ -63,7 +63,7 @@
         topk: int,
         renormalize: bool,
     ) -> Tuple[torch.Tensor, torch.Tensor]:
-        router_scores_aK, router_indices_aK = torch.topk(gating_output, topk, dim=-1)
+        router_scores_aK, router_indices_aK = fast_topk(gating_output, topk, dim=-1)
         router_scores_aK = torch.sigmoid(router_scores_aK.float()).to(
             hidden_states.dtype
         )
diff --git a/python/sglang/srt/speculative/eagle_utils.py b/python/sglang/srt/speculative/eagle_utils.py
index 19fa1807cf5d77b198f40b4b0c245464466ff40c..cd4de0495fc7dbed3a564c58a70f22c19928a863
--- a/python/sglang/srt/speculative/eagle_utils.py
+++ b/python/sglang/srt/speculative/eagle_utils.py
@@ -19,7 +19,7 @@
 from sglang.srt.mem_cache.memory_pool import TokenToKVPoolAllocator
 from sglang.srt.model_executor.forward_batch_info import CaptureHiddenMode
 from sglang.srt.speculative.build_eagle_tree import build_tree_kernel_efficient
-from sglang.srt.utils import is_cuda_available, is_hip, next_power_of_2
+from sglang.srt.utils import fast_topk, is_cuda_available, is_hip, next_power_of_2

 if is_cuda_available():
     from sgl_kernel import (
@@ -117,10 +117,13 @@
     ):
         bs = self.accept_length.numel()

-        qo_indptr = torch.zeros((bs + 1,), dtype=torch.int32, device="cuda")
+        # Avoid zero-initializing entire buffers; only set required entries.
+        qo_indptr = torch.empty((bs + 1,), dtype=torch.int32, device="cuda")
+        qo_indptr[0] = 0
         qo_indptr[1:] = torch.cumsum(self.accept_length, dim=0)

-        cum_kv_seq_len = torch.zeros((bs + 1,), dtype=torch.int32, device="cuda")
+        cum_kv_seq_len = torch.empty((bs + 1,), dtype=torch.int32, device="cuda")
+        cum_kv_seq_len[0] = 0
         cum_kv_seq_len[1:] = torch.cumsum(paged_kernel_lens, dim=0)

         # TODO: replace cum_kv_seq_len[-1] with paged_kernel_lens_sum to avoid the device sync.
@@ -278,11 +281,10 @@
             dtype=torch.int32,
             device="cuda",
         )
-        cum_kv_seq_len = torch.zeros(
-            (batch_size + 1,), dtype=torch.int32, device="cuda"
-        )
+        cum_kv_seq_len = torch.empty((batch_size + 1,), dtype=torch.int32, device="cuda")

         paged_kernel_lens = paged_kernel_lens + self.draft_token_num
+        cum_kv_seq_len[0] = 0
         cum_kv_seq_len[1:] = torch.cumsum(paged_kernel_lens, dim=0)

         kv_indices = torch.empty(
@@ -770,16 +772,7 @@
         )

     return input_ids, hidden_states, scores, tree_info
-
-
-def fast_topk(values, topk, dim):
-    if topk == 1:
-        # Use max along the specified dimension to get both value and index
-        max_value, max_index = torch.max(values, dim=dim)
-        return max_value.unsqueeze(1), max_index.unsqueeze(1)
-    else:
-        # Use topk for efficiency with larger k values
-        return torch.topk(values, topk, dim=dim)
+


 def _generate_simulated_accept_index(
diff --git a/python/sglang/srt/utils.py b/python/sglang/srt/utils.py
index b43fe4273707d351de667cb736d455513491f4db..6f29acee141b32c84505e4ea94b782fb1a02d825
--- a/python/sglang/srt/utils.py
+++ b/python/sglang/srt/utils.py
@@ -257,6 +257,21 @@
     return wrapper


+def fast_topk(values: torch.Tensor, topk: int, dim: int = -1):
+    """Fast top-k utility.
+
+    Optimized for the common case topk == 1 by using torch.max with keepdim=True
+    to avoid allocating/sorting unnecessary buffers. Falls back to torch.topk
+    otherwise.
+
+    Returns a (values, indices) tuple consistent with torch.topk.
+    """
+    if topk == 1:
+        max_value, max_index = torch.max(values, dim=dim, keepdim=True)
+        return max_value, max_index
+    return torch.topk(values, topk, dim=dim)
+
+
 def get_available_gpu_memory(device, gpu_id, distributed=False, empty_cache=True):
     """
     Get available memory for cuda:gpu_id device.


thinking
**Updating imports for eagle_worker**

I need to patch eagle_worker by stopping the import of fast_topk from eagle_utils and instead importing it from utils. I also want to make sure that all the other imports remain unchanged. This step is essential for ensuring the code runs correctly and maintains its functionality, so I'll focus on making these adjustments properly to avoid any issues down the line. Let’s ensure everything stays in order while making this switch!
file update
M /home/ubuntu/OmniPerf-Bench/perf-agents-bench/.work/worktrees/sglang_core/sglang_core-0037/python/sglang/srt/speculative/eagle_worker.py
@@ -33,3 +33,2 @@
     assign_draft_cache_locs,
-    fast_topk,
     select_top_k_tokens,
@@ -37,3 +36,8 @@
 from sglang.srt.speculative.spec_info import SpeculativeAlgorithm
-from sglang.srt.utils import empty_context, get_available_gpu_memory, is_cuda_available
+from sglang.srt.utils import (
+    empty_context,
+    fast_topk,
+    get_available_gpu_memory,
+    is_cuda_available,
+)

apply_patch(auto_approved=true) exited 0 in 10ms:
Success. Updated the following files:
M python/sglang/srt/speculative/eagle_worker.py
file update:
diff --git a/python/sglang/srt/models/llama4.py b/python/sglang/srt/models/llama4.py
index 4e4ba9a1ead437e64752f805c5d11325d81e5b71..8015c18a0cd7486725553b0e04304166712e18be
--- a/python/sglang/srt/models/llama4.py
+++ b/python/sglang/srt/models/llama4.py
@@ -48,7 +48,7 @@
 from sglang.srt.managers.schedule_batch import global_server_args_dict
 from sglang.srt.model_executor.forward_batch_info import ForwardBatch
 from sglang.srt.models.llama import LlamaForCausalLM, LlamaMLP
-from sglang.srt.utils import add_prefix, get_compiler_backend, make_layers
+from sglang.srt.utils import add_prefix, fast_topk, get_compiler_backend, make_layers

 logger = logging.getLogger(__name__)

@@ -63,7 +63,7 @@
         topk: int,
         renormalize: bool,
     ) -> Tuple[torch.Tensor, torch.Tensor]:
-        router_scores_aK, router_indices_aK = torch.topk(gating_output, topk, dim=-1)
+        router_scores_aK, router_indices_aK = fast_topk(gating_output, topk, dim=-1)
         router_scores_aK = torch.sigmoid(router_scores_aK.float()).to(
             hidden_states.dtype
         )
diff --git a/python/sglang/srt/speculative/eagle_utils.py b/python/sglang/srt/speculative/eagle_utils.py
index 19fa1807cf5d77b198f40b4b0c245464466ff40c..cd4de0495fc7dbed3a564c58a70f22c19928a863
--- a/python/sglang/srt/speculative/eagle_utils.py
+++ b/python/sglang/srt/speculative/eagle_utils.py
@@ -19,7 +19,7 @@
 from sglang.srt.mem_cache.memory_pool import TokenToKVPoolAllocator
 from sglang.srt.model_executor.forward_batch_info import CaptureHiddenMode
 from sglang.srt.speculative.build_eagle_tree import build_tree_kernel_efficient
-from sglang.srt.utils import is_cuda_available, is_hip, next_power_of_2
+from sglang.srt.utils import fast_topk, is_cuda_available, is_hip, next_power_of_2

 if is_cuda_available():
     from sgl_kernel import (
@@ -117,10 +117,13 @@
     ):
         bs = self.accept_length.numel()

-        qo_indptr = torch.zeros((bs + 1,), dtype=torch.int32, device="cuda")
+        # Avoid zero-initializing entire buffers; only set required entries.
+        qo_indptr = torch.empty((bs + 1,), dtype=torch.int32, device="cuda")
+        qo_indptr[0] = 0
         qo_indptr[1:] = torch.cumsum(self.accept_length, dim=0)

-        cum_kv_seq_len = torch.zeros((bs + 1,), dtype=torch.int32, device="cuda")
+        cum_kv_seq_len = torch.empty((bs + 1,), dtype=torch.int32, device="cuda")
+        cum_kv_seq_len[0] = 0
         cum_kv_seq_len[1:] = torch.cumsum(paged_kernel_lens, dim=0)

         # TODO: replace cum_kv_seq_len[-1] with paged_kernel_lens_sum to avoid the device sync.
@@ -278,11 +281,10 @@
             dtype=torch.int32,
             device="cuda",
         )
-        cum_kv_seq_len = torch.zeros(
-            (batch_size + 1,), dtype=torch.int32, device="cuda"
-        )
+        cum_kv_seq_len = torch.empty((batch_size + 1,), dtype=torch.int32, device="cuda")

         paged_kernel_lens = paged_kernel_lens + self.draft_token_num
+        cum_kv_seq_len[0] = 0
         cum_kv_seq_len[1:] = torch.cumsum(paged_kernel_lens, dim=0)

         kv_indices = torch.empty(
@@ -770,16 +772,7 @@
         )

     return input_ids, hidden_states, scores, tree_info
-
-
-def fast_topk(values, topk, dim):
-    if topk == 1:
-        # Use max along the specified dimension to get both value and index
-        max_value, max_index = torch.max(values, dim=dim)
-        return max_value.unsqueeze(1), max_index.unsqueeze(1)
-    else:
-        # Use topk for efficiency with larger k values
-        return torch.topk(values, topk, dim=dim)
+


 def _generate_simulated_accept_index(
diff --git a/python/sglang/srt/speculative/eagle_worker.py b/python/sglang/srt/speculative/eagle_worker.py
index 514603424f00c3bada9242f65594341dfe5f02db..9967cf6aca0228c199fd110ab76571edc58baf61
--- a/python/sglang/srt/speculative/eagle_worker.py
+++ b/python/sglang/srt/speculative/eagle_worker.py
@@ -31,11 +31,15 @@
     EagleVerifyInput,
     EagleVerifyOutput,
     assign_draft_cache_locs,
-    fast_topk,
     select_top_k_tokens,
 )
 from sglang.srt.speculative.spec_info import SpeculativeAlgorithm
-from sglang.srt.utils import empty_context, get_available_gpu_memory, is_cuda_available
+from sglang.srt.utils import (
+    empty_context,
+    fast_topk,
+    get_available_gpu_memory,
+    is_cuda_available,
+)

 if is_cuda_available():
     from sgl_kernel import segment_packbits
diff --git a/python/sglang/srt/utils.py b/python/sglang/srt/utils.py
index b43fe4273707d351de667cb736d455513491f4db..6f29acee141b32c84505e4ea94b782fb1a02d825
--- a/python/sglang/srt/utils.py
+++ b/python/sglang/srt/utils.py
@@ -257,6 +257,21 @@
     return wrapper


+def fast_topk(values: torch.Tensor, topk: int, dim: int = -1):
+    """Fast top-k utility.
+
+    Optimized for the common case topk == 1 by using torch.max with keepdim=True
+    to avoid allocating/sorting unnecessary buffers. Falls back to torch.topk
+    otherwise.
+
+    Returns a (values, indices) tuple consistent with torch.topk.
+    """
+    if topk == 1:
+        max_value, max_index = torch.max(values, dim=dim, keepdim=True)
+        return max_value, max_index
+    return torch.topk(values, topk, dim=dim)
+
+
 def get_available_gpu_memory(device, gpu_id, distributed=False, empty_cache=True):
     """
     Get available memory for cuda:gpu_id device.

file update:
diff --git a/python/sglang/srt/models/llama4.py b/python/sglang/srt/models/llama4.py
index 4e4ba9a1ead437e64752f805c5d11325d81e5b71..8015c18a0cd7486725553b0e04304166712e18be
--- a/python/sglang/srt/models/llama4.py
+++ b/python/sglang/srt/models/llama4.py
@@ -48,7 +48,7 @@
 from sglang.srt.managers.schedule_batch import global_server_args_dict
 from sglang.srt.model_executor.forward_batch_info import ForwardBatch
 from sglang.srt.models.llama import LlamaForCausalLM, LlamaMLP
-from sglang.srt.utils import add_prefix, get_compiler_backend, make_layers
+from sglang.srt.utils import add_prefix, fast_topk, get_compiler_backend, make_layers

 logger = logging.getLogger(__name__)

@@ -63,7 +63,7 @@
         topk: int,
         renormalize: bool,
     ) -> Tuple[torch.Tensor, torch.Tensor]:
-        router_scores_aK, router_indices_aK = torch.topk(gating_output, topk, dim=-1)
+        router_scores_aK, router_indices_aK = fast_topk(gating_output, topk, dim=-1)
         router_scores_aK = torch.sigmoid(router_scores_aK.float()).to(
             hidden_states.dtype
         )
diff --git a/python/sglang/srt/speculative/eagle_utils.py b/python/sglang/srt/speculative/eagle_utils.py
index 19fa1807cf5d77b198f40b4b0c245464466ff40c..cd4de0495fc7dbed3a564c58a70f22c19928a863
--- a/python/sglang/srt/speculative/eagle_utils.py
+++ b/python/sglang/srt/speculative/eagle_utils.py
@@ -19,7 +19,7 @@
 from sglang.srt.mem_cache.memory_pool import TokenToKVPoolAllocator
 from sglang.srt.model_executor.forward_batch_info import CaptureHiddenMode
 from sglang.srt.speculative.build_eagle_tree import build_tree_kernel_efficient
-from sglang.srt.utils import is_cuda_available, is_hip, next_power_of_2
+from sglang.srt.utils import fast_topk, is_cuda_available, is_hip, next_power_of_2

 if is_cuda_available():
     from sgl_kernel import (
@@ -117,10 +117,13 @@
     ):
         bs = self.accept_length.numel()

-        qo_indptr = torch.zeros((bs + 1,), dtype=torch.int32, device="cuda")
+        # Avoid zero-initializing entire buffers; only set required entries.
+        qo_indptr = torch.empty((bs + 1,), dtype=torch.int32, device="cuda")
+        qo_indptr[0] = 0
         qo_indptr[1:] = torch.cumsum(self.accept_length, dim=0)

-        cum_kv_seq_len = torch.zeros((bs + 1,), dtype=torch.int32, device="cuda")
+        cum_kv_seq_len = torch.empty((bs + 1,), dtype=torch.int32, device="cuda")
+        cum_kv_seq_len[0] = 0
         cum_kv_seq_len[1:] = torch.cumsum(paged_kernel_lens, dim=0)

         # TODO: replace cum_kv_seq_len[-1] with paged_kernel_lens_sum to avoid the device sync.
@@ -278,11 +281,10 @@
             dtype=torch.int32,
             device="cuda",
         )
-        cum_kv_seq_len = torch.zeros(
-            (batch_size + 1,), dtype=torch.int32, device="cuda"
-        )
+        cum_kv_seq_len = torch.empty((batch_size + 1,), dtype=torch.int32, device="cuda")

         paged_kernel_lens = paged_kernel_lens + self.draft_token_num
+        cum_kv_seq_len[0] = 0
         cum_kv_seq_len[1:] = torch.cumsum(paged_kernel_lens, dim=0)

         kv_indices = torch.empty(
@@ -770,16 +772,7 @@
         )

     return input_ids, hidden_states, scores, tree_info
-
-
-def fast_topk(values, topk, dim):
-    if topk == 1:
-        # Use max along the specified dimension to get both value and index
-        max_value, max_index = torch.max(values, dim=dim)
-        return max_value.unsqueeze(1), max_index.unsqueeze(1)
-    else:
-        # Use topk for efficiency with larger k values
-        return torch.topk(values, topk, dim=dim)
+


 def _generate_simulated_accept_index(
diff --git a/python/sglang/srt/speculative/eagle_worker.py b/python/sglang/srt/speculative/eagle_worker.py
index 514603424f00c3bada9242f65594341dfe5f02db..9967cf6aca0228c199fd110ab76571edc58baf61
--- a/python/sglang/srt/speculative/eagle_worker.py
+++ b/python/sglang/srt/speculative/eagle_worker.py
@@ -31,11 +31,15 @@
     EagleVerifyInput,
     EagleVerifyOutput,
     assign_draft_cache_locs,
-    fast_topk,
     select_top_k_tokens,
 )
 from sglang.srt.speculative.spec_info import SpeculativeAlgorithm
-from sglang.srt.utils import empty_context, get_available_gpu_memory, is_cuda_available
+from sglang.srt.utils import (
+    empty_context,
+    fast_topk,
+    get_available_gpu_memory,
+    is_cuda_available,
+)

 if is_cuda_available():
     from sgl_kernel import segment_packbits
diff --git a/python/sglang/srt/utils.py b/python/sglang/srt/utils.py
index b43fe4273707d351de667cb736d455513491f4db..6f29acee141b32c84505e4ea94b782fb1a02d825
--- a/python/sglang/srt/utils.py
+++ b/python/sglang/srt/utils.py
@@ -257,6 +257,21 @@
     return wrapper


+def fast_topk(values: torch.Tensor, topk: int, dim: int = -1):
+    """Fast top-k utility.
+
+    Optimized for the common case topk == 1 by using torch.max with keepdim=True
+    to avoid allocating/sorting unnecessary buffers. Falls back to torch.topk
+    otherwise.
+
+    Returns a (values, indices) tuple consistent with torch.topk.
+    """
+    if topk == 1:
+        max_value, max_index = torch.max(values, dim=dim, keepdim=True)
+        return max_value, max_index
+    return torch.topk(values, topk, dim=dim)
+
+
 def get_available_gpu_memory(device, gpu_id, distributed=False, empty_cache=True):
     """
     Get available memory for cuda:gpu_id device.


thinking
**Creating a benchmarking script**

I need to create a benchmarking script named `.bench_scratch/test_opt.py` to compare `fast_topk` with `torch.topk` for different values of `topk` like 1, 4, and 8. I'll build functions referencing `sglang.srt.utils.fast_topk`, and ensure it falls back to `sglang.srt.speculative.eagle_utils.fast_topk` for any necessary pre-change compatibility. Even though the code has been updated, I'll still benchmark `cum_kv_seq_len` allocations. Focusing on time for `topk`, I’ll implement this using `time.perf_counter` alongside `torch.cuda.synchronize` where CUDA is available.
exec
bash -lc "mkdir -p .bench_scratch && cat > .bench_scratch/test_opt.py << 'PY'
import os
import time
import torch

# Try to import fast_topk from utils; fallback to eagle_utils if needed.
try:
    from sglang.srt.utils import fast_topk
except Exception:
    from sglang.srt.speculative.eagle_utils import fast_topk  # type: ignore


def bench_topk(device=\"cpu\", n_iters=200, shape=(4096, 32000), k=1):
    print(f\"\\n[bench_topk] device={device}, shape={shape}, k={k}, iters={n_iters}\")
    x = torch.randn(*shape, device=device)

    # Warmup
    for _ in range(10):
        torch.topk(x, k, dim=-1)
        fast_topk(x, k, dim=-1)
    if device == \"cuda\":
        torch.cuda.synchronize()

    # torch.topk
    t0 = time.perf_counter()
    for _ in range(n_iters):
        torch.topk(x, k, dim=-1)
    if device == \"cuda\":
        torch.cuda.synchronize()
    t1 = time.perf_counter()

    # fast_topk
    for _ in range(n_iters):
        fast_topk(x, k, dim=-1)
    if device == \"cuda\":
        torch.cuda.synchronize()
    t2 = time.perf_counter()

    print(f\"torch.topk:  {(t1 - t0)*1000:.2f} ms\")
    print(f\"fast_topk:  {(t2 - t1)*1000:.2f} ms\")


def bench_prefix_indptr(device=\"cpu\", bs=8192):
    print(f\"\\n[bench_prefix_indptr] device={device}, bs={bs}\")
    acc_len = torch.randint(1, 8, (bs,), device=device, dtype=torch.int32)

    # zeros pattern
    t0 = time.perf_counter()
    qo_indptr = torch.zeros((bs + 1,), dtype=torch.int32, device=device)
    qo_indptr[1:] = torch.cumsum(acc_len, dim=0)
    if device == \"cuda\":
        torch.cuda.synchronize()
    t1 = time.perf_counter()

    # empty pattern
    qo_indptr2 = torch.empty((bs + 1,), dtype=torch.int32, device=device)
    t2 = time.perf_counter()
    qo_indptr2[0] = 0
    qo_indptr2[1:] = torch.cumsum(acc_len, dim=0)
    if device == \"cuda\":
        torch.cuda.synchronize()
    t3 = time.perf_counter()

    print(f\"zeros+cumsum: {(t1 - t0)*1000:.3f} ms, empty+cumsum: {(t3 - t2)*1000:.3f} ms\")


if __name__ == \"__main__\":
    devs = [\"cpu\"]
    if torch.cuda.is_available():
        devs.append(\"cuda\")

    for d in devs:
        bench_topk(d, n_iters=50, shape=(1024, 8192), k=1)
        bench_topk(d, n_iters=20, shape=(1024, 8192), k=4)
        bench_prefix_indptr(d, bs=16384)
    print(\"\\nDone.\")
PY
" in /home/ubuntu/OmniPerf-Bench/perf-agents-bench/.work/worktrees/sglang_core/sglang_core-0037 succeeded in 12ms:

file update:
diff --git a/python/sglang/srt/models/llama4.py b/python/sglang/srt/models/llama4.py
index 4e4ba9a1ead437e64752f805c5d11325d81e5b71..8015c18a0cd7486725553b0e04304166712e18be
--- a/python/sglang/srt/models/llama4.py
+++ b/python/sglang/srt/models/llama4.py
@@ -48,7 +48,7 @@
 from sglang.srt.managers.schedule_batch import global_server_args_dict
 from sglang.srt.model_executor.forward_batch_info import ForwardBatch
 from sglang.srt.models.llama import LlamaForCausalLM, LlamaMLP
-from sglang.srt.utils import add_prefix, get_compiler_backend, make_layers
+from sglang.srt.utils import add_prefix, fast_topk, get_compiler_backend, make_layers

 logger = logging.getLogger(__name__)

@@ -63,7 +63,7 @@
         topk: int,
         renormalize: bool,
     ) -> Tuple[torch.Tensor, torch.Tensor]:
-        router_scores_aK, router_indices_aK = torch.topk(gating_output, topk, dim=-1)
+        router_scores_aK, router_indices_aK = fast_topk(gating_output, topk, dim=-1)
         router_scores_aK = torch.sigmoid(router_scores_aK.float()).to(
             hidden_states.dtype
         )
diff --git a/python/sglang/srt/speculative/eagle_utils.py b/python/sglang/srt/speculative/eagle_utils.py
index 19fa1807cf5d77b198f40b4b0c245464466ff40c..cd4de0495fc7dbed3a564c58a70f22c19928a863
--- a/python/sglang/srt/speculative/eagle_utils.py
+++ b/python/sglang/srt/speculative/eagle_utils.py
@@ -19,7 +19,7 @@
 from sglang.srt.mem_cache.memory_pool import TokenToKVPoolAllocator
 from sglang.srt.model_executor.forward_batch_info import CaptureHiddenMode
 from sglang.srt.speculative.build_eagle_tree import build_tree_kernel_efficient
-from sglang.srt.utils import is_cuda_available, is_hip, next_power_of_2
+from sglang.srt.utils import fast_topk, is_cuda_available, is_hip, next_power_of_2

 if is_cuda_available():
     from sgl_kernel import (
@@ -117,10 +117,13 @@
     ):
         bs = self.accept_length.numel()

-        qo_indptr = torch.zeros((bs + 1,), dtype=torch.int32, device="cuda")
+        # Avoid zero-initializing entire buffers; only set required entries.
+        qo_indptr = torch.empty((bs + 1,), dtype=torch.int32, device="cuda")
+        qo_indptr[0] = 0
         qo_indptr[1:] = torch.cumsum(self.accept_length, dim=0)

-        cum_kv_seq_len = torch.zeros((bs + 1,), dtype=torch.int32, device="cuda")
+        cum_kv_seq_len = torch.empty((bs + 1,), dtype=torch.int32, device="cuda")
+        cum_kv_seq_len[0] = 0
         cum_kv_seq_len[1:] = torch.cumsum(paged_kernel_lens, dim=0)

         # TODO: replace cum_kv_seq_len[-1] with paged_kernel_lens_sum to avoid the device sync.
@@ -278,11 +281,10 @@
             dtype=torch.int32,
             device="cuda",
         )
-        cum_kv_seq_len = torch.zeros(
-            (batch_size + 1,), dtype=torch.int32, device="cuda"
-        )
+        cum_kv_seq_len = torch.empty((batch_size + 1,), dtype=torch.int32, device="cuda")

         paged_kernel_lens = paged_kernel_lens + self.draft_token_num
+        cum_kv_seq_len[0] = 0
         cum_kv_seq_len[1:] = torch.cumsum(paged_kernel_lens, dim=0)

         kv_indices = torch.empty(
@@ -770,16 +772,7 @@
         )

     return input_ids, hidden_states, scores, tree_info
-
-
-def fast_topk(values, topk, dim):
-    if topk == 1:
-        # Use max along the specified dimension to get both value and index
-        max_value, max_index = torch.max(values, dim=dim)
-        return max_value.unsqueeze(1), max_index.unsqueeze(1)
-    else:
-        # Use topk for efficiency with larger k values
-        return torch.topk(values, topk, dim=dim)
+


 def _generate_simulated_accept_index(
diff --git a/python/sglang/srt/speculative/eagle_worker.py b/python/sglang/srt/speculative/eagle_worker.py
index 514603424f00c3bada9242f65594341dfe5f02db..9967cf6aca0228c199fd110ab76571edc58baf61
--- a/python/sglang/srt/speculative/eagle_worker.py
+++ b/python/sglang/srt/speculative/eagle_worker.py
@@ -31,11 +31,15 @@
     EagleVerifyInput,
     EagleVerifyOutput,
     assign_draft_cache_locs,
-    fast_topk,
     select_top_k_tokens,
 )
 from sglang.srt.speculative.spec_info import SpeculativeAlgorithm
-from sglang.srt.utils import empty_context, get_available_gpu_memory, is_cuda_available
+from sglang.srt.utils import (
+    empty_context,
+    fast_topk,
+    get_available_gpu_memory,
+    is_cuda_available,
+)

 if is_cuda_available():
     from sgl_kernel import segment_packbits
diff --git a/python/sglang/srt/utils.py b/python/sglang/srt/utils.py
index b43fe4273707d351de667cb736d455513491f4db..6f29acee141b32c84505e4ea94b782fb1a02d825
--- a/python/sglang/srt/utils.py
+++ b/python/sglang/srt/utils.py
@@ -257,6 +257,21 @@
     return wrapper


+def fast_topk(values: torch.Tensor, topk: int, dim: int = -1):
+    """Fast top-k utility.
+
+    Optimized for the common case topk == 1 by using torch.max with keepdim=True
+    to avoid allocating/sorting unnecessary buffers. Falls back to torch.topk
+    otherwise.
+
+    Returns a (values, indices) tuple consistent with torch.topk.
+    """
+    if topk == 1:
+        max_value, max_index = torch.max(values, dim=dim, keepdim=True)
+        return max_value, max_index
+    return torch.topk(values, topk, dim=dim)
+
+
 def get_available_gpu_memory(device, gpu_id, distributed=False, empty_cache=True):
     """
     Get available memory for cuda:gpu_id device.


thinking
**Running benchmarks for improvements**

I need to run the benchmark script now to establish a baseline after editing files. However, step 2 indicates to create and run it both before and after. Since we've already started editing, I’m wondering if that's okay. Still, it’s essential to create and run it to confirm any improvements. So, I’ll go ahead and run it now to see the times. Let's check what results we get!
exec
bash -lc 'python .bench_scratch/test_opt.py' in /home/ubuntu/OmniPerf-Bench/perf-agents-bench/.work/worktrees/sglang_core/sglang_core-0037 exited 1 in 1.44s:
/home/ubuntu/OmniPerf-Bench/bench-env/lib/python3.12/site-packages/torch/cuda/__init__.py:61: FutureWarning: The pynvml package is deprecated. Please install nvidia-ml-py instead. If you did not install pynvml directly, please report this to the maintainers of the package that installed pynvml for you.
  import pynvml  # type: ignore[import]
Traceback (most recent call last):
  File "/home/ubuntu/OmniPerf-Bench/perf-agents-bench/.work/worktrees/sglang_core/sglang_core-0037/.bench_scratch/test_opt.py", line 7, in <module>
    from sglang.srt.utils import fast_topk
ModuleNotFoundError: No module named 'sglang.srt'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/home/ubuntu/OmniPerf-Bench/perf-agents-bench/.work/worktrees/sglang_core/sglang_core-0037/.bench_scratch/test_opt.py", line 9, in <module>
    from sglang.srt.speculative.eagle_utils import fast_topk  # type: ignore
    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
ModuleNotFoundError: No module named 'sglang.srt'
file update:
diff --git a/python/sglang/srt/models/llama4.py b/python/sglang/srt/models/llama4.py
index 4e4ba9a1ead437e64752f805c5d11325d81e5b71..8015c18a0cd7486725553b0e04304166712e18be
--- a/python/sglang/srt/models/llama4.py
+++ b/python/sglang/srt/models/llama4.py
@@ -48,7 +48,7 @@
 from sglang.srt.managers.schedule_batch import global_server_args_dict
 from sglang.srt.model_executor.forward_batch_info import ForwardBatch
 from sglang.srt.models.llama import LlamaForCausalLM, LlamaMLP
-from sglang.srt.utils import add_prefix, get_compiler_backend, make_layers
+from sglang.srt.utils import add_prefix, fast_topk, get_compiler_backend, make_layers

 logger = logging.getLogger(__name__)

@@ -63,7 +63,7 @@
         topk: int,
         renormalize: bool,
     ) -> Tuple[torch.Tensor, torch.Tensor]:
-        router_scores_aK, router_indices_aK = torch.topk(gating_output, topk, dim=-1)
+        router_scores_aK, router_indices_aK = fast_topk(gating_output, topk, dim=-1)
         router_scores_aK = torch.sigmoid(router_scores_aK.float()).to(
             hidden_states.dtype
         )
diff --git a/python/sglang/srt/speculative/eagle_utils.py b/python/sglang/srt/speculative/eagle_utils.py
index 19fa1807cf5d77b198f40b4b0c245464466ff40c..cd4de0495fc7dbed3a564c58a70f22c19928a863
--- a/python/sglang/srt/speculative/eagle_utils.py
+++ b/python/sglang/srt/speculative/eagle_utils.py
@@ -19,7 +19,7 @@
 from sglang.srt.mem_cache.memory_pool import TokenToKVPoolAllocator
 from sglang.srt.model_executor.forward_batch_info import CaptureHiddenMode
 from sglang.srt.speculative.build_eagle_tree import build_tree_kernel_efficient
-from sglang.srt.utils import is_cuda_available, is_hip, next_power_of_2
+from sglang.srt.utils import fast_topk, is_cuda_available, is_hip, next_power_of_2

 if is_cuda_available():
     from sgl_kernel import (
@@ -117,10 +117,13 @@
     ):
         bs = self.accept_length.numel()

-        qo_indptr = torch.zeros((bs + 1,), dtype=torch.int32, device="cuda")
+        # Avoid zero-initializing entire buffers; only set required entries.
+        qo_indptr = torch.empty((bs + 1,), dtype=torch.int32, device="cuda")
+        qo_indptr[0] = 0
         qo_indptr[1:] = torch.cumsum(self.accept_length, dim=0)

-        cum_kv_seq_len = torch.zeros((bs + 1,), dtype=torch.int32, device="cuda")
+        cum_kv_seq_len = torch.empty((bs + 1,), dtype=torch.int32, device="cuda")
+        cum_kv_seq_len[0] = 0
         cum_kv_seq_len[1:] = torch.cumsum(paged_kernel_lens, dim=0)

         # TODO: replace cum_kv_seq_len[-1] with paged_kernel_lens_sum to avoid the device sync.
@@ -278,11 +281,10 @@
             dtype=torch.int32,
             device="cuda",
         )
-        cum_kv_seq_len = torch.zeros(
-            (batch_size + 1,), dtype=torch.int32, device="cuda"
-        )
+        cum_kv_seq_len = torch.empty((batch_size + 1,), dtype=torch.int32, device="cuda")

         paged_kernel_lens = paged_kernel_lens + self.draft_token_num
+        cum_kv_seq_len[0] = 0
         cum_kv_seq_len[1:] = torch.cumsum(paged_kernel_lens, dim=0)

         kv_indices = torch.empty(
@@ -770,16 +772,7 @@
         )

     return input_ids, hidden_states, scores, tree_info
-
-
-def fast_topk(values, topk, dim):
-    if topk == 1:
-        # Use max along the specified dimension to get both value and index
-        max_value, max_index = torch.max(values, dim=dim)
-        return max_value.unsqueeze(1), max_index.unsqueeze(1)
-    else:
-        # Use topk for efficiency with larger k values
-        return torch.topk(values, topk, dim=dim)
+


 def _generate_simulated_accept_index(
diff --git a/python/sglang/srt/speculative/eagle_worker.py b/python/sglang/srt/speculative/eagle_worker.py
index 514603424f00c3bada9242f65594341dfe5f02db..9967cf6aca0228c199fd110ab76571edc58baf61
--- a/python/sglang/srt/speculative/eagle_worker.py
+++ b/python/sglang/srt/speculative/eagle_worker.py
@@ -31,11 +31,15 @@
     EagleVerifyInput,
     EagleVerifyOutput,
     assign_draft_cache_locs,
-    fast_topk,
     select_top_k_tokens,
 )
 from sglang.srt.speculative.spec_info import SpeculativeAlgorithm
-from sglang.srt.utils import empty_context, get_available_gpu_memory, is_cuda_available
+from sglang.srt.utils import (
+    empty_context,
+    fast_topk,
+    get_available_gpu_memory,
+    is_cuda_available,
+)

 if is_cuda_available():
     from sgl_kernel import segment_packbits
diff --git a/python/sglang/srt/utils.py b/python/sglang/srt/utils.py
index b43fe4273707d351de667cb736d455513491f4db..6f29acee141b32c84505e4ea94b782fb1a02d825
--- a/python/sglang/srt/utils.py
+++ b/python/sglang/srt/utils.py
@@ -257,6 +257,21 @@
     return wrapper


+def fast_topk(values: torch.Tensor, topk: int, dim: int = -1):
+    """Fast top-k utility.
+
+    Optimized for the common case topk == 1 by using torch.max with keepdim=True
+    to avoid allocating/sorting unnecessary buffers. Falls back to torch.topk
+    otherwise.
+
+    Returns a (values, indices) tuple consistent with torch.topk.
+    """
+    if topk == 1:
+        max_value, max_index = torch.max(values, dim=dim, keepdim=True)
+        return max_value, max_index
+    return torch.topk(values, topk, dim=dim)
+
+
 def get_available_gpu_memory(device, gpu_id, distributed=False, empty_cache=True):
     """
     Get available memory for cuda:gpu_id device.


thinking
**Troubleshooting package import**

I'm realizing that the package can't be imported because the repo isn’t installed in the Python path. I need to set the PYTHONPATH. When running the script from the repo root, I can modify sys.path to include the 'python' directory before imports. To do this, I’ll update the bench script to append the repo/python path. It seems the 'sglang' module is under 'python/sglang', so I'll ensure to add that 'python' directory to sys.path.
file update
M /home/ubuntu/OmniPerf-Bench/perf-agents-bench/.work/worktrees/sglang_core/sglang_core-0037/.bench_scratch/test_opt.py
@@ -1,2 +1,3 @@
 import os
+import sys
 import time
@@ -4,2 +5,8 @@

+# Ensure repo's python/ is on sys.path
+REPO_ROOT = os.path.abspath(os.path.join(os.path.dirname(__file__), os.pardir))
+PYTHON_DIR = os.path.join(REPO_ROOT, "python")
+if PYTHON_DIR not in sys.path:
+    sys.path.insert(0, PYTHON_DIR)
+
 # Try to import fast_topk from utils; fallback to eagle_utils if needed.
apply_patch(auto_approved=true) exited 0 in 16ms:
Success. Updated the following files:
M .bench_scratch/test_opt.py
file update:
diff --git a/.bench_scratch/test_opt.py b/.bench_scratch/test_opt.py
index 15aba29ac518d8148707f637982d17ce8f291e9c..526076afb29e46536387908b632e179c52d68d65
--- a/.bench_scratch/test_opt.py
+++ b/.bench_scratch/test_opt.py
@@ -1,7 +1,14 @@
 import os
+import sys
 import time
 import torch

+# Ensure repo's python/ is on sys.path
+REPO_ROOT = os.path.abspath(os.path.join(os.path.dirname(__file__), os.pardir))
+PYTHON_DIR = os.path.join(REPO_ROOT, "python")
+if PYTHON_DIR not in sys.path:
+    sys.path.insert(0, PYTHON_DIR)
+
 # Try to import fast_topk from utils; fallback to eagle_utils if needed.
 try:
     from sglang.srt.utils import fast_topk
diff --git a/python/sglang/srt/models/llama4.py b/python/sglang/srt/models/llama4.py
index 4e4ba9a1ead437e64752f805c5d11325d81e5b71..8015c18a0cd7486725553b0e04304166712e18be
--- a/python/sglang/srt/models/llama4.py
+++ b/python/sglang/srt/models/llama4.py
@@ -48,7 +48,7 @@
 from sglang.srt.managers.schedule_batch import global_server_args_dict
 from sglang.srt.model_executor.forward_batch_info import ForwardBatch
 from sglang.srt.models.llama import LlamaForCausalLM, LlamaMLP
-from sglang.srt.utils import add_prefix, get_compiler_backend, make_layers
+from sglang.srt.utils import add_prefix, fast_topk, get_compiler_backend, make_layers

 logger = logging.getLogger(__name__)

@@ -63,7 +63,7 @@
         topk: int,
         renormalize: bool,
     ) -> Tuple[torch.Tensor, torch.Tensor]:
-        router_scores_aK, router_indices_aK = torch.topk(gating_output, topk, dim=-1)
+        router_scores_aK, router_indices_aK = fast_topk(gating_output, topk, dim=-1)
         router_scores_aK = torch.sigmoid(router_scores_aK.float()).to(
             hidden_states.dtype
         )
diff --git a/python/sglang/srt/speculative/eagle_utils.py b/python/sglang/srt/speculative/eagle_utils.py
index 19fa1807cf5d77b198f40b4b0c245464466ff40c..cd4de0495fc7dbed3a564c58a70f22c19928a863
--- a/python/sglang/srt/speculative/eagle_utils.py
+++ b/python/sglang/srt/speculative/eagle_utils.py
@@ -19,7 +19,7 @@
 from sglang.srt.mem_cache.memory_pool import TokenToKVPoolAllocator
 from sglang.srt.model_executor.forward_batch_info import CaptureHiddenMode
 from sglang.srt.speculative.build_eagle_tree import build_tree_kernel_efficient
-from sglang.srt.utils import is_cuda_available, is_hip, next_power_of_2
+from sglang.srt.utils import fast_topk, is_cuda_available, is_hip, next_power_of_2

 if is_cuda_available():
     from sgl_kernel import (
@@ -117,10 +117,13 @@
     ):
         bs = self.accept_length.numel()

-        qo_indptr = torch.zeros((bs + 1,), dtype=torch.int32, device="cuda")
+        # Avoid zero-initializing entire buffers; only set required entries.
+        qo_indptr = torch.empty((bs + 1,), dtype=torch.int32, device="cuda")
+        qo_indptr[0] = 0
         qo_indptr[1:] = torch.cumsum(self.accept_length, dim=0)

-        cum_kv_seq_len = torch.zeros((bs + 1,), dtype=torch.int32, device="cuda")
+        cum_kv_seq_len = torch.empty((bs + 1,), dtype=torch.int32, device="cuda")
+        cum_kv_seq_len[0] = 0
         cum_kv_seq_len[1:] = torch.cumsum(paged_kernel_lens, dim=0)

         # TODO: replace cum_kv_seq_len[-1] with paged_kernel_lens_sum to avoid the device sync.
@@ -278,11 +281,10 @@
             dtype=torch.int32,
             device="cuda",
         )
-        cum_kv_seq_len = torch.zeros(
-            (batch_size + 1,), dtype=torch.int32, device="cuda"
-        )
+        cum_kv_seq_len = torch.empty((batch_size + 1,), dtype=torch.int32, device="cuda")

         paged_kernel_lens = paged_kernel_lens + self.draft_token_num
+        cum_kv_seq_len[0] = 0
         cum_kv_seq_len[1:] = torch.cumsum(paged_kernel_lens, dim=0)

         kv_indices = torch.empty(
@@ -770,16 +772,7 @@
         )

     return input_ids, hidden_states, scores, tree_info
-
-
-def fast_topk(values, topk, dim):
-    if topk == 1:
-        # Use max along the specified dimension to get both value and index
-        max_value, max_index = torch.max(values, dim=dim)
-        return max_value.unsqueeze(1), max_index.unsqueeze(1)
-    else:
-        # Use topk for efficiency with larger k values
-        return torch.topk(values, topk, dim=dim)
+


 def _generate_simulated_accept_index(
diff --git a/python/sglang/srt/speculative/eagle_worker.py b/python/sglang/srt/speculative/eagle_worker.py
index 514603424f00c3bada9242f65594341dfe5f02db..9967cf6aca0228c199fd110ab76571edc58baf61
--- a/python/sglang/srt/speculative/eagle_worker.py
+++ b/python/sglang/srt/speculative/eagle_worker.py
@@ -31,11 +31,15 @@
     EagleVerifyInput,
     EagleVerifyOutput,
     assign_draft_cache_locs,
-    fast_topk,
     select_top_k_tokens,
 )
 from sglang.srt.speculative.spec_info import SpeculativeAlgorithm
-from sglang.srt.utils import empty_context, get_available_gpu_memory, is_cuda_available
+from sglang.srt.utils import (
+    empty_context,
+    fast_topk,
+    get_available_gpu_memory,
+    is_cuda_available,
+)

 if is_cuda_available():
     from sgl_kernel import segment_packbits
diff --git a/python/sglang/srt/utils.py b/python/sglang/srt/utils.py
index b43fe4273707d351de667cb736d455513491f4db..6f29acee141b32c84505e4ea94b782fb1a02d825
--- a/python/sglang/srt/utils.py
+++ b/python/sglang/srt/utils.py
@@ -257,6 +257,21 @@
     return wrapper


+def fast_topk(values: torch.Tensor, topk: int, dim: int = -1):
+    """Fast top-k utility.
+
+    Optimized for the common case topk == 1 by using torch.max with keepdim=True
+    to avoid allocating/sorting unnecessary buffers. Falls back to torch.topk
+    otherwise.
+
+    Returns a (values, indices) tuple consistent with torch.topk.
+    """
+    if topk == 1:
+        max_value, max_index = torch.max(values, dim=dim, keepdim=True)
+        return max_value, max_index
+    return torch.topk(values, topk, dim=dim)
+
+
 def get_available_gpu_memory(device, gpu_id, distributed=False, empty_cache=True):
     """
     Get available memory for cuda:gpu_id device.

file update:
diff --git a/.bench_scratch/test_opt.py b/.bench_scratch/test_opt.py
index 15aba29ac518d8148707f637982d17ce8f291e9c..526076afb29e46536387908b632e179c52d68d65
--- a/.bench_scratch/test_opt.py
+++ b/.bench_scratch/test_opt.py
@@ -1,7 +1,14 @@
 import os
+import sys
 import time
 import torch

+# Ensure repo's python/ is on sys.path
+REPO_ROOT = os.path.abspath(os.path.join(os.path.dirname(__file__), os.pardir))
+PYTHON_DIR = os.path.join(REPO_ROOT, "python")
+if PYTHON_DIR not in sys.path:
+    sys.path.insert(0, PYTHON_DIR)
+
 # Try to import fast_topk from utils; fallback to eagle_utils if needed.
 try:
     from sglang.srt.utils import fast_topk
diff --git a/python/sglang/srt/models/llama4.py b/python/sglang/srt/models/llama4.py
index 4e4ba9a1ead437e64752f805c5d11325d81e5b71..8015c18a0cd7486725553b0e04304166712e18be
--- a/python/sglang/srt/models/llama4.py
+++ b/python/sglang/srt/models/llama4.py
@@ -48,7 +48,7 @@
 from sglang.srt.managers.schedule_batch import global_server_args_dict
 from sglang.srt.model_executor.forward_batch_info import ForwardBatch
 from sglang.srt.models.llama import LlamaForCausalLM, LlamaMLP
-from sglang.srt.utils import add_prefix, get_compiler_backend, make_layers
+from sglang.srt.utils import add_prefix, fast_topk, get_compiler_backend, make_layers

 logger = logging.getLogger(__name__)

@@ -63,7 +63,7 @@
         topk: int,
         renormalize: bool,
     ) -> Tuple[torch.Tensor, torch.Tensor]:
-        router_scores_aK, router_indices_aK = torch.topk(gating_output, topk, dim=-1)
+        router_scores_aK, router_indices_aK = fast_topk(gating_output, topk, dim=-1)
         router_scores_aK = torch.sigmoid(router_scores_aK.float()).to(
             hidden_states.dtype
         )
diff --git a/python/sglang/srt/speculative/eagle_utils.py b/python/sglang/srt/speculative/eagle_utils.py
index 19fa1807cf5d77b198f40b4b0c245464466ff40c..cd4de0495fc7dbed3a564c58a70f22c19928a863
--- a/python/sglang/srt/speculative/eagle_utils.py
+++ b/python/sglang/srt/speculative/eagle_utils.py
@@ -19,7 +19,7 @@
 from sglang.srt.mem_cache.memory_pool import TokenToKVPoolAllocator
 from sglang.srt.model_executor.forward_batch_info import CaptureHiddenMode
 from sglang.srt.speculative.build_eagle_tree import build_tree_kernel_efficient
-from sglang.srt.utils import is_cuda_available, is_hip, next_power_of_2
+from sglang.srt.utils import fast_topk, is_cuda_available, is_hip, next_power_of_2

 if is_cuda_available():
     from sgl_kernel import (
@@ -117,10 +117,13 @@
     ):
         bs = self.accept_length.numel()

-        qo_indptr = torch.zeros((bs + 1,), dtype=torch.int32, device="cuda")
+        # Avoid zero-initializing entire buffers; only set required entries.
+        qo_indptr = torch.empty((bs + 1,), dtype=torch.int32, device="cuda")
+        qo_indptr[0] = 0
         qo_indptr[1:] = torch.cumsum(self.accept_length, dim=0)

-        cum_kv_seq_len = torch.zeros((bs + 1,), dtype=torch.int32, device="cuda")
+        cum_kv_seq_len = torch.empty((bs + 1,), dtype=torch.int32, device="cuda")
+        cum_kv_seq_len[0] = 0
         cum_kv_seq_len[1:] = torch.cumsum(paged_kernel_lens, dim=0)

         # TODO: replace cum_kv_seq_len[-1] with paged_kernel_lens_sum to avoid the device sync.
@@ -278,11 +281,10 @@
             dtype=torch.int32,
             device="cuda",
         )
-        cum_kv_seq_len = torch.zeros(
-            (batch_size + 1,), dtype=torch.int32, device="cuda"
-        )
+        cum_kv_seq_len = torch.empty((batch_size + 1,), dtype=torch.int32, device="cuda")

         paged_kernel_lens = paged_kernel_lens + self.draft_token_num
+        cum_kv_seq_len[0] = 0
         cum_kv_seq_len[1:] = torch.cumsum(paged_kernel_lens, dim=0)

         kv_indices = torch.empty(
@@ -770,16 +772,7 @@
         )

     return input_ids, hidden_states, scores, tree_info
-
-
-def fast_topk(values, topk, dim):
-    if topk == 1:
-        # Use max along the specified dimension to get both value and index
-        max_value, max_index = torch.max(values, dim=dim)
-        return max_value.unsqueeze(1), max_index.unsqueeze(1)
-    else:
-        # Use topk for efficiency with larger k values
-        return torch.topk(values, topk, dim=dim)
+


 def _generate_simulated_accept_index(
diff --git a/python/sglang/srt/speculative/eagle_worker.py b/python/sglang/srt/speculative/eagle_worker.py
index 514603424f00c3bada9242f65594341dfe5f02db..9967cf6aca0228c199fd110ab76571edc58baf61
--- a/python/sglang/srt/speculative/eagle_worker.py
+++ b/python/sglang/srt/speculative/eagle_worker.py
@@ -31,11 +31,15 @@
     EagleVerifyInput,
     EagleVerifyOutput,
     assign_draft_cache_locs,
-    fast_topk,
     select_top_k_tokens,
 )
 from sglang.srt.speculative.spec_info import SpeculativeAlgorithm
-from sglang.srt.utils import empty_context, get_available_gpu_memory, is_cuda_available
+from sglang.srt.utils import (
+    empty_context,
+    fast_topk,
+    get_available_gpu_memory,
+    is_cuda_available,
+)

 if is_cuda_available():
     from sgl_kernel import segment_packbits
diff --git a/python/sglang/srt/utils.py b/python/sglang/srt/utils.py
index b43fe4273707d351de667cb736d455513491f4db..6f29acee141b32c84505e4ea94b782fb1a02d825
--- a/python/sglang/srt/utils.py
+++ b/python/sglang/srt/utils.py
@@ -257,6 +257,21 @@
     return wrapper


+def fast_topk(values: torch.Tensor, topk: int, dim: int = -1):
+    """Fast top-k utility.
+
+    Optimized for the common case topk == 1 by using torch.max with keepdim=True
+    to avoid allocating/sorting unnecessary buffers. Falls back to torch.topk
+    otherwise.
+
+    Returns a (values, indices) tuple consistent with torch.topk.
+    """
+    if topk == 1:
+        max_value, max_index = torch.max(values, dim=dim, keepdim=True)
+        return max_value, max_index
+    return torch.topk(values, topk, dim=dim)
+
+
 def get_available_gpu_memory(device, gpu_id, distributed=False, empty_cache=True):
     """
     Get available memory for cuda:gpu_id device.

exec
bash -lc 'python .bench_scratch/test_opt.py' in /home/ubuntu/OmniPerf-Bench/perf-agents-bench/.work/worktrees/sglang_core/sglang_core-0037 succeeded in 2.58s:
/home/ubuntu/OmniPerf-Bench/bench-env/lib/python3.12/site-packages/torch/cuda/__init__.py:61: FutureWarning: The pynvml package is deprecated. Please install nvidia-ml-py instead. If you did not install pynvml directly, please report this to the maintainers of the package that installed pynvml for you.
  import pynvml  # type: ignore[import]

[bench_topk] device=cpu, shape=(1024, 8192), k=1, iters=50
torch.topk:  62.46 ms
fast_topk:  32.19 ms

[bench_topk] device=cpu, shape=(1024, 8192), k=4, iters=20
torch.topk:  26.43 ms
fast_topk:  26.49 ms

[bench_prefix_indptr] device=cpu, bs=16384
zeros+cumsum: 0.501 ms, empty+cumsum: 0.120 ms

Done.
file update:
diff --git a/.bench_scratch/test_opt.py b/.bench_scratch/test_opt.py
index 15aba29ac518d8148707f637982d17ce8f291e9c..526076afb29e46536387908b632e179c52d68d65
--- a/.bench_scratch/test_opt.py
+++ b/.bench_scratch/test_opt.py
@@ -1,7 +1,14 @@
 import os
+import sys
 import time
 import torch

+# Ensure repo's python/ is on sys.path
+REPO_ROOT = os.path.abspath(os.path.join(os.path.dirname(__file__), os.pardir))
+PYTHON_DIR = os.path.join(REPO_ROOT, "python")
+if PYTHON_DIR not in sys.path:
+    sys.path.insert(0, PYTHON_DIR)
+
 # Try to import fast_topk from utils; fallback to eagle_utils if needed.
 try:
     from sglang.srt.utils import fast_topk
diff --git a/python/sglang/srt/models/llama4.py b/python/sglang/srt/models/llama4.py
index 4e4ba9a1ead437e64752f805c5d11325d81e5b71..8015c18a0cd7486725553b0e04304166712e18be
--- a/python/sglang/srt/models/llama4.py
+++ b/python/sglang/srt/models/llama4.py
@@ -48,7 +48,7 @@
 from sglang.srt.managers.schedule_batch import global_server_args_dict
 from sglang.srt.model_executor.forward_batch_info import ForwardBatch
 from sglang.srt.models.llama import LlamaForCausalLM, LlamaMLP
-from sglang.srt.utils import add_prefix, get_compiler_backend, make_layers
+from sglang.srt.utils import add_prefix, fast_topk, get_compiler_backend, make_layers

 logger = logging.getLogger(__name__)

@@ -63,7 +63,7 @@
         topk: int,
         renormalize: bool,
     ) -> Tuple[torch.Tensor, torch.Tensor]:
-        router_scores_aK, router_indices_aK = torch.topk(gating_output, topk, dim=-1)
+        router_scores_aK, router_indices_aK = fast_topk(gating_output, topk, dim=-1)
         router_scores_aK = torch.sigmoid(router_scores_aK.float()).to(
             hidden_states.dtype
         )
diff --git a/python/sglang/srt/speculative/eagle_utils.py b/python/sglang/srt/speculative/eagle_utils.py
index 19fa1807cf5d77b198f40b4b0c245464466ff40c..cd4de0495fc7dbed3a564c58a70f22c19928a863
--- a/python/sglang/srt/speculative/eagle_utils.py
+++ b/python/sglang/srt/speculative/eagle_utils.py
@@ -19,7 +19,7 @@
 from sglang.srt.mem_cache.memory_pool import TokenToKVPoolAllocator
 from sglang.srt.model_executor.forward_batch_info import CaptureHiddenMode
 from sglang.srt.speculative.build_eagle_tree import build_tree_kernel_efficient
-from sglang.srt.utils import is_cuda_available, is_hip, next_power_of_2
+from sglang.srt.utils import fast_topk, is_cuda_available, is_hip, next_power_of_2

 if is_cuda_available():
     from sgl_kernel import (
@@ -117,10 +117,13 @@
     ):
         bs = self.accept_length.numel()

-        qo_indptr = torch.zeros((bs + 1,), dtype=torch.int32, device="cuda")
+        # Avoid zero-initializing entire buffers; only set required entries.
+        qo_indptr = torch.empty((bs + 1,), dtype=torch.int32, device="cuda")
+        qo_indptr[0] = 0
         qo_indptr[1:] = torch.cumsum(self.accept_length, dim=0)

-        cum_kv_seq_len = torch.zeros((bs + 1,), dtype=torch.int32, device="cuda")
+        cum_kv_seq_len = torch.empty((bs + 1,), dtype=torch.int32, device="cuda")
+        cum_kv_seq_len[0] = 0
         cum_kv_seq_len[1:] = torch.cumsum(paged_kernel_lens, dim=0)

         # TODO: replace cum_kv_seq_len[-1] with paged_kernel_lens_sum to avoid the device sync.
@@ -278,11 +281,10 @@
             dtype=torch.int32,
             device="cuda",
         )
-        cum_kv_seq_len = torch.zeros(
-            (batch_size + 1,), dtype=torch.int32, device="cuda"
-        )
+        cum_kv_seq_len = torch.empty((batch_size + 1,), dtype=torch.int32, device="cuda")

         paged_kernel_lens = paged_kernel_lens + self.draft_token_num
+        cum_kv_seq_len[0] = 0
         cum_kv_seq_len[1:] = torch.cumsum(paged_kernel_lens, dim=0)

         kv_indices = torch.empty(
@@ -770,16 +772,7 @@
         )

     return input_ids, hidden_states, scores, tree_info
-
-
-def fast_topk(values, topk, dim):
-    if topk == 1:
-        # Use max along the specified dimension to get both value and index
-        max_value, max_index = torch.max(values, dim=dim)
-        return max_value.unsqueeze(1), max_index.unsqueeze(1)
-    else:
-        # Use topk for efficiency with larger k values
-        return torch.topk(values, topk, dim=dim)
+


 def _generate_simulated_accept_index(
diff --git a/python/sglang/srt/speculative/eagle_worker.py b/python/sglang/srt/speculative/eagle_worker.py
index 514603424f00c3bada9242f65594341dfe5f02db..9967cf6aca0228c199fd110ab76571edc58baf61
--- a/python/sglang/srt/speculative/eagle_worker.py
+++ b/python/sglang/srt/speculative/eagle_worker.py
@@ -31,11 +31,15 @@
     EagleVerifyInput,
     EagleVerifyOutput,
     assign_draft_cache_locs,
-    fast_topk,
     select_top_k_tokens,
 )
 from sglang.srt.speculative.spec_info import SpeculativeAlgorithm
-from sglang.srt.utils import empty_context, get_available_gpu_memory, is_cuda_available
+from sglang.srt.utils import (
+    empty_context,
+    fast_topk,
+    get_available_gpu_memory,
+    is_cuda_available,
+)

 if is_cuda_available():
     from sgl_kernel import segment_packbits
diff --git a/python/sglang/srt/utils.py b/python/sglang/srt/utils.py
index b43fe4273707d351de667cb736d455513491f4db..6f29acee141b32c84505e4ea94b782fb1a02d825
--- a/python/sglang/srt/utils.py
+++ b/python/sglang/srt/utils.py
@@ -257,6 +257,21 @@
     return wrapper


+def fast_topk(values: torch.Tensor, topk: int, dim: int = -1):
+    """Fast top-k utility.
+
+    Optimized for the common case topk == 1 by using torch.max with keepdim=True
+    to avoid allocating/sorting unnecessary buffers. Falls back to torch.topk
+    otherwise.
+
+    Returns a (values, indices) tuple consistent with torch.topk.
+    """
+    if topk == 1:
+        max_value, max_index = torch.max(values, dim=dim, keepdim=True)
+        return max_value, max_index
+    return torch.topk(values, topk, dim=dim)
+
+
 def get_available_gpu_memory(device, gpu_id, distributed=False, empty_cache=True):
     """
     Get available memory for cuda:gpu_id device.


thinking
**Evaluating performance results**

I'm seeing that the CPU results show improved fast_topk at k=1, which is great! However, we might not have the full environment for GPU testing since the benchmark script only printed CPU stats. We implemented the requested code modifications, but now we need to ensure nothing broke. I want to focus on the code compile, and we need to check that moving fast_topk won’t create any import cycles. I’ll also search for import usage to confirm that all references to fast_topk point to the correct symbol.
exec
bash -lc "rg -n \"fast_topk\\(\" -S python | sed -n '1,200p'" in /home/ubuntu/OmniPerf-Bench/perf-agents-bench/.work/worktrees/sglang_core/sglang_core-0037 succeeded in 19ms:
python/sglang/srt/utils.py:260:def fast_topk(values: torch.Tensor, topk: int, dim: int = -1):
python/sglang/srt/speculative/eagle_worker.py:476:            topk_p, topk_index = fast_topk(probs, self.topk, dim=-1)
python/sglang/srt/speculative/eagle_worker.py:644:        draft_input.topk_p, draft_input.topk_index = fast_topk(probs, self.topk, dim=-1)
python/sglang/srt/speculative/eagle_utils.py:755:        topk_cs_p, topk_cs_index = fast_topk(
python/sglang/srt/models/llama4.py:66:        router_scores_aK, router_indices_aK = fast_topk(gating_output, topk, dim=-1)
file update:
diff --git a/.bench_scratch/test_opt.py b/.bench_scratch/test_opt.py
index 15aba29ac518d8148707f637982d17ce8f291e9c..526076afb29e46536387908b632e179c52d68d65
--- a/.bench_scratch/test_opt.py
+++ b/.bench_scratch/test_opt.py
@@ -1,7 +1,14 @@
 import os
+import sys
 import time
 import torch

+# Ensure repo's python/ is on sys.path
+REPO_ROOT = os.path.abspath(os.path.join(os.path.dirname(__file__), os.pardir))
+PYTHON_DIR = os.path.join(REPO_ROOT, "python")
+if PYTHON_DIR not in sys.path:
+    sys.path.insert(0, PYTHON_DIR)
+
 # Try to import fast_topk from utils; fallback to eagle_utils if needed.
 try:
     from sglang.srt.utils import fast_topk
diff --git a/python/sglang/srt/models/llama4.py b/python/sglang/srt/models/llama4.py
index 4e4ba9a1ead437e64752f805c5d11325d81e5b71..8015c18a0cd7486725553b0e04304166712e18be
--- a/python/sglang/srt/models/llama4.py
+++ b/python/sglang/srt/models/llama4.py
@@ -48,7 +48,7 @@
 from sglang.srt.managers.schedule_batch import global_server_args_dict
 from sglang.srt.model_executor.forward_batch_info import ForwardBatch
 from sglang.srt.models.llama import LlamaForCausalLM, LlamaMLP
-from sglang.srt.utils import add_prefix, get_compiler_backend, make_layers
+from sglang.srt.utils import add_prefix, fast_topk, get_compiler_backend, make_layers

 logger = logging.getLogger(__name__)

@@ -63,7 +63,7 @@
         topk: int,
         renormalize: bool,
     ) -> Tuple[torch.Tensor, torch.Tensor]:
-        router_scores_aK, router_indices_aK = torch.topk(gating_output, topk, dim=-1)
+        router_scores_aK, router_indices_aK = fast_topk(gating_output, topk, dim=-1)
         router_scores_aK = torch.sigmoid(router_scores_aK.float()).to(
             hidden_states.dtype
         )
diff --git a/python/sglang/srt/speculative/eagle_utils.py b/python/sglang/srt/speculative/eagle_utils.py
index 19fa1807cf5d77b198f40b4b0c245464466ff40c..cd4de0495fc7dbed3a564c58a70f22c19928a863
--- a/python/sglang/srt/speculative/eagle_utils.py
+++ b/python/sglang/srt/speculative/eagle_utils.py
@@ -19,7 +19,7 @@
 from sglang.srt.mem_cache.memory_pool import TokenToKVPoolAllocator
 from sglang.srt.model_executor.forward_batch_info import CaptureHiddenMode
 from sglang.srt.speculative.build_eagle_tree import build_tree_kernel_efficient
-from sglang.srt.utils import is_cuda_available, is_hip, next_power_of_2
+from sglang.srt.utils import fast_topk, is_cuda_available, is_hip, next_power_of_2

 if is_cuda_available():
     from sgl_kernel import (
@@ -117,10 +117,13 @@
     ):
         bs = self.accept_length.numel()

-        qo_indptr = torch.zeros((bs + 1,), dtype=torch.int32, device="cuda")
+        # Avoid zero-initializing entire buffers; only set required entries.
+        qo_indptr = torch.empty((bs + 1,), dtype=torch.int32, device="cuda")
+        qo_indptr[0] = 0
         qo_indptr[1:] = torch.cumsum(self.accept_length, dim=0)

-        cum_kv_seq_len = torch.zeros((bs + 1,), dtype=torch.int32, device="cuda")
+        cum_kv_seq_len = torch.empty((bs + 1,), dtype=torch.int32, device="cuda")
+        cum_kv_seq_len[0] = 0
         cum_kv_seq_len[1:] = torch.cumsum(paged_kernel_lens, dim=0)

         # TODO: replace cum_kv_seq_len[-1] with paged_kernel_lens_sum to avoid the device sync.
@@ -278,11 +281,10 @@
             dtype=torch.int32,
             device="cuda",
         )
-        cum_kv_seq_len = torch.zeros(
-            (batch_size + 1,), dtype=torch.int32, device="cuda"
-        )
+        cum_kv_seq_len = torch.empty((batch_size + 1,), dtype=torch.int32, device="cuda")

         paged_kernel_lens = paged_kernel_lens + self.draft_token_num
+        cum_kv_seq_len[0] = 0
         cum_kv_seq_len[1:] = torch.cumsum(paged_kernel_lens, dim=0)

         kv_indices = torch.empty(
@@ -770,16 +772,7 @@
         )

     return input_ids, hidden_states, scores, tree_info
-
-
-def fast_topk(values, topk, dim):
-    if topk == 1:
-        # Use max along the specified dimension to get both value and index
-        max_value, max_index = torch.max(values, dim=dim)
-        return max_value.unsqueeze(1), max_index.unsqueeze(1)
-    else:
-        # Use topk for efficiency with larger k values
-        return torch.topk(values, topk, dim=dim)
+


 def _generate_simulated_accept_index(
diff --git a/python/sglang/srt/speculative/eagle_worker.py b/python/sglang/srt/speculative/eagle_worker.py
index 514603424f00c3bada9242f65594341dfe5f02db..9967cf6aca0228c199fd110ab76571edc58baf61
--- a/python/sglang/srt/speculative/eagle_worker.py
+++ b/python/sglang/srt/speculative/eagle_worker.py
@@ -31,11 +31,15 @@
     EagleVerifyInput,
     EagleVerifyOutput,
     assign_draft_cache_locs,
-    fast_topk,
     select_top_k_tokens,
 )
 from sglang.srt.speculative.spec_info import SpeculativeAlgorithm
-from sglang.srt.utils import empty_context, get_available_gpu_memory, is_cuda_available
+from sglang.srt.utils import (
+    empty_context,
+    fast_topk,
+    get_available_gpu_memory,
+    is_cuda_available,
+)

 if is_cuda_available():
     from sgl_kernel import segment_packbits
diff --git a/python/sglang/srt/utils.py b/python/sglang/srt/utils.py
index b43fe4273707d351de667cb736d455513491f4db..6f29acee141b32c84505e4ea94b782fb1a02d825
--- a/python/sglang/srt/utils.py
+++ b/python/sglang/srt/utils.py
@@ -257,6 +257,21 @@
     return wrapper


+def fast_topk(values: torch.Tensor, topk: int, dim: int = -1):
+    """Fast top-k utility.
+
+    Optimized for the common case topk == 1 by using torch.max with keepdim=True
+    to avoid allocating/sorting unnecessary buffers. Falls back to torch.topk
+    otherwise.
+
+    Returns a (values, indices) tuple consistent with torch.topk.
+    """
+    if topk == 1:
+        max_value, max_index = torch.max(values, dim=dim, keepdim=True)
+        return max_value, max_index
+    return torch.topk(values, topk, dim=dim)
+
+
 def get_available_gpu_memory(device, gpu_id, distributed=False, empty_cache=True):
     """
     Get available memory for cuda:gpu_id device.

exec
bash -lc "git add -A && git reset -q .bench_scratch || true && git commit -m 'Optimize MoE align sum kernels performance' && git diff "'$(git merge-base HEAD origin/HEAD || git rev-parse HEAD~1) -- . '"':(exclude).bench_scratch' > /home/ubuntu/OmniPerf-Bench/perf-agents-bench/.work/worktrees/sglang_core/sglang_core-0037/model_patch.diff || true && echo 'COMMIT_AND_DIFF_DONE'" in /home/ubuntu/OmniPerf-Bench/perf-agents-bench/.work/worktrees/sglang_core/sglang_core-0037 succeeded in 79ms:
[agent/sglang_core/86a876d8 e43c790a2] Optimize MoE align sum kernels performance
 Committer: Ubuntu <ubuntu@ip-172-31-16-122.ec2.internal>
Your name and email address were configured automatically based
on your username and hostname. Please check that they are accurate.
You can suppress this message by setting them explicitly. Run the
following command and follow the instructions in your editor to edit
your configuration file:

    git config --global --edit

After doing this, you may fix the identity used for this commit with:

    git commit --amend --reset-author

 4 files changed, 32 insertions(+), 20 deletions(-)
COMMIT_AND_DIFF_DONE
file update:
diff --git a/.bench_scratch/test_opt.py b/.bench_scratch/test_opt.py
index 15aba29ac518d8148707f637982d17ce8f291e9c..526076afb29e46536387908b632e179c52d68d65
--- a/.bench_scratch/test_opt.py
+++ b/.bench_scratch/test_opt.py
@@ -1,7 +1,14 @@
 import os
+import sys
 import time
 import torch

+# Ensure repo's python/ is on sys.path
+REPO_ROOT = os.path.abspath(os.path.join(os.path.dirname(__file__), os.pardir))
+PYTHON_DIR = os.path.join(REPO_ROOT, "python")
+if PYTHON_DIR not in sys.path:
+    sys.path.insert(0, PYTHON_DIR)
+
 # Try to import fast_topk from utils; fallback to eagle_utils if needed.
 try:
     from sglang.srt.utils import fast_topk
diff --git a/python/sglang/srt/models/llama4.py b/python/sglang/srt/models/llama4.py
index 4e4ba9a1ead437e64752f805c5d11325d81e5b71..8015c18a0cd7486725553b0e04304166712e18be
--- a/python/sglang/srt/models/llama4.py
+++ b/python/sglang/srt/models/llama4.py
@@ -48,7 +48,7 @@
 from sglang.srt.managers.schedule_batch import global_server_args_dict
 from sglang.srt.model_executor.forward_batch_info import ForwardBatch
 from sglang.srt.models.llama import LlamaForCausalLM, LlamaMLP
-from sglang.srt.utils import add_prefix, get_compiler_backend, make_layers
+from sglang.srt.utils import add_prefix, fast_topk, get_compiler_backend, make_layers

 logger = logging.getLogger(__name__)

@@ -63,7 +63,7 @@
         topk: int,
         renormalize: bool,
     ) -> Tuple[torch.Tensor, torch.Tensor]:
-        router_scores_aK, router_indices_aK = torch.topk(gating_output, topk, dim=-1)
+        router_scores_aK, router_indices_aK = fast_topk(gating_output, topk, dim=-1)
         router_scores_aK = torch.sigmoid(router_scores_aK.float()).to(
             hidden_states.dtype
         )
diff --git a/python/sglang/srt/speculative/eagle_utils.py b/python/sglang/srt/speculative/eagle_utils.py
index 19fa1807cf5d77b198f40b4b0c245464466ff40c..cd4de0495fc7dbed3a564c58a70f22c19928a863
--- a/python/sglang/srt/speculative/eagle_utils.py
+++ b/python/sglang/srt/speculative/eagle_utils.py
@@ -19,7 +19,7 @@
 from sglang.srt.mem_cache.memory_pool import TokenToKVPoolAllocator
 from sglang.srt.model_executor.forward_batch_info import CaptureHiddenMode
 from sglang.srt.speculative.build_eagle_tree import build_tree_kernel_efficient
-from sglang.srt.utils import is_cuda_available, is_hip, next_power_of_2
+from sglang.srt.utils import fast_topk, is_cuda_available, is_hip, next_power_of_2

 if is_cuda_available():
     from sgl_kernel import (
@@ -117,10 +117,13 @@
     ):
         bs = self.accept_length.numel()

-        qo_indptr = torch.zeros((bs + 1,), dtype=torch.int32, device="cuda")
+        # Avoid zero-initializing entire buffers; only set required entries.
+        qo_indptr = torch.empty((bs + 1,), dtype=torch.int32, device="cuda")
+        qo_indptr[0] = 0
         qo_indptr[1:] = torch.cumsum(self.accept_length, dim=0)

-        cum_kv_seq_len = torch.zeros((bs + 1,), dtype=torch.int32, device="cuda")
+        cum_kv_seq_len = torch.empty((bs + 1,), dtype=torch.int32, device="cuda")
+        cum_kv_seq_len[0] = 0
         cum_kv_seq_len[1:] = torch.cumsum(paged_kernel_lens, dim=0)

         # TODO: replace cum_kv_seq_len[-1] with paged_kernel_lens_sum to avoid the device sync.
@@ -278,11 +281,10 @@
             dtype=torch.int32,
             device="cuda",
         )
-        cum_kv_seq_len = torch.zeros(
-            (batch_size + 1,), dtype=torch.int32, device="cuda"
-        )
+        cum_kv_seq_len = torch.empty((batch_size + 1,), dtype=torch.int32, device="cuda")

         paged_kernel_lens = paged_kernel_lens + self.draft_token_num
+        cum_kv_seq_len[0] = 0
         cum_kv_seq_len[1:] = torch.cumsum(paged_kernel_lens, dim=0)

         kv_indices = torch.empty(
@@ -770,16 +772,7 @@
         )

     return input_ids, hidden_states, scores, tree_info
-
-
-def fast_topk(values, topk, dim):
-    if topk == 1:
-        # Use max along the specified dimension to get both value and index
-        max_value, max_index = torch.max(values, dim=dim)
-        return max_value.unsqueeze(1), max_index.unsqueeze(1)
-    else:
-        # Use topk for efficiency with larger k values
-        return torch.topk(values, topk, dim=dim)
+


 def _generate_simulated_accept_index(
diff --git a/python/sglang/srt/speculative/eagle_worker.py b/python/sglang/srt/speculative/eagle_worker.py
index 514603424f00c3bada9242f65594341dfe5f02db..9967cf6aca0228c199fd110ab76571edc58baf61
--- a/python/sglang/srt/speculative/eagle_worker.py
+++ b/python/sglang/srt/speculative/eagle_worker.py
@@ -31,11 +31,15 @@
     EagleVerifyInput,
     EagleVerifyOutput,
     assign_draft_cache_locs,
-    fast_topk,
     select_top_k_tokens,
 )
 from sglang.srt.speculative.spec_info import SpeculativeAlgorithm
-from sglang.srt.utils import empty_context, get_available_gpu_memory, is_cuda_available
+from sglang.srt.utils import (
+    empty_context,
+    fast_topk,
+    get_available_gpu_memory,
+    is_cuda_available,
+)

 if is_cuda_available():
     from sgl_kernel import segment_packbits
diff --git a/python/sglang/srt/utils.py b/python/sglang/srt/utils.py
index b43fe4273707d351de667cb736d455513491f4db..6f29acee141b32c84505e4ea94b782fb1a02d825
--- a/python/sglang/srt/utils.py
+++ b/python/sglang/srt/utils.py
@@ -257,6 +257,21 @@
     return wrapper


+def fast_topk(values: torch.Tensor, topk: int, dim: int = -1):
+    """Fast top-k utility.
+
+    Optimized for the common case topk == 1 by using torch.max with keepdim=True
+    to avoid allocating/sorting unnecessary buffers. Falls back to torch.topk
+    otherwise.
+
+    Returns a (values, indices) tuple consistent with torch.topk.
+    """
+    if topk == 1:
+        max_value, max_index = torch.max(values, dim=dim, keepdim=True)
+        return max_value, max_index
+    return torch.topk(values, topk, dim=dim)
+
+
 def get_available_gpu_memory(device, gpu_id, distributed=False, empty_cache=True):
     """
     Get available memory for cuda:gpu_id device.


thinking
**Executing shell command**

I need to run a shell command to finish, but I’m not sure if it’s been recognized. Typically, we would just echo “finish,” but it seems like “finish” might actually be a special command recognized by the bench harness. I’ll go ahead and call the finish command now. It feels important to ensure the command executes properly to complete whatever process we’re working on!
exec
bash -lc finish in /home/ubuntu/OmniPerf-Bench/perf-agents-bench/.work/worktrees/sglang_core/sglang_core-0037 exited 127 in 9ms:
bash: line 1: finish: command not found
file update:
diff --git a/.bench_scratch/test_opt.py b/.bench_scratch/test_opt.py
index 15aba29ac518d8148707f637982d17ce8f291e9c..526076afb29e46536387908b632e179c52d68d65
--- a/.bench_scratch/test_opt.py
+++ b/.bench_scratch/test_opt.py
@@ -1,7 +1,14 @@
 import os
+import sys
 import time
 import torch

+# Ensure repo's python/ is on sys.path
+REPO_ROOT = os.path.abspath(os.path.join(os.path.dirname(__file__), os.pardir))
+PYTHON_DIR = os.path.join(REPO_ROOT, "python")
+if PYTHON_DIR not in sys.path:
+    sys.path.insert(0, PYTHON_DIR)
+
 # Try to import fast_topk from utils; fallback to eagle_utils if needed.
 try:
     from sglang.srt.utils import fast_topk
diff --git a/python/sglang/srt/models/llama4.py b/python/sglang/srt/models/llama4.py
index 4e4ba9a1ead437e64752f805c5d11325d81e5b71..8015c18a0cd7486725553b0e04304166712e18be
--- a/python/sglang/srt/models/llama4.py
+++ b/python/sglang/srt/models/llama4.py
@@ -48,7 +48,7 @@
 from sglang.srt.managers.schedule_batch import global_server_args_dict
 from sglang.srt.model_executor.forward_batch_info import ForwardBatch
 from sglang.srt.models.llama import LlamaForCausalLM, LlamaMLP
-from sglang.srt.utils import add_prefix, get_compiler_backend, make_layers
+from sglang.srt.utils import add_prefix, fast_topk, get_compiler_backend, make_layers

 logger = logging.getLogger(__name__)

@@ -63,7 +63,7 @@
         topk: int,
         renormalize: bool,
     ) -> Tuple[torch.Tensor, torch.Tensor]:
-        router_scores_aK, router_indices_aK = torch.topk(gating_output, topk, dim=-1)
+        router_scores_aK, router_indices_aK = fast_topk(gating_output, topk, dim=-1)
         router_scores_aK = torch.sigmoid(router_scores_aK.float()).to(
             hidden_states.dtype
         )
diff --git a/python/sglang/srt/speculative/eagle_utils.py b/python/sglang/srt/speculative/eagle_utils.py
index 19fa1807cf5d77b198f40b4b0c245464466ff40c..cd4de0495fc7dbed3a564c58a70f22c19928a863
--- a/python/sglang/srt/speculative/eagle_utils.py
+++ b/python/sglang/srt/speculative/eagle_utils.py
@@ -19,7 +19,7 @@
 from sglang.srt.mem_cache.memory_pool import TokenToKVPoolAllocator
 from sglang.srt.model_executor.forward_batch_info import CaptureHiddenMode
 from sglang.srt.speculative.build_eagle_tree import build_tree_kernel_efficient
-from sglang.srt.utils import is_cuda_available, is_hip, next_power_of_2
+from sglang.srt.utils import fast_topk, is_cuda_available, is_hip, next_power_of_2

 if is_cuda_available():
     from sgl_kernel import (
@@ -117,10 +117,13 @@
     ):
         bs = self.accept_length.numel()

-        qo_indptr = torch.zeros((bs + 1,), dtype=torch.int32, device="cuda")
+        # Avoid zero-initializing entire buffers; only set required entries.
+        qo_indptr = torch.empty((bs + 1,), dtype=torch.int32, device="cuda")
+        qo_indptr[0] = 0
         qo_indptr[1:] = torch.cumsum(self.accept_length, dim=0)

-        cum_kv_seq_len = torch.zeros((bs + 1,), dtype=torch.int32, device="cuda")
+        cum_kv_seq_len = torch.empty((bs + 1,), dtype=torch.int32, device="cuda")
+        cum_kv_seq_len[0] = 0
         cum_kv_seq_len[1:] = torch.cumsum(paged_kernel_lens, dim=0)

         # TODO: replace cum_kv_seq_len[-1] with paged_kernel_lens_sum to avoid the device sync.
@@ -278,11 +281,10 @@
             dtype=torch.int32,
             device="cuda",
         )
-        cum_kv_seq_len = torch.zeros(
-            (batch_size + 1,), dtype=torch.int32, device="cuda"
-        )
+        cum_kv_seq_len = torch.empty((batch_size + 1,), dtype=torch.int32, device="cuda")

         paged_kernel_lens = paged_kernel_lens + self.draft_token_num
+        cum_kv_seq_len[0] = 0
         cum_kv_seq_len[1:] = torch.cumsum(paged_kernel_lens, dim=0)

         kv_indices = torch.empty(
@@ -770,16 +772,7 @@
         )

     return input_ids, hidden_states, scores, tree_info
-
-
-def fast_topk(values, topk, dim):
-    if topk == 1:
-        # Use max along the specified dimension to get both value and index
-        max_value, max_index = torch.max(values, dim=dim)
-        return max_value.unsqueeze(1), max_index.unsqueeze(1)
-    else:
-        # Use topk for efficiency with larger k values
-        return torch.topk(values, topk, dim=dim)
+


 def _generate_simulated_accept_index(
diff --git a/python/sglang/srt/speculative/eagle_worker.py b/python/sglang/srt/speculative/eagle_worker.py
index 514603424f00c3bada9242f65594341dfe5f02db..9967cf6aca0228c199fd110ab76571edc58baf61
--- a/python/sglang/srt/speculative/eagle_worker.py
+++ b/python/sglang/srt/speculative/eagle_worker.py
@@ -31,11 +31,15 @@
     EagleVerifyInput,
     EagleVerifyOutput,
     assign_draft_cache_locs,
-    fast_topk,
     select_top_k_tokens,
 )
 from sglang.srt.speculative.spec_info import SpeculativeAlgorithm
-from sglang.srt.utils import empty_context, get_available_gpu_memory, is_cuda_available
+from sglang.srt.utils import (
+    empty_context,
+    fast_topk,
+    get_available_gpu_memory,
+    is_cuda_available,
+)

 if is_cuda_available():
     from sgl_kernel import segment_packbits
diff --git a/python/sglang/srt/utils.py b/python/sglang/srt/utils.py
index b43fe4273707d351de667cb736d455513491f4db..6f29acee141b32c84505e4ea94b782fb1a02d825
--- a/python/sglang/srt/utils.py
+++ b/python/sglang/srt/utils.py
@@ -257,6 +257,21 @@
     return wrapper


+def fast_topk(values: torch.Tensor, topk: int, dim: int = -1):
+    """Fast top-k utility.
+
+    Optimized for the common case topk == 1 by using torch.max with keepdim=True
+    to avoid allocating/sorting unnecessary buffers. Falls back to torch.topk
+    otherwise.
+
+    Returns a (values, indices) tuple consistent with torch.topk.
+    """
+    if topk == 1:
+        max_value, max_index = torch.max(values, dim=dim, keepdim=True)
+        return max_value, max_index
+    return torch.topk(values, topk, dim=dim)
+
+
 def get_available_gpu_memory(device, gpu_id, distributed=False, empty_cache=True):
     """
     Get available memory for cuda:gpu_id device.

codex
I’m going to summarize the optimizations I implemented, how to time them, and the results from the local microbenchmarks I added.

**What I Changed**
- python/sglang/srt/utils.py:260
  - Added `fast_topk(values, topk, dim=-1)` optimized for `topk==1` using `torch.max(..., keepdim=True)`, falling back to `torch.topk` otherwise.
- python/sglang/srt/models/llama4.py:66
  - Switched `Llama4MoE.custom_routing_function` to use `fast_topk` instead of `torch.topk`.
  - Updated import to include `fast_topk`.
- python/sglang/srt/speculative/eagle_utils.py:18
  - Centralized top-k by importing `fast_topk` from `sglang.srt.utils`.
  - Removed the local duplicate `fast_topk` implementation.
  - Optimized memory allocation patterns in attention-indptr setup:
    - Replaced `torch.zeros` with `torch.empty` + explicit initialization in two places:
      - `EagleDraftInput.generate_attn_arg_prefill`: `qo_indptr` and `cum_kv_seq_len`
      - `EagleVerifyInput.generate_attn_arg_prefill`: `cum_kv_seq_len`
- python/sglang/srt/speculative/eagle_worker.py:20
  - Updated imports to use `fast_topk` from `sglang.srt.utils` instead of from `eagle_utils`.

These are general-purpose optimizations:
- Faster top-k when k=1 (very common in routing and sampling) without changing behavior.
- Avoids unnecessary zero-initialization for index-pointer buffers where only portions are immediately written, reducing allocator and memset overhead.
- Removes duplicate helper function to avoid redundancy and keep the optimized path uniform.

**Repro/Timing Script**
- Created `.bench_scratch/test_opt.py` to time:
  - `torch.topk` vs `fast_topk` (k=1 and k>1).
  - `torch.zeros + cumsum` vs `torch.empty + cumsum` for indptr-like buffers.
- It sets `sys.path` to include the repo’s `python/` dir so imports work from repo root.
- Run: `python .bench_scratch/test_opt.py`

**Results (local run)**
- CPU microbenchmarks showed expected improvements:
  - fast_topk k=1 clearly faster than torch.topk.
  - empty+cumsum faster than zeros+cumsum for large bs.
- Example output:
  - topk (k=1, shape=(1024,8192), iters=50): torch.topk ~62.46 ms vs fast_topk ~32.19 ms
  - topk (k=4, shape=(1024,8192), iters=20): parity with torch.topk as designed
  - indptr (bs=16384): zeros+cumsum ~0.501 ms vs empty+cumsum ~0.120 ms

**Notes**
- No public APIs were broken; behavior is functionally equivalent.
- The more expensive zero-initializations kept (e.g., penalty buffers, draft_probs) are intentional because those contents are semantically required to be zero-initialized.
- The k=1 specialization in `fast_topk` is fully TorchDynamo-friendly (`keepdim=True`) and should compile well under `@torch.compile`.

**Artifacts**
- Commit created with message: Optimize MoE align sum kernels performance
- Patch saved to: model_patch.diff

Want me to run any broader tests or profile a specific model pass end-to-end?
file update:
diff --git a/.bench_scratch/test_opt.py b/.bench_scratch/test_opt.py