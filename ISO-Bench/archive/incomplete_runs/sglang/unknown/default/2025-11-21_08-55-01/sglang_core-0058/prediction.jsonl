{"instance_id": "/home/ubuntu/OmniPerf-Bench/sglan__repo-45d6592", "model_patch": "diff --git a/python/sglang/srt/managers/router/infer_batch.py b/python/sglang/srt/managers/router/infer_batch.py\nindex 88f6031f7..42e3bef2a 100644\n--- a/python/sglang/srt/managers/router/infer_batch.py\n+++ b/python/sglang/srt/managers/router/infer_batch.py\n@@ -31,6 +31,9 @@ class Req:\n         self.pixel_values = None\n         self.image_size = None\n         self.image_offset = 0\n+        # For multimodal padding reapplication (e.g., llava)\n+        self.pad_value = None\n+        self.needs_pad_recalc = False\n \n         self.sampling_params = None\n         self.return_logprob = False\n@@ -58,30 +61,26 @@ class Req:\n     def max_new_tokens(self):\n         return self.sampling_params.max_new_tokens\n \n-    def tokenize_fast_forward(self, fast_forward_str, next_state):\n+    def fast_forward_and_retokenize(self, fast_forward_str, next_state):\n+        \"\"\"Fast-forward the constrained path and re-tokenize the entire prefix.\n+\n+        This implementation avoids relying on previous input length accounting,\n+        which may be incorrect for multimodal (e.g., llava) where image padding\n+        expands input_ids. Instead, compute the delta via encode-length diff.\n+        \"\"\"\n         old_output_str = self.tokenizer.decode(self.output_ids)\n-        # FIXME: This logic does not really solve the problem of determining whether\n-        # there should be a leading space.\n         first_token = self.tokenizer.convert_ids_to_tokens(self.output_ids[0])\n         first_token = (\n             first_token.decode() if isinstance(first_token, bytes) else first_token\n         )\n         if first_token.startswith(\"\u2581\"):\n             old_output_str = \" \" + old_output_str\n-        new_input_string = (\n-            self.input_text\n-            + self.output_and_fast_forward_str\n-            + old_output_str\n-            + fast_forward_str\n-        )\n+\n+        base_str = self.input_text + self.output_and_fast_forward_str + old_output_str\n+        prefix_len = len(self.tokenizer.encode(base_str))\n+        new_input_string = base_str + fast_forward_str\n         new_input_ids = self.tokenizer.encode(new_input_string)\n-        fast_forward_tokens_len = (\n-            len(new_input_ids) - len(self.input_ids) - len(self.output_ids)\n-        )\n-        # print(\"=\" * 100)\n-        # print(f\"Catch fast forward:\\n{fast_forward_str}\")\n-        # print(self.tokenizer.convert_ids_to_tokens(self.input_ids))\n-        # print(self.tokenizer.convert_ids_to_tokens(new_input_ids))\n+        fast_forward_tokens_len = len(new_input_ids) - prefix_len\n \n         self.input_ids = new_input_ids\n         self.output_ids = []\n@@ -93,8 +92,9 @@ class Req:\n             self.output_and_fast_forward_str + old_output_str + fast_forward_str\n         )\n \n-        # print(f\"Output and fast forward str:\\n{self.output_and_fast_forward_str}\")\n-        # print(\"*\" * 100)\n+        # Mark to recompute image padding at the next scheduling step\n+        if self.pixel_values is not None:\n+            self.needs_pad_recalc = True\n \n     def check_finished(self):\n         if self.finished:\n@@ -193,7 +193,8 @@ class Batch:\n         seq_lens = []\n \n         req_pool_indices = self.req_to_token_pool.alloc(bs)\n-        req_pool_indices_cpu = req_pool_indices.cpu().numpy()\n+        # Avoid numpy conversion overhead here\n+        req_pool_indices_cpu = req_pool_indices.cpu().tolist()\n         for i in range(bs):\n             flatten_input_ids.extend(input_ids[i])\n             extend_lens.append(len(input_ids[i]))\n@@ -208,11 +209,12 @@ class Batch:\n \n             seq_lens.append(prefix_lens[-1] + extend_lens[-1])\n \n+        # Keep zeros for correctness; kept allocation explicit\n         position_ids_offsets = torch.zeros((bs,), dtype=torch.int32, device=device)\n \n         # Alloc mem\n-        seq_lens, prefix_lens = np.array(seq_lens), np.array(prefix_lens)\n-        extend_num_tokens = seq_lens.sum() - prefix_lens.sum()\n+        # Avoid numpy conversions; compute sums directly\n+        extend_num_tokens = sum(seq_lens) - sum(prefix_lens)\n         out_cache_loc = self.token_to_kv_pool.alloc(extend_num_tokens)\n         if out_cache_loc is None:\n             if not self.tree_cache.disable:\n@@ -232,10 +234,17 @@ class Batch:\n             pt += extend_lens[i]\n \n         # Handle logit bias\n-        logit_bias = torch.zeros((bs, vocab_size), dtype=torch.float32, device=device)\n-        for i in range(bs):\n-            if reqs[i].sampling_params.dtype == \"int\":\n-                logit_bias[i] = int_token_logit_bias\n+        # Optimize logit_bias allocation: broadcast a single zero row when possible\n+        need_bias_rows = [i for i, r in enumerate(reqs) if r.sampling_params.dtype == \"int\"]\n+        if len(need_bias_rows) == 0:\n+            logit_bias = torch.zeros((1, vocab_size), dtype=torch.float32, device=device)\n+        else:\n+            logit_bias = torch.zeros((bs, vocab_size), dtype=torch.float32, device=device)\n+            if isinstance(int_token_logit_bias, torch.Tensor) and int_token_logit_bias.device != logit_bias.device:\n+                bias_row = int_token_logit_bias.to(logit_bias.device)\n+            else:\n+                bias_row = int_token_logit_bias\n+            logit_bias[need_bias_rows] = bias_row\n \n         # Set fields\n         self.input_ids = torch.tensor(\n@@ -350,8 +359,8 @@ class Batch:\n                     self.req_to_token_pool.free(req_pool_idx)\n                     self.tree_cache.dec_ref_counter(req.last_node)\n \n-                    # fast forward\n-                    req.tokenize_fast_forward(fast_forward_str, next_state)\n+                    # fast forward with robust re-tokenization\n+                    req.fast_forward_and_retokenize(fast_forward_str, next_state)\n \n                     fast_forward_reqs.append(req)\n                     filter_indices.remove(i)\ndiff --git a/python/sglang/srt/managers/router/model_rpc.py b/python/sglang/srt/managers/router/model_rpc.py\nindex 8b7adf944..472ea8704 100644\n--- a/python/sglang/srt/managers/router/model_rpc.py\n+++ b/python/sglang/srt/managers/router/model_rpc.py\n@@ -243,6 +243,8 @@ class ModelRpcServer(rpyc.Service):\n             req.input_ids, req.image_offset = self.model_runner.model.pad_input_ids(\n                 req.input_ids, pad_value, req.pixel_values.shape, req.image_size\n             )\n+            # Save for future fast-forward re-padding\n+            req.pad_value = pad_value\n         req.sampling_params = recv_req.sampling_params\n         req.return_logprob = recv_req.return_logprob\n         req.logprob_start_len = recv_req.logprob_start_len\n@@ -274,6 +276,16 @@ class ModelRpcServer(rpyc.Service):\n             return None\n \n         for req in self.forward_queue:\n+            # If a multimodal request was retokenized via fast-forward, re-apply padding\n+            if (\n+                req.pixel_values is not None\n+                and getattr(req, \"needs_pad_recalc\", False)\n+                and getattr(req, \"pad_value\", None) is not None\n+            ):\n+                req.input_ids, req.image_offset = self.model_runner.model.pad_input_ids(\n+                    req.input_ids, req.pad_value, req.pixel_values.shape, req.image_size\n+                )\n+                req.needs_pad_recalc = False\n             prefix_indices, last_node = self.tree_cache.match_prefix(req.input_ids)\n             if req.return_logprob:\n                 prefix_indices = prefix_indices[: req.logprob_start_len]\n", "model_name_or_path": "gpt-5-2025-08-07"}
