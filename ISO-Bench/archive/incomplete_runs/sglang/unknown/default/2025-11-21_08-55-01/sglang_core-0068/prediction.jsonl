{"instance_id": "/home/ubuntu/OmniPerf-Bench/sglan__repo-1e495e0", "model_patch": "diff --git a/python/sglang/srt/model_executor/model_runner.py b/python/sglang/srt/model_executor/model_runner.py\nindex 09b3c7127..e6a8f0170 100644\n--- a/python/sglang/srt/model_executor/model_runner.py\n+++ b/python/sglang/srt/model_executor/model_runner.py\n@@ -162,6 +162,13 @@ class ModelRunner:\n         return min_per_gpu_memory\n \n     def load_model(self):\n+        # Limit PyTorch CPU thread usage during model load to reduce overhead\n+        # and contention in safetensors/weight deserialization.\n+        try:\n+            torch.set_num_threads(1)\n+            torch.set_num_interop_threads(1)\n+        except Exception:\n+            pass\n         logger.info(\n             f\"Load weight begin. avail mem={get_available_gpu_memory(self.gpu_id):.2f} GB\"\n         )\n@@ -313,6 +320,7 @@ class ModelRunner:\n         return True, \"Succeeded to update model weights\"\n \n     def profile_max_num_token(self, total_gpu_memory: int):\n+        element_size = torch._utils._element_size(self.kv_cache_dtype)\n         available_gpu_memory = get_available_gpu_memory(\n             self.gpu_id, distributed=self.tp_size > 1\n         )\n@@ -323,7 +331,7 @@ class ModelRunner:\n             cell_size = (\n                 (self.model_config.kv_lora_rank + self.model_config.qk_rope_head_dim)\n                 * self.model_config.num_hidden_layers\n-                * torch._utils._element_size(self.kv_cache_dtype)\n+                * element_size\n             )\n         else:\n             cell_size = (\n@@ -331,7 +339,7 @@ class ModelRunner:\n                 * self.model_config.head_dim\n                 * self.model_config.num_hidden_layers\n                 * 2\n-                * torch._utils._element_size(self.kv_cache_dtype)\n+                * element_size\n             )\n         rest_memory = available_gpu_memory - total_gpu_memory * (\n             1 - self.mem_fraction_static\n@@ -415,8 +423,9 @@ class ModelRunner:\n         \"\"\"We need to run a small matmul to init cublas. Otherwise, it will raise some errors later.\"\"\"\n         dtype = torch.float16\n         device = \"cuda\"\n-        a = torch.ones((16, 16), dtype=dtype, device=device)\n-        b = torch.ones((16, 16), dtype=dtype, device=device)\n+        # Use empty() to avoid unnecessary memory initialization\n+        a = torch.empty((16, 16), dtype=dtype, device=device)\n+        b = torch.empty((16, 16), dtype=dtype, device=device)\n         c = a @ b\n         return c\n \n", "model_name_or_path": "gpt-5-2025-08-07"}
