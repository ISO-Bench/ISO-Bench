diff --git a/python/sglang/srt/managers/expert_location.py b/python/sglang/srt/managers/expert_location.py
index 615e0a440..51f29a3e0 100644
--- a/python/sglang/srt/managers/expert_location.py
+++ b/python/sglang/srt/managers/expert_location.py
@@ -26,6 +26,7 @@ from sglang.srt.configs.model_config import ModelConfig
 from sglang.srt.managers import eplb_algorithms
 from sglang.srt.model_loader import get_model_architecture
 from sglang.srt.server_args import ServerArgs
+from sglang.srt.managers.schedule_batch import global_server_args_dict
 
 logger = logging.getLogger(__name__)
 
@@ -35,7 +36,8 @@ class ExpertLocationMetadata:
     physical_to_logical_map: torch.Tensor  # (layers, num_physical_experts)
     logical_to_all_physical_map: torch.Tensor  # (layers, num_logical_experts, X)
     logical_to_all_physical_map_num_valid: torch.Tensor  # (layers, num_logical_experts)
-    logical_to_rank_dispatch_physical_map: torch.Tensor  # (layers, num_logical_experts)
+    # (layers, num_logical_experts). Optional when dispatch is non-static
+    logical_to_rank_dispatch_physical_map: Optional[torch.Tensor]
 
     # -------------------------------- properties ------------------------------------
 
@@ -70,11 +72,8 @@ class ExpertLocationMetadata:
         num_layers_2, num_logical_experts_1 = (
             self.logical_to_all_physical_map_num_valid.shape
         )
-        num_layers_3, num_logical_experts_2 = (
-            self.logical_to_rank_dispatch_physical_map.shape
-        )
-        assert num_layers_0 == num_layers_1 == num_layers_2 == num_layers_3
-        assert num_logical_experts_0 == num_logical_experts_1 == num_logical_experts_2
+        assert num_layers_0 == num_layers_1 == num_layers_2
+        assert num_logical_experts_0 == num_logical_experts_1
         assert num_physical_experts_0 == num_physical_experts_1
 
     # -------------------------------- construction ------------------------------------
@@ -200,17 +199,23 @@ class ExpertLocationMetadata:
             logical_to_all_physical_map != -1, dim=-1
         )
 
-        return ExpertLocationMetadata(
-            physical_to_logical_map=physical_to_logical_map,
-            logical_to_all_physical_map=logical_to_all_physical_map_padded,
-            logical_to_all_physical_map_num_valid=logical_to_all_physical_map_num_valid,
-            logical_to_rank_dispatch_physical_map=compute_logical_to_rank_dispatch_physical_map(
+        # Only precompute the rank dispatch map for static algorithm
+        ep_dispatch_algorithm = global_server_args_dict.get("ep_dispatch_algorithm")
+        logical_to_rank_dispatch_physical_map = None
+        if ep_dispatch_algorithm == "static":
+            logical_to_rank_dispatch_physical_map = compute_logical_to_rank_dispatch_physical_map(
                 logical_to_all_physical_map=logical_to_all_physical_map,
                 num_gpus=ep_size,
                 num_physical_experts=num_physical_experts,
                 # TODO improve when we have real EP rank
                 ep_rank=torch.distributed.get_rank() % ep_size,
-            ),
+            )
+
+        return ExpertLocationMetadata(
+            physical_to_logical_map=physical_to_logical_map,
+            logical_to_all_physical_map=logical_to_all_physical_map_padded,
+            logical_to_all_physical_map_num_valid=logical_to_all_physical_map_num_valid,
+            logical_to_rank_dispatch_physical_map=logical_to_rank_dispatch_physical_map,
         )
 
     # -------------------------------- mutation ------------------------------------
@@ -230,8 +235,15 @@ class ExpertLocationMetadata:
             "logical_to_all_physical_map_num_valid",
             "logical_to_rank_dispatch_physical_map",
         ]:
+            src = getattr(other, field)
             dst = getattr(self, field)
-            dst[...] = getattr(other, field)
+            if (dst is None) or (src is None):
+                # Either both are None or src is intentionally omitted
+                assert (dst is None) or (src is None)
+                # Keep existing dst if src is None; not used in non-static modes
+                continue
+            # In-place copy to preserve storage
+            dst[...] = src
 
     # -------------------------------- usage ------------------------------------
 
@@ -305,49 +317,56 @@ def compute_logical_to_rank_dispatch_physical_map(
     ep_rank: int,
     seed: int = 42,
 ):
+    # Optimized to compute only the slice needed for ep_rank
     r = random.Random(seed)
 
     num_local_physical_experts = num_physical_experts // num_gpus
     num_layers, num_logical_experts, _ = logical_to_all_physical_map.shape
     dtype = logical_to_all_physical_map.dtype
 
-    logical_to_rank_dispatch_physical_map = torch.full(
-        size=(num_gpus, num_layers, num_logical_experts),
-        fill_value=-1,
-        dtype=dtype,
-    )
-
+    # Collect results in Python list to reduce tensor indexing overhead
+    result = []
     for layer_id in range(num_layers):
+        row = []
         for logical_expert_id in range(num_logical_experts):
             candidate_physical_expert_ids = _logical_to_all_physical_raw(
                 logical_to_all_physical_map, layer_id, logical_expert_id
             )
-            output_partial = logical_to_rank_dispatch_physical_map[
-                :, layer_id, logical_expert_id
-            ]
-
-            for gpu_id in range(num_gpus):
-                same_gpu_physical_expert_ids = [
-                    physical_expert_id
-                    for physical_expert_id in candidate_physical_expert_ids
-                    if _compute_gpu_id_of_physical_expert(
-                        physical_expert_id, num_local_physical_experts
-                    )
-                    == gpu_id
-                ]
-                if len(same_gpu_physical_expert_ids) > 0:
-                    output_partial[gpu_id] = same_gpu_physical_expert_ids[0]
-
-            num_remain = torch.sum(output_partial == -1).item()
-            output_partial[output_partial == -1] = torch.tensor(
-                _fair_choices(candidate_physical_expert_ids, k=num_remain, r=r),
-                dtype=dtype,
+
+            # Determine GPUs that already have local candidates
+            gpus_with_local = set(
+                _compute_gpu_id_of_physical_expert(x, num_local_physical_experts)
+                for x in candidate_physical_expert_ids
             )
 
-    assert torch.all(logical_to_rank_dispatch_physical_map != -1)
+            if ep_rank in gpus_with_local:
+                # Pick the first candidate on the same GPU
+                for pid in candidate_physical_expert_ids:
+                    if (
+                        _compute_gpu_id_of_physical_expert(
+                            pid, num_local_physical_experts
+                        )
+                        == ep_rank
+                    ):
+                        row.append(pid)
+                        break
+                # Advance RNG state to keep deterministic behavior across layers
+                num_remain = num_gpus - len(gpus_with_local)
+                if num_remain > 0:
+                    _fair_choices(candidate_physical_expert_ids, k=num_remain, r=r)
+            else:
+                # Compute fair choices for remaining GPUs and select the element for ep_rank
+                num_remain = num_gpus - len(gpus_with_local)
+                ans = _fair_choices(
+                    candidate_physical_expert_ids, k=num_remain, r=r
+                )
+                # Position of ep_rank among remaining GPUs in ascending order
+                pos = sum((g not in gpus_with_local) for g in range(ep_rank))
+                row.append(ans[pos])
+        result.append(row)
 
     device = logical_to_all_physical_map.device
-    return logical_to_rank_dispatch_physical_map[ep_rank, :, :].to(device)
+    return torch.tensor(result, dtype=dtype, device=device)
 
 
 def _logical_to_all_physical_raw(
diff --git a/python/sglang/srt/managers/expert_location_dispatch.py b/python/sglang/srt/managers/expert_location_dispatch.py
index 6880b01a2..d9b9e3dde 100644
--- a/python/sglang/srt/managers/expert_location_dispatch.py
+++ b/python/sglang/srt/managers/expert_location_dispatch.py
@@ -23,9 +23,9 @@ from sglang.srt.managers.schedule_batch import global_server_args_dict
 
 @dataclass
 class ExpertLocationDispatchInfo:
-    ep_dispatch_algorithm: Literal["static", "random"]
+    ep_dispatch_algorithm: Literal["static", "dynamic", "fake"]
     # (num_logical_experts,)
-    partial_logical_to_rank_dispatch_physical_map: torch.Tensor
+    partial_logical_to_rank_dispatch_physical_map: Optional[torch.Tensor]
     # (num_logical_experts, X)
     partial_logical_to_all_physical_map: torch.Tensor
     # (num_logical_experts,)
@@ -42,9 +42,13 @@ class ExpertLocationDispatchInfo:
 
         return cls(
             ep_dispatch_algorithm=ep_dispatch_algorithm,
-            partial_logical_to_rank_dispatch_physical_map=expert_location_metadata.logical_to_rank_dispatch_physical_map[
-                layer_id, :
-            ],
+            partial_logical_to_rank_dispatch_physical_map=(
+                expert_location_metadata.logical_to_rank_dispatch_physical_map[
+                    layer_id, :
+                ]
+                if ep_dispatch_algorithm == "static"
+                else None
+            ),
             partial_logical_to_all_physical_map=expert_location_metadata.logical_to_all_physical_map[
                 layer_id, :
             ],
@@ -63,7 +67,8 @@ def transform_select_experts_inputs(
     if (info is not None) and (info.ep_dispatch_algorithm == "fake"):
         router_logits = torch.randn_like(router_logits)
         if correction_bias is not None:
-            correction_bias = torch.zeros_like(correction_bias)
+            # Zero in-place to avoid extra allocation
+            correction_bias.zero_()
     return router_logits, correction_bias
 
 
@@ -83,6 +88,7 @@ def topk_ids_logical_to_physical(
 def _topk_ids_logical_to_physical_static(
     topk_ids: torch.Tensor, info: Optional[ExpertLocationDispatchInfo]
 ) -> torch.Tensor:
+    assert info is not None and info.partial_logical_to_rank_dispatch_physical_map is not None
     return info.partial_logical_to_rank_dispatch_physical_map[topk_ids]
 
 
