{"instance_id": "/home/ubuntu/OmniPerf-Bench/sglan__repo-f081541", "model_patch": "diff --git a/python/sglang/srt/models/deepseek_v2.py b/python/sglang/srt/models/deepseek_v2.py\nindex 26c5e617a..8f2385d3e 100644\n--- a/python/sglang/srt/models/deepseek_v2.py\n+++ b/python/sglang/srt/models/deepseek_v2.py\n@@ -941,6 +941,12 @@ class DeepseekV2AttentionMLA(nn.Module):\n     ) -> torch.Tensor:\n \n         assert forward_batch.num_prefix_chunks is not None\n+        # Preallocate ping-pong buffers to avoid per-chunk allocations\n+        tmp_output_a = torch.empty_like(accum_output)\n+        tmp_output_b = torch.empty_like(accum_output)\n+        tmp_lse_a = torch.empty_like(accum_lse)\n+        tmp_lse_b = torch.empty_like(accum_lse)\n+\n         for i in range(forward_batch.num_prefix_chunks):\n             forward_batch.set_prefix_chunk_idx(i)\n \n@@ -977,10 +983,13 @@ class DeepseekV2AttentionMLA(nn.Module):\n \n             output, lse = self.attn_mha(q, k, v, forward_batch, save_kv_cache=False)\n             lse = torch.transpose(lse, 0, 1).contiguous()\n-            tmp_output = torch.empty_like(accum_output)\n-            tmp_lse = torch.empty_like(accum_lse)\n-            merge_state_v2(output, lse, accum_output, accum_lse, tmp_output, tmp_lse)\n-            accum_output, accum_lse = tmp_output, tmp_lse\n+            # Alternate between two buffers so src/dst never alias\n+            if (i & 1) == 0:\n+                out_buf, lse_buf = tmp_output_a, tmp_lse_a\n+            else:\n+                out_buf, lse_buf = tmp_output_b, tmp_lse_b\n+            merge_state_v2(output, lse, accum_output, accum_lse, out_buf, lse_buf)\n+            accum_output, accum_lse = out_buf, lse_buf\n \n         return accum_output\n \n@@ -1607,7 +1616,8 @@ class DeepseekV2ForCausalLM(nn.Module):\n                     \"up_proj.weight\",\n                     \"up_proj.weight_scale_inv\",\n                 ]\n-            names_to_remove = []\n+            # Use a set for O(1) membership checks and avoid duplicates\n+            names_to_remove = set()\n             for moe_layer in tqdm(\n                 range(\n                     self.config.first_k_dense_replace,\n@@ -1622,16 +1632,17 @@ class DeepseekV2ForCausalLM(nn.Module):\n                         shared_expert_weight_name = (\n                             f\"model.layers.{moe_layer}.mlp.shared_experts.{suffix}\"\n                         )\n+                        # Avoid unnecessary tensor cloning; reuse the same tensor\n                         weights_list.append(\n                             (\n                                 f\"model.layers.{moe_layer}.\"\n                                 f\"mlp.experts.\"\n                                 f\"{self.config.n_routed_experts + num_repeat}\"\n                                 f\".{suffix}\",\n-                                weights_dict[shared_expert_weight_name].clone(),\n+                                weights_dict[shared_expert_weight_name],\n                             )\n                         )\n-                        names_to_remove += [shared_expert_weight_name]\n+                        names_to_remove.add(shared_expert_weight_name)\n             weights = [w for w in weights_list if w[0] not in names_to_remove]\n \n         # Params for weights, fp8 weight scales, fp8 activation scales\n", "model_name_or_path": "gpt-5-2025-08-07"}
