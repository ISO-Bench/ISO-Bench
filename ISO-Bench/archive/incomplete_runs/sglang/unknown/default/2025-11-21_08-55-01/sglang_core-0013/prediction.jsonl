{"instance_id": "/home/ubuntu/OmniPerf-Bench/sglan__repo-e3bed74", "model_patch": "diff --git a/python/sglang/srt/layers/moe/topk.py b/python/sglang/srt/layers/moe/topk.py\nindex a7af87144..4f5ac1556 100644\n--- a/python/sglang/srt/layers/moe/topk.py\n+++ b/python/sglang/srt/layers/moe/topk.py\n@@ -35,6 +35,35 @@ if _is_cuda or _is_hip:\n expert_distribution_recorder = ExpertDistributionRecorder()\n \n \n+def _mask_topk_ids_padded_region(\n+    topk_ids: torch.Tensor, num_token_non_padded: Optional[torch.Tensor]\n+) -> None:\n+    \"\"\"Mask padded tokens' expert ids to -1 in-place.\n+\n+    Args:\n+        topk_ids: Tensor of shape [num_tokens, topk] or flattened, int dtype.\n+        num_token_non_padded: Number of valid tokens (scalar tensor or int). If None, do nothing.\n+    \"\"\"\n+    if num_token_non_padded is None:\n+        return\n+    # support both python int and 0-d tensor\n+    if isinstance(num_token_non_padded, torch.Tensor):\n+        if num_token_non_padded.numel() != 1:\n+            return\n+        n_valid = int(num_token_non_padded.item())\n+    else:\n+        n_valid = int(num_token_non_padded)\n+    if n_valid < 0:\n+        return\n+    # Reshape to 2D if needed and set padded part to -1\n+    if topk_ids.dim() == 1:\n+        if n_valid < topk_ids.shape[0]:\n+            topk_ids[n_valid:] = -1\n+    else:\n+        num_tokens = topk_ids.shape[0]\n+        if n_valid < num_tokens:\n+            topk_ids[n_valid:, :] = -1\n+\n def fused_topk_native(\n     hidden_states: torch.Tensor,\n     gating_output: torch.Tensor,\n@@ -44,11 +73,7 @@ def fused_topk_native(\n     assert (\n         hidden_states.shape[0] == gating_output.shape[0]\n     ), f\"Number of tokens mismatch, {hidden_states.shape=} vs {gating_output.shape=}\"\n-    M, _ = hidden_states.shape\n-    topk_weights = torch.empty(\n-        M, topk, dtype=torch.float32, device=hidden_states.device\n-    )\n-    topk_ids = torch.empty(M, topk, dtype=torch.int32, device=hidden_states.device)\n+    # Directly compute without preallocating unused buffers\n     topk_weights = F.softmax(gating_output.float(), dim=-1)\n     topk_weights, topk_ids = torch.topk(topk_weights, topk, dim=-1)\n     if renormalize:\n@@ -111,14 +136,17 @@ def grouped_topk(\n     group_idx = torch.topk(group_scores, k=topk_group, dim=-1, sorted=False)[\n         1\n     ]  # [n, top_k_group]\n-    group_mask = torch.zeros_like(group_scores)  # [n, n_group]\n-    group_mask.scatter_(1, group_idx, 1)  # [n, n_group]\n+    # build boolean mask to avoid dtype casts\n+    group_mask = torch.zeros(\n+        group_scores.shape, dtype=torch.bool, device=group_scores.device\n+    )  # [n, n_group]\n+    group_mask.scatter_(1, group_idx, True)  # [n, n_group]\n     score_mask = (\n         group_mask.unsqueeze(-1)\n         .expand(num_token, num_expert_group, scores.shape[-1] // num_expert_group)\n         .reshape(num_token, -1)\n     )  # [n, e]\n-    tmp_scores = scores.masked_fill(~score_mask.bool(), 0.0)  # [n, e]\n+    tmp_scores = scores.masked_fill(~score_mask, 0.0)  # [n, e]\n     topk_weights, topk_ids = torch.topk(tmp_scores, k=topk, dim=-1, sorted=False)\n     if n_share_experts_fusion:\n         topk_ids[:, -1] = torch.randint(\n@@ -166,16 +194,16 @@ def biased_grouped_topk_impl(\n     group_idx = torch.topk(group_scores, k=topk_group, dim=-1, sorted=False)[\n         1\n     ]  # [n, top_k_group]\n-    group_mask = torch.zeros_like(group_scores)  # [n, n_group]\n-    group_mask.scatter_(1, group_idx, 1)  # [n, n_group]\n+    group_mask = torch.zeros(\n+        group_scores.shape, dtype=torch.bool, device=group_scores.device\n+    )  # [n, n_group]\n+    group_mask.scatter_(1, group_idx, True)  # [n, n_group]\n     score_mask = (\n         group_mask.unsqueeze(-1)\n         .expand(num_token, num_expert_group, scores.shape[-1] // num_expert_group)\n         .reshape(num_token, -1)\n     )  # [n, e]\n-    tmp_scores = scores_for_choice.masked_fill(\n-        ~score_mask.bool(), float(\"-inf\")\n-    )  # [n, e]\n+    tmp_scores = scores_for_choice.masked_fill(~score_mask, float(\"-inf\"))  # [n, e]\n     _, topk_ids = torch.topk(tmp_scores, k=topk, dim=-1, sorted=False)\n     topk_weights = scores.gather(1, topk_ids)\n \n@@ -268,6 +296,7 @@ def select_experts(\n     correction_bias: Optional[torch.Tensor] = None,\n     torch_native: bool = False,\n     routed_scaling_factor: Optional[float] = None,\n+    num_token_non_padded: Optional[torch.Tensor] = None,\n ):\n     n_share_experts_fusion = global_server_args_dict[\"n_share_experts_fusion\"]\n     # DeepSeek V2/V3/R1 series models use grouped_top_k\n@@ -319,6 +348,9 @@ def select_experts(\n             renormalize=renormalize,\n         )\n \n+    # Mask padded tokens so dispatchers can skip them efficiently\n+    _mask_topk_ids_padded_region(topk_ids, num_token_non_padded)\n+\n     expert_distribution_recorder.record_new_token(topk_ids)\n \n     return topk_weights, topk_ids\ndiff --git a/python/sglang/srt/model_executor/cuda_graph_runner.py b/python/sglang/srt/model_executor/cuda_graph_runner.py\nindex e88022beb..1188223af 100644\n--- a/python/sglang/srt/model_executor/cuda_graph_runner.py\n+++ b/python/sglang/srt/model_executor/cuda_graph_runner.py\n@@ -282,16 +282,14 @@ class CudaGraphRunner:\n                 self.encoder_lens = None\n             if self.enable_dp_attention or self.enable_sp_layernorm:\n                 # TODO(ch-wan): SP layernorm should use a different logic to manage gathered_buffer\n-                self.gathered_buffer = torch.zeros(\n+                self.gathered_buffer = torch.empty(\n                     (\n                         self.max_bs * self.dp_size * self.num_tokens_per_bs,\n                         self.model_runner.model_config.hidden_size,\n                     ),\n                     dtype=self.model_runner.dtype,\n                 )\n-                self.global_num_tokens_gpu = torch.zeros(\n-                    (self.dp_size,), dtype=torch.int32\n-                )\n+                self.global_num_tokens_gpu = torch.empty((self.dp_size,), dtype=torch.int32)\n \n         # Capture\n         try:\ndiff --git a/python/sglang/srt/model_executor/forward_batch_info.py b/python/sglang/srt/model_executor/forward_batch_info.py\nindex 5018f92d5..817c721e4 100644\n--- a/python/sglang/srt/model_executor/forward_batch_info.py\n+++ b/python/sglang/srt/model_executor/forward_batch_info.py\n@@ -305,7 +305,7 @@ class ForwardBatch:\n             ).to(device, non_blocking=True)\n \n             sum_len = sum(batch.global_num_tokens)\n-            ret.gathered_buffer = torch.zeros(\n+            ret.gathered_buffer = torch.empty(\n                 (sum_len, model_runner.model_config.hidden_size),\n                 dtype=model_runner.dtype,\n                 device=device,\n@@ -683,7 +683,8 @@ def compute_position_torch(\n         ],\n         axis=0,\n     )\n-    extend_start_loc = torch.zeros_like(extend_seq_lens)\n+    extend_start_loc = torch.empty_like(extend_seq_lens)\n+    extend_start_loc[0] = 0\n     extend_start_loc[1:] = torch.cumsum(extend_seq_lens[:-1], dim=0)\n     return positions.to(torch.int64), extend_start_loc\n \ndiff --git a/python/sglang/srt/models/deepseek_v2.py b/python/sglang/srt/models/deepseek_v2.py\nindex 436e966db..ced783421 100644\n--- a/python/sglang/srt/models/deepseek_v2.py\n+++ b/python/sglang/srt/models/deepseek_v2.py\n@@ -287,12 +287,15 @@ class DeepseekV2MoE(nn.Module):\n             )\n \n     def forward(\n-        self, hidden_states: torch.Tensor, forward_mode: Optional[ForwardMode] = None\n+        self,\n+        hidden_states: torch.Tensor,\n+        forward_mode: Optional[ForwardMode] = None,\n+        num_token_non_padded: Optional[int] = None,\n     ) -> torch.Tensor:\n         if not global_server_args_dict[\"enable_deepep_moe\"]:\n             return self.forward_normal(hidden_states)\n         else:\n-            return self.forward_deepep(hidden_states, forward_mode)\n+            return self.forward_deepep(hidden_states, forward_mode, num_token_non_padded)\n \n     def forward_normal(self, hidden_states: torch.Tensor) -> torch.Tensor:\n         shared_output = self._forward_shared_experts(hidden_states)\n@@ -309,7 +312,10 @@ class DeepseekV2MoE(nn.Module):\n         return final_hidden_states\n \n     def forward_deepep(\n-        self, hidden_states: torch.Tensor, forward_mode: ForwardMode\n+        self,\n+        hidden_states: torch.Tensor,\n+        forward_mode: ForwardMode,\n+        num_token_non_padded: Optional[int] = None,\n     ) -> torch.Tensor:\n         shared_output = None\n         if (\n@@ -330,6 +336,7 @@ class DeepseekV2MoE(nn.Module):\n                 num_expert_group=self.num_expert_group,\n                 correction_bias=self.correction_bias,\n                 routed_scaling_factor=self.routed_scaling_factor,\n+                num_token_non_padded=num_token_non_padded,\n             )\n         else:\n             topk_idx = torch.full(\n@@ -1339,7 +1346,11 @@ class DeepseekV2DecoderLayer(nn.Module):\n             and (not self.info.is_sparse)\n             and hidden_states.shape[0] == 0\n         ):\n-            hidden_states = self.mlp(hidden_states, forward_batch.forward_mode)\n+            hidden_states = self.mlp(\n+                hidden_states,\n+                forward_batch.forward_mode,\n+                num_token_non_padded=forward_batch.input_ids.shape[0],\n+            )\n \n         if self.is_last_layer and self.attn_tp_size != 1:\n             hidden_states += residual\n", "model_name_or_path": "gpt-5-2025-08-07"}
