{"instance_id": "/home/ubuntu/OmniPerf-Bench/sglan__repo-5ac8b80", "model_patch": "diff --git a/README.md b/README.md\nindex 2ac666c6b..93ca1f79d 100644\n--- a/README.md\n+++ b/README.md\n@@ -378,6 +378,15 @@ python -m sglang.launch_server --model-path meta-llama/Llama-2-7b-chat-hf --port\n ```\n - See [hyperparameter_tuning.md](docs/hyperparameter_tuning.md) on tuning hyperparameters for better performance.\n \n+- Add `--nnodes 2` to run tensor parallelism on multiple nodes. If you have two nodes with two GPUs on each node and want to run TP=4, let `sgl-dev-1` be the hostname of the first node and `50000` be an available port.\n+```\n+# Node 0\n+python -m sglang.launch_server --model-path meta-llama/Llama-2-7b-chat-hf --tp 4 --nccl-init-addr sgl-dev-1:50000 --nnodes 2 --node-rank 0\n+\n+# Node 1\n+python -m sglang.launch_server --model-path meta-llama/Llama-2-7b-chat-hf --tp 4 --nccl-init-addr sgl-dev-1:50000 --nnodes 2 --node-rank 1\n+```\n+\n ### Supported Models\n - Llama\n - Mistral\ndiff --git a/benchmark/latency_throughput/bench_one.py b/benchmark/latency_throughput/bench_one.py\nindex cfd96b54c..51de90949 100644\n--- a/benchmark/latency_throughput/bench_one.py\n+++ b/benchmark/latency_throughput/bench_one.py\n@@ -14,6 +14,9 @@ import requests\n def run_one_batch_size(bs):\n     url = f\"{args.host}:{args.port}\"\n     max_new_tokens = args.max_tokens\n+    # Cache commonly used lens to avoid repeated attribute access\n+    input_len = args.input_len if args.input_len else 1\n+    output_len = max_new_tokens\n \n     if args.input_len:\n         input_ids = [\n@@ -94,10 +97,14 @@ def run_one_batch_size(bs):\n         ret = response\n     else:\n         ret = response.json()\n-    print(ret)\n+    # Avoid printing full payloads to reduce I/O overhead\n+    if isinstance(ret, dict):\n+        print({k: ret.get(k) for k in list(ret.keys())[:3]})\n+    else:\n+        print(str(ret)[:200])\n \n-    output_throughput = bs * max_new_tokens / latency\n-    overall_throughput = bs * (args.input_len + max_new_tokens) / latency\n+    output_throughput = bs * output_len / latency\n+    overall_throughput = bs * (input_len + output_len) / latency\n     print(f\"latency: {latency:.2f} s\")\n     print(f\"decode throughput: {output_throughput:.2f} token/s\")\n     print(f\"overall throughput: {overall_throughput:.2f} token/s\")\n@@ -105,8 +112,8 @@ def run_one_batch_size(bs):\n     with open(\"results.jsonl\", \"a\") as fout:\n         res = {\n             \"backend\": args.backend,\n-            \"input_len\": args.input_len,\n-            \"output_len\": args.max_tokens,\n+            \"input_len\": input_len,\n+            \"output_len\": output_len,\n             \"batch_size\": bs,\n             \"latency\": latency,\n             \"output_throughput\": output_throughput,\ndiff --git a/benchmark/latency_throughput/bench_serving.py b/benchmark/latency_throughput/bench_serving.py\nindex 23e8245f2..97df86f68 100644\n--- a/benchmark/latency_throughput/bench_serving.py\n+++ b/benchmark/latency_throughput/bench_serving.py\n@@ -175,15 +175,16 @@ async def send_request(\n \n     if backend != \"ginfer\":\n         timeout = aiohttp.ClientTimeout(total=3 * 3600)\n-        async with aiohttp.ClientSession(timeout=timeout) as session:\n+        connector = aiohttp.TCPConnector(limit=0, ttl_dns_cache=300)\n+        async with aiohttp.ClientSession(timeout=timeout, connector=connector) as session:\n             while True:\n                 async with session.post(\n                     api_url, headers=headers, json=pload\n                 ) as response:\n-                    chunks = []\n+                    buf = bytearray()\n                     async for chunk, _ in response.content.iter_chunks():\n-                        chunks.append(chunk)\n-                output = b\"\".join(chunks).decode(\"utf-8\")\n+                        buf.extend(chunk)\n+                output = bytes(buf).decode(\"utf-8\")\n                 output = json.loads(output)\n \n                 # Re-send the request if it failed.\ndiff --git a/python/sglang/README.md b/python/sglang/README.md\nindex c8c093706..7de19ef4f 100644\n--- a/python/sglang/README.md\n+++ b/python/sglang/README.md\n@@ -3,10 +3,10 @@\n - `backend`: Various backends for the language interpreter.\n - `lang`: The frontend language.\n - `srt`: The runtime for running local models.\n+- SRT supports tensor parallelism across multiple nodes via `--nnodes` and `--node-rank` in `server_args`.\n - `test`: Test utilities.\n - `api.py`: Public API.\n - `bench_latency.py`: Benchmark utilities.\n - `global_config.py`: The global configs and constants.\n - `launch_server.py`: The entry point of launching local server.\n - `utils.py`: Common utilities.\n-\ndiff --git a/python/sglang/srt/managers/controller/model_runner.py b/python/sglang/srt/managers/controller/model_runner.py\nindex d68d9af32..0b0d3283e 100644\n--- a/python/sglang/srt/managers/controller/model_runner.py\n+++ b/python/sglang/srt/managers/controller/model_runner.py\n@@ -187,8 +187,9 @@ class ModelRunner:\n         \"\"\"We need to run a small matmul to init cublas. Otherwise, it will raise some errors later.\"\"\"\n         dtype = torch.float16\n         device = \"cuda\"\n-        a = torch.ones((16, 16), dtype=dtype, device=device)\n-        b = torch.ones((16, 16), dtype=dtype, device=device)\n+        # Use empty to avoid unnecessary memory initialization.\n+        a = torch.empty((16, 16), dtype=dtype, device=device)\n+        b = torch.empty((16, 16), dtype=dtype, device=device)\n         c = a @ b\n         return c\n \ndiff --git a/python/sglang/srt/server.py b/python/sglang/srt/server.py\nindex 6cda67dea..4953b7fc2 100644\n--- a/python/sglang/srt/server.py\n+++ b/python/sglang/srt/server.py\n@@ -207,8 +207,9 @@ def launch_server(server_args: ServerArgs, pipe_finish_writer, model_overide_arg\n             logger.info(\n                 f\"[node_rank={server_args.node_rank}]: Listen for connections...\"\n             )\n+            # Avoid busy-wait to reduce CPU burn while waiting for rank 0.\n             while True:\n-                pass\n+                time.sleep(0.1)\n \n     # Launch processes\n     tokenizer_manager = TokenizerManager(server_args, port_args, model_overide_args)\n", "model_name_or_path": "gpt-5-2025-08-07"}
