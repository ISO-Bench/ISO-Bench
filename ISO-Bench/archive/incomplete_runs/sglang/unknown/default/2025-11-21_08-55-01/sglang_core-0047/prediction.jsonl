{"instance_id": "/home/ubuntu/OmniPerf-Bench/sglan__repo-ebaa2f3", "model_patch": "diff --git a/python/sglang/srt/constrained/outlines_backend.py b/python/sglang/srt/constrained/outlines_backend.py\nindex cc68b97f8..0d8ffeac4 100644\n--- a/python/sglang/srt/constrained/outlines_backend.py\n+++ b/python/sglang/srt/constrained/outlines_backend.py\n@@ -81,9 +81,23 @@ class OutlinesGrammar(BaseGrammarObject):\n     ):\n         self.state = next_state\n \n-    def fill_vocab_mask(self, vocab_mask: torch.Tensor):\n-        vocab_mask.fill_(1)\n-        vocab_mask[self.guide.get_next_instruction(self.state).tokens] = 0\n+    def allocate_vocab_mask(\n+        self, vocab_size: int, batch_size: int, device\n+    ) -> torch.Tensor:\n+        # Allocate without initializing to avoid redundant memset.\n+        # Individual rows will be filled on demand.\n+        return torch.empty(batch_size, vocab_size, dtype=torch.bool, device=device)\n+\n+    def fill_vocab_mask(self, vocab_mask: torch.Tensor, idx: int) -> None:\n+        # Work on a single row to reduce unnecessary writes.\n+        row = vocab_mask[idx]\n+        row.fill_(1)\n+        row[self.guide.get_next_instruction(self.state).tokens] = 0\n+\n+    @staticmethod\n+    def apply_vocab_mask(logits: torch.Tensor, vocab_mask: torch.Tensor) -> None:\n+        # In-place to avoid an extra tensor allocation.\n+        logits.masked_fill_(vocab_mask, float(\"-inf\"))\n \n     def copy(self):\n         return OutlinesGrammar(self.guide, self.jump_forward_map)\ndiff --git a/python/sglang/srt/constrained/xgrammar_backend.py b/python/sglang/srt/constrained/xgrammar_backend.py\nindex ab4df5c98..2bae7f527 100644\n--- a/python/sglang/srt/constrained/xgrammar_backend.py\n+++ b/python/sglang/srt/constrained/xgrammar_backend.py\n@@ -80,11 +80,20 @@ class XGrammarGrammar(BaseGrammarObject):\n         for i in range(k, len(new_output_ids)):\n             assert self.matcher.accept_token(new_output_ids[i])\n \n-    def fill_vocab_mask(self, vocab_mask: torch.Tensor):\n+    def allocate_vocab_mask(\n+        self, vocab_size: int, batch_size: int, device\n+    ) -> torch.Tensor:\n+        # Start with uninitialized memory; rows will be populated on demand.\n+        return torch.empty(batch_size, vocab_size, dtype=torch.bool, device=device)\n+\n+    def fill_vocab_mask(self, vocab_mask: torch.Tensor, idx: int) -> None:\n         # Note that this bitmask is a bitset, not bool\n         bitmask = self.matcher.get_next_token_bitmask()\n-        # Mask the tokens that are not allowed\n-        vocab_mask[\n+        # Mask the tokens that are not allowed for this row only\n+        row = vocab_mask[idx]\n+        # Ensure the row starts with no mask for allowed tokens\n+        row.fill_(0)\n+        row[\n             self.matcher.get_rejected_tokens_from_bitmask(bitmask, self.vocab_size)\n         ] = 1\n \ndiff --git a/python/sglang/srt/model_executor/model_runner.py b/python/sglang/srt/model_executor/model_runner.py\nindex 02750d5df..25181fc28 100644\n--- a/python/sglang/srt/model_executor/model_runner.py\n+++ b/python/sglang/srt/model_executor/model_runner.py\n@@ -645,7 +645,7 @@ class ModelRunner:\n \n         # Apply regex vocab_mask\n         if sampling_info.vocab_mask is not None:\n-            logits = logits.masked_fill(sampling_info.vocab_mask, float(\"-inf\"))\n+            logits.masked_fill_(sampling_info.vocab_mask, float(\"-inf\"))\n \n         return logits\n \ndiff --git a/python/sglang/srt/sampling/sampling_batch_info.py b/python/sglang/srt/sampling/sampling_batch_info.py\nindex a341c2b17..0c5eead8c 100644\n--- a/python/sglang/srt/sampling/sampling_batch_info.py\n+++ b/python/sglang/srt/sampling/sampling_batch_info.py\n@@ -137,15 +137,18 @@ class SamplingBatchInfo:\n             self.vocab_mask = None\n             return\n \n-        self.vocab_mask = torch.zeros(\n-            len(self.temperatures),\n-            self.vocab_size,\n-            dtype=torch.bool,\n-            device=self.device,\n+        bs = len(self.temperatures)\n+        # Allocate without initialization; fill rows on demand to reduce redundant memset.\n+        self.vocab_mask = torch.empty(\n+            bs, self.vocab_size, dtype=torch.bool, device=self.device\n         )\n         for i, grammar in enumerate(self.grammars):\n             if grammar is not None:\n-                grammar.fill_vocab_mask(self.vocab_mask[i])\n+                # Let the grammar populate the mask for this row only\n+                grammar.fill_vocab_mask(self.vocab_mask, i)\n+            else:\n+                # No grammar for this request -> no masking\n+                self.vocab_mask[i].fill_(0)\n \n     def filter_batch(self, unfinished_indices: List[int], new_indices: torch.Tensor):\n         if self.penalizer_orchestrator:\n@@ -178,11 +181,14 @@ class SamplingBatchInfo:\n                 shape, dtype = lhs.shape[1:], lhs.dtype\n             else:\n                 shape, dtype = rhs.shape[1:], rhs.dtype\n-            with torch.dtype(dtype):\n-                if lhs is None:\n-                    lhs = torch.empty((bs1, *shape), device=device).fill_(default)\n-                if rhs is None:\n-                    rhs = torch.empty((bs2, *shape), device=device).fill_(default)\n+            if lhs is None:\n+                lhs = torch.full(\n+                    (bs1, *shape), default, dtype=dtype, device=device\n+                )\n+            if rhs is None:\n+                rhs = torch.full(\n+                    (bs2, *shape), default, dtype=dtype, device=device\n+                )\n             return torch.cat([lhs, rhs])\n \n         return None\n", "model_name_or_path": "gpt-5-2025-08-07"}
