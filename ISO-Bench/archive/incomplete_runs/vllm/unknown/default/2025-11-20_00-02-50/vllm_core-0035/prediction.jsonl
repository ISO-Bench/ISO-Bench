{"instance_id": "/home/raven/coding-mess/kernel-corp/OmniPerf-Bench/vllm__repo-0e74d79", "model_patch": "diff --git a/vllm/model_executor/models/qwen2_5_vl.py b/vllm/model_executor/models/qwen2_5_vl.py\nindex ef3d28c80..cc4d6de37 100644\n--- a/vllm/model_executor/models/qwen2_5_vl.py\n+++ b/vllm/model_executor/models/qwen2_5_vl.py\n@@ -255,10 +255,12 @@ class Qwen2_5_VisionAttention(nn.Module):\n         return q, k, v\n \n     def forward(\n-        self,\n-        x: torch.Tensor,\n-        cu_seqlens: torch.Tensor,\n-        rotary_pos_emb: torch.Tensor,\n+            self,\n+            x: torch.Tensor,\n+            cu_seqlens: torch.Tensor,\n+            rotary_pos_emb: torch.Tensor,\n+            max_seqlen: Optional[int] = None,  # Only used for Flash Attention\n+            seqlens: Optional[list[int]] = None,  # Only used for xFormers\n     ) -> torch.Tensor:\n         # [s, b, c] --> [s, b, head * 3 * head_dim]\n         x, _ = self.qkv(x)\n@@ -285,7 +287,8 @@ class Qwen2_5_VisionAttention(nn.Module):\n \n             q, k, v = (rearrange(x, \"b s ... -> (b s) ...\") for x in [q, k, v])\n \n-            max_seqlen = (cu_seqlens[1:] - cu_seqlens[:-1]).max().item()\n+            if max_seqlen is None:\n+                max_seqlen = (cu_seqlens[1:] - cu_seqlens[:-1]).max().item()\n             output = flash_attn_varlen_func(q,\n                                             k,\n                                             v,\n@@ -321,7 +324,8 @@ class Qwen2_5_VisionAttention(nn.Module):\n             from xformers import ops as xops\n             from xformers.ops.fmha.attn_bias import BlockDiagonalMask\n \n-            seqlens = (cu_seqlens[1:] - cu_seqlens[:-1]).tolist()\n+            if seqlens is None:\n+                seqlens = (cu_seqlens[1:] - cu_seqlens[:-1]).tolist()\n             attn_bias = BlockDiagonalMask.from_seqlens(q_seqlen=seqlens,\n                                                        kv_seqlen=None,\n                                                        device=q.device)\n@@ -365,10 +369,14 @@ class Qwen2_5_VisionBlock(nn.Module):\n                                      prefix=f\"{prefix}.mlp\")\n \n     def forward(self, x: torch.Tensor, cu_seqlens: torch.Tensor,\n-                rotary_pos_emb: torch.Tensor) -> torch.Tensor:\n+                rotary_pos_emb: torch.Tensor,\n+                max_seqlen: Optional[int] = None,\n+                seqlens: Optional[list[int]] = None) -> torch.Tensor:\n         x = x + self.attn(self.norm1(x),\n                           cu_seqlens=cu_seqlens,\n-                          rotary_pos_emb=rotary_pos_emb)\n+                          rotary_pos_emb=rotary_pos_emb,\n+                          max_seqlen=max_seqlen,\n+                          seqlens=seqlens)\n         x = x + self.mlp(self.norm2(x))\n         return x\n \n@@ -557,7 +565,7 @@ class Qwen2_5_VisionTransformer(nn.Module):\n             pos_ids.append(\n                 torch.stack([hpos_ids, wpos_ids], dim=-1).repeat(t, 1))\n         pos_ids = torch.cat(pos_ids, dim=0)\n-        max_grid_size = grid_thw[:, 1:].max()\n+        max_grid_size = int(grid_thw[:, 1:].max().item())\n         rotary_pos_emb_full = self.rotary_pos_emb(max_grid_size)\n         rotary_pos_emb = rotary_pos_emb_full[pos_ids].flatten(1)\n         return rotary_pos_emb\n@@ -586,14 +594,14 @@ class Qwen2_5_VisionTransformer(nn.Module):\n             index_padded = index_padded.permute(0, 1, 3, 2, 4).reshape(\n                 grid_t, num_windows_h * num_windows_w, vit_merger_window_size,\n                 vit_merger_window_size)\n-            seqlens = (index_padded != -100).sum([2, 3]).reshape(-1)\n+            seqlens = (index_padded != -100).sum((2, 3)).reshape(-1)\n             index_padded = index_padded.reshape(-1)\n             index_new = index_padded[index_padded != -100]\n             window_index.append(index_new + window_index_id)\n             cu_seqlens_tmp = seqlens.cumsum(\n                 0) * self.spatial_merge_unit + cu_window_seqlens[-1]\n             cu_window_seqlens.extend(cu_seqlens_tmp.tolist())\n-            window_index_id += (grid_t * llm_grid_h * llm_grid_w).item()\n+            window_index_id += int(grid_t * llm_grid_h * llm_grid_w)\n         window_index = torch.cat(window_index, dim=0)\n         return window_index, cu_window_seqlens\n \n@@ -631,16 +639,30 @@ class Qwen2_5_VisionTransformer(nn.Module):\n                                                  dim=0, dtype=torch.int32)\n         cu_seqlens = F.pad(cu_seqlens, (1, 0), \"constant\", 0)\n \n+        # precompute seqlens information to avoid repeated syncs/computation\n+        _lens_full = (cu_seqlens[1:] - cu_seqlens[:-1])\n+        _max_seqlen_full = _lens_full.max().item()\n+        _seqlens_full = _lens_full.tolist()\n+        _lens_win = (cu_window_seqlens[1:] - cu_window_seqlens[:-1])\n+        _max_seqlen_win = _lens_win.max().item()\n+        _seqlens_win = _lens_win.tolist()\n+\n         # transformers\n         hidden_states = hidden_states.unsqueeze(1)\n         for layer_num, blk in enumerate(self.blocks):\n             if layer_num in self.fullatt_block_indexes:\n                 cu_seqlens_now = cu_seqlens\n+                _max_s = _max_seqlen_full\n+                _seqs = _seqlens_full\n             else:\n                 cu_seqlens_now = cu_window_seqlens\n+                _max_s = _max_seqlen_win\n+                _seqs = _seqlens_win\n             hidden_states = blk(hidden_states,\n                                 cu_seqlens=cu_seqlens_now,\n-                                rotary_pos_emb=rotary_pos_emb)\n+                                rotary_pos_emb=rotary_pos_emb,\n+                                max_seqlen=_max_s,\n+                                seqlens=_seqs)\n \n         # For Qwen2.5-VL-3B, float16 will overflow at last block\n         # for long visual tokens sequences.\ndiff --git a/vllm/model_executor/models/qwen2_vl.py b/vllm/model_executor/models/qwen2_vl.py\nindex ac3d154dd..2dae4e5dd 100644\n--- a/vllm/model_executor/models/qwen2_vl.py\n+++ b/vllm/model_executor/models/qwen2_vl.py\n@@ -303,10 +303,12 @@ class Qwen2VisionAttention(nn.Module):\n         return q, k, v\n \n     def forward(\n-        self,\n-        x: torch.Tensor,\n-        cu_seqlens: torch.Tensor,\n-        rotary_pos_emb: torch.Tensor,\n+            self,\n+            x: torch.Tensor,\n+            cu_seqlens: torch.Tensor,\n+            rotary_pos_emb: torch.Tensor,\n+            max_seqlen: Optional[int] = None,  # Only used for Flash Attention\n+            seqlens: Optional[list[int]] = None,  # Only used for xFormers\n     ) -> torch.Tensor:\n \n         # [s, b, c] --> [s, b, 3 * head * head_dim]\n@@ -329,7 +331,8 @@ class Qwen2VisionAttention(nn.Module):\n \n             q, k, v = (rearrange(x, \"b s ... -> (b s) ...\") for x in [q, k, v])\n \n-            max_seqlen = (cu_seqlens[1:] - cu_seqlens[:-1]).max().item()\n+            if max_seqlen is None:\n+                max_seqlen = (cu_seqlens[1:] - cu_seqlens[:-1]).max().item()\n             output = flash_attn_varlen_func(q,\n                                             k,\n                                             v,\n@@ -365,7 +368,8 @@ class Qwen2VisionAttention(nn.Module):\n             from xformers import ops as xops\n             from xformers.ops.fmha.attn_bias import BlockDiagonalMask\n \n-            seqlens = (cu_seqlens[1:] - cu_seqlens[:-1]).tolist()\n+            if seqlens is None:\n+                seqlens = (cu_seqlens[1:] - cu_seqlens[:-1]).tolist()\n             attn_bias = BlockDiagonalMask.from_seqlens(q_seqlen=seqlens,\n                                                        kv_seqlen=None,\n                                                        device=q.device)\n@@ -410,10 +414,14 @@ class Qwen2VisionBlock(nn.Module):\n                                   prefix=f\"{prefix}.mlp\")\n \n     def forward(self, x: torch.Tensor, cu_seqlens: torch.Tensor,\n-                rotary_pos_emb: torch.Tensor) -> torch.Tensor:\n+                rotary_pos_emb: torch.Tensor,\n+                max_seqlen: Optional[int] = None,\n+                seqlens: Optional[list[int]] = None) -> torch.Tensor:\n         x = x + self.attn(self.norm1(x),\n                           cu_seqlens=cu_seqlens,\n-                          rotary_pos_emb=rotary_pos_emb)\n+                          rotary_pos_emb=rotary_pos_emb,\n+                          max_seqlen=max_seqlen,\n+                          seqlens=seqlens)\n         x = x + self.mlp(self.norm2(x))\n         return x\n \n@@ -599,7 +607,7 @@ class Qwen2VisionTransformer(nn.Module):\n             pos_ids.append(\n                 torch.stack([hpos_ids, wpos_ids], dim=-1).repeat(t, 1))\n         pos_ids = torch.cat(pos_ids, dim=0)\n-        max_grid_size = grid_thw[:, 1:].max()\n+        max_grid_size = int(grid_thw[:, 1:].max().item())\n         rotary_pos_emb_full = self.rotary_pos_emb(max_grid_size)\n         rotary_pos_emb = rotary_pos_emb_full[pos_ids].flatten(1)\n         return rotary_pos_emb\n@@ -622,10 +630,19 @@ class Qwen2VisionTransformer(nn.Module):\n                                                  dim=0, dtype=torch.int32)\n         cu_seqlens = F.pad(cu_seqlens, (1, 0), \"constant\", 0)\n \n+        # precompute seqlens and max_seqlen once per forward to avoid syncs\n+        _lens = (cu_seqlens[1:] - cu_seqlens[:-1])\n+        _max_seqlen = _lens.max().item()\n+        _seqlens_list = _lens.tolist()\n+\n         # transformers\n         x = x.unsqueeze(1)\n         for blk in self.blocks:\n-            x = blk(x, cu_seqlens=cu_seqlens, rotary_pos_emb=rotary_pos_emb)\n+            x = blk(x,\n+                    cu_seqlens=cu_seqlens,\n+                    rotary_pos_emb=rotary_pos_emb,\n+                    max_seqlen=_max_seqlen,\n+                    seqlens=_seqlens_list)\n \n         # adapter\n         x = self.merger(x)\n", "model_name_or_path": "gpt-5-2025-08-07"}
