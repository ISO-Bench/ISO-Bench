{"instance_id": "/home/raven/coding-mess/kernel-corp/OmniPerf-Bench/vllm__repo-067c34a", "model_patch": "diff --git a/tests/v1/attention/test_attention_backends.py b/tests/v1/attention/test_attention_backends.py\nindex b4e0101a0..0913d7a0f 100644\n--- a/tests/v1/attention/test_attention_backends.py\n+++ b/tests/v1/attention/test_attention_backends.py\n@@ -155,7 +155,8 @@ def create_and_prepopulate_kv_cache(\n         perm = torch.arange(\n             1, blocks_end)  # Sequential order starting from block 1\n \n-    inv_perm = torch.zeros(blocks_end, dtype=torch.long, device=device)\n+    inv_perm = torch.empty(blocks_end, dtype=torch.long, device=device)\n+    inv_perm[0] = 0\n     inv_perm[1:] = torch.argsort(\n         perm) + 1  # Add 1 to account for starting from block 1\n     kv_cache[:, 1:blocks_end, ...] = kv_cache[:, perm, ...]\ndiff --git a/tests/v1/attention/utils.py b/tests/v1/attention/utils.py\nindex 30cfbdda5..2d944a6b3 100644\n--- a/tests/v1/attention/utils.py\n+++ b/tests/v1/attention/utils.py\n@@ -43,9 +43,10 @@ def create_common_attn_metadata(\n         max_block_idx: int = 1000) -> CommonAttentionMetadata:\n     \"\"\"Create CommonAttentionMetadata from a BatchSpec and ModelParams.\"\"\"\n     # Create query start locations\n-    query_start_loc = torch.zeros(batch_spec.batch_size + 1,\n+    query_start_loc = torch.empty(batch_spec.batch_size + 1,\n                                   dtype=torch.int32,\n                                   device=device)\n+    query_start_loc[0] = 0\n     query_start_loc[1:] = torch.tensor(batch_spec.query_lens,\n                                        dtype=torch.int32,\n                                        device=device).cumsum(0)\ndiff --git a/vllm/v1/attention/backends/flashinfer.py b/vllm/v1/attention/backends/flashinfer.py\nindex 953ef26c8..4e9360e35 100755\n--- a/vllm/v1/attention/backends/flashinfer.py\n+++ b/vllm/v1/attention/backends/flashinfer.py\n@@ -422,12 +422,12 @@ class FlashInferMetadataBuilder(AttentionMetadataBuilder[FlashInferMetadata]):\n                 < block_table_bounds.unsqueeze(1))\n         paged_kv_indices = block_table_tensor[mask]\n \n-        paged_kv_indptr = torch.cat([\n-            torch.zeros(1,\n-                        dtype=block_table_bounds.dtype,\n-                        device=block_table_bounds.device),\n-            block_table_bounds.cumsum(dim=0, dtype=torch.int32)\n-        ])\n+        cumsum = block_table_bounds.cumsum(dim=0, dtype=torch.int32)\n+        paged_kv_indptr = torch.empty(cumsum.numel() + 1,\n+                                      dtype=cumsum.dtype,\n+                                      device=cumsum.device)\n+        paged_kv_indptr[0] = 0\n+        paged_kv_indptr[1:] = cumsum\n \n         paged_kv_last_page_len = seq_lens % page_size\n         paged_kv_last_page_len = torch.where(paged_kv_last_page_len == 0,\n", "model_name_or_path": "gpt-5-2025-08-07"}
