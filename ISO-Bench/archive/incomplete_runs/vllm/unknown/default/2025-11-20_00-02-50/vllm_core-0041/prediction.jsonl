{"instance_id": "/home/raven/coding-mess/kernel-corp/OmniPerf-Bench/vllm__repo-5eda21e", "model_patch": "diff --git a/vllm/model_executor/layers/sampler.py b/vllm/model_executor/layers/sampler.py\nindex 5c376797a..121458f81 100644\n--- a/vllm/model_executor/layers/sampler.py\n+++ b/vllm/model_executor/layers/sampler.py\n@@ -220,7 +220,7 @@ def _apply_min_tokens_penalty(\n             seqs_to_penalize: List[int] = []\n             for j, seq_id in enumerate(seq_ids):\n                 seq_data = seq_group.seq_data[seq_id]\n-                if len(seq_data.output_token_ids) < min_tokens:\n+                if len(seq_data.output_token_ids_array) < min_tokens:\n                     seqs_to_penalize.append(j)\n \n             if seqs_to_penalize:\ndiff --git a/vllm/model_executor/sampling_metadata.py b/vllm/model_executor/sampling_metadata.py\nindex 390b5d173..8b97d81f9 100644\n--- a/vllm/model_executor/sampling_metadata.py\n+++ b/vllm/model_executor/sampling_metadata.py\n@@ -1,4 +1,6 @@\n import random\n+from array import array\n+\n from dataclasses import dataclass\n from typing import Dict, List, Optional, Tuple\n \n@@ -329,8 +331,8 @@ class SamplingTensors:\n             user-defined seed for each sequence.\n         extra_entropy: extra entropy to use when generating seeds.\n         \"\"\"\n-        prompt_tokens: List[List[int]] = []\n-        output_tokens: List[List[int]] = []\n+        prompt_tokens: List[array] = []\n+        output_tokens: List[array] = []\n         top_ks: List[int] = []\n         temperatures: List[float] = []\n         top_ps: List[float] = []\n@@ -437,8 +439,8 @@ class SamplingTensors:\n                 if seq_group.do_sample:\n                     for seq_id in seq_ids:\n                         seq_data = seq_group.seq_data[seq_id]\n-                        prompt_tokens.append(list(seq_data.prompt_token_ids))\n-                        output_tokens.append(list(seq_data.output_token_ids))\n+                        prompt_tokens.append(seq_data.prompt_token_ids_array)\n+                        output_tokens.append(seq_data.output_token_ids_array)\n \n         sampling_tensors = SamplingTensors.from_lists(\n             temperatures, top_ps, top_ks, min_ps, presence_penalties,\ndiff --git a/vllm/sequence.py b/vllm/sequence.py\nindex 0cd4c7e71..d1eddcff5 100644\n--- a/vllm/sequence.py\n+++ b/vllm/sequence.py\n@@ -9,6 +9,7 @@ from typing import (TYPE_CHECKING, Dict, List, Mapping, Optional, Set, Tuple,\n                     Union)\n \n import torch\n+from array import array\n \n from vllm.lora.request import LoRARequest\n from vllm.pooling_params import PoolingParams\n@@ -119,11 +120,16 @@ class SequenceData:\n         prompt_token_ids: List[int],\n         output_token_ids: Optional[List[int]] = None,\n     ) -> None:\n+        # Maintain list/tuple for API compatibility\n         self._prompt_token_ids: List[int] = list(prompt_token_ids)\n         self._prompt_token_ids_tuple: Tuple[int, ...] = tuple(prompt_token_ids)\n         self._output_token_ids: List[int] = (\n             list(output_token_ids) if output_token_ids is not None else [])\n \n+        # Fast path arrays for internal performance-sensitive operations\n+        self._prompt_token_ids_array: array = array('I', self._prompt_token_ids)\n+        self._output_token_ids_array: array = array('I', self._output_token_ids)\n+\n         self.cumulative_logprob = 0.0\n         # The number of tokens that are computed (that run against the model).\n         self._num_computed_tokens = 0\n@@ -143,6 +149,8 @@ class SequenceData:\n     def prompt_token_ids(self, new_prompt_token_ids) -> None:\n         self._prompt_token_ids = list(new_prompt_token_ids)\n         self._prompt_token_ids_tuple = tuple(new_prompt_token_ids)\n+        # keep array in sync\n+        self._prompt_token_ids_array = array('I', self._prompt_token_ids)\n         self._update_cached_all_tokens()\n \n     @property\n@@ -152,10 +160,22 @@ class SequenceData:\n     @output_token_ids.setter\n     def output_token_ids(self, new_output_token_ids) -> None:\n         self._output_token_ids = list(new_output_token_ids)\n+        # keep array in sync\n+        self._output_token_ids_array = array('I', self._output_token_ids)\n         self._update_cached_all_tokens()\n \n+    # Optimized accessors (no tuple/list allocations)\n+    @property\n+    def prompt_token_ids_array(self) -> array:\n+        return self._prompt_token_ids_array\n+\n+    @property\n+    def output_token_ids_array(self) -> array:\n+        return self._output_token_ids_array\n+\n     def append_token_id(self, token_id: int, logprob: float) -> None:\n         self._output_token_ids.append(token_id)\n+        self._output_token_ids_array.append(token_id)\n         self._cached_all_token_ids.append(token_id)\n         self.cumulative_logprob += logprob\n \n", "model_name_or_path": "gpt-5-2025-08-07"}
