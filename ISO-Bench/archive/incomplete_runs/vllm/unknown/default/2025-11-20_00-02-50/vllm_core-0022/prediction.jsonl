{"instance_id": "/home/raven/coding-mess/kernel-corp/OmniPerf-Bench/vllm__repo-edc4fa3", "model_patch": "diff --git a/model_patch.diff b/model_patch.diff\nnew file mode 100644\nindex 000000000..e69de29bb\ndiff --git a/requirements-common.txt b/requirements-common.txt\nindex b7c94cbdb..4028b12dd 100644\n--- a/requirements-common.txt\n+++ b/requirements-common.txt\n@@ -1,6 +1,7 @@\n psutil\n sentencepiece  # Required for LLaMA tokenizer.\n numpy < 2.0.0\n+numba == 0.60.0 # JIT for N-gram speculative decoding; 0.61 lacks Python 3.9 support\n requests >= 2.26.0\n tqdm\n blake3\ndiff --git a/vllm/v1/spec_decode/ngram_proposer.py b/vllm/v1/spec_decode/ngram_proposer.py\nindex 9b116e00a..45e741788 100644\n--- a/vllm/v1/spec_decode/ngram_proposer.py\n+++ b/vllm/v1/spec_decode/ngram_proposer.py\n@@ -3,6 +3,63 @@ from typing import List, Optional\n \n import numpy as np\n \n+try:\n+    from numba import njit\n+except Exception:  # Fallback when numba is unavailable\n+    def njit(*args, **kwargs):\n+        def deco(f):\n+            return f\n+        return deco\n+\n+\n+@njit(cache=True, nogil=True)\n+def _ngram_propose_impl(context_token_ids, n, k):\n+    context_len = context_token_ids.shape[0]\n+    if n <= 0 or context_len < n:\n+        return -1, 0\n+\n+    # Pattern: last n tokens\n+    # Use contiguous slice to help JIT\n+    pattern = context_token_ids[context_len - n:context_len]\n+\n+    # Build LPS array (KMP)\n+    lps = np.empty(n, dtype=np.int32)\n+    lps[0] = 0\n+    prev = 0\n+    i = 1\n+    while i < n:\n+        if pattern[i] == pattern[prev]:\n+            prev += 1\n+            lps[i] = prev\n+            i += 1\n+        else:\n+            if prev != 0:\n+                prev = lps[prev - 1]\n+            else:\n+                lps[i] = 0\n+                i += 1\n+\n+    i = 0\n+    j = 0\n+    end = context_len - n\n+    while i < end:\n+        if context_token_ids[i] == pattern[j]:\n+            i += 1\n+            j += 1\n+            if j == n:\n+                # Found; compute slice length bounded by context end\n+                max_len = context_len - i\n+                if k < max_len:\n+                    max_len = k\n+                return i, max_len\n+        else:\n+            if j != 0:\n+                j = lps[j - 1]\n+            else:\n+                i += 1\n+\n+    return -1, 0\n+\n \n class NgramProposer:\n \n@@ -41,9 +98,11 @@ class NgramProposer:\n               followed that pattern. Here we will return [4,2,3] because \n               we only have three tokens after the match.\n         \"\"\"\n-        # TODO: Use c++ to implement the _find_subarray_kmp to\n-        # improve the efficiency\n-        return self._find_subarray_kmp(context_token_ids, n, k)\n+        # Use a JIT-accelerated KMP matcher when available.\n+        start, length = _ngram_propose_impl(context_token_ids, n, k)\n+        if start == -1:\n+            return None\n+        return context_token_ids[start:start + length]\n \n     @staticmethod\n     def _kmp_lps_array(pattern: List[int]) -> List[int]:\ndiff --git a/vllm/v1/worker/gpu_model_runner.py b/vllm/v1/worker/gpu_model_runner.py\nindex 0ecc00acc..40d8be0f4 100644\n--- a/vllm/v1/worker/gpu_model_runner.py\n+++ b/vllm/v1/worker/gpu_model_runner.py\n@@ -154,10 +154,10 @@ class GPUModelRunner(LoRAModelRunnerMixin):\n         self.num_sms = self.device_properties.multi_processor_count\n \n         # Persistent buffers for CUDA graphs.\n-        self.input_ids = torch.zeros(self.max_num_tokens,\n+        self.input_ids = torch.empty(self.max_num_tokens,\n                                      dtype=torch.int32,\n                                      device=self.device)\n-        self.positions = torch.zeros(self.max_num_tokens,\n+        self.positions = torch.empty(self.max_num_tokens,\n                                      dtype=torch.int64,\n                                      device=self.device)\n         # None in the first PP rank. The rest are set after load_model.\n@@ -175,16 +175,16 @@ class GPUModelRunner(LoRAModelRunnerMixin):\n             # identical position IDs, making M-RoPE functionally equivalent to\n             # 1D-RoPE.\n             # See page 5 of https://arxiv.org/abs/2409.12191\n-            self.mrope_positions = torch.zeros((3, self.max_num_tokens + 1),\n+            self.mrope_positions = torch.empty((3, self.max_num_tokens + 1),\n                                                dtype=torch.int64,\n                                                device=self.device)\n-            self.mrope_positions_cpu = torch.zeros(\n+            self.mrope_positions_cpu = torch.empty(\n                 (3, self.max_num_tokens + 1),\n                 dtype=torch.int64,\n                 device=\"cpu\",\n                 pin_memory=self.pin_memory)\n \n-        self.inputs_embeds = torch.zeros(\n+        self.inputs_embeds = torch.empty(\n             (self.max_num_tokens, self.hidden_size),\n             dtype=self.dtype,\n             device=self.device)\n@@ -197,27 +197,27 @@ class GPUModelRunner(LoRAModelRunnerMixin):\n         # NOTE(woosuk): These tensors are \"stateless\", i.e., they are literally\n         # a faster version of creating a new tensor every time. Thus, we should\n         # not make any assumptions about the values in these tensors.\n-        self.input_ids_cpu = torch.zeros(self.max_num_tokens,\n+        self.input_ids_cpu = torch.empty(self.max_num_tokens,\n                                          dtype=torch.int32,\n                                          device=\"cpu\",\n                                          pin_memory=self.pin_memory)\n         self.input_ids_np = self.input_ids_cpu.numpy()\n-        self.positions_cpu = torch.zeros(self.max_num_tokens,\n+        self.positions_cpu = torch.empty(self.max_num_tokens,\n                                          dtype=torch.int64,\n                                          device=\"cpu\",\n                                          pin_memory=self.pin_memory)\n         self.positions_np = self.positions_cpu.numpy()\n-        self.slot_mapping_cpu = torch.zeros(self.max_num_tokens,\n+        self.slot_mapping_cpu = torch.empty(self.max_num_tokens,\n                                             dtype=torch.int32,\n                                             device=\"cpu\",\n                                             pin_memory=self.pin_memory)\n         self.slot_mapping_np = self.slot_mapping_cpu.numpy()\n-        self.query_start_loc_cpu = torch.zeros(self.max_num_reqs + 1,\n+        self.query_start_loc_cpu = torch.empty(self.max_num_reqs + 1,\n                                                dtype=torch.int32,\n                                                device=\"cpu\",\n                                                pin_memory=self.pin_memory)\n         self.query_start_loc_np = self.query_start_loc_cpu.numpy()\n-        self.seq_lens_cpu = torch.zeros(self.max_num_reqs,\n+        self.seq_lens_cpu = torch.empty(self.max_num_reqs,\n                                         dtype=torch.int32,\n                                         device=\"cpu\",\n                                         pin_memory=self.pin_memory)\n@@ -1353,7 +1353,7 @@ class GPUModelRunner(LoRAModelRunnerMixin):\n                     num_blocks, layer_spec.block_size, layer_spec.num_kv_heads,\n                     layer_spec.head_size)\n                 dtype = layer_spec.dtype\n-                kv_caches[layer_name] = torch.zeros(kv_cache_shape,\n+                kv_caches[layer_name] = torch.empty(kv_cache_shape,\n                                                     dtype=dtype,\n                                                     device=self.device)\n             else:\n", "model_name_or_path": "gpt-5-2025-08-07"}
