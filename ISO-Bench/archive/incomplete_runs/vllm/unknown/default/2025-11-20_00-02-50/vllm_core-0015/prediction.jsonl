{"instance_id": "/home/raven/coding-mess/kernel-corp/OmniPerf-Bench/vllm__repo-3cd91dc", "model_patch": "diff --git a/model_patch.diff b/model_patch.diff\nnew file mode 100644\nindex 000000000..e69de29bb\ndiff --git a/vllm/distributed/device_communicators/pynccl.py b/vllm/distributed/device_communicators/pynccl.py\nindex fda4d007c..8e92ee380 100644\n--- a/vllm/distributed/device_communicators/pynccl.py\n+++ b/vllm/distributed/device_communicators/pynccl.py\n@@ -10,6 +10,7 @@ from vllm.distributed.device_communicators.pynccl_wrapper import (\n     ncclRedOpTypeEnum, ncclUniqueId)\n from vllm.distributed.utils import StatelessProcessGroup\n from vllm.logger import init_logger\n+from vllm.utils import current_stream\n \n logger = init_logger(__name__)\n \n@@ -73,7 +74,7 @@ class PyNcclCommunicator:\n             self.unique_id = ncclUniqueId()\n \n         if not isinstance(group, StatelessProcessGroup):\n-            tensor = torch.ByteTensor(list(self.unique_id.internal))\n+            tensor = torch.tensor(self.unique_id.internal, dtype=torch.uint8)\n             ranks = dist.get_process_group_ranks(group)\n             # arg `src` in `broadcast` is the global rank\n             dist.broadcast(tensor, src=ranks[0], group=group)\n@@ -96,9 +97,9 @@ class PyNcclCommunicator:\n             self.comm: ncclComm_t = self.nccl.ncclCommInitRank(\n                 self.world_size, self.unique_id, self.rank)\n \n-            stream = torch.cuda.current_stream()\n+            stream = current_stream()\n             # A small all_reduce for warmup.\n-            data = torch.zeros(1, device=device)\n+            data = torch.empty(1, device=device)\n             self.all_reduce(data)\n             stream.synchronize()\n             del data\n@@ -119,7 +120,7 @@ class PyNcclCommunicator:\n         out_tensor = torch.empty_like(in_tensor)\n \n         if stream is None:\n-            stream = torch.cuda.current_stream()\n+            stream = current_stream()\n         self.nccl.ncclAllReduce(buffer_type(in_tensor.data_ptr()),\n                                 buffer_type(out_tensor.data_ptr()),\n                                 in_tensor.numel(),\n@@ -141,7 +142,7 @@ class PyNcclCommunicator:\n             f\"this nccl communicator is created to work on {self.device}, \"\n             f\"but the input tensor is on {input_tensor.device}\")\n         if stream is None:\n-            stream = torch.cuda.current_stream()\n+            stream = current_stream()\n         self.nccl.ncclAllGather(\n             buffer_type(input_tensor.data_ptr()),\n             buffer_type(output_tensor.data_ptr()), input_tensor.numel(),\n@@ -162,7 +163,7 @@ class PyNcclCommunicator:\n             f\"this nccl communicator is created to work on {self.device}, \"\n             f\"but the input tensor is on {input_tensor.device}\")\n         if stream is None:\n-            stream = torch.cuda.current_stream()\n+            stream = current_stream()\n         self.nccl.ncclReduceScatter(\n             buffer_type(input_tensor.data_ptr()),\n             buffer_type(output_tensor.data_ptr()), output_tensor.numel(),\n@@ -177,7 +178,7 @@ class PyNcclCommunicator:\n             f\"this nccl communicator is created to work on {self.device}, \"\n             f\"but the input tensor is on {tensor.device}\")\n         if stream is None:\n-            stream = torch.cuda.current_stream()\n+            stream = current_stream()\n         self.nccl.ncclSend(buffer_type(tensor.data_ptr()), tensor.numel(),\n                            ncclDataTypeEnum.from_torch(tensor.dtype), dst,\n                            self.comm, cudaStream_t(stream.cuda_stream))\n@@ -189,7 +190,7 @@ class PyNcclCommunicator:\n             f\"this nccl communicator is created to work on {self.device}, \"\n             f\"but the input tensor is on {tensor.device}\")\n         if stream is None:\n-            stream = torch.cuda.current_stream()\n+            stream = current_stream()\n         self.nccl.ncclRecv(buffer_type(tensor.data_ptr()), tensor.numel(),\n                            ncclDataTypeEnum.from_torch(tensor.dtype), src,\n                            self.comm, cudaStream_t(stream.cuda_stream))\n@@ -201,7 +202,7 @@ class PyNcclCommunicator:\n             f\"this nccl communicator is created to work on {self.device}, \"\n             f\"but the input tensor is on {tensor.device}\")\n         if stream is None:\n-            stream = torch.cuda.current_stream()\n+            stream = current_stream()\n         if src == self.rank:\n             sendbuff = buffer_type(tensor.data_ptr())\n             # NCCL requires the sender also to have a receive buffer\ndiff --git a/vllm/distributed/parallel_state.py b/vllm/distributed/parallel_state.py\nindex a837c1dc5..3450b721b 100644\n--- a/vllm/distributed/parallel_state.py\n+++ b/vllm/distributed/parallel_state.py\n@@ -39,7 +39,7 @@ import vllm.distributed.kv_transfer.kv_transfer_agent as kv_transfer\n import vllm.envs as envs\n from vllm.distributed.utils import StatelessProcessGroup\n from vllm.logger import init_logger\n-from vllm.utils import direct_register_custom_op, supports_custom_op\n+from vllm.utils import direct_register_custom_op, supports_custom_op, current_stream\n \n if TYPE_CHECKING:\n     from vllm.config import VllmConfig\n@@ -300,7 +300,7 @@ class GroupCoordinator:\n \n         # ensure all initialization operations complete before attempting to\n         # capture the graph on another stream\n-        curr_stream = torch.cuda.current_stream()\n+        curr_stream = current_stream()\n         if curr_stream != stream:\n             stream.wait_stream(curr_stream)\n \n@@ -360,7 +360,7 @@ class GroupCoordinator:\n         # TODO: pynccl should not use `stream=`\n         # it can just always use the current stream.\n         out = pynccl_comm.all_reduce(input_,\n-                                     stream=torch.cuda.current_stream())\n+                                     stream=current_stream())\n         if out is None:\n             # fall back to the default all-reduce using PyTorch.\n             # this usually happens during testing.\n@@ -1209,7 +1209,7 @@ def in_the_same_node_as(pg: Union[ProcessGroup, StatelessProcessGroup],\n         ranks = list(range(world_size))\n \n     # local tensor in each process to store the result\n-    is_in_the_same_node = torch.tensor([0] * world_size, dtype=torch.int32)\n+    is_in_the_same_node = torch.zeros(world_size, dtype=torch.int32)\n \n     magic_message = b\"magic_message\"\n     shm = None\ndiff --git a/vllm/utils.py b/vllm/utils.py\nindex a92b77efd..0d055d8e6 100644\n--- a/vllm/utils.py\n+++ b/vllm/utils.py\n@@ -56,6 +56,7 @@ if TYPE_CHECKING:\n \n logger = init_logger(__name__)\n \n+\n # Exception strings for non-implemented encoder/decoder scenarios\n \n # Reminder: Please update docs/source/features/compatibility_matrix.md\n@@ -865,7 +866,8 @@ def json_map_leaves(func: Callable[[T], U], value: JSONTree[T]) -> JSONTree[U]:\n \n def flatten_2d_lists(lists: List[List[T]]) -> List[T]:\n     \"\"\"Flatten a list of lists to a single list.\"\"\"\n-    return [item for sublist in lists for item in sublist]\n+    from itertools import chain\n+    return list(chain.from_iterable(lists))\n \n \n def full_groupby(values: Iterable[_V], *, key: Callable[[_V], _K]):\n@@ -1456,6 +1458,23 @@ def supports_custom_op() -> bool:\n     return hasattr(torch.library, \"custom_op\")\n \n \n+def current_stream():\n+    \"\"\"Return the current compute stream for the active device/platform.\n+\n+    This provides a single entry-point for getting the current stream across\n+    supported backends. For CUDA/ROCm, returns torch.cuda.current_stream().\n+    For XPU, returns torch.xpu.current_stream(). Other platforms should not\n+    call this utility.\n+    \"\"\"\n+    from vllm.platforms import current_platform\n+    if current_platform.is_xpu():\n+        # IPEX/oneAPI backend\n+        return torch.xpu.current_stream()  # type: ignore[attr-defined]\n+    # CUDA-like backends (CUDA/ROCm)\n+    return torch.cuda.current_stream()\n+\n+\n+\n class AtomicCounter:\n     \"\"\"An atomic, thread-safe counter\"\"\"\n \ndiff --git a/vllm/worker/multi_step_model_runner.py b/vllm/worker/multi_step_model_runner.py\nindex a2c2cebf8..d7664b37b 100644\n--- a/vllm/worker/multi_step_model_runner.py\n+++ b/vllm/worker/multi_step_model_runner.py\n@@ -14,7 +14,7 @@ from vllm.model_executor.layers.sampler import (PromptLogprobs, SampleLogprobs,\n                                                 get_pythonized_sample_results)\n from vllm.sequence import (CompletionSequenceGroupOutput, IntermediateTensors,\n                            Logprob, SequenceGroupMetadata, SequenceOutput)\n-from vllm.utils import PyObjectCache, async_tensor_h2d\n+from vllm.utils import PyObjectCache, async_tensor_h2d, current_stream\n from vllm.worker.model_runner import (GPUModelRunnerBase,\n                                       ModelInputForGPUWithSamplingMetadata)\n from vllm.worker.model_runner_base import (\n@@ -480,7 +480,7 @@ class MultiStepModelRunner(GPUModelRunnerBase[StatefulModelInput]):\n         # if CPU is ahead.\n         if self.is_driver_worker and get_pp_group().is_last_rank:\n             if self.pinned_sampled_token_ids is None:\n-                self.pinned_sampled_token_ids = torch.zeros(\n+                self.pinned_sampled_token_ids = torch.empty(\n                     (self.scheduler_config.max_num_seqs, 1),\n                     dtype=torch.long,\n                     device=\"cpu\",\n@@ -498,7 +498,7 @@ class MultiStepModelRunner(GPUModelRunnerBase[StatefulModelInput]):\n         #   appended sampler output from last iteration\n         #   - also maybe pythonize if CPU is ahead of GPU\n \n-        current_stream = torch.cuda.current_stream()\n+        curr_stream = current_stream()\n         if not model_input.is_first_multi_step:\n             # Explicitly block on the previous step's forward to make sure we\n             # don't clobber any GPU tensors still in use.\n@@ -541,7 +541,7 @@ class MultiStepModelRunner(GPUModelRunnerBase[StatefulModelInput]):\n                                                        num_steps=1)\n \n         # record the event for the current step so that the next step can sync\n-        model_input.record_step_event(current_stream)\n+        model_input.record_step_event(curr_stream)\n \n         if get_pp_group().is_last_rank and self.is_driver_worker:\n             assert isinstance(output, list)\n@@ -552,7 +552,7 @@ class MultiStepModelRunner(GPUModelRunnerBase[StatefulModelInput]):\n             # event for the pythonization so that we only pythonize if the\n             # tensors are ready. May be able to be combined with the step event\n             output_ready_event = torch.cuda.Event()\n-            output_ready_event.record(current_stream)\n+            output_ready_event.record(curr_stream)\n             if self.parallel_config.pipeline_parallel_size > 1:\n                 output[0].sampled_token_ids_cpu = output[\n                     0].sampled_token_ids.cpu()\n", "model_name_or_path": "gpt-5-2025-08-07"}
