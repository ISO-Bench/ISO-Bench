diff --git a/model_patch.diff b/model_patch.diff
new file mode 100644
index 000000000..e69de29bb
diff --git a/vllm/_custom_ops.py b/vllm/_custom_ops.py
index 5b5643748..bf4ffaca5 100644
--- a/vllm/_custom_ops.py
+++ b/vllm/_custom_ops.py
@@ -189,13 +189,36 @@ def gptq_marlin_gemm(a: torch.Tensor, b_q_weight: torch.Tensor,
 def scaled_fp8_quant(
     input: torch.Tensor,
     scale: Optional[torch.Tensor] = None,
+    batch_dim_padding: Optional[int] = None,
 ) -> Tuple[torch.Tensor, torch.Tensor]:
-    output = torch.empty_like(input, dtype=torch.float8_e4m3fn)
+    """
+    Quantize input tensor to FP8 and return quantized tensor and scale.
+
+    Supports both static and dynamic quantization: if scale is provided, static
+    scaling is used; otherwise dynamic scaling is computed from input. Optionally
+    pads the first dimension of the output tensor to at least batch_dim_padding
+    to enable downstream kernels to select better algorithms on some backends.
+
+    Only the leading slice matching the input shape is populated by the
+    quantization kernels when padding is requested.
+    """
+    # Allocate output, optionally with padding on the first dimension
+    if batch_dim_padding is not None and input.size(0) < batch_dim_padding:
+        padded_shape = list(input.shape)
+        padded_shape[0] = batch_dim_padding
+        output = torch.empty(padded_shape,
+                             dtype=torch.float8_e4m3fn,
+                             device=input.device)
+        out_view = output[:input.size(0)]
+    else:
+        output = torch.empty_like(input, dtype=torch.float8_e4m3fn)
+        out_view = output
+
     if scale is None:
-        scale = torch.zeros(1, device=input.device, dtype=torch.float32)
-        vllm_ops.dynamic_scaled_fp8_quant(output, input, scale)
+        scale = torch.empty(1, device=input.device, dtype=torch.float32)
+        vllm_ops.dynamic_scaled_fp8_quant(out_view, input, scale)
     else:
-        vllm_ops.static_scaled_fp8_quant(output, input, scale)
+        vllm_ops.static_scaled_fp8_quant(out_view, input, scale)
     return output, scale
 
 
diff --git a/vllm/model_executor/layers/quantization/fp8.py b/vllm/model_executor/layers/quantization/fp8.py
index b57e1dde8..efdd84108 100644
--- a/vllm/model_executor/layers/quantization/fp8.py
+++ b/vllm/model_executor/layers/quantization/fp8.py
@@ -199,7 +199,7 @@ class Fp8LinearMethod(LinearMethodBase):
                                                   layer.weight_scale[idx])
 
                 layer.weight[start:end, :] = per_tensor_quantize(
-                    weight_dq, layer.weight_scale.max())
+                    weight_dq, max_w_scale)
                 start = end
             layer.weight_scale = Parameter(max_w_scale, requires_grad=False)
 
@@ -231,7 +231,10 @@ class Fp8LinearMethod(LinearMethodBase):
         # ops.scaled_fp8_quant supports both dynamic and static quant.
         #   If dynamic, layer.act_scale is None and x_scale computed from x.
         #   If static,  layer.act_scale is scalar and x_scale set to act_scale.
-        qinput, x_scale = ops.scaled_fp8_quant(x, layer.act_scale)
+        # Enlarge the first dimension slightly to help GEMM algorithm selection on some backends.
+        pad = 17 if x.size(0) < 17 else None
+        qinput, x_scale = ops.scaled_fp8_quant(x, layer.act_scale,
+                                               batch_dim_padding=pad)
 
         # Fused GEMM_DQ
         output, _ = torch._scaled_mm(
@@ -242,24 +245,28 @@ class Fp8LinearMethod(LinearMethodBase):
             scale_b=layer.weight_scale,
             bias=bias,
         )
+        # Slice back to original batch dim if padding was applied
+        if pad is not None:
+            output = output[:x.size(0)]
 
         return output
 
 
 def all_close_1d(x: torch.Tensor) -> bool:
     assert len(x.shape) == 1
-    return all(torch.allclose(x[0], x[i]) for i in range(x.shape[0]))
+    return torch.allclose(x, x[0].expand_as(x))
 
 
 def per_tensor_quantize(tensor: torch.Tensor,
                         inv_scale: float) -> torch.Tensor:
     finfo = torch.finfo(torch.float8_e4m3fn)
-    qweight = (tensor / inv_scale).clamp(min=finfo.min, max=finfo.max)
-    return qweight.to(torch.float8_e4m3fn)
+    q = tensor / inv_scale  # avoid mutating the input tensor
+    q.clamp_(min=finfo.min, max=finfo.max)
+    return q.to(torch.float8_e4m3fn)
 
 
 def per_tensor_dequantize(tensor: torch.Tensor,
                           inv_scale: float) -> torch.Tensor:
-    fake_qweight = tensor.to(torch.float16)
-    dq_weight = fake_qweight * inv_scale
-    return dq_weight
+    dq = tensor.to(torch.float16)
+    dq.mul_(inv_scale)
+    return dq
