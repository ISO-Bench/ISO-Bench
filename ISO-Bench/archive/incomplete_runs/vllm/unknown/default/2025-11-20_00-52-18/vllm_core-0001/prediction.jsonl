{"instance_id": "/home/raven/coding-mess/kernel-corp/OmniPerf-Bench/vllm__repo-fbefc8a", "model_patch": "diff --git a/model_patch.diff b/model_patch.diff\nnew file mode 100644\nindex 000000000..e69de29bb\ndiff --git a/vllm/entrypoints/openai/reasoning_parsers/deepseek_r1_reasoning_parser.py b/vllm/entrypoints/openai/reasoning_parsers/deepseek_r1_reasoning_parser.py\nindex e5ab6e6b2..38b55623c 100644\n--- a/vllm/entrypoints/openai/reasoning_parsers/deepseek_r1_reasoning_parser.py\n+++ b/vllm/entrypoints/openai/reasoning_parsers/deepseek_r1_reasoning_parser.py\n@@ -1,6 +1,5 @@\n # SPDX-License-Identifier: Apache-2.0\n \n-import re\n from typing import Optional, Sequence, Tuple, Union\n \n from transformers import PreTrainedTokenizerBase\n@@ -27,9 +26,9 @@ class DeepSeekR1ReasoningParser(ReasoningParser):\n         super().__init__(tokenizer)\n         self.think_start_token = \"<think>\"\n         self.think_end_token = \"</think>\"\n-\n-        self.reasoning_regex = re.compile(\n-            rf\"{self.think_start_token}(.*?){self.think_end_token}\", re.DOTALL)\n+        # Cache token lengths to avoid repeated len() calls in hot paths.\n+        self._think_start_len = len(self.think_start_token)\n+        self._think_end_len = len(self.think_end_token)\n \n         if not self.model_tokenizer:\n             raise ValueError(\n@@ -62,23 +61,28 @@ class DeepSeekR1ReasoningParser(ReasoningParser):\n         - 'xyz' goes to content\n         \"\"\"\n         # Skip single special tokens\n-        if len(delta_token_ids) == 1 and (delta_token_ids[0] in [\n-                self.think_start_token_id, self.think_end_token_id\n-        ]):\n+        if (len(delta_token_ids) == 1 and delta_token_ids[0]\n+                in (self.think_start_token_id, self.think_end_token_id)):\n             return None\n \n+        # Precompute membership flags to avoid repeated O(n) scans.\n+        has_start_prev = self.think_start_token_id in previous_token_ids\n+        has_end_prev = self.think_end_token_id in previous_token_ids\n+        has_start_delta = self.think_start_token_id in delta_token_ids\n+        has_end_delta = self.think_end_token_id in delta_token_ids\n+\n         # Check if <think> is present in previous or delta.\n         # Keep compatibility with models that don't generate <think> tokens.\n-        if self.think_start_token_id in previous_token_ids:\n-            if self.think_end_token_id in delta_token_ids:\n+        if has_start_prev:\n+            if has_end_delta:\n                 # <think> in previous, </think> in delta,\n                 # extract reasoning content\n                 end_index = delta_text.find(self.think_end_token)\n                 reasoning_content = delta_text[:end_index]\n-                content = delta_text[end_index + len(self.think_end_token):]\n+                content = delta_text[end_index + self._think_end_len:]\n                 return DeltaMessage(reasoning_content=reasoning_content,\n                                     content=content if content else None)\n-            elif self.think_end_token_id in previous_token_ids:\n+            elif has_end_prev:\n                 # <think> in previous, </think> in previous,\n                 # reasoning content continues\n                 return DeltaMessage(content=delta_text)\n@@ -86,15 +90,14 @@ class DeepSeekR1ReasoningParser(ReasoningParser):\n                 # <think> in previous, no </think> in previous or delta,\n                 # reasoning content continues\n                 return DeltaMessage(reasoning_content=delta_text)\n-        elif self.think_start_token_id in delta_token_ids:\n-            if self.think_end_token_id in delta_token_ids:\n+        elif has_start_delta:\n+            if has_end_delta:\n                 # <think> in delta, </think> in delta, extract reasoning content\n                 start_index = delta_text.find(self.think_start_token)\n                 end_index = delta_text.find(self.think_end_token)\n                 reasoning_content = delta_text[start_index +\n-                                               len(self.think_start_token\n-                                                   ):end_index]\n-                content = delta_text[end_index + len(self.think_end_token):]\n+                                               self._think_start_len:end_index]\n+                content = delta_text[end_index + self._think_end_len:]\n                 return DeltaMessage(reasoning_content=reasoning_content,\n                                     content=content if content else None)\n             else:\n@@ -105,15 +108,15 @@ class DeepSeekR1ReasoningParser(ReasoningParser):\n             # No <think> in previous or delta, also need to check for </think>.\n             # Because the model may have generated </think> without <think>\n             # Ref https://huggingface.co/deepseek-ai/DeepSeek-R1/commit/8a58a132790c9935686eb97f042afa8013451c9f\n-            if self.think_end_token_id in delta_token_ids:\n+            if has_end_delta:\n                 # </think> in delta with more tokens,\n                 # extract reasoning content and content\n                 end_index = delta_text.find(self.think_end_token)\n                 reasoning_content = delta_text[:end_index]\n-                content = delta_text[end_index + len(self.think_end_token):]\n+                content = delta_text[end_index + self._think_end_len:]\n                 return DeltaMessage(reasoning_content=reasoning_content,\n                                     content=content if content else None)\n-            elif self.think_end_token_id in previous_token_ids:\n+            elif has_end_prev:\n                 # </think> in previous, thinking content ends\n                 return DeltaMessage(content=delta_text)\n             else:\n@@ -123,25 +126,35 @@ class DeepSeekR1ReasoningParser(ReasoningParser):\n     def extract_reasoning_content(\n             self, model_output: str, request: ChatCompletionRequest\n     ) -> Tuple[Optional[str], Optional[str]]:\n+        \"\"\"\n+        Extract reasoning content from the complete model output.\n \n+        For text \"<think>abc</think>xyz\":\n+        - reasoning_content = \"abc\"\n+        - content = \"xyz\"\n+\n+        If the model omits the opening token but includes the closing token\n+        (\"abc</think>xyz\"), treat the text before </think> as reasoning.\n+        If no closing token is present, return the whole output as reasoning.\n+        \"\"\"\n         # DeepSeek R1 doesn't generate <think> now.\n         # Thus we assume the reasoning content is always at the start.\n-        # Ref https://huggingface.co/deepseek-ai/DeepSeek-R1/commit/8a58a132790c9935686eb97f042afa8013451c9f\n-        if self.think_end_token not in model_output:\n+        # Fast path: no closing token, everything is reasoning.\n+        end_pos = model_output.find(self.think_end_token)\n+        if end_pos == -1:\n             return model_output, None\n-        else:\n-            # Add a start token if it's missing to keep compatibility.\n-            if self.think_start_token not in model_output:\n-                model_output = f\"{self.think_start_token}{model_output}\"\n-            # Use a regex to find the reasoning content\n-            reasoning_content = self.reasoning_regex.findall(model_output)[0]\n-\n-            end_index = len(\n-                f\"{self.think_start_token}{reasoning_content}{self.think_end_token}\"\n-            )\n-            final_output = model_output[end_index:]\n \n-            if len(final_output) == 0:\n-                return reasoning_content, None\n-\n-            return reasoning_content, final_output\n+        # Find opening token (if present before the closing token).\n+        start_pos = model_output.find(self.think_start_token)\n+        if start_pos != -1 and start_pos < end_pos:\n+            reasoning_content = model_output[start_pos +\n+                                             self._think_start_len:end_pos]\n+        else:\n+            # Missing opening token or appears after </think>:\n+            # consider reasoning from the start of the string.\n+            reasoning_content = model_output[:end_pos]\n+\n+        final_output = model_output[end_pos + self._think_end_len:]\n+        if not final_output:\n+            return reasoning_content, None\n+        return reasoning_content, final_output\ndiff --git a/vllm/model_executor/layers/fused_moe/configs/E=8,N=14336,device_name=AMD_Instinct_MI300X.json b/vllm/model_executor/layers/fused_moe/configs/E=8,N=14336,device_name=AMD_Instinct_MI300X.json\nindex 66f9106bd..4bf775347 100644\n--- a/vllm/model_executor/layers/fused_moe/configs/E=8,N=14336,device_name=AMD_Instinct_MI300X.json\n+++ b/vllm/model_executor/layers/fused_moe/configs/E=8,N=14336,device_name=AMD_Instinct_MI300X.json\n@@ -45,8 +45,8 @@\n     },\n     \"16\": {\n         \"BLOCK_SIZE_M\": 16,\n-        \"BLOCK_SIZE_N\": 16,\n-        \"BLOCK_SIZE_K\": 256,\n+        \"BLOCK_SIZE_N\": 64,\n+        \"BLOCK_SIZE_K\": 64,\n         \"GROUP_SIZE_M\": 1,\n         \"num_warps\": 2,\n         \"num_stages\": 2,\ndiff --git a/vllm/model_executor/layers/fused_moe/configs/E=8,N=3584,device_name=AMD_Instinct_MI300X.json b/vllm/model_executor/layers/fused_moe/configs/E=8,N=3584,device_name=AMD_Instinct_MI300X.json\nindex ed5b655d8..5a3f415d5 100644\n--- a/vllm/model_executor/layers/fused_moe/configs/E=8,N=3584,device_name=AMD_Instinct_MI300X.json\n+++ b/vllm/model_executor/layers/fused_moe/configs/E=8,N=3584,device_name=AMD_Instinct_MI300X.json\n@@ -45,8 +45,8 @@\n     },\n     \"16\": {\n         \"BLOCK_SIZE_M\": 16,\n-        \"BLOCK_SIZE_N\": 32,\n-        \"BLOCK_SIZE_K\": 256,\n+        \"BLOCK_SIZE_N\": 64,\n+        \"BLOCK_SIZE_K\": 128,\n         \"GROUP_SIZE_M\": 1,\n         \"num_warps\": 2,\n         \"num_stages\": 2,\ndiff --git a/vllm/model_executor/layers/fused_moe/configs/E=8,N=7168,device_name=AMD_Instinct_MI300X.json b/vllm/model_executor/layers/fused_moe/configs/E=8,N=7168,device_name=AMD_Instinct_MI300X.json\nindex 822f04e33..aee43f961 100644\n--- a/vllm/model_executor/layers/fused_moe/configs/E=8,N=7168,device_name=AMD_Instinct_MI300X.json\n+++ b/vllm/model_executor/layers/fused_moe/configs/E=8,N=7168,device_name=AMD_Instinct_MI300X.json\n@@ -1,8 +1,8 @@\n {\n     \"1\": {\n         \"BLOCK_SIZE_M\": 16,\n-        \"BLOCK_SIZE_N\": 32,\n-        \"BLOCK_SIZE_K\": 256,\n+        \"BLOCK_SIZE_N\": 64,\n+        \"BLOCK_SIZE_K\": 128,\n         \"GROUP_SIZE_M\": 1,\n         \"num_warps\": 2,\n         \"num_stages\": 2,\n", "model_name_or_path": "gpt-5-2025-08-07"}
