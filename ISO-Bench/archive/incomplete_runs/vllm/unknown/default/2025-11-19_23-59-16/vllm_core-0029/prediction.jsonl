{"instance_id": "/home/raven/coding-mess/kernel-corp/OmniPerf-Bench/vllm__repo-8d59dbb", "model_patch": "diff --git a/model_patch.diff b/model_patch.diff\nnew file mode 100644\nindex 000000000..e69de29bb\ndiff --git a/vllm/worker/neuron_worker.py b/vllm/worker/neuron_worker.py\nindex 5f0eb0019..6c59bd409 100644\n--- a/vllm/worker/neuron_worker.py\n+++ b/vllm/worker/neuron_worker.py\n@@ -42,6 +42,12 @@ class NeuronWorker(LoraNotSupportedWorkerBase, LocalOrDistributedWorkerBase):\n             vllm_config=vllm_config)\n         self.is_driver_worker = is_driver_worker\n \n+        # Internal flags for idempotent initialization\n+        self._device_initialized = False\n+        self._dist_env_initialized = False\n+        self._cached_available_blocks = None\n+\n+    @torch.inference_mode()\n     def execute_model(\n         self,\n         execute_model_req: Optional[ExecuteModelRequest] = None,\n@@ -53,15 +59,17 @@ class NeuronWorker(LoraNotSupportedWorkerBase, LocalOrDistributedWorkerBase):\n                     \"Cache operations are not supported for Neuron backend.\")\n         assert execute_model_req.num_lookahead_slots == 0, (\n             \"lookahead not supported for Neuron backend.\")\n-        output = LocalOrDistributedWorkerBase.execute_model(\n+        return LocalOrDistributedWorkerBase.execute_model(\n             self, execute_model_req)\n-        return output\n \n     def init_device(self) -> None:\n+        if getattr(self, \"_device_initialized\", False):\n+            return\n         self.init_distributed_environment()\n \n         # Set random seed.\n         set_random_seed(self.model_config.seed)\n+        self._device_initialized = True\n \n     def load_model(self):\n         self.model_runner.load_model()\n@@ -73,15 +81,21 @@ class NeuronWorker(LoraNotSupportedWorkerBase, LocalOrDistributedWorkerBase):\n \n         We configure num_gpu_blocks to be equal to max_num_seqs.\n         \"\"\"\n+        cached = getattr(self, \"_cached_available_blocks\", None)\n+        if cached is not None:\n+            return cached\n+\n         # Set the number of GPU blocks to be the same as the maximum number of\n         # sequences that can be processed in a single batch. This is equivalent\n         # to schedule without PagedAttention.\n-        num_gpu_blocks = self.scheduler_config.max_num_seqs\n+        num_gpu_blocks = self.scheduler_config.max_num_seqs + 1\n \n         # Swap not yet supported with Neuron backend.\n         num_cpu_blocks = 0\n \n-        return num_gpu_blocks, num_cpu_blocks\n+        result = (num_gpu_blocks, num_cpu_blocks)\n+        self._cached_available_blocks = result\n+        return result\n \n     def initialize_cache(self, num_gpu_blocks: int,\n                          num_cpu_blocks: int) -> None:\n@@ -90,7 +104,7 @@ class NeuronWorker(LoraNotSupportedWorkerBase, LocalOrDistributedWorkerBase):\n \n         # Different values are not tested.\n         assert num_cpu_blocks == 0\n-        assert num_gpu_blocks == self.scheduler_config.max_num_seqs\n+        assert num_gpu_blocks == self.scheduler_config.max_num_seqs + 1\n \n         self.cache_config.num_gpu_blocks = num_gpu_blocks\n         self.cache_config.num_cpu_blocks = num_cpu_blocks\n@@ -125,6 +139,11 @@ class NeuronWorker(LoraNotSupportedWorkerBase, LocalOrDistributedWorkerBase):\n         vLLM still needs the environment initialized when TP/PP > 1,\n         so we initialize a distributed environment with one process.\n         \"\"\"\n+        # Fast path: avoid redundant initialization\n+        if getattr(self, \"_dist_env_initialized\", False) or \\\n+                torch.distributed.is_available() and \\\n+                torch.distributed.is_initialized():\n+            return\n         init_distributed_environment(\n             world_size=1,\n             rank=0,\n@@ -136,3 +155,4 @@ class NeuronWorker(LoraNotSupportedWorkerBase, LocalOrDistributedWorkerBase):\n             1,\n             1,\n         )\n+        self._dist_env_initialized = True\n", "model_name_or_path": "gpt-5-2025-08-07"}
