{"instance_id": "/home/raven/coding-mess/kernel-corp/OmniPerf-Bench/vllm__repo-f728ab8", "model_patch": "diff --git a/model_patch.diff b/model_patch.diff\nnew file mode 100644\nindex 000000000..e69de29bb\ndiff --git a/tests/kernels/test_moe.py b/tests/kernels/test_moe.py\nindex 046f11d95..248e47b5f 100644\n--- a/tests/kernels/test_moe.py\n+++ b/tests/kernels/test_moe.py\n@@ -15,16 +15,18 @@ from vllm.model_executor.models.mixtral import MixtralMoE\n def torch_moe(a, w1, w2, score, topk):\n     B, D = a.shape\n     a = a.view(B, -1, D).repeat(1, topk, 1).reshape(-1, D)\n-    out = torch.zeros(B * topk, w2.shape[1], dtype=a.dtype, device=a.device)\n+    out = torch.empty(B * topk, w2.shape[1], dtype=a.dtype, device=a.device)\n     score = torch.softmax(score, dim=-1, dtype=torch.float32)\n     topk_weight, topk_ids = torch.topk(score, topk)\n     topk_weight = topk_weight.view(-1)\n     topk_ids = topk_ids.view(-1)\n+    silu_mul = SiluAndMul()\n+    w1T = w1.transpose(1, 2)\n+    w2T = w2.transpose(1, 2)\n     for i in range(w1.shape[0]):\n         mask = topk_ids == i\n         if mask.sum():\n-            out[mask] = SiluAndMul()(\n-                a[mask] @ w1[i].transpose(0, 1)) @ w2[i].transpose(0, 1)\n+            out[mask] = silu_mul(a[mask] @ w1T[i]) @ w2T[i]\n     return (out.view(B, -1, w2.shape[1]) *\n             topk_weight.view(B, -1, 1).to(out.dtype)).sum(dim=1)\n \n@@ -62,7 +64,7 @@ def test_mixtral_moe(dtype: torch.dtype):\n \n     # Instantiate our and huggingface's MoE blocks\n     config = MixtralConfig()\n-    hf_moe = MixtralSparseMoeBlock(config).to(dtype).to(\"cuda\")\n+    hf_moe = MixtralSparseMoeBlock(config).to(device=\"cuda\", dtype=dtype)\n     vllm_moe = MixtralMoE(\n         num_experts=config.num_local_experts,\n         top_k=config.num_experts_per_tok,\n@@ -70,18 +72,18 @@ def test_mixtral_moe(dtype: torch.dtype):\n         intermediate_size=config.intermediate_size,\n         params_dtype=dtype,\n         tp_size=1,\n-    ).cuda()\n+    ).to(\"cuda\")\n \n     # Load the weights\n     vllm_moe.gate.weight.data[:] = hf_moe.gate.weight.data\n     for i in range(config.num_local_experts):\n         weights = (hf_moe.experts[i].w1.weight.data,\n                    hf_moe.experts[i].w3.weight.data)\n-        vllm_moe.ws[i][:] = torch.cat(weights, dim=0)\n-        vllm_moe.w2s[i][:] = hf_moe.experts[i].w2.weight.data\n+        vllm_moe.w13_weight[i][:] = torch.cat(weights, dim=0)\n+        vllm_moe.w2_weight[i][:] = hf_moe.experts[i].w2.weight.data\n \n     # Generate input batch of dimensions [batch_size, seq_len, hidden_dim]\n-    hf_inputs = torch.randn((1, 64, config.hidden_size)).to(dtype).to(\"cuda\")\n+    hf_inputs = torch.randn((1, 64, config.hidden_size), dtype=dtype, device=\"cuda\")\n     # vLLM uses 1D query [num_tokens, hidden_dim]\n     vllm_inputs = hf_inputs.flatten(0, 1)\n \ndiff --git a/vllm/model_executor/models/mixtral.py b/vllm/model_executor/models/mixtral.py\nindex 9ff9ba298..746314e9c 100644\n--- a/vllm/model_executor/models/mixtral.py\n+++ b/vllm/model_executor/models/mixtral.py\n@@ -78,6 +78,7 @@ class MixtralMoE(nn.Module):\n         self.top_k = top_k\n         self.hidden_size = hidden_size\n         self.intermediate_size = intermediate_size // self.tp_size\n+        self.quant_config = quant_config\n         # FIXME(pcmoritz): Make this more general to support different\n         # quantization schemes\n         self.use_fp8 = isinstance(quant_config, Fp8Config)\n@@ -92,21 +93,21 @@ class MixtralMoE(nn.Module):\n                                      params_dtype=self.params_dtype,\n                                      quant_config=None)\n \n-        self.ws = nn.Parameter(\n+        self.w13_weight = nn.Parameter(\n             torch.empty(self.num_total_experts,\n                         2 * self.intermediate_size,\n                         self.hidden_size,\n                         dtype=self.params_dtype))\n-        self.w2s = nn.Parameter(\n+        self.w2_weight = nn.Parameter(\n             torch.empty(self.num_total_experts,\n                         self.hidden_size,\n                         self.intermediate_size,\n                         dtype=self.params_dtype))\n \n-        set_weight_attrs(self.ws, {\n+        set_weight_attrs(self.w13_weight, {\n             \"weight_loader\": self.weight_loader,\n         })\n-        set_weight_attrs(self.w2s, {\n+        set_weight_attrs(self.w2_weight, {\n             \"weight_loader\": self.weight_loader,\n         })\n \n@@ -154,15 +155,15 @@ class MixtralMoE(nn.Module):\n \n     def process_weights_after_loading(self):\n         if self.use_fp8:\n-            ws = torch.empty_like(self.ws.data, dtype=torch.float8_e4m3fn)\n-            w2s = torch.empty_like(self.w2s.data, dtype=torch.float8_e4m3fn)\n+            ws = torch.empty_like(self.w13_weight.data, dtype=torch.float8_e4m3fn)\n+            w2s = torch.empty_like(self.w2_weight.data, dtype=torch.float8_e4m3fn)\n             for expert in range(self.num_total_experts):\n                 ws[expert, :, :], self.ws_scale[expert] = ops.scaled_fp8_quant(\n-                    self.ws.data[expert, :, :])\n+                    self.w13_weight.data[expert, :, :])\n                 w2s[expert, :, :], self.w2s_scale[\n-                    expert] = ops.scaled_fp8_quant(self.w2s.data[expert, :, :])\n-            self.ws = nn.Parameter(ws, requires_grad=False)\n-            self.w2s = nn.Parameter(w2s, requires_grad=False)\n+                    expert] = ops.scaled_fp8_quant(self.w2_weight.data[expert, :, :])\n+            self.w13_weight = nn.Parameter(ws, requires_grad=False)\n+            self.w2_weight = nn.Parameter(w2s, requires_grad=False)\n \n     def forward(self, hidden_states: torch.Tensor) -> torch.Tensor:\n         num_tokens, hidden_size = hidden_states.shape\n@@ -170,8 +171,8 @@ class MixtralMoE(nn.Module):\n         # router_logits: (num_tokens, n_experts)\n         router_logits, _ = self.gate(hidden_states)\n         final_hidden_states = fused_moe(hidden_states,\n-                                        self.ws,\n-                                        self.w2s,\n+                                        self.w13_weight,\n+                                        self.w2_weight,\n                                         router_logits,\n                                         self.top_k,\n                                         renormalize=True,\n@@ -463,7 +464,7 @@ class MixtralForCausalLM(nn.Module):\n         expert_params_mapping = [\n             # These are the weights for the experts\n             # (param_name, weight_name, expert_id)\n-            (\"ws\" if weight_name in [\"w1\", \"w3\"] else \"w2s\",\n+            (\"w13_weight\" if weight_name in [\"w1\", \"w3\"] else \"w2_weight\",\n              f\"experts.{expert_id}.{weight_name}.weight\", expert_id)\n             for expert_id in range(self.config.num_local_experts)\n             for weight_name in [\"w1\", \"w2\", \"w3\"]\n", "model_name_or_path": "gpt-5-2025-08-07"}
