diff --git a/model_patch.diff b/model_patch.diff
new file mode 100644
index 000000000..e69de29bb
diff --git a/vllm/model_executor/models/qwen2_5_vl.py b/vllm/model_executor/models/qwen2_5_vl.py
index 5904ad1f1..fd4296b7e 100644
--- a/vllm/model_executor/models/qwen2_5_vl.py
+++ b/vllm/model_executor/models/qwen2_5_vl.py
@@ -198,7 +198,7 @@ class Qwen2_5_VisionMLP(nn.Module):
 def all_gather_interleave(local_tensor, hidden_size: int, tp_size: int):
     """All-gather the input tensor interleavely across model parallel group."""
     import torch.distributed as dist
-    gathered_tensors = [torch.zeros_like(local_tensor) for _ in range(tp_size)]
+    gathered_tensors = [torch.empty_like(local_tensor) for _ in range(tp_size)]
     dist.all_gather(gathered_tensors,
                     local_tensor,
                     group=parallel_state.get_tp_group().device_group)
@@ -478,8 +478,8 @@ class Qwen2_5_VisionRotaryEmbedding(nn.Module):
         super().__init__()
         self.dim = dim
         self.theta = theta
-        inv_freq = 1.0 / (theta
-                          **(torch.arange(0, dim, 2, dtype=torch.float) / dim))
+        inv_freq = 1.0 / (theta**(
+            torch.arange(0, dim, 2, dtype=torch.float, device='cpu') / dim))
         self.register_buffer("inv_freq", inv_freq, persistent=False)
         self._seq_len_cached = 0
         self._freqs_cached = None
@@ -488,9 +488,7 @@ class Qwen2_5_VisionRotaryEmbedding(nn.Module):
         if seqlen > self._seq_len_cached:
             seqlen *= 2
             self._seq_len_cached = seqlen
-            self.inv_freq = 1.0 / (self.theta**(torch.arange(
-                0, self.dim, 2, dtype=torch.float, device=self.inv_freq.device)
-                                                / self.dim))
+
             seq = torch.arange(seqlen,
                                device=self.inv_freq.device,
                                dtype=self.inv_freq.dtype)
@@ -527,6 +525,10 @@ class Qwen2_5_VisionTransformer(nn.Module):
         self.fullatt_block_indexes = vision_config.fullatt_block_indexes
         self.spatial_merge_unit = self.spatial_merge_size**2
 
+        # Caches to avoid recomputing indices for repeated grids
+        self._pos_ids_cache = {}
+        self._window_index_cache = {}
+
         self.patch_embed = Qwen2_5_VisionPatchEmbed(
             patch_size=patch_size,
             temporal_patch_size=temporal_patch_size,
@@ -568,31 +570,49 @@ class Qwen2_5_VisionTransformer(nn.Module):
         return self.patch_embed.proj.weight.device
 
     def rot_pos_emb(self, grid_thw: torch.Tensor) -> torch.Tensor:
-        pos_ids = []
-        for t, h, w in grid_thw:
-            hpos_ids = torch.arange(h).unsqueeze(1).expand(-1, w)
-            wpos_ids = torch.arange(w).unsqueeze(0).expand(h, -1)
-            hpos_ids = hpos_ids.reshape(
-                h // self.spatial_merge_size,
-                self.spatial_merge_size,
-                w // self.spatial_merge_size,
-                self.spatial_merge_size,
-            ).permute(0, 2, 1, 3).flatten()
-            wpos_ids = wpos_ids.reshape(
-                h // self.spatial_merge_size,
-                self.spatial_merge_size,
-                w // self.spatial_merge_size,
-                self.spatial_merge_size,
-            ).permute(0, 2, 1, 3).flatten()
-            pos_ids.append(
-                torch.stack([hpos_ids, wpos_ids], dim=-1).repeat(t, 1))
-        pos_ids = torch.cat(pos_ids, dim=0)
-        max_grid_size = grid_thw[:, 1:].max()
+        # Cache pos_ids per unique grid to avoid recomputation
+        key = tuple(tuple(map(int, row.tolist())) for row in grid_thw.cpu())
+        pos_ids = self._pos_ids_cache.get(key)
+        if pos_ids is None:
+            pos_ids_list = []
+            for t, h, w in grid_thw:
+                h = int(h.item()) if isinstance(h, torch.Tensor) else int(h)
+                w = int(w.item()) if isinstance(w, torch.Tensor) else int(w)
+                t = int(t.item()) if isinstance(t, torch.Tensor) else int(t)
+                hpos_ids = torch.arange(h, device="cpu").unsqueeze(1).expand(-1, w)
+                wpos_ids = torch.arange(w, device="cpu").unsqueeze(0).expand(h, -1)
+                hpos_ids = hpos_ids.reshape(
+                    h // self.spatial_merge_size,
+                    self.spatial_merge_size,
+                    w // self.spatial_merge_size,
+                    self.spatial_merge_size,
+                ).permute(0, 2, 1, 3).flatten()
+                wpos_ids = wpos_ids.reshape(
+                    h // self.spatial_merge_size,
+                    self.spatial_merge_size,
+                    w // self.spatial_merge_size,
+                    self.spatial_merge_size,
+                ).permute(0, 2, 1, 3).flatten()
+                pos_ids_list.append(
+                    torch.stack([hpos_ids, wpos_ids], dim=-1).repeat(t, 1))
+            pos_ids = torch.cat(pos_ids_list, dim=0).to(torch.long).contiguous()
+            # store on CPU to reduce device-specific duplication
+            self._pos_ids_cache[key] = pos_ids.cpu()
+        else:
+            # reuse cached CPU indices
+            pos_ids = pos_ids
+        max_grid_size = int(grid_thw[:, 1:].max().item())
         rotary_pos_emb_full = self.rotary_pos_emb(max_grid_size)
-        rotary_pos_emb = rotary_pos_emb_full[pos_ids].flatten(1)
+        pos_ids_dev = pos_ids.to(device=rotary_pos_emb_full.device)
+        rotary_pos_emb = rotary_pos_emb_full[pos_ids_dev].flatten(1)
         return rotary_pos_emb
 
     def get_window_index(self, grid_thw):
+        # Cache window indices per unique grid to avoid recomputation
+        key = tuple(tuple(map(int, row.tolist())) for row in grid_thw.cpu())
+        cached = self._window_index_cache.get(key)
+        if cached is not None:
+            return cached
         window_index: list = []
         cu_window_seqlens: list = [0]
         window_index_id = 0
@@ -620,12 +640,13 @@ class Qwen2_5_VisionTransformer(nn.Module):
             index_padded = index_padded.reshape(-1)
             index_new = index_padded[index_padded != -100]
             window_index.append(index_new + window_index_id)
-            cu_seqlens_tmp = seqlens.cumsum(
-                0) * self.spatial_merge_unit + cu_window_seqlens[-1]
+            cu_seqlens_tmp = seqlens.cumsum(0, dtype=torch.int32) * self.spatial_merge_unit + cu_window_seqlens[-1]
             cu_window_seqlens.extend(cu_seqlens_tmp.tolist())
             window_index_id += (grid_t * llm_grid_h * llm_grid_w).item()
         window_index = torch.cat(window_index, dim=0)
-        return window_index, cu_window_seqlens
+        result = (window_index, cu_window_seqlens)
+        self._window_index_cache[key] = result
+        return result
 
     def compute_attn_mask_seqlen(
         self,
