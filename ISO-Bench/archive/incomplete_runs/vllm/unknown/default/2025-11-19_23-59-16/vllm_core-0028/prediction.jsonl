{"instance_id": "/home/raven/coding-mess/kernel-corp/OmniPerf-Bench/vllm__repo-526078a", "model_patch": "diff --git a/model_patch.diff b/model_patch.diff\nnew file mode 100644\nindex 000000000..e69de29bb\ndiff --git a/vllm/model_executor/models/qwen2_5_vl.py b/vllm/model_executor/models/qwen2_5_vl.py\nindex 5904ad1f1..fd4296b7e 100644\n--- a/vllm/model_executor/models/qwen2_5_vl.py\n+++ b/vllm/model_executor/models/qwen2_5_vl.py\n@@ -198,7 +198,7 @@ class Qwen2_5_VisionMLP(nn.Module):\n def all_gather_interleave(local_tensor, hidden_size: int, tp_size: int):\n     \"\"\"All-gather the input tensor interleavely across model parallel group.\"\"\"\n     import torch.distributed as dist\n-    gathered_tensors = [torch.zeros_like(local_tensor) for _ in range(tp_size)]\n+    gathered_tensors = [torch.empty_like(local_tensor) for _ in range(tp_size)]\n     dist.all_gather(gathered_tensors,\n                     local_tensor,\n                     group=parallel_state.get_tp_group().device_group)\n@@ -478,8 +478,8 @@ class Qwen2_5_VisionRotaryEmbedding(nn.Module):\n         super().__init__()\n         self.dim = dim\n         self.theta = theta\n-        inv_freq = 1.0 / (theta\n-                          **(torch.arange(0, dim, 2, dtype=torch.float) / dim))\n+        inv_freq = 1.0 / (theta**(\n+            torch.arange(0, dim, 2, dtype=torch.float, device='cpu') / dim))\n         self.register_buffer(\"inv_freq\", inv_freq, persistent=False)\n         self._seq_len_cached = 0\n         self._freqs_cached = None\n@@ -488,9 +488,7 @@ class Qwen2_5_VisionRotaryEmbedding(nn.Module):\n         if seqlen > self._seq_len_cached:\n             seqlen *= 2\n             self._seq_len_cached = seqlen\n-            self.inv_freq = 1.0 / (self.theta**(torch.arange(\n-                0, self.dim, 2, dtype=torch.float, device=self.inv_freq.device)\n-                                                / self.dim))\n+\n             seq = torch.arange(seqlen,\n                                device=self.inv_freq.device,\n                                dtype=self.inv_freq.dtype)\n@@ -527,6 +525,10 @@ class Qwen2_5_VisionTransformer(nn.Module):\n         self.fullatt_block_indexes = vision_config.fullatt_block_indexes\n         self.spatial_merge_unit = self.spatial_merge_size**2\n \n+        # Caches to avoid recomputing indices for repeated grids\n+        self._pos_ids_cache = {}\n+        self._window_index_cache = {}\n+\n         self.patch_embed = Qwen2_5_VisionPatchEmbed(\n             patch_size=patch_size,\n             temporal_patch_size=temporal_patch_size,\n@@ -568,31 +570,49 @@ class Qwen2_5_VisionTransformer(nn.Module):\n         return self.patch_embed.proj.weight.device\n \n     def rot_pos_emb(self, grid_thw: torch.Tensor) -> torch.Tensor:\n-        pos_ids = []\n-        for t, h, w in grid_thw:\n-            hpos_ids = torch.arange(h).unsqueeze(1).expand(-1, w)\n-            wpos_ids = torch.arange(w).unsqueeze(0).expand(h, -1)\n-            hpos_ids = hpos_ids.reshape(\n-                h // self.spatial_merge_size,\n-                self.spatial_merge_size,\n-                w // self.spatial_merge_size,\n-                self.spatial_merge_size,\n-            ).permute(0, 2, 1, 3).flatten()\n-            wpos_ids = wpos_ids.reshape(\n-                h // self.spatial_merge_size,\n-                self.spatial_merge_size,\n-                w // self.spatial_merge_size,\n-                self.spatial_merge_size,\n-            ).permute(0, 2, 1, 3).flatten()\n-            pos_ids.append(\n-                torch.stack([hpos_ids, wpos_ids], dim=-1).repeat(t, 1))\n-        pos_ids = torch.cat(pos_ids, dim=0)\n-        max_grid_size = grid_thw[:, 1:].max()\n+        # Cache pos_ids per unique grid to avoid recomputation\n+        key = tuple(tuple(map(int, row.tolist())) for row in grid_thw.cpu())\n+        pos_ids = self._pos_ids_cache.get(key)\n+        if pos_ids is None:\n+            pos_ids_list = []\n+            for t, h, w in grid_thw:\n+                h = int(h.item()) if isinstance(h, torch.Tensor) else int(h)\n+                w = int(w.item()) if isinstance(w, torch.Tensor) else int(w)\n+                t = int(t.item()) if isinstance(t, torch.Tensor) else int(t)\n+                hpos_ids = torch.arange(h, device=\"cpu\").unsqueeze(1).expand(-1, w)\n+                wpos_ids = torch.arange(w, device=\"cpu\").unsqueeze(0).expand(h, -1)\n+                hpos_ids = hpos_ids.reshape(\n+                    h // self.spatial_merge_size,\n+                    self.spatial_merge_size,\n+                    w // self.spatial_merge_size,\n+                    self.spatial_merge_size,\n+                ).permute(0, 2, 1, 3).flatten()\n+                wpos_ids = wpos_ids.reshape(\n+                    h // self.spatial_merge_size,\n+                    self.spatial_merge_size,\n+                    w // self.spatial_merge_size,\n+                    self.spatial_merge_size,\n+                ).permute(0, 2, 1, 3).flatten()\n+                pos_ids_list.append(\n+                    torch.stack([hpos_ids, wpos_ids], dim=-1).repeat(t, 1))\n+            pos_ids = torch.cat(pos_ids_list, dim=0).to(torch.long).contiguous()\n+            # store on CPU to reduce device-specific duplication\n+            self._pos_ids_cache[key] = pos_ids.cpu()\n+        else:\n+            # reuse cached CPU indices\n+            pos_ids = pos_ids\n+        max_grid_size = int(grid_thw[:, 1:].max().item())\n         rotary_pos_emb_full = self.rotary_pos_emb(max_grid_size)\n-        rotary_pos_emb = rotary_pos_emb_full[pos_ids].flatten(1)\n+        pos_ids_dev = pos_ids.to(device=rotary_pos_emb_full.device)\n+        rotary_pos_emb = rotary_pos_emb_full[pos_ids_dev].flatten(1)\n         return rotary_pos_emb\n \n     def get_window_index(self, grid_thw):\n+        # Cache window indices per unique grid to avoid recomputation\n+        key = tuple(tuple(map(int, row.tolist())) for row in grid_thw.cpu())\n+        cached = self._window_index_cache.get(key)\n+        if cached is not None:\n+            return cached\n         window_index: list = []\n         cu_window_seqlens: list = [0]\n         window_index_id = 0\n@@ -620,12 +640,13 @@ class Qwen2_5_VisionTransformer(nn.Module):\n             index_padded = index_padded.reshape(-1)\n             index_new = index_padded[index_padded != -100]\n             window_index.append(index_new + window_index_id)\n-            cu_seqlens_tmp = seqlens.cumsum(\n-                0) * self.spatial_merge_unit + cu_window_seqlens[-1]\n+            cu_seqlens_tmp = seqlens.cumsum(0, dtype=torch.int32) * self.spatial_merge_unit + cu_window_seqlens[-1]\n             cu_window_seqlens.extend(cu_seqlens_tmp.tolist())\n             window_index_id += (grid_t * llm_grid_h * llm_grid_w).item()\n         window_index = torch.cat(window_index, dim=0)\n-        return window_index, cu_window_seqlens\n+        result = (window_index, cu_window_seqlens)\n+        self._window_index_cache[key] = result\n+        return result\n \n     def compute_attn_mask_seqlen(\n         self,\n", "model_name_or_path": "gpt-5-2025-08-07"}
