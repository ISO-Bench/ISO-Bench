{"instance_id": "/home/raven/coding-mess/kernel-corp/OmniPerf-Bench/vllm__repo-0032903", "model_patch": "diff --git a/model_patch.diff b/model_patch.diff\nnew file mode 100644\nindex 000000000..e69de29bb\ndiff --git a/vllm/model_executor/layers/utils.py b/vllm/model_executor/layers/utils.py\nindex a9ef97391..b6034806f 100644\n--- a/vllm/model_executor/layers/utils.py\n+++ b/vllm/model_executor/layers/utils.py\n@@ -12,12 +12,10 @@ def get_token_bin_counts_and_mask(\n ) -> Tuple[torch.Tensor, torch.Tensor]:\n     # Compute the bin counts for the tokens.\n     # vocab_size + 1 for padding.\n-    bin_counts = torch.zeros((num_seqs, vocab_size + 1),\n-                             dtype=torch.long,\n-                             device=tokens.device)\n+    bin_counts = tokens.new_zeros((num_seqs, vocab_size + 1))\n     bin_counts.scatter_add_(1, tokens, torch.ones_like(tokens))\n     bin_counts = bin_counts[:, :vocab_size]\n-    mask = bin_counts > 0\n+    mask = bin_counts.ne(0)\n \n     return bin_counts, mask\n \n@@ -45,14 +43,13 @@ def apply_penalties(logits: torch.Tensor, prompt_tokens_tensor: torch.Tensor,\n                                                    vocab_size, num_seqs)\n     output_bin_counts, output_mask = get_token_bin_counts_and_mask(\n         output_tokens_tensor, vocab_size, num_seqs)\n-    repetition_penalties = repetition_penalties.unsqueeze(dim=1).repeat(\n-        1, vocab_size)\n-    logits[logits > 0] /= torch.where(prompt_mask | output_mask,\n-                                      repetition_penalties, 1.0)[logits > 0]\n-    logits[logits <= 0] *= torch.where(prompt_mask | output_mask,\n-                                       repetition_penalties, 1.0)[logits <= 0]\n+    mask_any = prompt_mask | output_mask\n+    rep = repetition_penalties.unsqueeze(1)\n+    penalties = torch.where(mask_any, rep, 1.0)\n+    scaling = torch.where(logits > 0, 1.0 / penalties, penalties)\n+    logits *= scaling\n     # We follow the definition in OpenAI API.\n     # Refer to https://platform.openai.com/docs/api-reference/parameter-details\n-    logits -= frequency_penalties.unsqueeze(dim=1) * output_bin_counts\n-    logits -= presence_penalties.unsqueeze(dim=1) * output_mask\n+    logits -= frequency_penalties.unsqueeze(1).to(logits.dtype) * output_bin_counts.to(logits.dtype)\n+    logits -= presence_penalties.unsqueeze(1).to(logits.dtype) * output_mask.to(logits.dtype)\n     return logits\n", "model_name_or_path": "gpt-5-2025-08-07"}
