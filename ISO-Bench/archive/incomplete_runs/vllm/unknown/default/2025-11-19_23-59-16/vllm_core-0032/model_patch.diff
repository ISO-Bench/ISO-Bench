diff --git a/model_patch.diff b/model_patch.diff
new file mode 100644
index 000000000..e69de29bb
diff --git a/tests/multi_step/test_correctness_async_llm.py b/tests/multi_step/test_correctness_async_llm.py
index d054ca341..0cbe8371e 100644
--- a/tests/multi_step/test_correctness_async_llm.py
+++ b/tests/multi_step/test_correctness_async_llm.py
@@ -103,13 +103,13 @@ async def test_multi_step(
         model,
         server_args + distributed_args,
         num_logprobs,
-        max_wait_seconds=3 * 240)
+        max_wait_seconds=5 * 240)
     test_completions = await completions_with_server_args(
         prompts,
         model,
         ms_server_args + distributed_args,
         num_logprobs,
-        max_wait_seconds=3 * 240)
+        max_wait_seconds=5 * 240)
 
     # Assert multi-step scheduling produces identical tokens
     # to single-step scheduling.
diff --git a/vllm/engine/async_llm_engine.py b/vllm/engine/async_llm_engine.py
index 159281dab..e0bee193c 100644
--- a/vllm/engine/async_llm_engine.py
+++ b/vllm/engine/async_llm_engine.py
@@ -106,7 +106,7 @@ class AsyncStream:
             while True:
                 result = await self._queue.get()
                 if self._is_raisable(result):
-                    if result == STOP_ITERATION:
+                    if result is STOP_ITERATION:
                         return
                     raise result
                 yield result
diff --git a/vllm/sequence.py b/vllm/sequence.py
index 87b3d21fa..99a8a2baa 100644
--- a/vllm/sequence.py
+++ b/vllm/sequence.py
@@ -1150,7 +1150,7 @@ class HiddenStates(msgspec.Struct, array_like=True,
             # Adding dummy hidden_states to this to maintain same shape
             self.second_last_token_hidden_states = torch.cat([
                 self.second_last_token_hidden_states,
-                torch.zeros_like(hidden_states)
+                torch.empty_like(hidden_states)
                 if second_last_token_hidden_states is None else
                 second_last_token_hidden_states
             ])
diff --git a/vllm/worker/model_runner.py b/vllm/worker/model_runner.py
index 8a3c99a45..3a857cb54 100644
--- a/vllm/worker/model_runner.py
+++ b/vllm/worker/model_runner.py
@@ -1225,8 +1225,8 @@ class GPUModelRunnerBase(ModelRunnerBase[TModelInputForGPU]):
 
         # Prepare dummy inputs. These will be reused for all batch sizes.
         max_batch_size = self.max_batchsize_to_capture
-        input_tokens = torch.zeros(max_batch_size, dtype=torch.long).cuda()
-        input_positions = torch.zeros(max_batch_size, dtype=torch.long).cuda()
+        input_tokens = torch.zeros(max_batch_size, dtype=torch.long, device=self.device)
+        input_positions = torch.zeros(max_batch_size, dtype=torch.long, device=self.device)
 
         # Prepare dummy previous_hidden_states only if needed by the model.
         # This is used by draft models such as EAGLE.
diff --git a/vllm/worker/multi_step_model_runner.py b/vllm/worker/multi_step_model_runner.py
index be0c75bc0..b51a6be5d 100644
--- a/vllm/worker/multi_step_model_runner.py
+++ b/vllm/worker/multi_step_model_runner.py
@@ -298,7 +298,7 @@ class MultiStepModelRunner(GPUModelRunnerBase[StatefulModelInput]):
         # if CPU is ahead.
         if self.is_driver_worker and get_pp_group().is_last_rank:
             if self.pinned_sampled_token_ids is None:
-                self.pinned_sampled_token_ids = torch.zeros(
+                self.pinned_sampled_token_ids = torch.empty(
                     (self.scheduler_config.max_num_seqs, 1),
                     dtype=torch.long,
                     device="cpu",
@@ -362,8 +362,19 @@ class MultiStepModelRunner(GPUModelRunnerBase[StatefulModelInput]):
             output_ready_event = torch.cuda.Event()
             output_ready_event.record(current_stream)
             if self.parallel_config.pipeline_parallel_size > 1:
-                output[0].sampled_token_ids_cpu = output[
-                    0].sampled_token_ids.cpu()
+                stids = output[0].sampled_token_ids
+                if stids is not None:
+                    num_rows = stids.shape[0]
+                    if self.pinned_sampled_token_ids is None:
+                        self.pinned_sampled_token_ids = torch.empty(
+                            (self.scheduler_config.max_num_seqs, 1),
+                            dtype=torch.long,
+                            device="cpu",
+                            pin_memory=True)
+                    # Copy into pinned CPU buffer for faster HtoD next step
+                    dst = self.pinned_sampled_token_ids[:num_rows, :]
+                    dst.copy_(stids, non_blocking=True)
+                    output[0].sampled_token_ids_cpu = dst
             model_input.cached_outputs.append(
                 ModelOutput(output[0], output_ready_event,
                             output[0].sampled_token_ids, False,
diff --git a/vllm/worker/multi_step_worker.py b/vllm/worker/multi_step_worker.py
index 517b0ab78..2eb818ceb 100644
--- a/vllm/worker/multi_step_worker.py
+++ b/vllm/worker/multi_step_worker.py
@@ -128,7 +128,7 @@ class MultiStepWorker(Worker):
             # execute_model_req
             assert execute_model_req.last_sampled_token_ids is not None
             model_input.last_sampled_token_ids = (
-                execute_model_req.last_sampled_token_ids.cuda())
+                execute_model_req.last_sampled_token_ids.cuda(non_blocking=True))
             model_input.add_sampler_output(
                 SamplerOutput(outputs=[], sampled_token_ids=None),
                 model_input.last_sampled_token_ids)
