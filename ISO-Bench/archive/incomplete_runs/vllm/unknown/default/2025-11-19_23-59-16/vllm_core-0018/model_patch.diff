diff --git a/tests/v1/sample/test_topk_topp_sampler.py b/tests/v1/sample/test_topk_topp_sampler.py
new file mode 100644
index 000000000..c9064c4e0
--- /dev/null
+++ b/tests/v1/sample/test_topk_topp_sampler.py
@@ -0,0 +1,34 @@
+# SPDX-License-Identifier: Apache-2.0
+import torch
+from torch import Generator
+
+from vllm.v1.sample.ops.topk_topp_sampler import apply_top_k_top_p
+
+DEVICE = "cpu"
+
+BATCH_SIZE = 64
+VOCAB_SIZE = 8192
+
+
+def test_topk_impl_equivalance():
+    with torch.device(DEVICE):
+        generator = Generator(device=DEVICE).manual_seed(33)
+
+        logits = torch.rand((BATCH_SIZE, VOCAB_SIZE), generator=generator)
+
+        # Random top-k values between 1 and 64.
+        k = torch.randint(1, 65, (BATCH_SIZE,), generator=generator)
+
+        # Reference: mask all but top-k using topk + scatter
+        topk_values, topk_indices = torch.topk(logits, k, dim=-1)
+        ref = torch.empty_like(logits)
+        ref.fill_(-float("inf"))
+        ref.scatter_(-1, topk_indices, topk_values)
+
+        # Under test: our optimized apply_top_k_top_p
+        out = apply_top_k_top_p(logits, k, None)
+
+        assert torch.equal(ref.isfinite(), out.isfinite())
+        # For finite entries (top-k), values must match
+        mask = ref.isfinite()
+        assert torch.allclose(ref[mask], out[mask])
diff --git a/vllm/v1/sample/ops/topk_topp_sampler.py b/vllm/v1/sample/ops/topk_topp_sampler.py
index 1dea71187..e3c1210b0 100644
--- a/vllm/v1/sample/ops/topk_topp_sampler.py
+++ b/vllm/v1/sample/ops/topk_topp_sampler.py
@@ -85,6 +85,30 @@ class TopKTopPSampler(nn.Module):
         p: Optional[torch.Tensor],
     ) -> torch.Tensor:
         """PyTorch-native implementation of top-k and top-p sampling."""
+        # Fast path: top-k only avoids full sort-based masking
+        if k is not None and p is None:
+            # Keep only top-k logits, mask others to -inf without building a bool mask
+            if torch.is_tensor(k) and (k.dim() == 0 or k.numel() == 1):
+                k_int = int(k.item()) if torch.is_tensor(k) else int(k)
+                topk_values, topk_indices = torch.topk(logits, k_int, dim=-1)
+                masked = torch.empty_like(logits)
+                masked.fill_(-float("inf"))
+                masked.scatter_(-1, topk_indices, topk_values)
+            else:
+                # Variable k per batch
+                if not torch.is_tensor(k):
+                    k = torch.tensor(k, device=logits.device)
+                k_max = int(k.max().item())
+                topk_values, topk_indices = torch.topk(logits, k_max, dim=-1)
+                masked = torch.empty_like(logits)
+                masked.fill_(-float("inf"))
+                ar = torch.arange(k_max, device=logits.device)
+                sel = ar.unsqueeze(0) < k.unsqueeze(1)
+                rows = torch.arange(logits.shape[0], device=logits.device).unsqueeze(1)
+                rows = rows.expand_as(sel)
+                masked[rows[sel], topk_indices[sel]] = topk_values[sel]
+            probs = masked.softmax(dim=-1, dtype=torch.float32)
+            return random_sample(probs, generators)
         logits = apply_top_k_top_p(logits, k, p)
         probs = logits.softmax(dim=-1, dtype=torch.float32)
         return random_sample(probs, generators)
@@ -115,11 +139,27 @@ class TopKTopPSampler(nn.Module):
         # If only top-k is specified, use pytorch's builtin topk op. This leads
         # to significant speed up on TPU compared to using apply_top_k_top_p.
         if k is not None and p is None:
-            topk_values, topk_indices = torch.topk(logits, k, dim=-1)
-
-            mask = torch.ones_like(logits, dtype=torch.bool)
-            mask.scatter_(-1, topk_indices, False)
-            logits.masked_fill_(mask, float('-inf'))
+            if torch.is_tensor(k) and (k.dim() == 0 or k.numel() == 1):
+                k_int = int(k.item()) if torch.is_tensor(k) else int(k)
+                topk_values, topk_indices = torch.topk(logits, k_int, dim=-1)
+                masked = torch.empty_like(logits)
+                masked.fill_(-float('inf'))
+                masked.scatter_(-1, topk_indices, topk_values)
+                logits = masked
+            else:
+                # Variable k per batch element
+                if not torch.is_tensor(k):
+                    k = torch.tensor(k, device=logits.device)
+                k_max = int(k.max().item())
+                topk_values, topk_indices = torch.topk(logits, k_max, dim=-1)
+                masked = torch.empty_like(logits)
+                masked.fill_(-float('inf'))
+                ar = torch.arange(k_max, device=logits.device)
+                sel = ar.unsqueeze(0) < k.unsqueeze(1)
+                rows = torch.arange(logits.shape[0], device=logits.device).unsqueeze(1)
+                rows = rows.expand_as(sel)
+                masked[rows[sel], topk_indices[sel]] = topk_values[sel]
+                logits = masked
         else:
             # TODO Placeholder for TPU optimized topp kernel
             # logits = apply_top_k_top_p(logits, k, p)
@@ -138,6 +178,37 @@ def apply_top_k_top_p(
 
     This function sorts the logits tensor, which can be slow for large batches.
     """
+    if k is not None and p is None:
+        # Fast path for top-k only: avoid global sort and boolean mask
+        if torch.is_tensor(k):
+            # Variable k per batch element
+            if k.dim() == 0 or k.numel() == 1:
+                k_int = int(k.item())
+                topk_values, topk_indices = torch.topk(logits, k_int, dim=-1)
+                out = torch.empty_like(logits)
+                out.fill_(-float("inf"))
+                out.scatter_(-1, topk_indices, topk_values)
+                return out
+            k_max = int(k.max().item())
+            # Get top-k_max per row once
+            topk_values, topk_indices = torch.topk(logits, k_max, dim=-1)
+            out = torch.empty_like(logits)
+            out.fill_(-float("inf"))
+            # Build mask selecting first k[i] entries for each row
+            ar = torch.arange(k_max, device=logits.device)
+            sel = ar.unsqueeze(0) < k.unsqueeze(1)
+            rows = torch.arange(logits.shape[0], device=logits.device).unsqueeze(1)
+            rows = rows.expand_as(sel)
+            out[rows[sel], topk_indices[sel]] = topk_values[sel]
+            return out
+        else:
+            # Scalar k provided
+            topk_values, topk_indices = torch.topk(logits, k, dim=-1)
+            out = torch.empty_like(logits)
+            out.fill_(-float("inf"))
+            out.scatter_(-1, topk_indices, topk_values)
+            return out
+
     if k is None and p is None:
         return logits
     logits_sort, logits_idx = logits.sort(dim=-1, descending=False)
@@ -152,7 +223,7 @@ def apply_top_k_top_p(
 
     if p is not None:
         # Apply top-p.
-        probs_sort = logits_sort.softmax(dim=-1)
+        probs_sort = logits_sort.softmax(dim=-1, dtype=torch.float32)
         probs_sum = probs_sort.cumsum(dim=-1)
         top_p_mask = probs_sum <= 1 - p.unsqueeze(dim=1)
         # at least one
