{"instance_id": "/home/raven/coding-mess/kernel-corp/OmniPerf-Bench/vllm__repo-5460070", "model_patch": "diff --git a/tests/v1/sample/test_topk_topp_sampler.py b/tests/v1/sample/test_topk_topp_sampler.py\nnew file mode 100644\nindex 000000000..c9064c4e0\n--- /dev/null\n+++ b/tests/v1/sample/test_topk_topp_sampler.py\n@@ -0,0 +1,34 @@\n+# SPDX-License-Identifier: Apache-2.0\n+import torch\n+from torch import Generator\n+\n+from vllm.v1.sample.ops.topk_topp_sampler import apply_top_k_top_p\n+\n+DEVICE = \"cpu\"\n+\n+BATCH_SIZE = 64\n+VOCAB_SIZE = 8192\n+\n+\n+def test_topk_impl_equivalance():\n+    with torch.device(DEVICE):\n+        generator = Generator(device=DEVICE).manual_seed(33)\n+\n+        logits = torch.rand((BATCH_SIZE, VOCAB_SIZE), generator=generator)\n+\n+        # Random top-k values between 1 and 64.\n+        k = torch.randint(1, 65, (BATCH_SIZE,), generator=generator)\n+\n+        # Reference: mask all but top-k using topk + scatter\n+        topk_values, topk_indices = torch.topk(logits, k, dim=-1)\n+        ref = torch.empty_like(logits)\n+        ref.fill_(-float(\"inf\"))\n+        ref.scatter_(-1, topk_indices, topk_values)\n+\n+        # Under test: our optimized apply_top_k_top_p\n+        out = apply_top_k_top_p(logits, k, None)\n+\n+        assert torch.equal(ref.isfinite(), out.isfinite())\n+        # For finite entries (top-k), values must match\n+        mask = ref.isfinite()\n+        assert torch.allclose(ref[mask], out[mask])\ndiff --git a/vllm/v1/sample/ops/topk_topp_sampler.py b/vllm/v1/sample/ops/topk_topp_sampler.py\nindex 1dea71187..e3c1210b0 100644\n--- a/vllm/v1/sample/ops/topk_topp_sampler.py\n+++ b/vllm/v1/sample/ops/topk_topp_sampler.py\n@@ -85,6 +85,30 @@ class TopKTopPSampler(nn.Module):\n         p: Optional[torch.Tensor],\n     ) -> torch.Tensor:\n         \"\"\"PyTorch-native implementation of top-k and top-p sampling.\"\"\"\n+        # Fast path: top-k only avoids full sort-based masking\n+        if k is not None and p is None:\n+            # Keep only top-k logits, mask others to -inf without building a bool mask\n+            if torch.is_tensor(k) and (k.dim() == 0 or k.numel() == 1):\n+                k_int = int(k.item()) if torch.is_tensor(k) else int(k)\n+                topk_values, topk_indices = torch.topk(logits, k_int, dim=-1)\n+                masked = torch.empty_like(logits)\n+                masked.fill_(-float(\"inf\"))\n+                masked.scatter_(-1, topk_indices, topk_values)\n+            else:\n+                # Variable k per batch\n+                if not torch.is_tensor(k):\n+                    k = torch.tensor(k, device=logits.device)\n+                k_max = int(k.max().item())\n+                topk_values, topk_indices = torch.topk(logits, k_max, dim=-1)\n+                masked = torch.empty_like(logits)\n+                masked.fill_(-float(\"inf\"))\n+                ar = torch.arange(k_max, device=logits.device)\n+                sel = ar.unsqueeze(0) < k.unsqueeze(1)\n+                rows = torch.arange(logits.shape[0], device=logits.device).unsqueeze(1)\n+                rows = rows.expand_as(sel)\n+                masked[rows[sel], topk_indices[sel]] = topk_values[sel]\n+            probs = masked.softmax(dim=-1, dtype=torch.float32)\n+            return random_sample(probs, generators)\n         logits = apply_top_k_top_p(logits, k, p)\n         probs = logits.softmax(dim=-1, dtype=torch.float32)\n         return random_sample(probs, generators)\n@@ -115,11 +139,27 @@ class TopKTopPSampler(nn.Module):\n         # If only top-k is specified, use pytorch's builtin topk op. This leads\n         # to significant speed up on TPU compared to using apply_top_k_top_p.\n         if k is not None and p is None:\n-            topk_values, topk_indices = torch.topk(logits, k, dim=-1)\n-\n-            mask = torch.ones_like(logits, dtype=torch.bool)\n-            mask.scatter_(-1, topk_indices, False)\n-            logits.masked_fill_(mask, float('-inf'))\n+            if torch.is_tensor(k) and (k.dim() == 0 or k.numel() == 1):\n+                k_int = int(k.item()) if torch.is_tensor(k) else int(k)\n+                topk_values, topk_indices = torch.topk(logits, k_int, dim=-1)\n+                masked = torch.empty_like(logits)\n+                masked.fill_(-float('inf'))\n+                masked.scatter_(-1, topk_indices, topk_values)\n+                logits = masked\n+            else:\n+                # Variable k per batch element\n+                if not torch.is_tensor(k):\n+                    k = torch.tensor(k, device=logits.device)\n+                k_max = int(k.max().item())\n+                topk_values, topk_indices = torch.topk(logits, k_max, dim=-1)\n+                masked = torch.empty_like(logits)\n+                masked.fill_(-float('inf'))\n+                ar = torch.arange(k_max, device=logits.device)\n+                sel = ar.unsqueeze(0) < k.unsqueeze(1)\n+                rows = torch.arange(logits.shape[0], device=logits.device).unsqueeze(1)\n+                rows = rows.expand_as(sel)\n+                masked[rows[sel], topk_indices[sel]] = topk_values[sel]\n+                logits = masked\n         else:\n             # TODO Placeholder for TPU optimized topp kernel\n             # logits = apply_top_k_top_p(logits, k, p)\n@@ -138,6 +178,37 @@ def apply_top_k_top_p(\n \n     This function sorts the logits tensor, which can be slow for large batches.\n     \"\"\"\n+    if k is not None and p is None:\n+        # Fast path for top-k only: avoid global sort and boolean mask\n+        if torch.is_tensor(k):\n+            # Variable k per batch element\n+            if k.dim() == 0 or k.numel() == 1:\n+                k_int = int(k.item())\n+                topk_values, topk_indices = torch.topk(logits, k_int, dim=-1)\n+                out = torch.empty_like(logits)\n+                out.fill_(-float(\"inf\"))\n+                out.scatter_(-1, topk_indices, topk_values)\n+                return out\n+            k_max = int(k.max().item())\n+            # Get top-k_max per row once\n+            topk_values, topk_indices = torch.topk(logits, k_max, dim=-1)\n+            out = torch.empty_like(logits)\n+            out.fill_(-float(\"inf\"))\n+            # Build mask selecting first k[i] entries for each row\n+            ar = torch.arange(k_max, device=logits.device)\n+            sel = ar.unsqueeze(0) < k.unsqueeze(1)\n+            rows = torch.arange(logits.shape[0], device=logits.device).unsqueeze(1)\n+            rows = rows.expand_as(sel)\n+            out[rows[sel], topk_indices[sel]] = topk_values[sel]\n+            return out\n+        else:\n+            # Scalar k provided\n+            topk_values, topk_indices = torch.topk(logits, k, dim=-1)\n+            out = torch.empty_like(logits)\n+            out.fill_(-float(\"inf\"))\n+            out.scatter_(-1, topk_indices, topk_values)\n+            return out\n+\n     if k is None and p is None:\n         return logits\n     logits_sort, logits_idx = logits.sort(dim=-1, descending=False)\n@@ -152,7 +223,7 @@ def apply_top_k_top_p(\n \n     if p is not None:\n         # Apply top-p.\n-        probs_sort = logits_sort.softmax(dim=-1)\n+        probs_sort = logits_sort.softmax(dim=-1, dtype=torch.float32)\n         probs_sum = probs_sort.cumsum(dim=-1)\n         top_p_mask = probs_sum <= 1 - p.unsqueeze(dim=1)\n         # at least one\n", "model_name_or_path": "gpt-5-2025-08-07"}
