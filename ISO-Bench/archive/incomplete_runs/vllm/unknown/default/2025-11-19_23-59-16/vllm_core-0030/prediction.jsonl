{"instance_id": "/home/raven/coding-mess/kernel-corp/OmniPerf-Bench/vllm__repo-5c04bb8", "model_patch": "diff --git a/model_patch.diff b/model_patch.diff\nnew file mode 100644\nindex 000000000..e69de29bb\ndiff --git a/vllm/core/block_manager_v1.py b/vllm/core/block_manager_v1.py\nindex e29eba375..d81648caa 100644\n--- a/vllm/core/block_manager_v1.py\n+++ b/vllm/core/block_manager_v1.py\n@@ -700,5 +700,5 @@ class BlockSpaceManagerV1(BlockSpaceManager):\n \n     def mark_blocks_as_computed(self, seq_group: SequenceGroup):\n         if self.enable_caching:\n-            for seq in seq_group.seqs_dict.values():\n+            for seq in seq_group.get_seqs():\n                 self.compute_full_blocks_in_seq(seq)\ndiff --git a/vllm/sequence.py b/vllm/sequence.py\nindex ab50cfdfd..8e8f9f2dc 100644\n--- a/vllm/sequence.py\n+++ b/vllm/sequence.py\n@@ -444,6 +444,7 @@ class SequenceGroup:\n         prompt_adapter_request: Optional[PromptAdapterRequest] = None,\n     ) -> None:\n         self.request_id = request_id\n+        self.seqs = seqs\n         self.seqs_dict = {seq.seq_id: seq for seq in seqs}\n         self.sampling_params = sampling_params\n         self.metrics = RequestMetrics(arrival_time=arrival_time,\n@@ -458,7 +459,7 @@ class SequenceGroup:\n         self.prompt_adapter_request = prompt_adapter_request\n         self.encoder_seq = encoder_seq\n         self.trace_headers = trace_headers\n-        self._first_seq = next(iter(self.seqs_dict.values()))\n+        self._first_seq = seqs[0]\n \n     @property\n     def prompt(self) -> Optional[str]:\n@@ -548,8 +549,8 @@ class SequenceGroup:\n         self,\n         status: Optional[SequenceStatus] = None,\n     ) -> List[Sequence]:\n-        return list(self.seqs_dict.values()) if status is None else [\n-            seq for seq in self.seqs_dict.values() if seq.status == status\n+        return self.seqs if status is None else [\n+            seq for seq in self.seqs if seq.status == status\n         ]\n \n     def is_encoder_decoder(self) -> bool:\n@@ -560,15 +561,15 @@ class SequenceGroup:\n \n     def get_unfinished_seqs(self) -> List[Sequence]:\n         return [\n-            seq for seq in self.seqs_dict.values() if not seq.is_finished()\n+            seq for seq in self.seqs if not seq.is_finished()\n         ]\n \n     def get_finished_seqs(self) -> List[Sequence]:\n-        return [seq for seq in self.seqs_dict.values() if seq.is_finished()]\n+        return [seq for seq in self.seqs if seq.is_finished()]\n \n     def update_num_computed_tokens(self, num_new_computed_tokens: int):\n         \"\"\"Update number of tokens computed so far.\"\"\"\n-        for seq in self.seqs_dict.values():\n+        for seq in self.seqs:\n             if not seq.is_finished():\n                 seq.data.update_num_computed_tokens(num_new_computed_tokens)\n \n@@ -583,7 +584,7 @@ class SequenceGroup:\n         # Optimization. We don't need to call get_seqs if we don't need to\n         # filter by states.\n         if status is None:\n-            return len(self.seqs_dict)\n+            return len(self.seqs)\n \n         return len(self.get_seqs(status))\n \n@@ -602,11 +603,19 @@ class SequenceGroup:\n         if seq.seq_id in self.seqs_dict:\n             raise ValueError(f\"Sequence {seq.seq_id} already exists.\")\n         self.seqs_dict[seq.seq_id] = seq\n+        self.seqs.append(seq)\n \n     def remove(self, seq_id: int) -> None:\n         if seq_id not in self.seqs_dict:\n             raise ValueError(f\"Sequence {seq_id} not found.\")\n+        seq = self.seqs_dict[seq_id]\n         del self.seqs_dict[seq_id]\n+        # Keep the list in sync\n+        try:\n+            self.seqs.remove(seq)\n+        except ValueError:\n+            # Should not happen, but avoid crashing if out of sync\n+            pass\n \n     def is_finished(self) -> bool:\n         return all(seq.is_finished() for seq in self.get_seqs())\n@@ -618,7 +627,7 @@ class SequenceGroup:\n     def __repr__(self) -> str:\n         return (f\"SequenceGroup(request_id={self.request_id}, \"\n                 f\"sampling_params={self.sampling_params}, \"\n-                f\"num_seqs={len(self.seqs_dict)})\")\n+                f\"num_seqs={len(self.seqs)})\")\n \n \n class SequenceGroupMetadata:\ndiff --git a/vllm/transformers_utils/detokenizer.py b/vllm/transformers_utils/detokenizer.py\nindex 76f418674..89cb2a933 100644\n--- a/vllm/transformers_utils/detokenizer.py\n+++ b/vllm/transformers_utils/detokenizer.py\n@@ -40,7 +40,7 @@ class Detokenizer:\n         assert prms is not None\n \n         # We can pick any sequence for the prompt.\n-        seq = next(iter(seq_group.seqs_dict.values()))\n+        seq = seq_group.get_seqs()[0]\n         # Only prompt, without the generated token.\n         all_token_ids = seq.get_token_ids()\n         prompt_token_ids = all_token_ids[:-1]\n@@ -186,7 +186,10 @@ def _convert_tokens_to_string_with_added_encoders(\n     # even when the loop body is very simple.\n     sub_texts: List[str] = []\n     current_sub_text: List[str] = []\n-    all_special_tokens = set(tokenizer.all_special_tokens)\n+    all_special_tokens = getattr(tokenizer, \"_all_special_tokens_set\", None)\n+    if all_special_tokens is None:\n+        all_special_tokens = set(tokenizer.all_special_tokens)\n+        setattr(tokenizer, \"_all_special_tokens_set\", all_special_tokens)\n     for token in output_tokens:\n         if skip_special_tokens and token in all_special_tokens:\n             continue\n", "model_name_or_path": "gpt-5-2025-08-07"}
