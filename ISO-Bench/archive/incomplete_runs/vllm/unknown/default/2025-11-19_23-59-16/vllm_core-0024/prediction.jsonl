{"instance_id": "/home/raven/coding-mess/kernel-corp/OmniPerf-Bench/vllm__repo-0df4d9b", "model_patch": "diff --git a/model_patch.diff b/model_patch.diff\nnew file mode 100644\nindex 000000000..e69de29bb\ndiff --git a/vllm/model_executor/layers/quantization/compressed_tensors/triton_scaled_mm.py b/vllm/model_executor/layers/quantization/compressed_tensors/triton_scaled_mm.py\nindex 3ff162170..de612332b 100644\n--- a/vllm/model_executor/layers/quantization/compressed_tensors/triton_scaled_mm.py\n+++ b/vllm/model_executor/layers/quantization/compressed_tensors/triton_scaled_mm.py\n@@ -88,12 +88,13 @@ def scaled_mm_kernel(a_ptr, b_ptr, scale_a_ptr, scale_b_ptr, c_ptr, bias_ptr,\n     # (BLOCK_SIZE_M, 1) then it will broadcast to its own shape. Same goes\n     # for scale_b below.\n     scale_a = scale_a.broadcast_to((BLOCK_SIZE_M, 1))\n-    accumulator = scale_a * accumulator.to(tl.float32)\n+    accumulator = accumulator.to(tl.float32)\n+    accumulator = scale_a * accumulator\n \n     masks_scale_b = masks_scale_bn[:, None] & (tl.arange(0, 1) < 1)[None, :]\n     scale_b = tl.load(scale_b_ptrs[:, None], masks_scale_b)\n     scale_b = scale_b.broadcast_to((BLOCK_SIZE_N, 1))\n-    accumulator = scale_b.T * accumulator.to(tl.float32)\n+    accumulator = scale_b.T * accumulator\n \n     # Convert to output format.\n     c = accumulator.to(c_ptr.type.element_ty)\n@@ -128,7 +129,7 @@ def triton_scaled_mm(input: torch.Tensor,\n                      bias: Optional[torch.Tensor] = None,\n                      block_size_m: int = 32,\n                      block_size_n: int = 32,\n-                     block_size_k: int = 32) -> torch.Tensor:\n+                     block_size_k: int = 32, use_heuristic: bool = True, num_warps: Optional[int] = None, num_stages: Optional[int] = None) -> torch.Tensor:\n     M, K = input.shape\n     N = weight.shape[1]\n \n@@ -144,16 +145,34 @@ def triton_scaled_mm(input: torch.Tensor,\n     assert bias is None or bias.is_floating_point()\n     assert is_weak_contiguous(input)\n     assert is_weak_contiguous(weight)\n-\n-    grid = lambda META: (triton.cdiv(M, META['BLOCK_SIZE_M']) * triton.cdiv(\n-        N, META['BLOCK_SIZE_N']), )\n+    # Heuristic block sizes and kernel launch parameters for better performance\n+    if use_heuristic:\n+        is_small_N = N < 8192\n+        next_power_of_2_M = max(32, triton.next_power_of_2(M))\n+        if next_power_of_2_M <= 32:\n+            block_size_m, block_size_n, block_size_k = ((64, 64, 256) if is_small_N else (64, 128, 256))\n+        elif next_power_of_2_M <= 64:\n+            block_size_m, block_size_n, block_size_k = (64, 64, 256)\n+        elif next_power_of_2_M <= 128:\n+            block_size_m, block_size_n, block_size_k = (64, 128, 128)\n+        else:\n+            block_size_m, block_size_n, block_size_k = (128, 128, 128)\n+\n+    if num_warps is None:\n+        num_warps = 8 if (block_size_m >= 128 or block_size_n >= 128) else 4\n+    if num_stages is None:\n+        num_stages = 3\n+\n+\n+    grid = ( ((M + block_size_m - 1) // block_size_m) * ((N + block_size_n - 1) // block_size_n), )\n \n     result = torch.empty((M, N), dtype=out_dtype, device=input.device)\n \n-    has_scalar = lambda x: x.shape[0] == 1 and x.shape[1] == 1\n+    is_scalar_a = scale_a.numel() == 1\n+    is_scalar_b = scale_b.numel() == 1\n \n-    block_size_sa = 1 if has_scalar(scale_a) else block_size_m\n-    block_size_sb = 1 if has_scalar(scale_b) else block_size_n\n+    block_size_sa = 1 if is_scalar_a else block_size_m\n+    block_size_sb = 1 if is_scalar_b else block_size_n\n \n     accumulator_dtype = tl.float32 if input.is_floating_point() else tl.int32\n \n@@ -174,11 +193,11 @@ def triton_scaled_mm(input: torch.Tensor,\n                            weight.stride(1),\n                            result.stride(0),\n                            result.stride(1),\n-                           accumulator_dtype,\n+                           accumulator_dtype, num_warps=num_warps, num_stages=num_stages,\n                            BLOCK_SIZE_M=block_size_m,\n                            BLOCK_SIZE_N=block_size_n,\n                            BLOCK_SIZE_K=block_size_k,\n                            BLOCK_SIZE_SCALE_A=block_size_sa,\n                            BLOCK_SIZE_SCALE_B=block_size_sb)\n \n-    return result.to(out_dtype)\n+    return result\n", "model_name_or_path": "gpt-5-2025-08-07"}
